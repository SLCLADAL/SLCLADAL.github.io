{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "![An interactive LADAL notebook](https://slcladal.github.io/images/uq1.jpg)\n",
                "\n",
                "***\n",
                "\n",
                "Please copy this Jupyter notebook so that you are able to edit it.\n",
                "\n",
                "Simply go to: File > Save a copy in Drive.\n",
                "\n",
                "Once you have done that, you are good to go.\n",
                "\n",
                "***\n",
                "\n",
                "This tutorial is the interactive Jupyter notebook accompanying the [Language Technology and Data Analysis Laboratory (LADAL) tutorial on part-of-speech tagging and dependency parsing with R](https://ladal.edu.au/postag.html). \n",
                "\n",
                "\n",
                "\n",
                "***\n",
                "\n",
                "\n",
                "**Preparation and session set up**\n",
                "\n",
                "If you are using this notebook on Google Colab or your own computer and you have not already installed the R packages listed below, you need to install them. You can install them by running the code chunk below. \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# install packages\n",
                "install.packages(\"dplyr\") \n",
                "install.packages(\"stringr\")\n",
                "install.packages(\"udpipe\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "***\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# activate packages\n",
                "library(dplyr)\n",
                "library(stringr) \n",
                "library(udpipe) \n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Once you have initiated the session by executing the code shown above, you are good to go.\n",
                "\n",
                "***\n",
                "\n",
                "## Using your own data\n",
                "\n",
                "While the tutorial uses data from the LADAL website, you can also use your own data. You can see below what you need to do to upload and use your own data.\n",
                "\n",
                "The code chunk below allows you to upload two files from your own computer. To be able to load your own data, you need to click on the folder symbol to the left of the screen:\n",
                "\n",
                "![Colab Folder Symbol](https://slcladal.github.io/images/ColabFolder.png)\n",
                "\n",
                "Then on the upload symbol.\n",
                "\n",
                "![Colab Upload Symbol](https://slcladal.github.io/images/ColabUpload.png)\n",
                "\n",
                "Next, upload the files you want to analyze and then the respective files names in the file argument of the scan function. When you then execute the code (like to code chunk below, you will upload your own data.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "mytext1 <- scan(file = \"linguistics01.txt\",\n",
                "            what = \"char\", \n",
                "            sep = \"\", \n",
                "            quote = \"\", \n",
                "            quiet = T, \n",
                "            skipNul = T) %>%\n",
                "            paste0(collapse = \" \")\n",
                "mytext2 <- scan(file = \"linguistics02.txt\",\n",
                "            what = \"char\", \n",
                "            sep = \"\", \n",
                "            quote = \"\", \n",
                "            quiet = T, \n",
                "            skipNul = T) %>%\n",
                "            paste0(collapse = \" \")\n",
                "# inspect\n",
                "mytext1; mytext2\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Keep in mind though that you need to adapt the names of the texts in the code chunks below so that the code below work on your own texts!**\n",
                "\n",
                "***\n",
                "\n",
                "# POS-Tagging with UDPipe\n",
                "\n",
                "UDPipe allows you to access numerous language models for 64 languages (for an overview of the supported languages and language models see [here](https://ladal.edu.au/postag.html#POS-Tagging_with_UDPipe)). \n",
                "\n",
                "To download any of these models, we can use the `udpipe_download_model` function. For example, to download the `english-ewt` model, we would use the call: `m_eng\t<- udpipe::udpipe_download_model(language = \"english-ewt\")`. \n",
                "\n",
                "We start by loading  a text\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# load text\n",
                "text <- readLines(\"https://slcladal.github.io/data/testcorpus/linguistics06.txt\", skipNul = T) %>%\n",
                " str_squish() %>%\n",
                "  .[1]\n",
                "# inspect\n",
                "text\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now that we have a text that we can work with, we will download a pre-trained language model.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# download language model\n",
                "m_eng\t<- udpipe::udpipe_download_model(language = \"english-ewt\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We now load language model.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# load language model\n",
                "m_eng <- udpipe_load_model(file = \"/content/english-ewt-ud-2.5-191206.udpipe\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We can now use the model to annotate out text.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# tokenise, tag, dependency parsing\n",
                "text_anndf <- udpipe::udpipe_annotate(m_eng, x = text) %>%\n",
                "  as.data.frame() %>%\n",
                "  dplyr::select(-sentence)\n",
                "# inspect\n",
                "head(text_anndf, 10)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "It can be useful to extract only the words and their pos-tags and convert them back into a text format (rather than a tabular format). \n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "tagged_text <- paste(text_anndf$token, \"/\", text_anndf$xpos, collapse = \" \", sep = \"\")\n",
                "# inspect tagged text\n",
                "tagged_text\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# POS-Tagging non-English texts\n",
                "\n",
                "We can apply the same method for annotating, e.g. adding pos-tags, to other languages. For this, we could train our own model, or, we can use one of the many pre-trained language models that `udpipe` provides.\n",
                "\n",
                "Let us explore how to do this by using  example texts from different languages, here from German and Spanish (but we could also annotate texts from any of the wide variety of languages for which UDPipe provides pre-trained models.\n",
                "\n",
                "\n",
                "We begin by loading a German and a Dutch text.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# load texts\n",
                "gertext <- readLines(\"https://slcladal.github.io/data/german.txt\") \n",
                "duttext <- readLines(\"https://slcladal.github.io/data/dutch.txt\") \n",
                "# inspect texts\n",
                "gertext; duttext\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Next, we install the pre-trained language models.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# download language model\n",
                "m_ger\t<- udpipe::udpipe_download_model(language = \"german-gsd\")\n",
                "m_dut\t<- udpipe::udpipe_download_model(language = \"dutch-alpino\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Or we load them from our machine (if we have downloaded and saved them before).\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# load language model from your computer after you have downloaded it once\n",
                "m_ger\t<- udpipe::udpipe_load_model(file = \"/content/german-gsd-ud-2.5-191206.udpipe\")\n",
                "m_dut\t<- udpipe::udpipe_load_model(file = \"/content/dutch-alpino-ud-2.5-191206.udpipe\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now, pos-tag the German text.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "ger_pos <- udpipe::udpipe_annotate(m_ger, x = gertext) %>%\n",
                "  as.data.frame() %>%\n",
                "  dplyr::summarise(postxt = paste(token, \"/\", xpos, collapse = \" \", sep = \"\")) %>%\n",
                "  dplyr::pull(unique(postxt))\n",
                "# inspect\n",
                "ger_pos\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "And finally, we also pos-tag the Dutch text.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "nl_pos <- udpipe::udpipe_annotate(m_dut, x = duttext) %>%\n",
                "   as.data.frame() %>%\n",
                "  dplyr::summarise(postxt = paste(token, \"/\", xpos, collapse = \" \", sep = \"\")) %>%\n",
                "  dplyr::pull(unique(postxt))\n",
                "# inspect\n",
                "nl_pos\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Dependency Parsing Using UDPipe\n",
                "\n",
                "In addition to pos-tagging, we can also generate plots showing the syntactic dependencies of the different constituents of a sentence. For this, we generate an object that contains a sentence (in this case, the sentence *Linguistics is the scientific study of language*), and we then plot (or visualize) the dependencies using the `textplot_dependencyparser` function.  \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# parse text\n",
                "sent <- udpipe::udpipe_annotate(m_eng, x = \"Linguistics is the scientific study of language\") %>%\n",
                "  as.data.frame()\n",
                "# inspect\n",
                "head(sent)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Before we can generate the plot, we need to install additional packages.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# install and activate necessary packages\n",
                "install.packages(\"igraph\")\n",
                "install.packages(\"ggraph\")\n",
                "install.packages(\"textplot\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We now generate the plot.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# load package\n",
                "library(textplot)\n",
                "# generate dependency plot\n",
                "dplot <- textplot_dependencyparser(sent, size = 5) \n",
                "# show plot\n",
                "dplot\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "That's it for this tutorial. We hope that you have enjoyed this tutorial and learned how to annotate texts using language models and perform pos-tagging and dependency parsing.\n",
                "\n",
                "\n",
                "\n",
                "[Back to LADAL](https://ladal.edu.au/postag.html)\n"
            ]
        }
    ],
    "metadata": {
        "anaconda-cloud": "",
        "kernelspec": {
            "display_name": "R",
            "langauge": "R",
            "name": "ir"
        },
        "language_info": {
            "codemirror_mode": "r",
            "file_extension": ".r",
            "mimetype": "text/x-r-source",
            "name": "R",
            "pygments_lexer": "r",
            "version": "3.4.1"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}
