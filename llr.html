<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />
<link rel="icon" 
      type="image/x-icon" 
      href="favicon.ico" />


<meta name="author" content="Martin Schweinberger" />

<meta name="date" content="2021-11-01" />

<title>Analyzing learner language using R</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cosmo.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<link href="site_libs/anchor-sections-1.0/anchor-sections.css" rel="stylesheet" />
<script src="site_libs/anchor-sections-1.0/anchor-sections.js"></script>
<script src="site_libs/clipboard-1.7.1/clipboard.min.js"></script>
<link href="site_libs/primer-tooltips-1.4.0/build.css" rel="stylesheet" />
<link href="site_libs/klippy-0.0.0.9500/css/klippy.min.css" rel="stylesheet" />
<script src="site_libs/klippy-0.0.0.9500/js/klippy.min.js"></script>
<link href="site_libs/tabwid-1.0.0/tabwid.css" rel="stylesheet" />

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>





<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #ffffff;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>


<!-- added by SKC for LADAL Style -->
<link rel="stylesheet" href="styles.css">
</head>

<body>


<div class="container-fluid main-container">





<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">



<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  
  <!-- Added by SKC - LADAL image and thicker top with   -->
  <div class="container-fluid navbar-top" >
    <a href="index.html"> <!-- Make entire top row and text clickable home link  -->
        <div class="row">
            <div class="navbar-brand col-md-12">
              <img src="ladal_icon_cas_tran_white_trimed.png" class="navbar-icon" alt="LADAL"/>
              <span class="navbar-title-note navbar-collapse collapse" >Language Technology and Data Analysis Laboratory</span>
            </div>
        </div>
    </a>
  </div>
  
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <!-- SKC removed  navbar brand -->
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">HOME</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    ABOUT LADAL
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="people.html">People | Collabs</a>
    </li>
    <li>
      <a href="news.html">News</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    EVENTS
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="opening.html">Opening Webinar Series</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    R BASICS
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li class="dropdown-header">Introduction to R</li>
    <li>
      <a href="intror.html">Getting started</a>
    </li>
    <li>
      <a href="string.html">String Processing</a>
    </li>
    <li>
      <a href="regex.html">Regular Expressions</a>
    </li>
    <li>
      <a href="table.html">Handling tables in R</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    DATA SCIENCE BASICS
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li class="dropdown-header">Introduction to Data Science</li>
    <li>
      <a href="comp.html">Working with Computers</a>
    </li>
    <li>
      <a href="repro.html">Data Management and Reproducibility</a>
    </li>
    <li>
      <a href="introquant.html">Introduction to Quantitative Reasoning</a>
    </li>
    <li>
      <a href="basicquant.html">Basic Concepts in Quantitative Research</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    TUTORIALS
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li class="dropdown-header">Data Visualization</li>
    <li>
      <a href="introviz.html">Introduction to Data Viz</a>
    </li>
    <li>
      <a href="dviz.html">Data Visualization with R</a>
    </li>
    <li>
      <a href="maps.html">Displaying Geo-Spatial Data</a>
    </li>
    <li>
      <a href="motion.html">Interactive Charts</a>
    </li>
    <li class="divider"></li>
    <li class="dropdown-header">Statistics</li>
    <li>
      <a href="dstats.html">Descriptive Statistics</a>
    </li>
    <li>
      <a href="basicstatz.html">Basic Inferential Statistics</a>
    </li>
    <li>
      <a href="regression.html">Regression Analysis</a>
    </li>
    <li>
      <a href="tree.html">Tree-Based Models</a>
    </li>
    <li>
      <a href="clust.html">Cluster and Correspondence Analysis</a>
    </li>
    <li>
      <a href="lexsim.html">Introduction to Lexical Similarity</a>
    </li>
    <li>
      <a href="svm.html">Semantic Vector Space Models</a>
    </li>
    <li>
      <a href="pwr.html">Power Analysis</a>
    </li>
    <li class="divider"></li>
    <li class="dropdown-header">Text Analytics</li>
    <li>
      <a href="textanalysis.html">Text Analysis and Distant Reading</a>
    </li>
    <li>
      <a href="kwics.html">Concordancing (keywords-in-context)</a>
    </li>
    <li>
      <a href="net.html">Network Analysis</a>
    </li>
    <li>
      <a href="coll.html">Co-occurrence and Collocation Analysis</a>
    </li>
    <li>
      <a href="topicmodels.html">Topic Modeling</a>
    </li>
    <li>
      <a href="sentiment.html">Sentiment Analysis</a>
    </li>
    <li>
      <a href="tagging.html">Tagging and Parsing</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    FOCUS STUDIES
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="corplingr.html">Corpus Linguistics with R</a>
    </li>
    <li>
      <a href="lex.html">Lexicography and Dictionaries with R</a>
    </li>
    <li>
      <a href="surveys.html">Questionnaires and Surveys with R</a>
    </li>
    <li>
      <a href="vc.html">Phonetics: Creating Vowel Charts with Praat and R</a>
    </li>
    <li>
      <a href="litsty.html">Computational Literary Stylistics with R</a>
    </li>
    <li class="divider"></li>
    <li class="dropdown-header">Useful How-To Tutorials</li>
    <li>
      <a href="pdf2txt.html">Converting PDFs to txt</a>
    </li>
    <li>
      <a href="webcrawling.html">Web Crawling using R</a>
    </li>
    <li>
      <a href="gutenberg.html">Downloading Texts from Project Gutenberg</a>
    </li>
    <li>
      <a href="rename.html">Renaming files with R</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    RESOURCES
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="links.html">Links</a>
    </li>
    <li>
      <a href="services.html">Services | Contact</a>
    </li>
    <li>
      <a href="base.html">Tutorial stylesheet</a>
    </li>
  </ul>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Analyzing learner language using R</h1>
<h4 class="author">Martin Schweinberger</h4>
<h4 class="date">2021-11-01</h4>

</div>



<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-130562131-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-130562131-1');
</script>

<p><img src="https://slcladal.github.io/images/uq1.jpg" width="100%" /></p>
<div id="introduction" class="section level1 unnumbered">
<h1>Introduction</h1>
<p>This tutorial focuses on learner language and how to analyze differences between learners and L1 speakers of English using R. The entire R markdown document for this tutorial can be downloaded <a href="https://slcladal.github.io/llr.Rmd">here</a>.</p>
<p>The aim of this tutorial is to showcase how to extract information from essays from learners and L1 speakers of English and how to analyze these essays. The aim is not to provide a fully-fledged analysis but rather to show and exemplyfy some common methods for data extraction, processing, and analysis.</p>
<p><strong>Preparation and session set up</strong></p>
<p>This tutorial is based on R. If you have not installed R or are new to it, you will find an introduction to and more information how to use R <a href="https://slcladal.github.io/intror.html">here</a>. For this tutorials, we need to install certain <em>packages</em> from an R <em>library</em> so that the scripts shown below are executed without errors. Before turning to the code below, please install the packages by running the code below this paragraph. If you have already installed the packages mentioned below, then you can skip ahead and ignore this section. To install the necessary packages, simply run the following code - it may take some time (between 1 and 5 minutes to install all of the packages so you do not need to worry if it takes some time).</p>
<pre class="r"><code># install packages
install.packages(&quot;quanteda&quot;)
install.packages(&quot;flextable&quot;)
install.packages(&quot;quanteda&quot;)
install.packages(&quot;tidyverse&quot;)
install.packages(&quot;tm&quot;)
install.packages(&quot;tidytext&quot;)
install.packages(&quot;tidyr&quot;)
install.packages(&quot;NLP&quot;)
install.packages(&quot;openNLP&quot;)
install.packages(&quot;openNLPdata&quot;)
install.packages(&quot;koRpus&quot;)
install.packages(&quot;stringi&quot;)
install.packages(&quot;pacman&quot;)
# install the language support package
koRpus::install.koRpus.lang(&quot;en&quot;)
# install klippy for copy-to-clipboard button in code chunks
remotes::install_github(&quot;rlesur/klippy&quot;)</code></pre>
<p>Now that we have installed the packages, we can activate them as shown below.</p>
<pre class="r"><code># set options
options(stringsAsFactors = F)
options(scipen = 999)
options(max.print=1000)
options(java.parameters = c(&quot;-XX:+UseConcMarkSweepGC&quot;, &quot;-Xmx8192m&quot;))
gc()</code></pre>
<pre><code>##          used (Mb) gc trigger (Mb) max used (Mb)
## Ncells 428574 22.9     902641 48.3   643711 34.4
## Vcells 788608  6.1    8388608 64.0  1650532 12.6</code></pre>
<pre class="r"><code># load packages
library(tidyverse)
library(flextable)
library(tm)
library(tidytext)
library(tidyr)
library(NLP)
library(openNLP)
library(quanteda)
library(quanteda.textstats)
library(koRpus)
library(koRpus.lang.en)
library(stringi)
library(pacman)
pacman::p_load_gh(&quot;trinker/entity&quot;)
# activate klippy for copy-to-clipboard button
klippy::klippy()</code></pre>
<script>
  addClassKlippyTo("pre.r, pre.markdown");
  addKlippy('left', 'top', 'auto', '1', 'Copy code', 'Copied!');
</script>
<p>Once you have installed R and RStudio and once you have also initiated the session by executing the code shown above, you are good to go.</p>
<p><strong>Loading data</strong></p>
<p>We use 7 essays written by learners from the <a href="https://uclouvain.be/en/research-institutes/ilc/cecl/icle.html"><em>International Corpus of Learner English</em> (ICLE)</a> and two files containing a-level essays written by L1-English British students from <a href="https://uclouvain.be/en/research-institutes/ilc/cecl/locness.html"><em>The Louvain Corpus of Native English Essays</em> (LOCNESS)</a> which was compiled by the <em>Centre for English Corpus Linguistics</em> (CECL), Université catholique de Louvain, Belgium. The code chunk below loads the data from the LADAL repository on GitHub into R.</p>
<pre class="r"><code># load essays from l1 speakers
ns1 &lt;- base::readRDS(url(&quot;https://slcladal.github.io/data/LCorpus/ns1.rda&quot;, &quot;rb&quot;))
ns2 &lt;- base::readRDS(url(&quot;https://slcladal.github.io/data/LCorpus/ns2.rda&quot;, &quot;rb&quot;))
# load essays from l2 speakers
es &lt;- base::readRDS(url(&quot;https://slcladal.github.io/data/LCorpus/es.rda&quot;, &quot;rb&quot;))
de &lt;- base::readRDS(url(&quot;https://slcladal.github.io/data/LCorpus/de.rda&quot;, &quot;rb&quot;))
fr &lt;- base::readRDS(url(&quot;https://slcladal.github.io/data/LCorpus/fr.rda&quot;, &quot;rb&quot;))
it &lt;- base::readRDS(url(&quot;https://slcladal.github.io/data/LCorpus/it.rda&quot;, &quot;rb&quot;))
pl &lt;- base::readRDS(url(&quot;https://slcladal.github.io/data/LCorpus/pl.rda&quot;, &quot;rb&quot;))
ru &lt;- base::readRDS(url(&quot;https://slcladal.github.io/data/LCorpus/ru.rda&quot;, &quot;rb&quot;))</code></pre>
<p>The table below shows the first 3 text elements from the essay written a Russian learner of English to provide an idea of what the data look like.</p>
<template id="de7a23bb-c18b-4e88-a024-fb4a1b2edbf2"><style>
.tabwid table{
  border-spacing:0px !important;
  border-collapse:collapse;
  line-height:1;
  margin-left:auto;
  margin-right:auto;
  border-width: 0;
  display: table;
  margin-top: 1.275em;
  margin-bottom: 1.275em;
  border-color: transparent;
}
.tabwid_left table{
  margin-left:0;
}
.tabwid_right table{
  margin-right:0;
}
.tabwid td {
    padding: 0;
}
.tabwid a {
  text-decoration: none;
}
.tabwid thead {
    background-color: transparent;
}
.tabwid tfoot {
    background-color: transparent;
}
.tabwid table tr {
background-color: transparent;
}
</style><div class="tabwid"><style>.cl-c3acc3ee{table-layout:auto;width:75%;}.cl-c3a43e36{font-family:'Arial';font-size:12pt;font-weight:bold;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-c3a43e37{font-family:'Arial';font-size:12pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-c3a46636{margin:0;text-align:center;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-c3a4b41a{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c3a4b41b{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c3a4b41c{background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c3a4b41d{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}</style><table class='cl-c3acc3ee'>
<caption class="Table Caption">
<p>First 3 text elements of the Russian learner data.</p>
</caption>
<thead><tr style="overflow-wrap:break-word;"><td class="cl-c3a4b41d"><p class="cl-c3a46636"><span class="cl-c3a43e36">.</span></p></td></tr></thead><tbody><tr style="overflow-wrap:break-word;"><td class="cl-c3a4b41a"><p class="cl-c3a46636"><span class="cl-c3a43e37"></span></p></td></tr><tr style="overflow-wrap:break-word;"><td class="cl-c3a4b41b"><p class="cl-c3a46636"><span class="cl-c3a43e37">It is now a very wide spread opinion, that in the modern world there is no place for dreaming and imagination. Those who share this point of view usually say that at present we are so very much under the domination of science, industry, technology, ever-increasing tempo of our lives and so on, that neither dreaming nor imagination can possibly survive. Their usual argument is very simple - they suggest to their opponents to look at some samples of the modern art and to compare them to the masterpieces of the "Old Masters" of painting, music, literature.</span></p></td></tr><tr style="overflow-wrap:break-word;"><td class="cl-c3a4b41a"><p class="cl-c3a46636"><span class="cl-c3a43e37">As everything which is simple, the argument sounds very convincing. Of course, it is evident, that no modern writer, painter or musician can be compare to such names as Bach, Pushkin&lt; Byron, Mozart, Rembrandt, Raffael et cetera. Modern pictures, in the majority of cases, seem to be merely repetitions or combinations of the images and methods of painting, invented very long before. The same is also true to modern verses, novels and songs.</span></p></td></tr><tr style="overflow-wrap:break-word;"><td class="cl-c3a4b41c"><p class="cl-c3a46636"><span class="cl-c3a43e37">But, I think, those, who put forward this argument, play - if I may put it like this - not fair game with their opponents, because such an approach presupposes the firm conviction, that dreaming and imagination can deal only with Arts, moreover, only with this "well-established set" of Arts, which includes music, painting, architecture, sculpture and literature. That is, a person, who follows the above-mentioned point of view tries to make his opponent take for granted the statement, the evidence of which is, to say the least, doubtful.</span></p></td></tr></tbody></table></div></template>
<div class="flextable-shadow-host" id="d0c4c235-d083-42f9-b438-82eb8ad750d1"></div>
<script>
var dest = document.getElementById("d0c4c235-d083-42f9-b438-82eb8ad750d1");
var template = document.getElementById("de7a23bb-c18b-4e88-a024-fb4a1b2edbf2");
var caption = template.content.querySelector("caption");
if(caption) {
  caption.style.cssText = "display:block;text-align:center;";
  var newcapt = document.createElement("p");
  newcapt.appendChild(caption)
  dest.parentNode.insertBefore(newcapt, dest.previousSibling);
}
var fantome = dest.attachShadow({mode: 'open'});
var templateContent = template.content;
fantome.appendChild(templateContent);
</script>

<p>Now that we have loaded some data, we can go ahead and extract information from the texts and process the data to analyze differences between L1 speakers and learners of English.</p>
</div>
<div id="concordancing" class="section level1 unnumbered">
<h1>Concordancing</h1>
<p>Concordancing refers to the extraction of words or phrases from a given text or texts <span class="citation">(Lindquist <a href="#ref-lindquist2009corpus" role="doc-biblioref">2009</a>)</span>. Commonly, concordances are displayed in the form of key-word in contexts (KWIC) where the search term is shown with some preceding and following context. Thus, such displays are referred to as key word in context concordances. A more elaborate tutorial on how to perform concordancing with R is available <a href="https://slcladal.github.io/kwics.html">here</a>.</p>
<p>Concordancing is helpful for seeing how a given term or phrased is used in the data, for inspecting how often a given word occurs in a text or a collection of texts, for extracting examples, and it also represents a basic procedure, and often the first step, in more sophisticated analyses.</p>
<p>We begin by creating KWIC displays of the term <em>problem</em> as shown below.</p>
<pre class="r"><code># combine data from l1 speakers
l1 &lt;- c(ns1, ns2)
# combine data from learners
learner &lt;- c(de, es, fr, it, pl, ru)
# extract kwic for term &quot;problem&quot; in learner data
kwic &lt;- quanteda::kwic(pattern = &quot;problem.*&quot;, 
                       learner, 
                       valuetype = &quot;regex&quot;, 
                       window = 10) %&gt;%
  as.data.frame()
# inspect
head(kwic)</code></pre>
<pre><code>##   docname from  to                                                     pre
## 1  text12    8   8                      Many of the drug addits have legal
## 2  text12   31  31     countries , like Spain , illegal . They have social
## 3  text30   11  11     In our society there is a growing concern about the
## 4  text33  108 108 that once the availability of guns has been removed the
## 5  text33  139 139    honest way and remove any causes that could worsen a
## 6  text34   54  54       violence in our society . In order to analise the
##    keyword                                                       post   pattern
## 1 problems       because they steal money for buying the drug that is problem.*
## 2 problems         too because people are afraid of them and the drug problem.*
## 3  problem       of violent crime . In fact , particular attention is problem.*
## 4  problem of violence simply vanishes , but in this caotic situation problem.*
## 5  problem                    which is already particularly serious . problem.*
## 6  problem            in its complexity and allow people to live in a problem.*</code></pre>
<p>The output shows that the term <em>problem</em> occurs six times in the learner data.</p>
<p>We can also arrange the output according to what comes before or after the search term as shown below.</p>
<pre class="r"><code># arrange kwic alphabetically by what comes after the key term
kwic %&gt;%
  dplyr::arrange(post)</code></pre>
<pre><code>##   docname from  to                                                          pre
## 1  text12    8   8                           Many of the drug addits have legal
## 2  text39  131 131 , greatest ideas were produced and solutions to many serious
## 3  text34   54  54            violence in our society . In order to analise the
## 4  text33  108 108      that once the availability of guns has been removed the
## 5  text30   11  11          In our society there is a growing concern about the
## 6  text12   31  31          countries , like Spain , illegal . They have social
## 7  text33  139 139         honest way and remove any causes that could worsen a
##    keyword                                                        post
## 1 problems        because they steal money for buying the drug that is
## 2 problems found . Most wonderful pieces of literature were created in
## 3  problem             in its complexity and allow people to live in a
## 4  problem  of violence simply vanishes , but in this caotic situation
## 5  problem        of violent crime . In fact , particular attention is
## 6 problems          too because people are afraid of them and the drug
## 7  problem                     which is already particularly serious .
##     pattern
## 1 problem.*
## 2 problem.*
## 3 problem.*
## 4 problem.*
## 5 problem.*
## 6 problem.*
## 7 problem.*</code></pre>
<pre class="r"><code># arrange kwic alphabetically by what comes before the key term
kwic %&gt;%
  dplyr::mutate(prerev = stringi::stri_reverse(pre)) %&gt;%
  dplyr::arrange(prerev) %&gt;%
  dplyr::select(-prerev)</code></pre>
<pre><code>##   docname from  to                                                          pre
## 1  text33  139 139         honest way and remove any causes that could worsen a
## 2  text33  108 108      that once the availability of guns has been removed the
## 3  text34   54  54            violence in our society . In order to analise the
## 4  text30   11  11          In our society there is a growing concern about the
## 5  text12    8   8                           Many of the drug addits have legal
## 6  text12   31  31          countries , like Spain , illegal . They have social
## 7  text39  131 131 , greatest ideas were produced and solutions to many serious
##    keyword                                                        post
## 1  problem                     which is already particularly serious .
## 2  problem  of violence simply vanishes , but in this caotic situation
## 3  problem             in its complexity and allow people to live in a
## 4  problem        of violent crime . In fact , particular attention is
## 5 problems        because they steal money for buying the drug that is
## 6 problems          too because people are afraid of them and the drug
## 7 problems found . Most wonderful pieces of literature were created in
##     pattern
## 1 problem.*
## 2 problem.*
## 3 problem.*
## 4 problem.*
## 5 problem.*
## 6 problem.*
## 7 problem.*</code></pre>
<p>We can also search for phrases rather than individual words. To do this, we need to use the <code>phrase</code> function in the <code>pattern</code> argument as shown below. In the code chunk below, we look for any combination of the word <em>very</em> and any following word. It we would wish, we could of course also sort (or order) the concordances as we have done above.</p>
<pre class="r"><code>kwic &lt;- quanteda::kwic(pattern = phrase(&quot;^very [a-z]{1,}&quot;), 
                       learner, valuetype = &quot;regex&quot;) %&gt;%
  as.data.frame()</code></pre>
<template id="aa3bb0a7-9093-440f-b6a9-2e30d2ed18e6"><style>
.tabwid table{
  border-spacing:0px !important;
  border-collapse:collapse;
  line-height:1;
  margin-left:auto;
  margin-right:auto;
  border-width: 0;
  display: table;
  margin-top: 1.275em;
  margin-bottom: 1.275em;
  border-color: transparent;
}
.tabwid_left table{
  margin-left:0;
}
.tabwid_right table{
  margin-right:0;
}
.tabwid td {
    padding: 0;
}
.tabwid a {
  text-decoration: none;
}
.tabwid thead {
    background-color: transparent;
}
.tabwid tfoot {
    background-color: transparent;
}
.tabwid table tr {
background-color: transparent;
}
</style><div class="tabwid"><style>.cl-c3fa6f0e{table-layout:auto;width:75%;}.cl-c3ee4328{font-family:'Arial';font-size:12pt;font-weight:bold;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-c3ee4329{font-family:'Arial';font-size:12pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-c3ee432a{margin:0;text-align:center;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-c3ee432b{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-c3ee9170{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c3ee9171{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c3ee9172{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c3ee9173{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c3ee9174{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c3ee9175{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c3ee9176{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c3ee9177{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c3ee9178{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c3ee9179{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c3ee917a{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c3eeb862{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c3eeb863{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c3eeb864{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c3eeb865{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c3eeb866{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}</style><table class='cl-c3fa6f0e'>
<caption class="Table Caption">
<p>First 6 rows of the concordance for very + any other word in the learner data.</p>
</caption>
<thead><tr style="overflow-wrap:break-word;"><td class="cl-c3eeb863"><p class="cl-c3ee432a"><span class="cl-c3ee4328">docname</span></p></td><td class="cl-c3eeb866"><p class="cl-c3ee432b"><span class="cl-c3ee4328">from</span></p></td><td class="cl-c3eeb866"><p class="cl-c3ee432b"><span class="cl-c3ee4328">to</span></p></td><td class="cl-c3eeb865"><p class="cl-c3ee432a"><span class="cl-c3ee4328">pre</span></p></td><td class="cl-c3eeb865"><p class="cl-c3ee432a"><span class="cl-c3ee4328">keyword</span></p></td><td class="cl-c3eeb865"><p class="cl-c3ee432a"><span class="cl-c3ee4328">post</span></p></td><td class="cl-c3eeb864"><p class="cl-c3ee432a"><span class="cl-c3ee4328">pattern</span></p></td></tr></thead><tbody><tr style="overflow-wrap:break-word;"><td class="cl-c3ee9172"><p class="cl-c3ee432a"><span class="cl-c3ee4329">text3</span></p></td><td class="cl-c3ee9170"><p class="cl-c3ee432b"><span class="cl-c3ee4329">193</span></p></td><td class="cl-c3ee9170"><p class="cl-c3ee432b"><span class="cl-c3ee4329">194</span></p></td><td class="cl-c3ee9171"><p class="cl-c3ee432a"><span class="cl-c3ee4329">in black trousers and only</span></p></td><td class="cl-c3ee9171"><p class="cl-c3ee432a"><span class="cl-c3ee4329">very seldom</span></p></td><td class="cl-c3ee9171"><p class="cl-c3ee432a"><span class="cl-c3ee4329">in skirts , because she</span></p></td><td class="cl-c3ee9173"><p class="cl-c3ee432a"><span class="cl-c3ee4329">^very [a-z]{1,}</span></p></td></tr><tr style="overflow-wrap:break-word;"><td class="cl-c3ee9177"><p class="cl-c3ee432a"><span class="cl-c3ee4329">text4</span></p></td><td class="cl-c3ee9174"><p class="cl-c3ee432b"><span class="cl-c3ee4329">9</span></p></td><td class="cl-c3ee9174"><p class="cl-c3ee432b"><span class="cl-c3ee4329">10</span></p></td><td class="cl-c3ee9176"><p class="cl-c3ee432a"><span class="cl-c3ee4329">is admirable is that she's</span></p></td><td class="cl-c3ee9176"><p class="cl-c3ee432a"><span class="cl-c3ee4329">very active</span></p></td><td class="cl-c3ee9176"><p class="cl-c3ee432a"><span class="cl-c3ee4329">in doing sports and that</span></p></td><td class="cl-c3ee9175"><p class="cl-c3ee432a"><span class="cl-c3ee4329">^very [a-z]{1,}</span></p></td></tr><tr style="overflow-wrap:break-word;"><td class="cl-c3ee9172"><p class="cl-c3ee432a"><span class="cl-c3ee4329">text4</span></p></td><td class="cl-c3ee9170"><p class="cl-c3ee432b"><span class="cl-c3ee4329">27</span></p></td><td class="cl-c3ee9170"><p class="cl-c3ee432b"><span class="cl-c3ee4329">28</span></p></td><td class="cl-c3ee9171"><p class="cl-c3ee432a"><span class="cl-c3ee4329">managed by her in a</span></p></td><td class="cl-c3ee9171"><p class="cl-c3ee432a"><span class="cl-c3ee4329">very simple</span></p></td><td class="cl-c3ee9171"><p class="cl-c3ee432a"><span class="cl-c3ee4329">way . She's very interested</span></p></td><td class="cl-c3ee9173"><p class="cl-c3ee432a"><span class="cl-c3ee4329">^very [a-z]{1,}</span></p></td></tr><tr style="overflow-wrap:break-word;"><td class="cl-c3ee9177"><p class="cl-c3ee432a"><span class="cl-c3ee4329">text4</span></p></td><td class="cl-c3ee9174"><p class="cl-c3ee432b"><span class="cl-c3ee4329">32</span></p></td><td class="cl-c3ee9174"><p class="cl-c3ee432b"><span class="cl-c3ee4329">33</span></p></td><td class="cl-c3ee9176"><p class="cl-c3ee432a"><span class="cl-c3ee4329">very simple way . She's</span></p></td><td class="cl-c3ee9176"><p class="cl-c3ee432a"><span class="cl-c3ee4329">very interested</span></p></td><td class="cl-c3ee9176"><p class="cl-c3ee432a"><span class="cl-c3ee4329">in cycling , swimming and</span></p></td><td class="cl-c3ee9175"><p class="cl-c3ee432a"><span class="cl-c3ee4329">^very [a-z]{1,}</span></p></td></tr><tr style="overflow-wrap:break-word;"><td class="cl-c3ee917a"><p class="cl-c3ee432a"><span class="cl-c3ee4329">text5</span></p></td><td class="cl-c3ee9178"><p class="cl-c3ee432b"><span class="cl-c3ee4329">3</span></p></td><td class="cl-c3ee9178"><p class="cl-c3ee432b"><span class="cl-c3ee4329">4</span></p></td><td class="cl-c3ee9179"><p class="cl-c3ee432a"><span class="cl-c3ee4329">She's also</span></p></td><td class="cl-c3ee9179"><p class="cl-c3ee432a"><span class="cl-c3ee4329">very intelligent</span></p></td><td class="cl-c3ee9179"><p class="cl-c3ee432a"><span class="cl-c3ee4329">and because of that she</span></p></td><td class="cl-c3eeb862"><p class="cl-c3ee432a"><span class="cl-c3ee4329">^very [a-z]{1,}</span></p></td></tr></tbody></table></div></template>
<div class="flextable-shadow-host" id="4322e277-f2ec-41cb-b54d-f030a134d48c"></div>
<script>
var dest = document.getElementById("4322e277-f2ec-41cb-b54d-f030a134d48c");
var template = document.getElementById("aa3bb0a7-9093-440f-b6a9-2e30d2ed18e6");
var caption = template.content.querySelector("caption");
if(caption) {
  caption.style.cssText = "display:block;text-align:center;";
  var newcapt = document.createElement("p");
  newcapt.appendChild(caption)
  dest.parentNode.insertBefore(newcapt, dest.previousSibling);
}
var fantome = dest.attachShadow({mode: 'open'});
var templateContent = template.content;
fantome.appendChild(templateContent);
</script>

</div>
<div id="splitting-texts-into-sentences" class="section level1 unnumbered">
<h1>Splitting texts into sentences</h1>
<p>It can be every useful to split texts into individual sentences. This can be done, e.g., to extract the average sentence length or simply to inspect or annotate individual sentences. To split a text into sentences, we clean the data by removing file identifiers and html tags as well as quotation marks within sentences. As we are dealing with several texts, we write a function that performs this task and that we can then apply to the individual texts.</p>
<pre class="r"><code>cleanText &lt;- function(x,...){
  require(tokenizers)
  # paste text together
  x &lt;- paste0(x)
  # remove file identifiers
  x &lt;- stringr::str_remove_all(x, &quot;&lt;.*?&gt;&quot;)
  # remove quotation marks
  x &lt;- stringr::str_remove_all(x, fixed(&quot;\&quot;&quot;))
  # remove empty elements
  x &lt;- x[!x==&quot;&quot;]
  # split text into sentences
  x &lt;- tokenize_sentences(x)
  x &lt;- unlist(x)
}
# clean texts
ns1_sen &lt;- cleanText(ns1)
ns2_sen &lt;- cleanText(ns2)
de_sen &lt;- cleanText(de)
es_sen &lt;- cleanText(es)
fr_sen &lt;- cleanText(fr)
it_sen &lt;- cleanText(it)
pl_sen &lt;- cleanText(pl)
ru_sen &lt;- cleanText(ru)</code></pre>
<template id="714c7b3c-0ce7-4bb8-b07a-0fa99c4bb8d0"><style>
.tabwid table{
  border-spacing:0px !important;
  border-collapse:collapse;
  line-height:1;
  margin-left:auto;
  margin-right:auto;
  border-width: 0;
  display: table;
  margin-top: 1.275em;
  margin-bottom: 1.275em;
  border-color: transparent;
}
.tabwid_left table{
  margin-left:0;
}
.tabwid_right table{
  margin-right:0;
}
.tabwid td {
    padding: 0;
}
.tabwid a {
  text-decoration: none;
}
.tabwid thead {
    background-color: transparent;
}
.tabwid tfoot {
    background-color: transparent;
}
.tabwid table tr {
background-color: transparent;
}
</style><div class="tabwid"><style>.cl-c419a1a8{table-layout:auto;width:75%;}.cl-c412059c{font-family:'Arial';font-size:12pt;font-weight:bold;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-c412059d{font-family:'Arial';font-size:12pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-c4122d38{margin:0;text-align:center;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-c4125394{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c4125395{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c4125396{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c4125397{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}</style><table class='cl-c419a1a8'>
<caption class="Table Caption">
<p>First 6 sentences of the Russian learner data.</p>
</caption>
<thead><tr style="overflow-wrap:break-word;"><td class="cl-c4125397"><p class="cl-c4122d38"><span class="cl-c412059c">.</span></p></td></tr></thead><tbody><tr style="overflow-wrap:break-word;"><td class="cl-c4125394"><p class="cl-c4122d38"><span class="cl-c412059d">It is now a very wide spread opinion, that in the modern world there is no place for dreaming and imagination.</span></p></td></tr><tr style="overflow-wrap:break-word;"><td class="cl-c4125395"><p class="cl-c4122d38"><span class="cl-c412059d">Those who share this point of view usually say that at present we are so very much under the domination of science, industry, technology, ever-increasing tempo of our lives and so on, that neither dreaming nor imagination can possibly survive.</span></p></td></tr><tr style="overflow-wrap:break-word;"><td class="cl-c4125394"><p class="cl-c4122d38"><span class="cl-c412059d">Their usual argument is very simple - they suggest to their opponents to look at some samples of the modern art and to compare them to the masterpieces of the Old Masters of painting, music, literature.</span></p></td></tr><tr style="overflow-wrap:break-word;"><td class="cl-c4125395"><p class="cl-c4122d38"><span class="cl-c412059d">As everything which is simple, the argument sounds very convincing.</span></p></td></tr><tr style="overflow-wrap:break-word;"><td class="cl-c4125396"><p class="cl-c4122d38"><span class="cl-c412059d">Of course, it is evident, that no modern writer, painter or musician can be compare to such names as Bach, Pushkin&lt; Byron, Mozart, Rembrandt, Raffael et cetera.</span></p></td></tr></tbody></table></div></template>
<div class="flextable-shadow-host" id="1205c35c-5986-4997-91dc-b5f50a764989"></div>
<script>
var dest = document.getElementById("1205c35c-5986-4997-91dc-b5f50a764989");
var template = document.getElementById("714c7b3c-0ce7-4bb8-b07a-0fa99c4bb8d0");
var caption = template.content.querySelector("caption");
if(caption) {
  caption.style.cssText = "display:block;text-align:center;";
  var newcapt = document.createElement("p");
  newcapt.appendChild(caption)
  dest.parentNode.insertBefore(newcapt, dest.previousSibling);
}
var fantome = dest.attachShadow({mode: 'open'});
var templateContent = template.content;
fantome.appendChild(templateContent);
</script>

<p>Now that we have split the texts into individual sentences, we can easily extract and visualize the average sentence lengths of L1 speakers and learners of English.</p>
</div>
<div id="analyzing-sentence-length" class="section level1 unnumbered">
<h1>Analyzing sentence length</h1>
<p>The most basic complexity measure is average sentence length. In the following, we will extract the average sentence length for L1-speakers and learners of English with different language backgrounds.</p>
<p>In a first step, we write a function that extracts the sentence length for each individual sentence in the different texts. To check if the function works, we apply it to learner data from L1 Russian learners of English. The function first cleans the sentences that we extracted above and then splits the sentences into tokens by splitting whenever there is a white space. Finally, the function counts the number of elements resulting from the splitting process.</p>
<pre class="r"><code>senLen &lt;- function(x, ...){
  sapply(x, function(x){
    x &lt;- stringr::str_remove_all(x, &quot;[^[:alnum:] ]&quot;) %&gt;%
      stringr::str_squish() %&gt;%
      stringr::str_split(&quot; &quot;)
    y &lt;- sapply(x, function(y){
      y &lt;- length(y)
      })
    return(as.vector(y))
    })
}
# apply function to text
senLen(ru_sen)[1:3]</code></pre>
<pre><code>##                                                                                                                                      It is now a very wide spread opinion, that in the modern world there is no place for dreaming and imagination. 
##                                                                                                                                                                                                                                                  21 
## Those who share this point of view usually say that at present we are so very much under the domination of science, industry, technology, ever-increasing tempo of our lives and so on, that neither dreaming nor imagination can possibly survive. 
##                                                                                                                                                                                                                                                  40 
##                                          Their usual argument is very simple - they suggest to their opponents to look at some samples of the modern art and to compare them to the masterpieces of the Old Masters of painting, music, literature. 
##                                                                                                                                                                                                                                                  35</code></pre>
<p>We can now apply the function to all texts and generate a table (a data frame) of the results and add the L1 of the speaker who produced the sentence.</p>
<pre class="r"><code># extract sentences lengths
ns1_sl &lt;- senLen(ns1_sen)
ns2_sl &lt;- senLen(ns2_sen)
de_sl &lt;- senLen(de_sen)
es_sl &lt;- senLen(es_sen)
fr_sl &lt;- senLen(fr_sen)
it_sl &lt;- senLen(it_sen)
pl_sl &lt;- senLen(pl_sen)
ru_sl &lt;- senLen(ru_sen)
# create a data frame from the results
sl_df &lt;- data.frame(c(ns1_sl, ns2_sl, de_sl, es_sl, fr_sl, it_sl, pl_sl, ru_sl)) %&gt;%
  dplyr::rename(sentenceLength = 1) %&gt;%
  dplyr::mutate(l1 = c(rep(&quot;en&quot;, length(ns1_sl)),
                       rep(&quot;en&quot;, length(ns2_sl)),
                       rep(&quot;de&quot;, length(de_sl)),
                       rep(&quot;es&quot;, length(es_sl)),
                       rep(&quot;fr&quot;, length(fr_sl)),
                       rep(&quot;it&quot;, length(it_sl)),
                       rep(&quot;pl&quot;, length(pl_sl)),
                       rep(&quot;ru&quot;, length(ru_sl))))</code></pre>
<template id="e3e5134e-c85f-4e3d-94ce-85115575ddba"><style>
.tabwid table{
  border-spacing:0px !important;
  border-collapse:collapse;
  line-height:1;
  margin-left:auto;
  margin-right:auto;
  border-width: 0;
  display: table;
  margin-top: 1.275em;
  margin-bottom: 1.275em;
  border-color: transparent;
}
.tabwid_left table{
  margin-left:0;
}
.tabwid_right table{
  margin-right:0;
}
.tabwid td {
    padding: 0;
}
.tabwid a {
  text-decoration: none;
}
.tabwid thead {
    background-color: transparent;
}
.tabwid tfoot {
    background-color: transparent;
}
.tabwid table tr {
background-color: transparent;
}
</style><div class="tabwid"><style>.cl-c4619e22{table-layout:auto;width:75%;}.cl-c459652c{font-family:'Arial';font-size:12pt;font-weight:bold;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-c459652d{font-family:'Arial';font-size:12pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-c459652e{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-c459652f{margin:0;text-align:center;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-c459b306{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c459b307{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c459b308{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c459b309{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c459b30a{background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c459b30b{background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c459b30c{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c459b30d{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}</style><table class='cl-c4619e22'>
<caption class="Table Caption">
<p>First 6 rows of the table holding the sentences lengths and the L1 of the speakers that produced them.</p>
</caption>
<thead><tr style="overflow-wrap:break-word;"><td class="cl-c459b30d"><p class="cl-c459652e"><span class="cl-c459652c">sentenceLength</span></p></td><td class="cl-c459b30c"><p class="cl-c459652f"><span class="cl-c459652c">l1</span></p></td></tr></thead><tbody><tr style="overflow-wrap:break-word;"><td class="cl-c459b307"><p class="cl-c459652e"><span class="cl-c459652d">2</span></p></td><td class="cl-c459b306"><p class="cl-c459652f"><span class="cl-c459652d">en</span></p></td></tr><tr style="overflow-wrap:break-word;"><td class="cl-c459b309"><p class="cl-c459652e"><span class="cl-c459652d">17</span></p></td><td class="cl-c459b308"><p class="cl-c459652f"><span class="cl-c459652d">en</span></p></td></tr><tr style="overflow-wrap:break-word;"><td class="cl-c459b307"><p class="cl-c459652e"><span class="cl-c459652d">23</span></p></td><td class="cl-c459b306"><p class="cl-c459652f"><span class="cl-c459652d">en</span></p></td></tr><tr style="overflow-wrap:break-word;"><td class="cl-c459b309"><p class="cl-c459652e"><span class="cl-c459652d">17</span></p></td><td class="cl-c459b308"><p class="cl-c459652f"><span class="cl-c459652d">en</span></p></td></tr><tr style="overflow-wrap:break-word;"><td class="cl-c459b307"><p class="cl-c459652e"><span class="cl-c459652d">20</span></p></td><td class="cl-c459b306"><p class="cl-c459652f"><span class="cl-c459652d">en</span></p></td></tr><tr style="overflow-wrap:break-word;"><td class="cl-c459b30b"><p class="cl-c459652e"><span class="cl-c459652d">34</span></p></td><td class="cl-c459b30a"><p class="cl-c459652f"><span class="cl-c459652d">en</span></p></td></tr></tbody></table></div></template>
<div class="flextable-shadow-host" id="c4fb8990-b5db-44e2-9155-a0cdc0a5ab05"></div>
<script>
var dest = document.getElementById("c4fb8990-b5db-44e2-9155-a0cdc0a5ab05");
var template = document.getElementById("e3e5134e-c85f-4e3d-94ce-85115575ddba");
var caption = template.content.querySelector("caption");
if(caption) {
  caption.style.cssText = "display:block;text-align:center;";
  var newcapt = document.createElement("p");
  newcapt.appendChild(caption)
  dest.parentNode.insertBefore(newcapt, dest.previousSibling);
}
var fantome = dest.attachShadow({mode: 'open'});
var templateContent = template.content;
fantome.appendChild(templateContent);
</script>

<p>Now, we can use the resulting table to create a box plot showing the results.</p>
<pre class="r"><code>sl_df %&gt;%
  ggplot(aes(x = reorder(l1, -sentenceLength, mean), y = sentenceLength, fill = l1)) +
  geom_boxplot() +
  # adapt y-axis labels
  labs(y = &quot;Sentence lenghts&quot;) +
  # adapt tick labels
  scale_x_discrete(&quot;L1 of learners&quot;, 
                   breaks = names(table(sl_df$l1)), 
                   labels = c(&quot;en&quot; = &quot;English&quot;,
                              &quot;de&quot; = &quot;German&quot;,
                              &quot;es&quot; = &quot;Spanish&quot;,
                              &quot;fr&quot; = &quot;French&quot;,
                              &quot;it&quot; = &quot;Italian&quot;,
                              &quot;pl&quot; = &quot;Polish&quot;,
                              &quot;ru&quot; = &quot;Russian&quot;)) +
  theme_bw() +
  theme(legend.position = &quot;none&quot;)</code></pre>
<p><img src="llr_files/figure-html/senl8-1.png" width="672" /></p>
</div>
<div id="extracting-n-grams" class="section level1 unnumbered">
<h1>Extracting N-grams</h1>
<p>In a next step, we extract n-grams using the <code>tokens_ngrams</code> function from the <code>quanteda</code> package. In a first step, we take the sentence data, convert it to lower case and remove punctuation. Then we apply the <code>tokens_ngrams</code> function to extract the n-grams (in this case 2-grams).</p>
<pre class="r"><code>ns1_tok &lt;- ns1_sen %&gt;%
  tolower() %&gt;%
  quanteda::tokens(remove_punct = TRUE)
# extract n-grams
ns1_2gram &lt;- quanteda::tokens_ngrams(ns1_tok, n = 2)
# inspect
head(ns1_2gram[[2]], 10)</code></pre>
<pre><code>##  [1] &quot;the_basic&quot;        &quot;basic_dilema&quot;     &quot;dilema_facing&quot;    &quot;facing_the&quot;      
##  [5] &quot;the_uk&#39;s&quot;         &quot;uk&#39;s_rail&quot;        &quot;rail_and&quot;         &quot;and_road&quot;        
##  [9] &quot;road_transport&quot;   &quot;transport_system&quot;</code></pre>
<p>We now apply the same procedure to all texts as shown below.</p>
<pre class="r"><code>ns1_tok &lt;- ns1_sen %&gt;% tolower() %&gt;% quanteda::tokens(remove_punct = TRUE)
ns2_tok &lt;- ns2_sen %&gt;% tolower() %&gt;% quanteda::tokens(remove_punct = TRUE)
de_tok &lt;- de_sen %&gt;% tolower() %&gt;% quanteda::tokens(remove_punct = TRUE)
es_tok &lt;- es_sen %&gt;% tolower() %&gt;% quanteda::tokens(remove_punct = TRUE)
fr_tok &lt;- fr_sen %&gt;% tolower() %&gt;% quanteda::tokens(remove_punct = TRUE)
it_tok &lt;- it_sen %&gt;% tolower() %&gt;% quanteda::tokens(remove_punct = TRUE)
pl_tok &lt;- pl_sen %&gt;% tolower() %&gt;% quanteda::tokens(remove_punct = TRUE)
ru_tok &lt;- ru_sen %&gt;% tolower() %&gt;% quanteda::tokens(remove_punct = TRUE)
# extract n-grams
ns1_2gram &lt;- as.vector(unlist(quanteda::tokens_ngrams(ns1_tok, n = 2)))
ns2_2gram &lt;- as.vector(unlist(quanteda::tokens_ngrams(ns2_tok, n = 2)))
de_2gram &lt;- as.vector(unlist(quanteda::tokens_ngrams(de_tok, n = 2)))
es_2gram &lt;- as.vector(unlist(quanteda::tokens_ngrams(es_tok, n = 2)))
fr_2gram &lt;- as.vector(unlist(quanteda::tokens_ngrams(fr_tok, n = 2)))
it_2gram &lt;- as.vector(unlist(quanteda::tokens_ngrams(it_tok, n = 2)))
pl_2gram &lt;- as.vector(unlist(quanteda::tokens_ngrams(pl_tok, n = 2)))
ru_2gram &lt;- as.vector(unlist(quanteda::tokens_ngrams(ru_tok, n = 2)))</code></pre>
<p>Next, we generate a table with the ngrams and the L1 background of the speaker that produced the bi-grams.</p>
<pre class="r"><code>ngram_df &lt;- c(ns1_2gram, ns2_2gram, de_2gram, es_2gram, 
              fr_2gram, it_2gram, pl_2gram, ru_2gram) %&gt;%
  as.data.frame() %&gt;%
  dplyr::rename(ngram = 1) %&gt;%
  dplyr::mutate(l1 = c(rep(&quot;en&quot;, length(ns1_2gram)),
                       rep(&quot;en&quot;, length(ns2_2gram)),
                       rep(&quot;de&quot;, length(de_2gram)),
                       rep(&quot;es&quot;, length(es_2gram)),
                       rep(&quot;fr&quot;, length(fr_2gram)),
                       rep(&quot;it&quot;, length(it_2gram)),
                       rep(&quot;pl&quot;, length(pl_2gram)),
                       rep(&quot;ru&quot;, length(ru_2gram))),
                learner = ifelse(l1 == &quot;en&quot;, &quot;no&quot;, &quot;yes&quot;))
# inspect
head(ngram_df)</code></pre>
<pre><code>##           ngram l1 learner
## 1  transport_01 en      no
## 2     the_basic en      no
## 3  basic_dilema en      no
## 4 dilema_facing en      no
## 5    facing_the en      no
## 6      the_uk&#39;s en      no</code></pre>
<p>Now, we process the table further to add frequency information, i.e., how often a given n-gram occurs in each the language of speakers with distinct L1 backgrounds.</p>
<pre class="r"><code>ngram_fdf &lt;- ngram_df %&gt;%
  dplyr::group_by(ngram, learner) %&gt;%
  dplyr::summarise(freq = n()) %&gt;%
  dplyr::arrange(-freq)
# inspect
head(ngram_fdf)</code></pre>
<pre><code>## # A tibble: 6 x 3
## # Groups:   ngram [5]
##   ngram            learner  freq
##   &lt;chr&gt;            &lt;chr&gt;   &lt;int&gt;
## 1 of_the           no         72
## 2 to_the           no         40
## 3 in_the           no         39
## 4 public_transport no         35
## 5 of_the           yes        33
## 6 number_of        no         32</code></pre>
<p>As the word counts of the texts are quite different, we normalize the frequencies to per-1,000-word frequencies which are comparable across texts of different lengths.</p>
<pre class="r"><code>ngram_nfdf &lt;- ngram_fdf %&gt;%
  dplyr::group_by(ngram) %&gt;%
  dplyr::mutate(total_ngram = sum(freq)) %&gt;%
  dplyr::arrange(-total_ngram) %&gt;%
  # total by learner
  dplyr::group_by(learner) %&gt;%
  dplyr::mutate(total_learner = sum(freq),
                rfreq = freq/total_learner*1000)
# inspect
head(ngram_nfdf, 10)</code></pre>
<pre><code>## # A tibble: 10 x 6
## # Groups:   learner [2]
##    ngram            learner  freq total_ngram total_learner rfreq
##    &lt;chr&gt;            &lt;chr&gt;   &lt;int&gt;       &lt;int&gt;         &lt;int&gt; &lt;dbl&gt;
##  1 of_the           no         72         105          9452  7.62
##  2 of_the           yes        33         105          3395  9.72
##  3 in_the           no         39          49          9452  4.13
##  4 in_the           yes        10          49          3395  2.95
##  5 to_the           no         40          47          9452  4.23
##  6 to_the           yes         7          47          3395  2.06
##  7 it_is            no         23          44          9452  2.43
##  8 it_is            yes        21          44          3395  6.19
##  9 public_transport no         35          35          9452  3.70
## 10 number_of        no         32          35          9452  3.39</code></pre>
<p>We now reformat the table so that we have relative frequencies for both learners and L1 speakers even if a particular n-gram does not occur in the text produced by either a learner or a L1 speaker.</p>
<pre class="r"><code>ngram_rel &lt;- ngram_nfdf %&gt;%
  dplyr::select(ngram, learner, rfreq, total_ngram) %&gt;%
  tidyr::spread(learner, rfreq) %&gt;%
  dplyr::mutate(no = ifelse(is.na(no), 0, no),
                yes = ifelse(is.na(yes), 0, yes)) %&gt;%
  tidyr::gather(learner, rfreq, no:yes) %&gt;%
  dplyr::arrange(-total_ngram)
# inspect
head(ngram_rel)</code></pre>
<pre><code>## # A tibble: 6 x 4
##   ngram  total_ngram learner rfreq
##   &lt;chr&gt;        &lt;int&gt; &lt;chr&gt;   &lt;dbl&gt;
## 1 of_the         105 no       7.62
## 2 of_the         105 yes      9.72
## 3 in_the          49 no       4.13
## 4 in_the          49 yes      2.95
## 5 to_the          47 no       4.23
## 6 to_the          47 yes      2.06</code></pre>
<p>Finally, we visualize the most frequent n-grams in the data in a bar chart.</p>
<pre class="r"><code>ngram_rel %&gt;%
  head(20) %&gt;%
  ggplot(aes(y = rfreq, x = reorder(ngram, -total_ngram), group = learner, fill = learner)) +
  geom_bar(stat = &quot;identity&quot;, position = position_dodge()) +
  theme_bw() +
  theme(axis.text.x = element_text(size=8, angle=90),
        legend.position = &quot;top&quot;) +
  labs(y = &quot;Relative frequnecy\n(per 1,000 words)&quot;, x = &quot;n-gram&quot;)</code></pre>
<p><img src="llr_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
</div>
<div id="analyzing-differences-in-ngram-use" class="section level1 unnumbered">
<h1>Analyzing differences in ngram use</h1>
<p>Next, we will set out to identify differences in ngram frequencies between learners and L1 speakers.</p>
<pre class="r"><code>sdif_ngram &lt;- ngram_fdf %&gt;%
  tidyr::spread(learner, freq) %&gt;%
  dplyr::mutate(no = ifelse(is.na(no), 0, no),
                yes = ifelse(is.na(yes), 0, yes)) %&gt;%
  dplyr::rename(l1speaker = no, 
                learner = yes) %&gt;%
  dplyr::mutate(total_ngram = l1speaker+learner) %&gt;%
  dplyr::ungroup() %&gt;%
  dplyr::mutate(total_learner = sum(learner),
              total_l1 = sum(l1speaker)) %&gt;%
  dplyr::mutate(a = l1speaker,
                b = learner) %&gt;%
  dplyr::mutate(c = total_l1-a,
                d = total_learner-b) %&gt;%
  # perform fishers exact test and extract estimate and p
  dplyr::rowwise() %&gt;%
  dplyr::mutate(fisher_p = fisher.test(matrix(c(a,c,b,d), nrow= 2))$p.value,
                fisher_est = fisher.test(matrix(c(a,c,b,d), nrow= 2))$estimate,
                # calculate bonferroni correction
                crit = .05/nrow(.),
                sig_corr = ifelse(fisher_p &lt; crit, &quot;p&lt;.05&quot;, &quot;n.s.&quot;))# %&gt;%
  #dplyr::filter(sig_corr != &quot;n.s.&quot;)
# inspect
head(sdif_ngram)</code></pre>
<pre><code>## # A tibble: 6 x 14
## # Rowwise: 
##   ngram   l1speaker learner total_ngram total_learner total_l1     a     b     c
##   &lt;chr&gt;       &lt;dbl&gt;   &lt;dbl&gt;       &lt;dbl&gt;         &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
## 1 -to_cr~         1       0           1          3395     9452     1     0  9451
## 2 `_t             0       1           1          3395     9452     0     1  9452
## 3 £_1mil~         1       0           1          3395     9452     1     0  9451
## 4 £_bill~         1       0           1          3395     9452     1     0  9451
## 5 +_even          1       0           1          3395     9452     1     0  9451
## 6 +_peop~         1       0           1          3395     9452     1     0  9451
## # ... with 5 more variables: d &lt;dbl&gt;, fisher_p &lt;dbl&gt;, fisher_est &lt;dbl&gt;,
## #   crit &lt;dbl&gt;, sig_corr &lt;chr&gt;</code></pre>
<p>In our case, there are no n-grams that differ significantly in their use by learners and L1-speakers once we have corrected for repeated testing.</p>
</div>
<div id="part-of-speech-tagging" class="section level1 unnumbered">
<h1>Part-of-speech tagging</h1>
<p>Part-of-speech tagging is a vry useful procedure for many analyses. Here, we automatically identify parts of speech (word classes) in the text which, for a well-studied language like English, is approximately 95% accurate.</p>
<p>The code chunk below defines a function which applies this kind of tagging to any text fed into the function.</p>
<pre class="r"><code>POStag &lt;- function(x){
  # load necessary packages
  require(&quot;stringr&quot;)
  require(&quot;NLP&quot;)
  require(&quot;openNLP&quot;)
  # define annotators
  sent_token_annotator &lt;- openNLP::Maxent_Sent_Token_Annotator()
  word_token_annotator &lt;- openNLP::Maxent_Word_Token_Annotator()
  pos_tag_annotator &lt;- openNLP::Maxent_POS_Tag_Annotator(language = &quot;en&quot;, probs = FALSE)
  # convert all file content to strings
  strings &lt;- lapply(x, function(x){
    x &lt;- as.String(x)  })
  # loop over file contents
  sapply(strings, function(x){
    a &lt;- NLP::annotate(x, list(sent_token_annotator, word_token_annotator))
    p &lt;- NLP::annotate(x, pos_tag_annotator, a)
    w &lt;- subset(p, type == &quot;word&quot;)
    tags &lt;- sapply(w$features, &#39;[[&#39;, &quot;POS&quot;)
    as &lt;- sprintf(&quot;%s/%s&quot;, x[w], tags)
    at &lt;- paste(as, collapse = &quot; &quot;)
    return(at)  
    })
  }</code></pre>
<p>We now apply this function to a test sentence to see if the function does what we want it to and to chck the output format.</p>
<pre class="r"><code># generate test text
text &lt;- &quot;It is now a very wide spread opinion, that in the modern world there is no place for dreaming and imagination.&quot;
# apply pos-tag function to test text
tagged_text &lt;- POStag(text)
# inspect result
tagged_text</code></pre>
<pre><code>## [1] &quot;It/PRP is/VBZ now/RB a/DT very/RB wide/JJ spread/NN opinion/NN ,/, that/IN in/IN the/DT modern/JJ world/NN there/EX is/VBZ no/DT place/NN for/IN dreaming/VBG and/CC imagination/NN ./.&quot;</code></pre>
<p>The tags which you see here are from the tag set developed for the <em>Penn Treebank</em>, a corpus of English text with syntactic annotations. The tags are not always transparent, and this is very much the case for the word class we will be looking at - the tag for an adjective is <code>/JJ</code>!</p>
<p>The next step is to tag all our texts.</p>
<pre class="r"><code>comText &lt;- function(x,...){
  # paste text together
  x &lt;- paste0(x)
  # remove file identifiers
  x &lt;- stringr::str_remove_all(x, &quot;&lt;.*?&gt;&quot;)
  # remove quotation marks
  x &lt;- stringr::str_remove_all(x, fixed(&quot;\&quot;&quot;))
  # remove superfluous white spaces
  x &lt;- stringr::str_squish(x)
  # remove empty elements
  x &lt;- x[!x==&quot;&quot;]
}</code></pre>
<p>Now we apply the function to the texts, generate a corpus object from the texts and apply the pos-tagging function.</p>
<pre class="r"><code># combine texts
ns1_com &lt;- comText(ns1)
ns2_com &lt;- comText(ns2)
de_com &lt;- comText(de)
es_com &lt;- comText(es)
fr_com &lt;- comText(fr)
it_com &lt;- comText(it)
pl_com &lt;- comText(pl)
ru_com &lt;- comText(ru)</code></pre>
<p>Now we apply the function to the texts, generate a corpus object from the texts and apply the pos-tagging function.</p>
<pre class="r"><code># apply pos-tag function to data
ns1_pos &lt;- as.vector(unlist(POStag(ns1_com)))
ns2_pos &lt;- as.vector(unlist(POStag(ns2_com)))
de_pos &lt;- as.vector(unlist(POStag(de_com)))
es_pos &lt;- as.vector(unlist(POStag(es_com)))
fr_pos &lt;- as.vector(unlist(POStag(fr_com)))
it_pos &lt;- as.vector(unlist(POStag(it_com)))
pl_pos &lt;- as.vector(unlist(POStag(pl_com)))
ru_pos &lt;- as.vector(unlist(POStag(ru_com)))
# inspect
head(ns1_pos)</code></pre>
<pre><code>## [1] &quot;Transport/NNP 01/CD&quot;                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   
## [2] &quot;The/DT basic/JJ dilema/NN facing/VBG the/DT UK/NNP &#39;s/POS rail/NN and/CC road/NN transport/NN system/NN is/VBZ the/DT general/JJ rise/NN in/IN population/NN ./. This/DT leads/VBZ to/TO an/DT increase/NN in/IN the/DT number/NN of/IN commuters/NNS and/CC transport/NN users/NNS every/DT year/NN ,/, consequently/RB putting/VBG pressure/NN on/IN the/DT UKs/NNP transports/VBZ network/NN ./.&quot;                                                                                                                                                   
## [3] &quot;The/DT biggest/JJS worry/NN to/TO the/DT system/NN is/VBZ the/DT rapid/JJ rise/NN of/IN car/NN users/NNS outside/IN the/DT major/JJ cities/NNS ./. Most/JJS large/JJ cities/NNS have/VBP managed/VBN to/TO incourage/VB commuters/NNS to/TO use/VB public/JJ transport/NN thus/RB decreasing/VBG major/JJ conjestion/NN in/IN Rush/NNP hour/NN periods/NNS ./.&quot;                                                                                                                                                                                        
## [4] &quot;Public/NNP transport/NN is/VBZ the/DT obvious/JJ solution/NN to/TO to/TO the/DT increase/NN in/IN population/NN if/IN it/PRP is/VBZ made/VBN cheep/NN to/TO commuters/NNS ,/, clean/JJ ,/, easy/JJ and/CC efficient/JJ then/RB it/PRP could/MD take/VB the/DT strain/NN of/IN the/DT overloaded/VBN British/JJ roads/NNS ./.&quot;                                                                                                                                                                                                                          
## [5] &quot;For/IN commuters/NNS who/WP regularly/RB travel/VBP long/JJ distances/NNS rail/NN transport/NN should/MD be/VB made/VBN more/RBR appealing/JJ ,/, more/RBR comfortable/JJ and/CC cheaper/JJR ./. Motorways/NNP and/CC other/JJ transport/NN links/NNS are/VBP constantly/RB being/VBG extended/VBN ,/, widened/VBN and/CC slowly/RB turning/VBG the/DT country/NN into/IN a/DT concrete/JJ jungle/NN yet/CC it/PRP is/VBZ only/RB trying/VBG to/TO cope/VB with/IN the/DT increase/NN in/IN traffic/NN ,/, we/PRP are/VBP our/PRP$ own/JJ enemy/NN !/.&quot;
## [6] &quot;Another/DT major/JJ problem/NN created/VBN by/IN the/DT mass/NN of/IN vehicle/NN transport/NN is/VBZ the/DT pollution/NN emitted/VBN into/IN the/DT atmosphere/NN damaging/VBG the/DT ozone/NN layer/NN ,/, creating/VBG smog/NN and/CC forming/VBG acid/JJ rain/NN ./. Tourturing/VBG the/DT Earth/NN we/PRP are/VBP living/VBG on/IN ./.&quot;</code></pre>
<p>Next we get a frequency list from each of the tagged corpora, then we extract the adjectives from the data by subsetting the data frame using the grepl() function. This function returns TRUE or FALSE, so we only need to call the function as the conditional part of the subset() function.</p>
</div>
<div id="vocabulary-size" class="section level1 unnumbered">
<h1>Vocabulary size</h1>
<p>We can now extract lexical diversity measures of the texts. In teh present case, we will extract</p>
<ul>
<li><code>TTR</code>: <em>type-token ratio</em></li>
<li><code>C</code>: Herdan’s C (see <span class="citation">J and Baayen (<a href="#ref-tweedie1988lexdiv" role="doc-biblioref">1998</a>)</span>; sometimes referred to as LogTTR)</li>
<li><code>R</code>: Guiraud’s Root TTR (see <span class="citation">J and Baayen (<a href="#ref-tweedie1988lexdiv" role="doc-biblioref">1998</a>)</span>)</li>
<li><code>CTTR</code>: Carroll’s Corrected TTR</li>
<li><code>U</code>: Dugast’s Uber Index (see <span class="citation">J and Baayen (<a href="#ref-tweedie1988lexdiv" role="doc-biblioref">1998</a>)</span>)</li>
<li><code>S</code>: Summer’s index</li>
<li><code>Maas</code>: Maas’ indices</li>
</ul>
<p>Before we extract the lexical diversity measures, however, we split the data into individual essays.</p>
<pre class="r"><code>cleanEss &lt;- function(x){
  x %&gt;%
  paste0(collapse = &quot; &quot;) %&gt;%
  stringr::str_split(&quot;Transport [0-9]{1,2}&quot;) %&gt;%
  unlist() %&gt;%
  stringr::str_squish() %&gt;%
  .[. != &quot;&quot;]
}
# apply function
ns1_ess &lt;- cleanEss(ns1)
ns2_ess &lt;- cleanEss(ns2)
de_ess &lt;- cleanEss(de)
es_ess &lt;- cleanEss(es)
fr_ess &lt;- cleanEss(fr)
it_ess &lt;- cleanEss(it)
pl_ess &lt;- cleanEss(pl)
ru_ess &lt;- cleanEss(ru)
# inspect
head(ns1_ess, 1)</code></pre>
<pre><code>## [1] &quot;The basic dilema facing the UK&#39;s rail and road transport system is the general rise in population. This leads to an increase in the number of commuters and transport users every year, consequently putting pressure on the UKs transports network. The biggest worry to the system is the rapid rise of car users outside the major cities. Most large cities have managed to incourage commuters to use public transport thus decreasing major conjestion in Rush hour periods. Public transport is the obvious solution to to the increase in population if it is made cheep to commuters, clean, easy and efficient then it could take the strain of the overloaded British roads. For commuters who regularly travel long distances rail transport should be made more appealing, more comfortable and cheaper. Motorways and other transport links are constantly being extended, widened and slowly turning the country into a concrete jungle yet it is only trying to cope with the increase in traffic, we are our own enemy! Another major problem created by the mass of vehicle transport is the pollution emitted into the atmosphere damaging the ozone layer, creating smog and forming acid rain. Tourturing the Earth we are living on. In concluding I wish to propose clean, efficient comfortable and cheap public transport for the near future.&quot;</code></pre>
<pre class="r"><code># extract lex. div. measures
ns1_lds &lt;- lapply(ns1_ess, function(x){
  x &lt;- koRpus::lex.div(x, force.lang = &#39;en&#39;, # define language 
                       segment = 20,      # define segment width
                       window = 20,       # define window width
                       quiet = T,
                       # define lex div measures
                       measure=c(&quot;TTR&quot;, &quot;C&quot;, &quot;R&quot;, &quot;CTTR&quot;, &quot;U&quot;, &quot;Maas&quot;),
                       char=c(&quot;TTR&quot;, &quot;C&quot;, &quot;R&quot;, &quot;CTTR&quot;,&quot;U&quot;, &quot;Maas&quot;))
})</code></pre>
<pre><code>## No TreeTagger command specified. Using tokenize() as fallback
## No TreeTagger command specified. Using tokenize() as fallback
## No TreeTagger command specified. Using tokenize() as fallback
## No TreeTagger command specified. Using tokenize() as fallback
## No TreeTagger command specified. Using tokenize() as fallback
## No TreeTagger command specified. Using tokenize() as fallback
## No TreeTagger command specified. Using tokenize() as fallback
## No TreeTagger command specified. Using tokenize() as fallback
## No TreeTagger command specified. Using tokenize() as fallback
## No TreeTagger command specified. Using tokenize() as fallback
## No TreeTagger command specified. Using tokenize() as fallback
## No TreeTagger command specified. Using tokenize() as fallback
## No TreeTagger command specified. Using tokenize() as fallback
## No TreeTagger command specified. Using tokenize() as fallback
## No TreeTagger command specified. Using tokenize() as fallback
## No TreeTagger command specified. Using tokenize() as fallback</code></pre>
<pre class="r"><code># inspect
ns1_lds[1]</code></pre>
<pre><code>## [[1]]
## 
## Total number of tokens: 217 
## Total number of types:  134
## 
## Type-Token Ratio
##                TTR: 0.62 
## 
## TTR characteristics:
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##  0.6140  0.6322  0.6429  0.6888  0.7129  0.9000 
##    SD
##  0.0852
## 
## 
## Herdan&#39;s C
##                  C: 0.91 
## 
## C characteristics:
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##  0.8614  0.9059  0.9131  0.9157  0.9164  0.9648 
##    SD
##  0.0186
## 
## 
## Guiraud&#39;s R
##                  R: 9.1 
## 
## R characteristics:
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##   1.789   5.459   6.484   6.596   8.158   9.080 
##    SD
##  1.8316
## 
## 
## Carroll&#39;s CTTR
##               CTTR: 6.43 
## 
## CTTR characteristics:
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##   1.265   3.860   4.585   4.664   5.769   6.420 
##    SD
##  1.2951
## 
## 
## Uber Index
##                  U: 26.08 
## 
## U characteristics:
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##   5.041  21.104  22.588  23.564  26.226  36.992 
##    SD
##  4.5831
## 
## 
## Maas&#39; Indices
##                  a: 0.2 
##               lgV0: 5.14 
##              lgeV0: 11.84 
## 
## Relative vocabulary growth (first half to full text)
##                  a: 0 
##               lgV0: 1.83 
##                 V&#39;: 0 (0 new types every 100 tokens)
## 
## Maas Indices characteristics:
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##  0.1644  0.1953  0.2104  0.2112  0.2177  0.4454 
##    SD
##  0.0392
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##   1.185   4.106   4.506   4.428   4.955   5.223 
##    SD
##  0.7145
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##   2.729   9.455  10.376  10.196  11.410  12.026 
##    SD
##  1.6452</code></pre>
<pre><code>## 
## Note: Analysis was conducted case insensitive.</code></pre>
<p>We now go aheda and extract the lexical diversity scores for the other essays.</p>
<pre class="r"><code>lexDiv &lt;- function(x){
  lapply(x, function(y){
    koRpus::lex.div(y, force.lang = &#39;en&#39;,  segment = 20, window = 20,  
                    quiet = T, measure=c(&quot;TTR&quot;, &quot;C&quot;, &quot;R&quot;, &quot;CTTR&quot;, &quot;U&quot;, &quot;Maas&quot;),
                    char=c(&quot;TTR&quot;, &quot;C&quot;, &quot;R&quot;, &quot;CTTR&quot;,&quot;U&quot;, &quot;Maas&quot;))
  })
}

# extract lex. div. measures
ns2_lds &lt;- lexDiv(ns2_ess)</code></pre>
<pre><code>## No TreeTagger command specified. Using tokenize() as fallback</code></pre>
<pre class="r"><code>de_lds &lt;- lexDiv(de_ess)</code></pre>
<pre><code>## No TreeTagger command specified. Using tokenize() as fallback</code></pre>
<pre class="r"><code>es_lds &lt;- lexDiv(es_ess)</code></pre>
<pre><code>## No TreeTagger command specified. Using tokenize() as fallback</code></pre>
<pre class="r"><code>fr_lds &lt;- lexDiv(fr_ess)</code></pre>
<pre><code>## No TreeTagger command specified. Using tokenize() as fallback</code></pre>
<pre class="r"><code>it_lds &lt;- lexDiv(it_ess)</code></pre>
<pre><code>## No TreeTagger command specified. Using tokenize() as fallback</code></pre>
<pre class="r"><code>pl_lds &lt;- lexDiv(pl_ess)</code></pre>
<pre><code>## No TreeTagger command specified. Using tokenize() as fallback</code></pre>
<pre class="r"><code>ru_lds &lt;- lexDiv(ru_ess)</code></pre>
<pre><code>## No TreeTagger command specified. Using tokenize() as fallback</code></pre>
<p>Inspect differences in lexical diversity across l1-backgrounds</p>
<pre class="r"><code>cttr &lt;- data.frame(c(as.vector(sapply(ns1_lds, &#39;[&#39;, &quot;CTTR&quot;)), 
                     as.vector(sapply(ns2_lds, &#39;[&#39;, &quot;CTTR&quot;)), 
                     as.vector(sapply(de_lds, &#39;[&#39;, &quot;CTTR&quot;)), 
                     as.vector(sapply(es_lds, &#39;[&#39;, &quot;CTTR&quot;)),
                     as.vector(sapply(fr_lds, &#39;[&#39;, &quot;CTTR&quot;)), 
                     as.vector(sapply(it_lds, &#39;[&#39;, &quot;CTTR&quot;)), 
                     as.vector(sapply(pl_lds, &#39;[&#39;, &quot;CTTR&quot;)), 
                     as.vector(sapply(ru_lds, &#39;[&#39;, &quot;CTTR&quot;))),
          c(rep(&quot;en&quot;, length(as.vector(sapply(ns1_lds, &#39;[&#39;, &quot;CTTR&quot;)))),
            rep(&quot;en&quot;, length(as.vector(sapply(ns2_lds, &#39;[&#39;, &quot;CTTR&quot;)))),
            rep(&quot;de&quot;, length(as.vector(sapply(de_lds, &#39;[&#39;, &quot;CTTR&quot;)))),
            rep(&quot;es&quot;, length(as.vector(sapply(es_lds, &#39;[&#39;, &quot;CTTR&quot;)))),
            rep(&quot;fr&quot;, length(as.vector(sapply(fr_lds, &#39;[&#39;, &quot;CTTR&quot;)))),
            rep(&quot;it&quot;, length(as.vector(sapply(it_lds, &#39;[&#39;, &quot;CTTR&quot;)))),
            rep(&quot;pl&quot;, length(as.vector(sapply(pl_lds, &#39;[&#39;, &quot;CTTR&quot;)))),
            rep(&quot;ru&quot;, length(as.vector(sapply(ru_lds, &#39;[&#39;, &quot;CTTR&quot;)))))) %&gt;%
  dplyr::rename(CTTR = 1,
                l1 = 2)
# inspect
head(cttr)</code></pre>
<pre><code>##   CTTR l1
## 1 6.43 en
## 2 8.84 en
## 3 8.20 en
## 4 8.34 en
## 5 7.34 en
## 6 8.78 en</code></pre>
<pre class="r"><code>cttr %&gt;%
  dplyr::group_by(l1) %&gt;%
  dplyr::summarise(CTTR = mean(CTTR)) %&gt;%
  ggplot(aes(x = reorder(l1, CTTR, mean), y = CTTR)) +
  geom_point() +
  # adapt y-axis labels
  labs(y = &quot;Lexical diversity (CTTR)&quot;) +
  # adapt tick labels
  scale_x_discrete(&quot;L1 of learners&quot;, 
                   breaks = names(table(cttr$l1)), 
                   labels = c(&quot;en&quot; = &quot;English&quot;,
                              &quot;de&quot; = &quot;German&quot;,
                              &quot;es&quot; = &quot;Spanish&quot;,
                              &quot;fr&quot; = &quot;French&quot;,
                              &quot;it&quot; = &quot;Italian&quot;,
                              &quot;pl&quot; = &quot;Polish&quot;,
                              &quot;ru&quot; = &quot;Russian&quot;)) +
  theme_bw() +
  coord_cartesian(ylim = c(0, 15)) +
  theme(legend.position = &quot;none&quot;)</code></pre>
<p><img src="llr_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
</div>
<div id="readability" class="section level1 unnumbered">
<h1>Readability</h1>
<pre class="r"><code>ns1_read &lt;- quanteda.textstats::textstat_readability(ns1_ess)
ns2_read &lt;- quanteda.textstats::textstat_readability(ns2_ess)
de_read &lt;- quanteda.textstats::textstat_readability(de_ess)
es_read &lt;- quanteda.textstats::textstat_readability(es_ess)
fr_read &lt;- quanteda.textstats::textstat_readability(fr_ess)
it_read &lt;- quanteda.textstats::textstat_readability(it_ess)
pl_read &lt;- quanteda.textstats::textstat_readability(pl_ess)
ru_read &lt;- quanteda.textstats::textstat_readability(ru_ess)
# inspect
ns1_read</code></pre>
<pre><code>##    document   Flesch
## 1     text1 43.12767
## 2     text2 62.34563
## 3     text3 63.16179
## 4     text4 62.90455
## 5     text5 53.53250
## 6     text6 56.92020
## 7     text7 53.89138
## 8     text8 59.28742
## 9     text9 62.26228
## 10   text10 53.60807
## 11   text11 58.24022
## 12   text12 58.36792
## 13   text13 55.85388
## 14   text14 48.55222
## 15   text15 55.41899
## 16   text16 62.98538</code></pre>
<p>Generate table</p>
<pre class="r"><code>read &lt;- rbind(ns1_read, ns2_read, de_read, es_read, fr_read, it_read, pl_read, ru_read) %&gt;%
  dplyr::mutate(l1 = c(rep(&quot;en&quot;, nrow(ns1_read)),
                       rep(&quot;en&quot;, nrow(ns2_read)),
                       rep(&quot;de&quot;, nrow(de_read)),
                       rep(&quot;es&quot;, nrow(es_read)),
                       rep(&quot;fr&quot;, nrow(fr_read)),
                       rep(&quot;it&quot;, nrow(it_read)),
                       rep(&quot;pl&quot;, nrow(pl_read)),
                       rep(&quot;ru&quot;, nrow(ru_read)))) %&gt;%
  dplyr::group_by(l1) %&gt;%
  dplyr::summarise(Flesch = mean(Flesch))
# inspect
head(read)</code></pre>
<pre><code>## # A tibble: 6 x 2
##   l1    Flesch
##   &lt;chr&gt;  &lt;dbl&gt;
## 1 de      65.2
## 2 en      56.7
## 3 es      57.6
## 4 fr      66.4
## 5 it      55.4
## 6 pl      62.5</code></pre>
<pre class="r"><code>read %&gt;%
  ggplot(aes(x = l1, y = Flesch, label = round(Flesch, 1))) +
  geom_bar(stat = &quot;identity&quot;) +
  geom_text(vjust=1.6, color = &quot;white&quot;)+
  # adapt tick labels
  scale_x_discrete(&quot;L1 of learners&quot;, 
                   breaks = names(table(read$l1)), 
                   labels = c(&quot;en&quot; = &quot;English&quot;,
                              &quot;de&quot; = &quot;German&quot;,
                              &quot;es&quot; = &quot;Spanish&quot;,
                              &quot;fr&quot; = &quot;French&quot;,
                              &quot;it&quot; = &quot;Italian&quot;,
                              &quot;pl&quot; = &quot;Polish&quot;,
                              &quot;ru&quot; = &quot;Russian&quot;)) +
  theme_bw() +
  coord_cartesian(ylim = c(0, 75)) +
  theme(legend.position = &quot;none&quot;)</code></pre>
<p><img src="llr_files/figure-html/unnamed-chunk-17-1.png" width="672" /></p>
</div>
<div id="citation-session-info" class="section level1 unnumbered">
<h1>Citation &amp; Session Info</h1>
<p>Schweinberger, Martin. 2021. <em>Analyzing learner language using R</em>. Brisbane: The University of Queensland. url: <a href="https://slcladal.github.io/llr.html" class="uri">https://slcladal.github.io/llr.html</a> (Version 2021.11.01).</p>
<pre><code>@manual{schweinberger2021llr,
  author = {Schweinberger, Martin},
  title = {Analyzing learner language using R},
  note = {https://slcladal.github.io/pwr.html},
  year = {2021},
  organization = &quot;The University of Queensland, Australia. School of Languages and Cultures},
  address = {Brisbane},
  edition = {2021.11.01}
}</code></pre>
<pre class="r"><code>sessionInfo()</code></pre>
<pre><code>## R version 4.1.1 (2021-08-10)
## Platform: x86_64-w64-mingw32/x64 (64-bit)
## Running under: Windows 10 x64 (build 19043)
## 
## Matrix products: default
## 
## locale:
## [1] LC_COLLATE=German_Germany.1252  LC_CTYPE=German_Germany.1252   
## [3] LC_MONETARY=German_Germany.1252 LC_NUMERIC=C                   
## [5] LC_TIME=German_Germany.1252    
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] tokenizers_0.2.1          entity_0.1.0             
##  [3] pacman_0.5.1              stringi_1.7.5            
##  [5] koRpus.lang.en_0.1-4      koRpus_0.13-8            
##  [7] sylly_0.1-6               quanteda.textstats_0.94.1
##  [9] quanteda_3.1.0            openNLP_0.2-7            
## [11] tidytext_0.3.2            tm_0.7-8                 
## [13] NLP_0.2-1                 flextable_0.6.9          
## [15] forcats_0.5.1             stringr_1.4.0            
## [17] dplyr_1.0.7               purrr_0.3.4              
## [19] readr_2.0.2               tidyr_1.1.4              
## [21] tibble_3.1.5              ggplot2_3.3.5            
## [23] tidyverse_1.3.1          
## 
## loaded via a namespace (and not attached):
##  [1] fs_1.5.0            lubridate_1.8.0     httr_1.4.2         
##  [4] SnowballC_0.7.0     tools_4.1.1         backports_1.3.0    
##  [7] utf8_1.2.2          R6_2.5.1            DBI_1.1.1          
## [10] colorspace_2.0-2    openNLPdata_1.5.3-4 withr_2.4.2        
## [13] tidyselect_1.1.1    proxyC_0.2.2        compiler_4.1.1     
## [16] cli_3.1.0           rvest_1.0.2         xml2_1.3.2         
## [19] officer_0.4.0       labeling_0.4.2      slam_0.1-48        
## [22] scales_1.1.1        systemfonts_1.0.3   digest_0.6.27      
## [25] rmarkdown_2.5       base64enc_0.1-3     pkgconfig_2.0.3    
## [28] htmltools_0.5.2     dbplyr_2.1.1        fastmap_1.1.0      
## [31] highr_0.9           rlang_0.4.11        readxl_1.3.1       
## [34] rstudioapi_0.13     farver_2.1.0        generics_0.1.1     
## [37] jsonlite_1.7.2      zip_2.2.0           magrittr_2.0.1     
## [40] Matrix_1.3-4        Rcpp_1.0.7          munsell_0.5.0      
## [43] fansi_0.5.0         gdtools_0.2.3       lifecycle_1.0.1    
## [46] yaml_2.2.1          grid_4.1.1          parallel_4.1.1     
## [49] crayon_1.4.1        lattice_0.20-44     haven_2.4.3        
## [52] hms_1.1.1           klippy_0.0.0.9500   knitr_1.36         
## [55] pillar_1.6.4        uuid_1.0-2          stopwords_2.3      
## [58] fastmatch_1.1-3     reprex_2.0.1.9000   glue_1.4.2         
## [61] evaluate_0.14       data.table_1.14.2   RcppParallel_5.1.4 
## [64] modelr_0.1.8        vctrs_0.3.8         tzdb_0.2.0         
## [67] cellranger_1.1.0    gtable_0.3.0        assertthat_0.2.1   
## [70] xfun_0.26           sylly.en_0.1-3      broom_0.7.9        
## [73] janeaustenr_0.1.5   nsyllable_1.0       rJava_1.0-5        
## [76] ellipsis_0.3.2</code></pre>
<hr />
<p><a href="#introduction">Back to top</a></p>
<p><a href="https://slcladal.github.io/index.html">Back to HOME</a></p>
<hr />
</div>
<div id="references" class="section level1 unnumbered">
<h1>References</h1>
<div id="refs" class="references">
<div id="ref-tweedie1988lexdiv">
<p>J, Tweedie F, and R H Baayen. 1998. “How Variable May a Constant Be? Measures of Lexical Richness in Perspective.” <em>Computers and the Humanities</em> 32 (5): 323–52.</p>
</div>
<div id="ref-lindquist2009corpus">
<p>Lindquist, Hans. 2009. <em>Corpus Linguistics and the Description of English</em>. Edinburgh: Edinburgh University Press.</p>
</div>
</div>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>


</body>
</html>
