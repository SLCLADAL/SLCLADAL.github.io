<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />


<meta name="author" content="UQ SLC Digital Team" />

<meta name="date" content="2019-01-25" />

<title>Advanced Inferential Statistics</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cosmo.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<link href="site_libs/font-awesome-5.0.13/css/fa-svg-with-js.css" rel="stylesheet" />
<script src="site_libs/font-awesome-5.0.13/js/fontawesome-all.min.js"></script>
<script src="site_libs/font-awesome-5.0.13/js/fa-v4-shims.min.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>


</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}

.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>


<div class="container-fluid main-container">

<!-- tabsets -->
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->




<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_').toLowerCase();
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}


.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
  padding-left: 25px;
  text-indent: 0;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>

<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">LADAL</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    <span class="fa fa-play-circle"></span>
     
    Basics
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li class="dropdown-header">Basics</li>
    <li>
      <a href="introquant.html">Introduction To Quantitative Reasoning</a>
    </li>
    <li>
      <a href="basicquant.html">Basic Concepts In Quantitative Reasoning</a>
    </li>
    <li>
      <a href="researchdesigns.html">Research Designs</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    <span class="fa fa-gear"></span>
     
    Data Processing
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li class="dropdown-header">Data Processing</li>
    <li>
      <a href="intror.html">Basics: Getting started with R</a>
    </li>
    <li>
      <a href="loading.html">Loading and saving data</a>
    </li>
    <li>
      <a href="introtables.html">Tabulating data</a>
    </li>
    <li>
      <a href="webcrawling.html">Web Crawling</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    <span class="fa fa-bar-chart"></span>
     
    Visualization
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li class="dropdown-header">Visualization</li>
    <li>
      <a href="basicgraphs.html">Basic Visualization Techniques</a>
    </li>
    <li>
      <a href="advancedgraphs.html">Advanced Visualization Techniques</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    <span class="fa fa-eye"></span>
     
    Statistics
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li class="dropdown-header">Statistics</li>
    <li>
      <a href="descriptives.html">Descriptive Statistics</a>
    </li>
    <li>
      <a href="basicstatz.html">Basic Interential Statistics</a>
    </li>
    <li>
      <a href="advancedstatz.html">Advanced Interential Statistics</a>
    </li>
    <li>
      <a href="groupingstatz.html">Clustering</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    <span class="fa fa-bars"></span>
     
    Text Analysis/Corpus Linguistics
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li class="dropdown-header">Text Analysis and Corpus Linguistics</li>
    <li>
      <a href="page-c.html">Network Analysis</a>
    </li>
    <li>
      <a href="page-c.html">Topic Modeling</a>
    </li>
    <li class="divider"></li>
    <li>
      <a href="corplingr.html">Corpus Linguistics in R</a>
    </li>
    <li>
      <a href="corplingantconcexcel.html">Corpus Linguistics with AntConc, TextPad and Excel</a>
    </li>
    <li>
      <a href="available.html">Available Software</a>
    </li>
  </ul>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="about.html">
    <span class="fa fa-info"></span>
     
    Contact
  </a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Advanced Inferential Statistics</h1>
<h4 class="author"><em>UQ SLC Digital Team</em></h4>
<h4 class="date"><em>2019-01-25</em></h4>

</div>


<p><img src="images/uq1.jpg" width="100%" /></p>
<div id="multiple-linear-regression" class="section level1">
<h1><span class="header-section-number">1</span> Multiple Linear Regression</h1>
<p>In contrast to simple linear regression, which estimates the effect of a single predictor, multiple linear regression estimates the effect of various predictor (cf.Â equation ()). A multiple linear regression can thus test the effects of various predictors simultaneously.</p>
<span class="math display">\[\begin{equation}

f_{(x)} = \alpha + \beta_{1}x_{i} + \beta_{2}x_{i+1} + \dots + \beta_{n}x_{i+n} + \epsilon

\end{equation}\]</span>
<p>There exists a wealth of literature focusing on multiple linear regressions and the concepts it is based on. For instance, there are <span class="citation">(Achen 1982)</span>, <span class="citation">(Bortz 2006)</span>, <span class="citation">(Crawley 2005)</span>, <span class="citation">(Faraway 2002)</span>, <span class="citation">(A. Field, Miles, and Field 2012)</span> (my personal favourite), and <span class="citation">(Wilcox 2009)</span> to name just a few. Introductions to regression modelling in <code>R</code> are <span class="citation">(Baayen 2008)</span>, <span class="citation">(Crawley 2012)</span>, or <span class="citation">(Gries 2009)</span>.</p>
<p>The model diagnostics we are dealing with here are partly identical to the diagnostic methods discussed in the section on simple linear regression. Because of this overlap, diagnostics will only be described in more detail if they have not been described in the section on simple linear regression.</p>
<p>A brief note on minimum necessary sample or data set size appears necessary here. Although there appears to be a general assumption that 25 data points per group are sufficient, this is not necessarily correct (it is merely a general rule of thumb that is actually often incorrect). Such rules of thumb are inadequate because the required sample size depends on the number of variables in a given model, the size of the effect and the variance of the effect. If a model contains many variables, then this requires a larger sample size than a model which only uses very few predictors. Also, to detect an effect with a very minor effect size, one needs a substantially larger sample compared to cases where the effect is very strong. In fact, when dealing with small effects, model require a minimum of 600 cases to reliably detect these effects. Finally, effects that very robust and do not vary much require a much smaller sample size compared with effects that are spurious and vary substantially. Since the sample size depends on the effect size and variance as well as the number of variables, there is no final one-size-fits-all answer to what the best sample size is.</p>
<p>Another, slightly better but still incorrect, rule of thumb is that the more data, the better. This is not correct because models based on too many cases are prone for overfitting and thus report correlations as being significant that are not. However, given that there are procedures that can correct for overfitting, larger data sets are still preferable to data sets that are simply too small to warrant reliable results. In conclusion, it remains true that the sample size depends on the effect under investigation.</p>
<p>Despite there being no ultimate rule of thumb, <span class="citation">A. Field, Miles, and Field (2012)</span> 273-275), based on <span class="citation">(Green 1991)</span>, provide data-driven suggestions for the minimal size of data required for regression models that aim to find medium sized effects (k = number of predictors; categorical variables with more than two levels should be transformed into dummy variables):</p>
<ul>
<li>If one is merely interested in the overall model fit (something I have not encountered), then the sample size should be at least 50 + k (k = number of predictors in model).</li>
<li>If one is only interested in the effect of specific variables, then the sample size should be at least 104 + k (k = number of predictors in model).</li>
<li>If one is only interested in both model fit and the effect of specific variables, then the sample size should be at least the higher value of 50 + k or 104 + k (k = number of predictors in model).</li>
</ul>
<p>You will see in the <code>R</code>-code below that there is already a function that test whether or not the sample size is sufficient. (include figure on <span class="citation">(A. Field, Miles, and Field 2012, 275)</span>).</p>
<div id="example-gifts-and-availability" class="section level2">
<h2><span class="header-section-number">1.1</span> Example: Gifts and Availability</h2>
<p>The example we will go through here is taken from <span class="citation">A. Field, Miles, and Field (2012)</span>. In this example, the research question is if the money that men spend on presents for women depends on the womenâs attractiveness and their relationship status. To answer this research question, we will implement a multiple linear regression and start by preparing the <code>R</code>-session (cleaning the workspace, setting options necessary, installing and activating necessary packages, and loading functions).</p>
<pre class="r"><code>rm(list=ls(all=T)) # clean workspace
options(&quot;scipen&quot; = 100, &quot;digits&quot; = 4)  # set options (supress math annotation)
#install.packages(&quot;ggplot2&quot;)           # install ggplot2 package (remove # to activate)
#install.packages(&quot;ggplot2&quot;)           # install car package (remove # to activate)
#install.packages(&quot;QuantPsyc&quot;)         # install QuantPsyc package (remove # to activate)
#install.packages(&quot;boot&quot;)              # install boot package (remove # to activate)
library(ggplot2)                       # activate ggplot2 package
library(car)                           # activate car package
library(QuantPsyc)                     # activate QuantPsyc package
library(boot)                          # activate boot package
source(&quot;rscripts/multiplot_ggplot2.r&quot;) # load function multiplot_ggplot2
source(&quot;rscripts/mlinr.summary.r&quot;)     # load function mlinr.summary
source(&quot;rscripts/SampleSizeMLR.r&quot;)     # load function SampleSizeMLR
source(&quot;rscripts/ExpR.r&quot;)              # load function ExpR</code></pre>
<p>After preparing the session, we can now load the data and inspect its structure and properties.</p>
<pre class="r"><code>mlrdata &lt;- read.delim(&quot;data/mlrdata.txt&quot;, header = TRUE) # load data
head(mlrdata)    # inspect first 6 lines</code></pre>
<pre><code>##         status    attraction money
## 1 Relationship NotInterested 86.33
## 2 Relationship NotInterested 45.58
## 3 Relationship NotInterested 68.43
## 4 Relationship NotInterested 52.93
## 5 Relationship NotInterested 61.86
## 6 Relationship NotInterested 48.47</code></pre>
<pre class="r"><code>str(mlrdata)     # inspect structure</code></pre>
<pre><code>## &#39;data.frame&#39;:    100 obs. of  3 variables:
##  $ status    : Factor w/ 2 levels &quot;Relationship&quot;,..: 1 1 1 1 1 1 1 1 1 1 ...
##  $ attraction: Factor w/ 2 levels &quot;Interested&quot;,&quot;NotInterested&quot;: 2 2 2 2 2 2 2 2 2 2 ...
##  $ money     : num  86.3 45.6 68.4 52.9 61.9 ...</code></pre>
<pre class="r"><code>summary(mlrdata) # summarize data</code></pre>
<pre><code>##           status           attraction     money       
##  Relationship:50   Interested   :50   Min.   :  0.93  
##  Single      :50   NotInterested:50   1st Qu.: 49.84  
##                                       Median : 81.73  
##                                       Mean   : 88.38  
##                                       3rd Qu.:121.57  
##                                       Max.   :200.99</code></pre>
<p>The data set consist of three variables stored in three columns. The first column contains the relationship status of the women, the second whether the man is interested in the woman, and the third column represents the money spend on the present. The data set represents 100 cases and the mean amount of money spend on a present is 88.38 dollars. In a next step, we visualize the data to get a more detailed impression of the relationships between variables.</p>
<pre class="r"><code># plot 1
p1 &lt;- ggplot(mlrdata,                   # create plot based on mlrdata
             aes(status, money)) +      # define x- and y-axes
  geom_boxplot(fill=c(&quot;gold&quot;, &quot;indianred4&quot;)) + # define colors
  theme_set(theme_bw(base_size = 8))+ # define theme (black and white theme)
  labs(x = &quot;&quot;) +                        # x-axis label
  labs(y = &quot;Money spent on present (AUD)&quot;, cex = .75) +   # y-axis label
  coord_cartesian(ylim = c(0, 250)) +   # y-axis range
  guides(fill = FALSE) +                # supress legend
  ggtitle(&quot;Status&quot;)                     # define title
# plot 2
p2 &lt;- ggplot(mlrdata,                   # create plot based on mlrdata
             aes(attraction, money)) +  # define x- and y-axes
  geom_boxplot(fill=c(&quot;grey30&quot;, &quot;grey70&quot;)) +  # define colors
theme_set(theme_bw(base_size = 8))+    # define theme (black and white theme)
  labs(x = &quot;&quot;) +                        # x-axis label
  labs(y = &quot;Money spent on present (AUD)&quot;) +  # y-axis label
  coord_cartesian(ylim = c(0, 250)) +   # y-axis range
  guides(fill = FALSE) +                # supress legend
  ggtitle(&quot;Attraction&quot;)                 # define title
# plot 3
p3 &lt;- ggplot(mlrdata,                   # create plot based on mlrdata
             aes(x = money)) +          # define y-axis
  geom_histogram(aes(y=..density..),    # add density statistic
                 binwidth = 10,         # define bin width
                 colour = &quot;black&quot;,      # define bar edge colour
                 fill = &quot;white&quot;) +      # define bar colour
    theme_bw() +                        # black-white theme
  geom_density(alpha=.2, fill = &quot;#FF6666&quot;) # define colour of transparent overlay
# plot 4
p4 &lt;- ggplot(mlrdata, aes(status, money)) +    # create plot based on mlrdata
  geom_boxplot(notch = F, aes(fill = factor(status))) + # cerate boxplot
  scale_fill_brewer(palette=&quot;Set1&quot;) +          # define colour palette
  facet_wrap(~ attraction, nrow = 1) +         # cerate separate panels for attraction
theme_set(theme_bw(base_size = 8)) +          # define theme (black and white theme)
  labs(x = &quot;&quot;) +                               # define x-axis label
  labs(y = &quot;Money spent on present (Euro)&quot;) +  # define y-axis label
  coord_cartesian(ylim = c(0, 250)) +          # define y-axis range
  guides(fill = FALSE)                         # supress legend
multiplot(p1, p3, p2, p4, cols = 2)</code></pre>
<p><img src="advancedstatz_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
<p>The upper left figure consists of a boxplot which shows how much money was spent based on the relationship status of the moan. The figure suggests that men spend more on women who are not in a relationship. The next figure shows the relationship between the money spend on presents and whether or not the men were interested in the women.</p>
<p>The boxplot in the upper right panel suggests that men spend substantially more on women if the men are interested in them. The next figure depicts the distribution of the amounts of money spend on women. In addition, the figure indicates the existence of two outliers (dots in the boxplot)</p>
<p>The historgram in the lower left panel shows that, although the mean amount of money spent on presents is 88.38 dollars, the distribution peaks around 50 dollars indicating that on average, men spend about 50 dollars on presents. Finally, we will plot the amount of money spend on presents against relationship status by attraction in order to check whether the money spent on presents is affected by an interaction between attraction and relationship status.</p>
<p>The boxplot in the lower right panel confirms the existence of an interaction (a non-additive term) as men only spend more money on single women if the men are interested in the women. If men are not interested in the women, then the relationship has no effect as they spend an equal amount of money on the women regardless of whether they are in a relationship or not.</p>
<p>We will now start to implement the regression model. In a first step, we initialize four base-line models: two minimal base-line models that only use the intercept as their sole predictor and two saturated base-line models that contain all possible predictors. The model pairs are generated with the <code>lm</code> and the <code>glm</code> function as these functions offer different model parameters in their output.</p>
<pre class="r"><code>m0.mlr = lm(                       # generate regression object using the lm function
  money ~ 1,                       # define rgression formula (1 = intercept) 
  data = mlrdata)                  # define data set
m0.glm = glm(                      # generate regression object using the glm function
  money ~ 1,                       # define rgression formula (1 = intercept) 
  family = gaussian,               # define linkage function
  data = mlrdata)                  # define data set
m1.mlr = lm(                       # generate regression object using the lm function
  money ~ (status + attraction)^2, # define rgression formula
  data = mlrdata)                  # define data set
m1.glm = glm(                      # generate regression object using the glm function
  money ~ status * attraction,     # define rgression formula 
  family = gaussian,               # define linkage function
  data = mlrdata)                  # define data set</code></pre>
<p>After generating the saturated base-line models we can now start with the model fitting. Model fitting refers to a process that aims at find the model that explains a maximum of variance with a minimum of predictors <span class="citation">(cf. A. Field, Miles, and Field 2012, 318)</span>. Model fitting is therefore based on the <em>principle of parsimony</em> which is related to Occamâs razor according to which explanations that require fewer assumptions are more likely to be true.</p>
</div>
<div id="automatic-model-fitting" class="section level2">
<h2><span class="header-section-number">1.2</span> Automatic Model Fitting</h2>
<p>In this section, we will use a step-wise step-down procedure that uses decreases in AIC (Akaike information criterion) as the criterion to minimize the model in a step-wise manner. This procedure aims at finding the model with the lowest AIC values by evaluating - step-by-step - whether the removal of a predictor (term) leads to a lower AIC value.</p>
<p>The AIC is calculated using the equation below. The lower the AIC value, the better the balance between explained variance and the number of predictors. AIC values can and should only be compared for models that are fit on the same dataset with the same (number of) cases (<span class="math inline">\(LL\)</span> stands for LogLikelihood and <span class="math inline">\(k\)</span> represents the number of predictors in the model).</p>
<span class="math display">\[\begin{equation}

-2LL + 2k
\label{eq:aic}

\end{equation}\]</span>
<p>Interactions are evaluated first and only if all interactions have been removed would the procedure start removing main effects. Other model fitting procedures (forced entry, step-wise step up, hierarchical) are discussed during the implementation of other regression models. We cannot discuss all procedures here as model fitting is rather complex and a discussion of even the most common procedures would to lengthy and time consuming at this point. It is important to note though that there is not perfect model fitting procedure and automated approaches should be handled with care as they are likely to ignore violations of model parameters that can be detected during manual - but time consuming - model fitting procedures. As a general rule of thumb, it is advisable to fit models as carefully and deliberately as possible. We will now begin to fit the model.</p>
<pre class="r"><code>step(m1.mlr, direction = &quot;both&quot;) # automated AIC based model fitting</code></pre>
<pre><code>## Start:  AIC=592.5
## money ~ (status + attraction)^2
## 
##                     Df Sum of Sq   RSS AIC
## &lt;none&gt;                           34558 593
## - status:attraction  1     24947 59505 645</code></pre>
<pre><code>## 
## Call:
## lm(formula = money ~ (status + attraction)^2, data = mlrdata)
## 
## Coefficients:
##                          (Intercept)  
##                                 99.2  
##                         statusSingle  
##                                 57.7  
##              attractionNotInterested  
##                                -47.7  
## statusSingle:attractionNotInterested  
##                                -63.2</code></pre>
<p>The automated model fitting procedure informs us that removing predictors ahs not caused a decrease in the AIC. The saturated model is thus also the final minimal adequate model. We will now inspect the final minimal model and go over the model report.</p>
<pre class="r"><code>m2.mlr = lm(                       # generate regression object using the lm function
  money ~ (status + attraction)^2, # define regression formula
  data = mlrdata)                  # define data set
m2.glm = glm(                      # generate regression object using the glm function
  money ~ (status + attraction)^2, # define regression formula  
  family = gaussian,               # define linkage function 
  data = mlrdata)                  # define data set
summary(m2.mlr)                    # inspect final minimal model</code></pre>
<pre><code>## 
## Call:
## lm(formula = money ~ (status + attraction)^2, data = mlrdata)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -45.08 -14.26   0.46  11.93  44.14 
## 
## Coefficients:
##                                      Estimate Std. Error t value
## (Intercept)                             99.15       3.79   26.13
## statusSingle                            57.69       5.37   10.75
## attractionNotInterested                -47.66       5.37   -8.88
## statusSingle:attractionNotInterested   -63.18       7.59   -8.32
##                                                  Pr(&gt;|t|)    
## (Intercept)                          &lt; 0.0000000000000002 ***
## statusSingle                         &lt; 0.0000000000000002 ***
## attractionNotInterested                 0.000000000000038 ***
## statusSingle:attractionNotInterested    0.000000000000581 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 19 on 96 degrees of freedom
## Multiple R-squared:  0.852,  Adjusted R-squared:  0.847 
## F-statistic:  184 on 3 and 96 DF,  p-value: &lt;0.0000000000000002</code></pre>
<p>The first element of the report is called <em>Call</em> and it reports the regression formula of the model. Then, the report provides the residual distribution (the range, median and quartiles of the residuals) which allows drawing inferences about the distribution of differences between observed and expected values. If the residuals are distributed unevenly, then this is a strong indicator that the model is unstable and unreliable because mathematical assumptions on which the model is based are violated.</p>
<p>Next, the model summary reports the most important part: a table with model statistics of the fixed-effects structure of the model. The table contains the estimates (coefficients of the predictors), standard errors, t-values, and the p-values which show whether a predictor significantly correlates with the dependent variable that the model investigates.</p>
<p>All main effects (status and attraction) as well as the interaction between status and attraction is reported as being significantly correlated with the dependent variable (money). An interaction occurs if a correlation between the dependent variable and a predictor is affect by another predictor.</p>
<p>The top most term is called intercept and has a value of 99.15 which represents the base estimate to which all other estimates refer. To exemplify what this means, let us consider what the model would predict a man would spend on a present for a women who is single but the man is not attracted to her: The amount he would spend (based on the model would be 99.15 dollars (the intercept) plus 57.69 dollars (because she is single) minus 47.66 dollars (because he is not interested in her) minus 63.18 dollars because of the interaction between status and attraction.</p>
<pre class="r"><code>#intercept  Single  NotInterested  Single:NotInterested
99.15     + 57.69  + 0           + 0     # 156.8 single + interested </code></pre>
<pre><code>## [1] 156.8</code></pre>
<pre class="r"><code>99.15     + 57.69  - 47.66       - 63.18 # 46.00 single + not interested</code></pre>
<pre><code>## [1] 46</code></pre>
<pre class="r"><code>99.15     - 0      + 0           - 0     # 99.15 relationship + interested</code></pre>
<pre><code>## [1] 99.15</code></pre>
<pre class="r"><code>99.15     - 0      - 47.66       - 0     # 51.49 relationship + not interested</code></pre>
<pre><code>## [1] 51.49</code></pre>
<p>Interestingly, the model predicts that a man would invest even less money in a women that he is not interested in if she were single compared to being in a relationship! We can derive the same results easier using the <code>predict</code> function.</p>
<pre class="r"><code>prediction &lt;- predict(m2.mlr,            # make prediction based on the model
                      newdata = mlrdata) # for original data
table(round(prediction,2))               # inspect predictions</code></pre>
<pre><code>## 
##  46.01  51.49  99.15 156.85 
##     25     25     25     25</code></pre>
<p>Below the table of coefficient, the summary reports model statistics that provide information about how well the model performs. The difference between the values and the values in the coefficients table is that the model statistics refer to the model as a whole rather than focusing on individual predictors.</p>
<p>The multiple R<sup>2</sup>-value is a measure of how much variance the model explains. A multiple R<sup>2</sup>-value of 0 would inform us that the model does not explain any variance while a value of .852 mean that the model explains 85.2 percent of the variance. A value of 1 would inform us that the model explains 100 percent of the variance and that the predictions of the model match the observed values perfectly. Multiplying the multiple R<sup>2</sup>-value thus provides the percentage of explained variance. Models that have a multiple R<sup>2</sup>-value equal or higher than .05 are deemed substantially significant <span class="citation">(cf Szmrecsanyi 2006, 55)</span>. It has been claimed that models should explain a minimum of 5 percent of variance but this is problematic as itis not uncommon for models to have very low explanatory power while still performing significantly and systematically better than chance. In addition, the total amount of variance is negligible in cases where one is interested in very weak but significant effects. It is much more important for model to perform significantly better than minimal base-line models because if this is not the case, then the model does not have any predictive and therefore no explanatory power.</p>
<p>The adjusted R<sup>2</sup>-value considers the amount of explained variance in light of the number of predictors in the model (it is thus somewhat similar to the AIC and BIC) and informs about how well the model would perform if it were applied to the population that the sample is drawn from. Ideally, the difference between multiple and adjusted R<sup>2</sup>-value should be very small as this means that the model is not overfitted. If, however, the difference between multiple and adjusted R<sup>2</sup>-value is substantial, then this would strongly suggest that the model is instable and overfitted to the data while being inadequate for drawing inferences about the population. Differences between multiple and adjusted R<sup>2</sup>-values indicate that the data contains outliers that cause the distribution of the data on which the model is based to differ from the distributions that the model mathematically requires to provide reliable estimates. The difference between multiple and adjusted R<sup>2</sup>-value in our model is very small (85.2-84.7=.05) and should not cause concern.</p>
<p>Before continuing, we will calculate the confidence intervals of the coefficients.</p>
<pre class="r"><code>confint(m2.mlr)       # extract confidence intervals of the coefficients</code></pre>
<pre><code>##                                       2.5 % 97.5 %
## (Intercept)                           91.62 106.69
## statusSingle                          47.04  68.34
## attractionNotInterested              -58.31 -37.01
## statusSingle:attractionNotInterested -78.24 -48.11</code></pre>
<pre class="r"><code>anova(m0.mlr, m2.mlr) # compare baseline- and minimal adequate model</code></pre>
<pre><code>## Analysis of Variance Table
## 
## Model 1: money ~ 1
## Model 2: money ~ (status + attraction)^2
##   Res.Df    RSS Df Sum of Sq   F              Pr(&gt;F)    
## 1     99 233562                                         
## 2     96  34558  3    199005 184 &lt;0.0000000000000002 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>Now, we compare the final minimal adequate model to the base-line model to test whether then final model significantly outperforms the baseline model.</p>
<pre class="r"><code>Anova(m0.mlr, m2.mlr, type = &quot;III&quot;) # compare baseline- and minimal adequate model</code></pre>
<pre><code>## Anova Table (Type III tests)
## 
## Response: money
##             Sum Sq Df F value              Pr(&gt;F)    
## (Intercept) 781016  1     331 &lt;0.0000000000000002 ***
## Residuals    34558 96                                
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>The comparison between the two model confirms that the minimal adequate model performs significantly better (makes significantly more accurate estimates of the outcome variable) compared with the baseline model.</p>
</div>
<div id="outlier-detection" class="section level2">
<h2><span class="header-section-number">1.3</span> Outlier Detection</h2>
<p>After implementing the multiple regression, we now need to look for outliers and perform the model diagnostics by testing whether removing data points disproportionately decreases model fit. To begin with, we generate diagnostic plots.</p>
<pre class="r"><code># start plotting
par(mfrow = c(1, 4))           # display plots in 3 rows and 2 columns
plot(m2.mlr)                   # plot fitted values</code></pre>
<p><img src="advancedstatz_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<pre class="r"><code>par(mfrow = c(1, 1))           # restore original 1 plot per row and column settings</code></pre>
<pre class="r"><code># determine a cutoff for data points that have D-values higher than 4/(n-k-1) 
cutoff &lt;- 4/((nrow(mlrdata)-length(m2.mlr$coefficients)-2))
# start plotting
par(mfrow = c(1, 2))           # display plots in 3 rows and 2 columns
qqPlot(m2.mlr, main=&quot;QQ Plot&quot;) # cerate qq-plot</code></pre>
<pre><code>## [1] 52 83</code></pre>
<pre class="r"><code>plot(m2.mlr, which=4, cook.levels = cutoff) # plot cook*s distance</code></pre>
<p><img src="advancedstatz_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<pre class="r"><code>par(mfrow = c(1, 1))           # restore original 1 plot per row and column settings</code></pre>
<p>The graphs indicate that data points 52, 64, and 83 may be problematic. We will therefore statistically evaluate whether these data points need to be removed. In order to find out which data points require removal, we extract the influence measure statistics and add them to out data set.</p>
<pre class="r"><code>infl &lt;- influence.measures(m2.mlr)       # extract influence statistics
mlrdata &lt;- data.frame(mlrdata, infl[[1]], infl[[2]]) # add infl. statistics to data
# annotate too influential data points
remove &lt;- apply(infl$is.inf, 1, function(x) {
  ifelse(x == TRUE, return(&quot;remove&quot;), return(&quot;keep&quot;)) } )
mlrdata &lt;- data.frame(mlrdata, remove)   # add annotation to data
nrow(mlrdata)                            # number of rows before removing outliers</code></pre>
<pre><code>## [1] 100</code></pre>
<pre class="r"><code>mlrdata &lt;- mlrdata[mlrdata$remove == &quot;keep&quot;, ] # remove outliers
nrow(mlrdata)                             # number of rows after removing outliers</code></pre>
<pre><code>## [1] 98</code></pre>
<p>The difference in row in the data set before and after removing data points indicate that two data points which represented outliers have been removed.</p>
</div>
<div id="rerun-regression" class="section level2">
<h2><span class="header-section-number">1.4</span> Rerun Regression</h2>
<p>As we have a different data set now, we need to rerun the regression analysis. As the steps are identical to the regression analysis performed above, the steps will not be described in greater detail.</p>
<pre class="r"><code>m0.mlr = lm(                       # generate regression object using the lm function
  money ~ 1,                       # define regression formula (1 = intercept) 
  data = mlrdata)                  # define data set
m0.glm = glm(                      # generate regression object using the glm function
  money ~ 1,                       # define regression formula (1 = intercept) 
  family = gaussian,               # define linkage function
  data = mlrdata)                  # define data set
m1.mlr = lm(                       # generate regression object using the lm function
  money ~ (status + attraction)^2, # define regression formula
  data = mlrdata)                  # define data set
m1.glm = glm(                      # generate regression object using the glm function
  money ~ status * attraction,     # define regression formula 
  family = gaussian,               # define linkage function
  data = mlrdata)                  # define data set
step(m1.mlr, direction = &quot;both&quot;)   # automated AIC based model fitting</code></pre>
<pre><code>## Start:  AIC=570.3
## money ~ (status + attraction)^2
## 
##                     Df Sum of Sq   RSS AIC
## &lt;none&gt;                           30411 570
## - status:attraction  1     21647 52058 621</code></pre>
<pre><code>## 
## Call:
## lm(formula = money ~ (status + attraction)^2, data = mlrdata)
## 
## Coefficients:
##                          (Intercept)  
##                                 99.2  
##                         statusSingle  
##                                 55.9  
##              attractionNotInterested  
##                                -47.7  
## statusSingle:attractionNotInterested  
##                                -59.5</code></pre>
<pre class="r"><code>m2.mlr = lm(                       # generate regression object using the lm function
  money ~ (status + attraction)^2, # define regression formula
  data = mlrdata)                  # define data set
m2.glm = glm(                      # generate regression object using the glm function
  money ~ status * attraction,     # define regression formula 
  family = gaussian,               # define linkage function
  data = mlrdata)                  # define data set
summary(m2.mlr)                    # inspect final minimal model</code></pre>
<pre><code>## 
## Call:
## lm(formula = money ~ (status + attraction)^2, data = mlrdata)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -35.76 -13.51  -0.99  10.60  38.77 
## 
## Coefficients:
##                                      Estimate Std. Error t value
## (Intercept)                             99.15       3.60   27.56
## statusSingle                            55.85       5.14   10.87
## attractionNotInterested                -47.66       5.09   -9.37
## statusSingle:attractionNotInterested   -59.46       7.27   -8.18
##                                                  Pr(&gt;|t|)    
## (Intercept)                          &lt; 0.0000000000000002 ***
## statusSingle                         &lt; 0.0000000000000002 ***
## attractionNotInterested                 0.000000000000004 ***
## statusSingle:attractionNotInterested    0.000000000001338 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 18 on 94 degrees of freedom
## Multiple R-squared:  0.857,  Adjusted R-squared:  0.853 
## F-statistic:  188 on 3 and 94 DF,  p-value: &lt;0.0000000000000002</code></pre>
<pre class="r"><code>confint(m2.mlr)       # extract confidence intervals of the coefficients</code></pre>
<pre><code>##                                       2.5 % 97.5 %
## (Intercept)                           92.01 106.30
## statusSingle                          45.65  66.06
## attractionNotInterested              -57.76 -37.56
## statusSingle:attractionNotInterested -73.89 -45.03</code></pre>
<pre class="r"><code>anova(m0.mlr, m2.mlr)               # compare baseline with final model</code></pre>
<pre><code>## Analysis of Variance Table
## 
## Model 1: money ~ 1
## Model 2: money ~ (status + attraction)^2
##   Res.Df    RSS Df Sum of Sq   F              Pr(&gt;F)    
## 1     97 213227                                         
## 2     94  30411  3    182816 188 &lt;0.0000000000000002 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<pre class="r"><code>Anova(m0.mlr, m2.mlr, type = &quot;III&quot;) # compare baseline with final model</code></pre>
<pre><code>## Anova Table (Type III tests)
## 
## Response: money
##             Sum Sq Df F value              Pr(&gt;F)    
## (Intercept) 760953  1     346 &lt;0.0000000000000002 ***
## Residuals    30411 94                                
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
</div>
<div id="additional-model-diagnostics" class="section level2">
<h2><span class="header-section-number">1.5</span> Additional Model Diagnostics</h2>
<p>After rerunning the regression analysis on the updated data set, we again cerate diagnostic plots in order to check whether there are potentially problematic data points.</p>
<pre class="r"><code># start plotting
par(mfrow = c(2, 2))           # display plots in 2 rows and 2 columns
plot(m2.mlr)                   # plot fitted values</code></pre>
<p><img src="advancedstatz_files/figure-html/unnamed-chunk-17-1.png" width="672" /></p>
<pre class="r"><code>par(mfrow = c(1, 1))           # restore original 1 plot per row and column settings</code></pre>
<pre class="r"><code># determine a cutoff for data points that have D-values higher than 4/(n-k-1) 
cutoff &lt;- 4/((nrow(mlrdata)-length(m2.mlr$coefficients)-2))
# start plotting
par(mfrow = c(1, 2))           # display plots in 1 row and 2 columns
qqPlot(m2.mlr, main=&quot;QQ Plot&quot;) # cerate qq-plot</code></pre>
<pre><code>## 84 88 
## 82 86</code></pre>
<pre class="r"><code>plot(m2.mlr, which=4, cook.levels = cutoff) # plot cook*s distance</code></pre>
<p><img src="advancedstatz_files/figure-html/unnamed-chunk-18-1.png" width="672" /></p>
<pre class="r"><code>par(mfrow = c(1, 1))           # restore original 1 plot per row and column settings</code></pre>
<p>Although the diagnostic plots indicate that additional points may be problematic, but these data points deviate substantially less from the trend than was the case with the data points that have already been removed. To make sure that retaining the data points that are deemed potentially problematic by the diagnostic plots, is acceptable, we extract diagnostic statistics and add them to the data.</p>
<pre class="r"><code># addieren von modelldiagnostiken zum datasatz
mlrdata$residuals &lt;- resid(m2.mlr)
mlrdata$standardized.residuals &lt;- rstandard(m2.mlr)
mlrdata$studentized.residuals &lt;- rstudent(m2.mlr)
mlrdata$cooks.distance &lt;- cooks.distance(m2.mlr)
mlrdata$dffit &lt;- dffits(m2.mlr)
mlrdata$leverage &lt;- hatvalues(m2.mlr)
mlrdata$covariance.ratios &lt;- covratio(m2.mlr)
mlrdata$fitted &lt;- m2.mlr$fitted.values</code></pre>
<p>We can now use these diagnostic statistics to create more precise diagnostic plots.</p>
<pre class="r"><code># plot 5
p5 &lt;- ggplot(mlrdata, 
             aes(studentized.residuals)) +
  theme(legend.position = &quot;none&quot;) +
  theme_set(theme_bw(base_size = 8))+    # define theme (black and white theme)
  geom_histogram(aes(y=..density..),
                 binwidth = 1,
                 colour=&quot;black&quot;,
                 fill=&quot;white&quot;) +
  labs(x = &quot;Studentized Residual&quot;, y = &quot;Density&quot;) + 
  stat_function(fun = dnorm, 
                args = list(mean = mean(mlrdata$studentized.residuals, na.rm = TRUE), 
                            sd = sd(mlrdata$studentized.residuals, na.rm = TRUE)), 
                colour = &quot;red&quot;, size = 1)
# plot 6
p6 &lt;- ggplot(mlrdata, aes(fitted, studentized.residuals)) +
  geom_point() + 
  geom_smooth(method = &quot;lm&quot;, colour = &quot;Red&quot;)+
    theme_set(theme_bw(base_size = 8))+    # define theme (black and white theme)
  labs(x = &quot;Fitted Values&quot;, 
       y = &quot;Studentized Residual&quot;)
# plot 7
p7 &lt;- qplot(sample = mlrdata$studentized.residuals, stat=&quot;qq&quot;) + 
    theme_set(theme_bw(base_size = 8))+    # define theme (black and white theme)
  labs(x = &quot;Theoretical Values&quot;, 
       y = &quot;Observed Values&quot;)
multiplot(p5, p6, p7, cols = 3)</code></pre>
<p><img src="advancedstatz_files/figure-html/unnamed-chunk-20-1.png" width="672" /></p>
<p>The new diagnostic plots do not indicate outliers that require removal. With respect to such data points the following parameters should be considered:</p>
<ul>
<li><p>Data points with standardised residuals &gt; 3.29 should be removed <span class="citation">(A. Field, Miles, and Field 2012, 269)</span></p></li>
<li><p>If more than 1 percent of data points have standardized residuals exceeding values &gt; 2.58, then the error rate of the model is inacceptable <span class="citation">(A. Field, Miles, and Field 2012, 269)</span>.</p></li>
<li><p>If more than 5 percent of data points have standardized residuals exceeding values &gt; 1.96, then the error rate of the model is inacceptable <span class="citation">(A. Field, Miles, and Field 2012, 269)</span></p></li>
<li><p>In addition, data points with Cookâs D-values &gt; 1 should be removed <span class="citation">(A. Field, Miles, and Field 2012, 269)</span></p></li>
<li><p>Also, data points with leverage values <span class="math inline">\(3(k + 1)/n\)</span> (k = Number of predictors, N = Number of cases in model) should be removed <span class="citation">(A. Field, Miles, and Field 2012, 270)</span></p></li>
<li><p>There should not be (any) autocorrelation among predictors. This means that independent variables cannot be correlated with itself (for instance, because data points come from the same subject). If there is autocorrelation among predictors, then a Repeated Measures Design or a (hierarchical) mixed-effects model should be implemented instead.</p></li>
<li><p>Predictors cannot substantially correlate with each other (multicollinearity). If a model contains predictors that have variance inflation factors (VIF) &gt; 10 the model is completely unreliable <span class="citation">(Myers 1990)</span> and predictors causing such VIFs should be removed. Indeed, even VIFs of 2.5 can be problematic <span class="citation">(Szmrecsanyi 2006, 215)</span> and <span class="citation">(Zuur, Ieno, and Elphick 2010)</span> proposes that variables with VIFs exceeding 3 should be removed!</p></li>
<li><p>Data points with 1/VIF values <span class="math inline">\(&lt;\)</span> .1 must be removed (data points with values above .2 are considered problematic) <span class="citation">(Menard 1995)</span>.</p></li>
<li><p>The mean value of VIFs should be <span class="math inline">\(&lt;\)</span> 1 <span class="citation">(Bowerman and OâConnell 1990)</span>.</p></li>
</ul>
<pre class="r"><code># 1: optimal = 0
# (aufgelistete datenpunkte sollten entfernt werden)
which(mlrdata$standardized.residuals &gt; 3.29)</code></pre>
<pre><code>## integer(0)</code></pre>
<pre class="r"><code># 2: optimal = 1
# (listed data points should be removed)
stdres_258 &lt;- as.vector(sapply(mlrdata$standardized.residuals, function(x) {
ifelse(sqrt((x^2)) &gt; 2.58, 1, 0) } ))
(sum(stdres_258) / length(stdres_258)) * 100</code></pre>
<pre><code>## [1] 0</code></pre>
<pre class="r"><code># 3: optimal = 5
# (listed data points should be removed)
stdres_196 &lt;- as.vector(sapply(mlrdata$standardized.residuals, function(x) {
ifelse(sqrt((x^2)) &gt; 1.96, 1, 0) } ))
(sum(stdres_196) / length(stdres_196)) * 100</code></pre>
<pre><code>## [1] 6.122</code></pre>
<pre class="r"><code># 4: optimal = 0
# (listed data points should be removed)
which(mlrdata$cooks.distance &gt; 1)</code></pre>
<pre><code>## integer(0)</code></pre>
<pre class="r"><code># 5: optimal = 0
# (data points should be removed if cooks distance is close to 1)
which(mlrdata$leverage &gt;= (3*mean(mlrdata$leverage)))</code></pre>
<pre><code>## integer(0)</code></pre>
<pre class="r"><code># 6: checking autocorrelation:
# Durbin-Watson test (optimal: grosser p-wert)
dwt(m2.mlr)</code></pre>
<pre><code>##  lag Autocorrelation D-W Statistic p-value
##    1        -0.01433         1.968   0.644
##  Alternative hypothesis: rho != 0</code></pre>
<pre class="r"><code># 7: test multicolliniarity 1
vif(m2.mlr)</code></pre>
<pre><code>##            status        attraction status:attraction 
##              2.00              1.96              2.96</code></pre>
<pre class="r"><code># 8: test multicolliniarity 2
1/vif(m2.mlr)</code></pre>
<pre><code>##            status        attraction status:attraction 
##            0.5000            0.5102            0.3378</code></pre>
<pre class="r"><code># 9: mean vif should not exceed 1 
mean(vif(m2.mlr))</code></pre>
<pre><code>## [1] 2.307</code></pre>
<p>Except for the mean VIF value (2.307) which should not exceed 1, all diagnostics are acceptable. We will now test whether the sample size is sufficient for our model. With respect to the minimal sample size and based on <span class="citation">(Green 1991)</span>, <span class="citation">(A. Field, Miles, and Field 2012, 273â74)</span> offer the following rules of thumb (k = number of predictors; categorical predictors with more than two levels should be recoded as dummy variables):</p>
</div>
<div id="evaluation-of-sample-size" class="section level2">
<h2><span class="header-section-number">1.6</span> Evaluation of Sample Size</h2>
<p>After performing the diagnostics, we will now test whether the sample size is adequate and what the values of <code>R</code> would be based on a random distribution in order to be able to estimate how likely a <span class="math inline">\(\beta\)</span>-error is given the present sample size <span class="citation">(cf. A. Field, Miles, and Field 2012, 274)</span>. Beta errors (or <span class="math inline">\(\beta\)</span>-errors) refer to the erroneous assumption that a predictor is not significant (based on the analysis and given the sample) although it does have an effect in the population. In other words, <span class="math inline">\(\beta\)</span>-error means to overlook a significant effect because of weaknesses of the analysis. The test statistics ranges between 0 and 1 where lower values are better. If the values approximate 1, then there is serious concern as the model is not reliable given the sample size. In such cases, unfortunately, the best option is to increase the sample size.</p>
<pre class="r"><code>smplesz(m2.mlr) # check if sample size is sufficient</code></pre>
<pre><code>## [1] &quot;Sample too small: please increase your sample by  9  data points&quot;</code></pre>
<pre class="r"><code>expR(m2.mlr)    # check for beta-error likelihood </code></pre>
<pre><code>## [1] &quot;Based on the sample size expect a false positive correlation of 0.0309 between the predictors and the predicted&quot;</code></pre>
<p>The function <code>smplesz</code> reports that the sample size is insufficient by 9 data points according to <span class="citation">(Green 1991)</span>. The likelihood of <span class="math inline">\(\beta\)</span>-errors, however, is very small (0.0309). As a last step, we summarize the results of the regression analysis.</p>
<pre class="r"><code>mlr.summary(m2.mlr, m2.glm, ia = T)  # tabulate regression results</code></pre>
<pre><code>##                                      Estimate  VIF CI(2.5%) CI(97.5%)
## (Intercept)                             99.15          92.1    106.21
## statusSingle                            55.85    2    45.78     65.93
## attractionNotInterested                -47.66 1.96   -57.63    -37.69
## statusSingle:attractionNotInterested   -59.46 2.96   -73.71    -45.21
## Model statistics                                                     
## Number of cases in model                                             
## Residual Standard Error on 94 DF                                     
## Multiple R2                                                          
## Adjusted R2                                                          
## AIC                                                                  
## BIC                                                                  
## F-statistic                                                          
##                                               Std. Error      t value
## (Intercept)                                          3.6        27.56
## statusSingle                                        5.14        10.87
## attractionNotInterested                             5.09        -9.37
## statusSingle:attractionNotInterested                7.27        -8.18
## Model statistics                                                     
## Number of cases in model                                             
## Residual Standard Error on 94 DF                                     
## Multiple R2                                                          
## Adjusted R2                                                          
## AIC                                                                  
## BIC                                                                  
## F-statistic                          F-statistic: 188.36 DF: 3 and 94
##                                        Pr(&gt;|t|) Significance
## (Intercept)                                   0  p &lt; .001***
## statusSingle                                  0  p &lt; .001***
## attractionNotInterested                       0  p &lt; .001***
## statusSingle:attractionNotInterested          0  p &lt; .001***
## Model statistics                                       Value
## Number of cases in model                                  98
## Residual Standard Error on 94 DF                       17.99
## Multiple R2                                            0.857
## Adjusted R2                                            0.853
## AIC                                                    850.4
## BIC                                                   863.32
## F-statistic                          p-value: 0  p &lt; .001***</code></pre>
<p>(Falls signifikante Interaktionen vorliegen, sollten die Haupteffekte der PrÃ¤dikatoren, die an der/n Interaktion/en beteiligt sind, nicht interpretiert werden. Sie werden hier dennoch interpretiert, um zu verdeutlichen, wie die Ergebnisse einer multiplen linearen Regression verschriftlicht werden kÃ¶nnen.)</p>
<p>The results of the regression analysis can be summarized as follows:</p>
<p>A multiple linear regression was fitted to the data in a step-wise step-down, AIC-based (Akaikeâs Information Criterion) procedure to the data and arrived at a final minimal model. During the model diagnostics, two outliers were detected and removed. Further diagnostics did not find other issues after the removal. The final minimal adequate regression model is based on 98 data points and performs highly significantly better than a minimal baseline model (Multiple R<sup>2</sup>: .857, Adjusted R<sup>2</sup>: .853, F-statistic (3, 94): 154.4, AIC: 850.4, BIC: 863.32, p&lt;.001<span class="math inline">\(***\)</span>). The final minimal adequate regression model reports <em>attraction</em> and <em>status</em> as significant main effects. The relationship status of women correlates highly significantly and positively with the amount of money spend on the womenâs presents (SE: 5.14, t-value: 10.87, p&lt;.001<span class="math inline">\(***\)</span>). This shows that men spend 156.8 dollars on presents are single while they spend 99,15 dollars if the women are in a relationship. Whether men are attracted to women also correlates highly significantly and positively with the money they spend on women (SE: 5.09, t-values: -9.37, p&lt;.001<span class="math inline">\(***\)</span>). If men are not interested in women, they spend 47.66 dollar less on a present for women compared with women the men are interested in.</p>
<p>Furthermore, the final minimal adequate regression model reports a highly significant interaction between relationship <em>status</em> and <em>attraction</em> (SE: 7.27, t-value: -8.18, p&lt;.001<span class="math inline">\(***\)</span>): If women are single but man are not interested in them, men spend 59.46 dollars less on their presents compared to all other constellations.</p>
</div>
<div id="exercises" class="section level2">
<h2><span class="header-section-number">1.7</span> Exercises</h2>
<ol style="list-style-type: decimal">
<li>Download the data set called <code>exdatamlr</code> from <code>http://martinschweinberger.de/docs/data/exdatamlr.txt</code> and apply what you have learned by implementing a multiple linear regression model so that you can answer how movement (move) and food intake (food) affect weight (given the data at hand).</li>
</ol>
</div>
</div>
<div id="linear-mixed-effects-regression-models" class="section level1">
<h1><span class="header-section-number">2</span> Linear Mixed-Effects Regression Models </h1>
<p>The following focuses on an extension of ordinary multiple linear regressions: mixed-effects regression linear regression.</p>
<div id="introduction" class="section level2">
<h2><span class="header-section-number">2.1</span> Introduction</h2>
<p>So far, the regression models that we have used only had fixed-effects. having only fixed-effecst measn that all data points are treated as if they are completely independent and thus on the same hierarchical level. However, it is very common, that the data is nested in the sense that data points are not independent because they are, for instance produced by the same speaker or are grouped by some other characteristsi. In such cases, the data is consiederd hierarchical and statistical models should incororate such structiral features of the data they work upon. With respect to regression modelling, hierarchical structures are incorporated by what is called <em>random effects</em>. When models only have a fixed-effects structure, then they make use of only a single intercept and/or slope (as in the left panel in the figure below), while mixed effects models have intercepts for each level of a random effect. If the randon effect structure represents speakers then this would mean that a mixed-model would have a separate intercept and or slope for each speaker.</p>
<p><img src="advancedstatz_files/figure-html/unnamed-chunk-24-1.png" width="672" /></p>
<p><em>Random Effects</em> have two parameters: the intercept (the point where the regression line cross the y-axis) and the slope (the acclivity of the regression line). In contrast to fixed-effects models have only 1 intercept and one slope (left panel of the Figure above) while mixed-effects models can have various <em>random intercepts</em> (center left panel ) or various <em>random slopes</em> (center right panel ), or both, various <em>random intercepts</em> and various <em>random slopes</em> (right panel ). In the follwoing, we will onyl focus on models with random interecpts becasue this is the by far more common method and because including both random incetrcepts and random slopes requires huge amounts of data. Consider the Figure below to understand what is meant by ârandom interceptsâ.</p>
<p><img src="advancedstatz_files/figure-html/unnamed-chunk-25-1.png" width="672" /></p>
<p>The left panel merely shows the data while the center panel includes the regression line for a regression that estimates Weight basedon Height. The right panel shows the regression line and, in addition, random incercepts each each of the three groups.</p>
<p>After adding random intercepts, predictors (or fixed effects) are added to the model (just like with multiple rgeression). So mixed-effects are called mixed-effects because they contain both random and fixed effects.</p>
<p>In terms of general procedure, random effects are added first, and only after we have ascertained that including random effects is warranted, we test whether including fixed-effects is warranted <span class="citation">(vgl. A. Field, Miles, and Field 2012)</span>. We test whteher including random effects is warranted by comparing a model, that bases its estiamtes of the dependend variable solely on the base intercept (the mean), with a model, that bases its estiamtes of the dependend variable solely on the intercepts of the random effect. If the random-effect model explains significantly more variance than the simple model without random effect structure, then we continue with the mixed-effects model. In other words, including random effects is justified if they reduce residual deviance.</p>
</div>
<div id="example-preposition-use-across-time-by-genre" class="section level2">
<h2><span class="header-section-number">2.2</span> Example: Preposition Use across Time by Genre</h2>
<p>To explore how to implement a mixed-effects model in <code>R</code> we revisit the preposition data that contains relative frequencies of prepositions in English texts written between 1150 and 1913. As a first step, and to prepare our analysis, we load neccessary <code>R</code> packages, specify oprions, and load as well as provide an overview of the data.</p>
<pre class="r"><code># activate packages
library(RLRsim)
#library(car)
#library(QuantPsyc)
#library(boot)
library(nlme)
library(lme4)
#library(ez)
library(ggplot2)
# set options
options(&quot;scipen&quot; = 100, &quot;digits&quot; = 4)      # supress scientific notation
options(stringsAsFactors = F)              # do not convert strings into factors
mydata &lt;- read.delim(&quot;data/lmemdata.txt&quot;, header = TRUE) # read in data
mydata$date &lt;- as.numeric(mydata$date)     # convert date into a numeric variable
head(mydata); nrow(mydata)                 # inspect updated data set</code></pre>
<pre><code>##   date         genre    text  pptw region
## 1 1736 SCIENCE_OTHER   albin 166.0  north
## 2 1711 EDUC_TREATISE    anon 139.9  north
## 3 1808  LETTERS_PRIV  austen 130.8  north
## 4 1878 EDUC_TREATISE    bain 151.3  north
## 5 1743 EDUC_TREATISE barclay 145.7  north
## 6 1908 EDUC_TREATISE  benson 120.8  north</code></pre>
<pre><code>## [1] 537</code></pre>
<p>The data set contains the date when the text was written (<code>date</code>), the genre of the text (<code>genre</code>), the name of the text (<code>text</code>), the relative frequency of prepositions in the text (<code>pptw</code>), and the region in which the text was written (<code>region</code>). We now plot the data to get a first impression of its structure.</p>
<pre class="r"><code># visualize variables (2 plots per row)
# 3 plots in 1 window
def.par &lt;- par(no.readonly = TRUE)
nf &lt;- layout(matrix(c(1, 1, 2, 3), 2, 2, byrow = T))
plot(mydata$pptw ~ mydata$date, ylab = &quot;Frequency&quot;, xlab = &quot;year of publication&quot;)
abline(lm(mydata$pptw ~ mydata$date), lty = 3, lwd = 2, col = &quot;red&quot;)
# re-set margins to fit the labels
par(mar = c(7.2, 4, 1, 2) + 0.1)
# reorder genre by median
genrebymedian &lt;- with(mydata, reorder(genre, -pptw, median))
#   generate plots
plot(mydata$pptw ~ genrebymedian,
  col = &quot;lightgrey&quot;,
  ylab = &quot;Frequency&quot;,
  xlab = &quot;&quot;,
  las = 2,
  cex.axis = .7,
  cex = .5)
# re-set margins
par(mar = c(5, 4, 1, 2) + 0.1)
x = mydata$pptw
h = hist(mydata$pptw,
    ylim =c(0, 150),
    xlim = c(50, 200),
    xlab = &quot;prepositions per text&quot;,
    col = &quot;lightgrey&quot;,
    main = &quot;&quot;)
xfit &lt;- seq(min(mydata$pptw), max(mydata$pptw), length = 40)
yfit &lt;- dnorm(xfit, mean = mean(mydata$pptw),sd = sd(mydata$pptw))
yfit &lt;- yfit*diff(h$mids[1:2])*length(x)
lines(xfit, yfit, lty = 2, lwd=2)</code></pre>
<p><img src="advancedstatz_files/figure-html/unnamed-chunk-27-1.png" width="672" /></p>
<pre class="r"><code># restore original graphic&#39;s parameters
par(def.par)</code></pre>
<p>The scatter plot in the upper panel indicates that the use of prepositions has moderattely increased over time while the boxplots in the lower left panel show show that the gernres differ quite substantailly with respect to their median frequencies of preositions per text. Finally, the histogram in the lower right panel show that preposition use is distributed normally with a mean of 132.2 prepositions per text.</p>
<pre class="r"><code># plot 8
p8 &lt;- ggplot(mydata, aes(date, pptw)) +
  geom_point() +
  labs(x = &quot;Year&quot;) +
  labs(y = &quot;Prepositions per 1,000 words&quot;) +
  geom_smooth(method = &quot;lm&quot;)  + 
  theme_set(theme_bw(base_size = 10))
# plot 9
p9 &lt;- ggplot(mydata, aes(region, pptw)) +
  geom_boxplot() +
  labs(x = &quot;Region&quot;) +
  labs(y = &quot;Prepositions per 1,000 words&quot;) +
  geom_smooth(method = &quot;lm&quot;) # with linear model smoothing!
# include genre (lowess)
multiplot(p8, p9, cols = 2)</code></pre>
<p><img src="advancedstatz_files/figure-html/unnamed-chunk-28-1.png" width="672" /></p>
<pre class="r"><code>ggplot(mydata, aes(date, pptw)) +
  geom_point() +
  facet_wrap(~ genre, nrow = 4) +
  geom_smooth(method = &quot;lm&quot;) +
  theme_bw() +
  labs(x = &quot;Year&quot;) +
  labs(y = &quot;Prepositions per 1,000 words&quot;) +
  coord_cartesian(ylim = c(0, 220))</code></pre>
<p><img src="advancedstatz_files/figure-html/unnamed-chunk-29-1.png" width="672" /></p>
<p>Centering or scaling numeric variables is useful for later interpretation of regression models: if the date variable was not centered, the regression would show the effects of variables at year 0(!). If numeric variables are scaled, other variables are variables are considered relative not to 0 but to the mean of that variable (in this case the mean of years in our data). Centering simply means that the mean of the numeric vairbale is subtracted from each value.</p>
<pre><code>##     date         genre    text  pptw region
## 1 109.87 SCIENCE_OTHER   albin 166.0  north
## 2  84.87 EDUC_TREATISE    anon 139.9  north
## 3 181.87  LETTERS_PRIV  austen 130.8  north
## 4 251.87 EDUC_TREATISE    bain 151.3  north
## 5 116.87 EDUC_TREATISE barclay 145.7  north
## 6 281.87 EDUC_TREATISE  benson 120.8  north</code></pre>
<pre><code>## &#39;data.frame&#39;:    537 obs. of  5 variables:
##  $ date  : num [1:537, 1] 109.9 84.9 181.9 251.9 116.9 ...
##   ..- attr(*, &quot;scaled:center&quot;)= num 1626
##  $ genre : chr  &quot;SCIENCE_OTHER&quot; &quot;EDUC_TREATISE&quot; &quot;LETTERS_PRIV&quot; &quot;EDUC_TREATISE&quot; ...
##  $ text  : chr  &quot;albin&quot; &quot;anon&quot; &quot;austen&quot; &quot;bain&quot; ...
##  $ pptw  : num  166 140 131 151 146 ...
##  $ region: chr  &quot;north&quot; &quot;north&quot; &quot;north&quot; &quot;north&quot; ...</code></pre>
</div>
<div id="testing-random-effects" class="section level2">
<h2><span class="header-section-number">2.3</span> Testing Random Effects</h2>
<pre><code>## [[1]]
## [1] 220.9
## 
## [[2]]
## [1] 0.000000000000000000000000000000000000000000000001082</code></pre>
<pre><code>## Data: mydata
## Models:
## m0.lmer1: pptw ~ (1 | genre) + 1
## m0.lmer2: pptw ~ (1 | region) + 1
## m0.lmer3: pptw ~ (1 | genre/region) + 1
##          Df  AIC  BIC logLik deviance Chisq Chi Df          Pr(&gt;Chisq)    
## m0.lmer1  3 4502 4515  -2248     4496                                     
## m0.lmer2  3 4719 4731  -2356     4713     0      0                   1    
## m0.lmer3  4 4501 4518  -2246     4493   220      1 &lt;0.0000000000000002 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<pre><code>## 
##  simulated finite sample distribution of RLRT.
##  
##  (p-value based on 10000 simulated values)
## 
## data:  
## RLRT = 220, p-value &lt;0.0000000000000002</code></pre>
<pre><code>##        Model df  AIC  BIC logLik   Test L.Ratio p-value
## m0.lme     1  3 4502 4515  -2248                       
## m1.lme     2  5 4496 4517  -2243 1 vs 2   10.12  0.0063</code></pre>
<pre><code>## Linear mixed-effects model fit by maximum likelihood
##  Data: mydata 
##    AIC  BIC logLik
##   4496 4517  -2243
## 
## Random effects:
##  Formula: ~1 | genre
##         (Intercept)
## StdDev:       12.05
## 
##  Formula: ~1 | region %in% genre
##         (Intercept) Residual
## StdDev:       3.453    14.97
## 
## Fixed effects: pptw ~ date 
##              Value Std.Error  DF t-value p-value
## (Intercept) 133.95     3.183 505   42.09  0.0000
## date          0.02     0.007 505    2.70  0.0071
##  Correlation: 
##      (Intr)
## date 0.003 
## 
## Standardized Within-Group Residuals:
##      Min       Q1      Med       Q3      Max 
## -3.74687 -0.66308  0.01827  0.64043  3.62269 
## 
## Number of Observations: 537
## Number of Groups: 
##             genre region %in% genre 
##                16                31</code></pre>
<pre><code>##             numDF denDF F-value p-value
## (Intercept)     1   505  1770.7  &lt;.0001
## date            1   505     7.3  0.0071</code></pre>
<pre><code>## Data: mydata
## Models:
## m0.lmer: pptw ~ (1 | genre/region) + 1
## m1.lmer: pptw ~ (1 | genre/region) + date
##         Df  AIC  BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq)   
## m0.lmer  4 4501 4518  -2246     4493                           
## m1.lmer  5 4496 4517  -2243     4486  7.03      1      0.008 **
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<pre><code>## Approximate 95% confidence intervals
## 
##  Fixed effects:
##                  lower      est.     upper
## (Intercept) 127.713505 133.95499 140.19647
## date          0.004896   0.01787   0.03083
## attr(,&quot;label&quot;)
## [1] &quot;Fixed effects:&quot;
## 
##  Random Effects:
##   Level: genre 
##                 lower  est. upper
## sd((Intercept))  8.19 12.05 17.73
##   Level: region 
##                 lower  est. upper
## sd((Intercept)) 1.172 3.453 10.17
## 
##  Within-group standard error:
## lower  est. upper 
## 14.07 14.97 15.93</code></pre>
</div>
<div id="model-diagnostics" class="section level2">
<h2><span class="header-section-number">2.4</span> Model Diagnostics</h2>
<p>diagnostic plot: examining residuals (Pinheiro &amp; Bates 2000:175)</p>
<p><img src="advancedstatz_files/figure-html/unnamed-chunk-33-1.png" width="672" /></p>
<p>The plot shows that there are some outliers (points outside the boxes) and that the variability within letters is greater than in other genres we therefore examine the genres in isolation standardized residuals versus fitted values</p>
<p><img src="advancedstatz_files/figure-html/unnamed-chunk-34-1.png" width="672" /></p>
<p>The plot showing the standardized residuals versus fitted values confirms that there are outliers in the letters because there are obviously differences in the variance, we create a new model which uses weights to compensate variance heterogeneiety of variance (cf.Â Pinheiro &amp; Bates 2000:177)</p>
<pre><code>##        Model df  AIC  BIC logLik   Test L.Ratio p-value
## m1.lme     1  5 4496 4517  -2243                       
## m2.lme     2 20 4483 4569  -2222 1 vs 2   42.75  0.0002</code></pre>
<pre><code>## Linear mixed-effects model fit by maximum likelihood
##  Data: mydata 
##    AIC  BIC logLik
##   4483 4569  -2222
## 
## Random effects:
##  Formula: ~1 | genre
##         (Intercept)
## StdDev:       12.14
## 
##  Formula: ~1 | region %in% genre
##         (Intercept) Residual
## StdDev:       4.183    13.89
## 
## Variance function:
##  Structure: Different standard deviations per stratum
##  Formula: ~1 | genre 
##  Parameter estimates:
##             BIBLE   BIOGRAPHY_OTHER        DIARY_PRIV     EDUC_TREATISE 
##            1.0000            0.3582            0.8994            0.7324 
##           FICTION    HANDBOOK_OTHER           HISTORY               LAW 
##            0.8895            1.1417            1.0185            0.7591 
##  LETTERS_NON-PRIV      LETTERS_PRIV        PHILOSOPHY PROCEEDINGS_TRIAL 
##            1.2641            1.2302            0.7753            1.2262 
##    RELIG_TREATISE     SCIENCE_OTHER            SERMON        TRAVELOGUE 
##            1.0108            0.8293            0.9821            1.0663 
## Fixed effects: pptw ~ date 
##              Value Std.Error  DF t-value p-value
## (Intercept) 134.01     3.209 505   41.76  0.0000
## date          0.02     0.006 505    3.36  0.0008
##  Correlation: 
##      (Intr)
## date 0.002 
## 
## Standardized Within-Group Residuals:
##      Min       Q1      Med       Q3      Max 
## -3.29018 -0.67307  0.03261  0.64633  3.08450 
## 
## Number of Observations: 537
## Number of Groups: 
##             genre region %in% genre 
##                16                31</code></pre>
<pre><code>##             numDF denDF F-value p-value
## (Intercept)     1   505  1743.6  &lt;.0001
## date            1   505    11.3  0.0008</code></pre>
<pre><code>##        Model df  AIC  BIC logLik   Test L.Ratio p-value
## m0.lme     1  3 4502 4515  -2248                       
## m2.lme     2 20 4483 4569  -2222 1 vs 2   52.87  &lt;.0001</code></pre>
<pre><code>## Approximate 95% confidence intervals
## 
##  Fixed effects:
##                  lower      est.     upper
## (Intercept) 127.719320 134.01179 140.30426
## date          0.008414   0.02022   0.03203
## attr(,&quot;label&quot;)
## [1] &quot;Fixed effects:&quot;
## 
##  Random Effects:
##   Level: genre 
##                 lower  est. upper
## sd((Intercept)) 8.253 12.14 17.87
##   Level: region 
##                 lower  est. upper
## sd((Intercept)) 2.092 4.183 8.363
## 
##  Variance function:
##                    lower   est.  upper
## BIOGRAPHY_OTHER   0.2233 0.3582 0.5747
## DIARY_PRIV        0.6532 0.8994 1.2385
## EDUC_TREATISE     0.5210 0.7324 1.0295
## FICTION           0.6426 0.8895 1.2312
## HANDBOOK_OTHER    0.8170 1.1417 1.5955
## HISTORY           0.7583 1.0185 1.3682
## LAW               0.5499 0.7591 1.0480
## LETTERS_NON-PRIV  0.9974 1.2641 1.6020
## LETTERS_PRIV      0.9907 1.2302 1.5276
## PHILOSOPHY        0.4907 0.7753 1.2250
## PROCEEDINGS_TRIAL 0.8344 1.2262 1.8019
## RELIG_TREATISE    0.6776 1.0108 1.5078
## SCIENCE_OTHER     0.5702 0.8293 1.2063
## SERMON            0.7351 0.9821 1.3121
## TRAVELOGUE        0.7574 1.0663 1.5012
## attr(,&quot;label&quot;)
## [1] &quot;Variance function:&quot;
## 
##  Within-group standard error:
## lower  est. upper 
## 11.62 13.89 16.61</code></pre>
</div>
<div id="effect-sizes" class="section level2">
<h2><span class="header-section-number">2.5</span> Effect Sizes</h2>
<p>Extract effect sizes (in the example: the effect size of date) calculate effect size (this effect size measure works for all fixed effects)to calculate the effect size, take the square root of the t-value squared divided by the t-value squared plus the degrees of freedom: r = sqrt(t<sup>2/(t</sup>2+df)) WARNING: only apply this function to main effecst not involved in interactions or higher level interactions but not to the main effects involved in interactions as they are meaningless (cf.Â Field, Miles &amp; Field 2012:641)</p>
<pre><code>## [1] &quot;Pearson&#39;s r =  0.148&quot;</code></pre>
<p>Set up m1 model but using the lmer function from the lme4 package</p>
<p>How to calculate the variance explained when only a simple random effect is involved:</p>
<pre><code>## Linear mixed model fit by maximum likelihood  [&#39;lmerMod&#39;]
## Formula: pptw ~ (1 | genre/region) + date
##    Data: mydata
## 
##      AIC      BIC   logLik deviance df.resid 
##     4496     4517    -2243     4486      532 
## 
## Scaled residuals: 
##    Min     1Q Median     3Q    Max 
## -3.747 -0.663  0.018  0.640  3.623 
## 
## Random effects:
##  Groups       Name        Variance Std.Dev.
##  region:genre (Intercept)  11.9     3.45   
##  genre        (Intercept) 145.2    12.05   
##  Residual                 224.1    14.97   
## Number of obs: 537, groups:  region:genre, 31; genre, 16
## 
## Fixed effects:
##             Estimate Std. Error t value
## (Intercept) 133.9550     3.1768   42.17
## date          0.0179     0.0066    2.71
## 
## Correlation of Fixed Effects:
##      (Intr)
## date 0.003</code></pre>
<p>Variance of random effect (145.2) devided by variance of random effect plus residual variance (145.2+224.1) times 100 gives the variance explained by the random effect:(145.2/(145.2+2284.1))*100 # percentage of variance explained by random effect</p>
<p>Craete lmer with complex random effect structure</p>
<p>An alternative for testing if including the random intercepts is permitted.</p>
<p>WARNING: this method is not as good as applying a restricted likelihood ratio test(!) because the p-value is only an approximation IMPORTANT: the second model is a glm object</p>
<pre><code>## [1] 0.00000000000000000000000000000000000000000000000005159</code></pre>
</div>
<div id="rerun-model-diagnostics" class="section level2">
<h2><span class="header-section-number">2.6</span> Rerun Model Diagnostics</h2>
<p>Diagnostic plot (Pinheiro &amp; Bates 2000:11, 182) what we wish to see: a cloud of dots in the middle of the window without structure what we do not want to see: a funnel-shaped cloud because this indicates an increase of the errors/residuals with an increase of the predictor(s) (because this would indicate heteroscedasticity) in short: observed valuesagainst fitted values (cf.Â Pinheiro &amp; Bates 2000:182)</p>
<pre class="r"><code># start plotting
par(mfrow = c(2, 2))           # display plots in 2 rows and 2 columns
plot(m2.lme)</code></pre>
<p><img src="advancedstatz_files/figure-html/unnamed-chunk-40-1.png" width="672" /></p>
<pre class="r"><code>par(mfrow = c(1, 1))</code></pre>
<pre class="r"><code># diagnostic plot (Pinheiro &amp; Bates 2000:21)
plot(m2.lme, form = resid(., type = &quot;p&quot;) ~ fitted(.) | genre, abline = 0, cex = .5)</code></pre>
<p><img src="advancedstatz_files/figure-html/unnamed-chunk-41-1.png" width="672" /></p>
<pre class="r"><code># diagnostic plot: residuals of fitted values against observed values (cf. Pinheiro &amp; Bates 2000:182)
qqnorm(m2.lme)</code></pre>
<p><img src="advancedstatz_files/figure-html/unnamed-chunk-42-1.png" width="672" /></p>
<pre class="r"><code># normal plot of the estimated date %in% genre random effects
qqnorm(m2.lme, ~ranef(., level = 2), id = 0.05, cex = 0.7, xlim = c(-40, 40))</code></pre>
<p><img src="advancedstatz_files/figure-html/unnamed-chunk-43-1.png" width="672" /></p>
<pre class="r"><code># diagnostic plot: normal plots of the residuals by genre (cf. Pinheiro &amp; Bates 2000:22, 179)
qqnorm(m2.lme, ~resid(.) | genre )</code></pre>
<p><img src="advancedstatz_files/figure-html/unnamed-chunk-44-1.png" width="672" /></p>
<pre class="r"><code># inspect the observed responses versus the within-group fitted values
# (cf. Pinheiro &amp; Bates 2000:178)
plot(m2.lme, pptw ~ fitted(.), id = 0.05, adj = -0.3, xlim = c(80, 220), cex = .8)</code></pre>
<p><img src="advancedstatz_files/figure-html/unnamed-chunk-45-1.png" width="672" /></p>
</div>
<div id="reporting-results" class="section level2">
<h2><span class="header-section-number">2.7</span> Reporting Results</h2>
<pre><code>## Linear mixed model fit by maximum likelihood  [&#39;lmerMod&#39;]
## Formula: pptw ~ (1 | genre/region) + date
##    Data: mydata
## 
##      AIC      BIC   logLik deviance df.resid 
##     4496     4517    -2243     4486      532 
## 
## Scaled residuals: 
##    Min     1Q Median     3Q    Max 
## -3.747 -0.663  0.018  0.640  3.623 
## 
## Random effects:
##  Groups       Name        Variance Std.Dev.
##  region:genre (Intercept)  11.9     3.45   
##  genre        (Intercept) 145.2    12.05   
##  Residual                 224.1    14.97   
## Number of obs: 537, groups:  region:genre, 31; genre, 16
## 
## Fixed effects:
##             Estimate Std. Error t value
## (Intercept) 133.9550     3.1768   42.17
## date          0.0179     0.0066    2.71
## 
## Correlation of Fixed Effects:
##      (Intr)
## date 0.003</code></pre>
</div>
</div>
<div id="multiple-binomial-logistic-regression" class="section level1">
<h1><span class="header-section-number">3</span> Multiple Binomial Logistic Regression</h1>
<pre><code>##   file.speaker.id text.id spk.ref  sex   age ethnicity suf.eh
## 1   &lt;S1A-001#1:M&gt;  S1A001       M male young    Pakeha      0
## 2   &lt;S1A-001#1:M&gt;  S1A001       M male young    Pakeha      1
## 3   &lt;S1A-001#1:M&gt;  S1A001       M male young    Pakeha      0
## 4   &lt;S1A-001#1:M&gt;  S1A001       M male young    Pakeha      0
## 5   &lt;S1A-001#1:M&gt;  S1A001       M male young    Pakeha      1
## 6   &lt;S1A-001#1:M&gt;  S1A001       M male young    Pakeha      1</code></pre>
<pre><code>## &#39;data.frame&#39;:    25821 obs. of  7 variables:
##  $ file.speaker.id: chr  &quot;&lt;S1A-001#1:M&gt;&quot; &quot;&lt;S1A-001#1:M&gt;&quot; &quot;&lt;S1A-001#1:M&gt;&quot; &quot;&lt;S1A-001#1:M&gt;&quot; ...
##  $ text.id        : chr  &quot;S1A001&quot; &quot;S1A001&quot; &quot;S1A001&quot; &quot;S1A001&quot; ...
##  $ spk.ref        : chr  &quot;M&quot; &quot;M&quot; &quot;M&quot; &quot;M&quot; ...
##  $ sex            : Factor w/ 2 levels &quot;female&quot;,&quot;male&quot;: 2 2 2 2 2 2 2 2 2 2 ...
##  $ age            : Factor w/ 2 levels &quot;young&quot;,&quot;old&quot;: 1 1 1 1 1 1 1 1 1 1 ...
##  $ ethnicity      : Factor w/ 2 levels &quot;Pakeha&quot;,&quot;Maori&quot;: 1 1 1 1 1 1 1 1 1 1 ...
##  $ suf.eh         : num  0 1 0 0 1 1 0 0 0 1 ...</code></pre>
<pre><code>##  file.speaker.id      text.id            spk.ref              sex       
##  Length:25821       Length:25821       Length:25821       female:15852  
##  Class :character   Class :character   Class :character   male  : 9969  
##  Mode  :character   Mode  :character   Mode  :character                 
##                                                                         
##                                                                         
##                                                                         
##     age         ethnicity         suf.eh     
##  young:19150   Pakeha:20024   Min.   :0.000  
##  old  : 6671   Maori : 5797   1st Qu.:0.000  
##                               Median :0.000  
##                               Mean   :0.337  
##                               3rd Qu.:1.000  
##                               Max.   :1.000</code></pre>
<p><img src="advancedstatz_files/figure-html/unnamed-chunk-47-1.png" width="672" /><img src="advancedstatz_files/figure-html/unnamed-chunk-47-2.png" width="672" /></p>
<pre><code>## 
## Call:
## glm(formula = suf.eh ~ 1, family = binomial, data = mydata)
## 
## Deviance Residuals: 
##    Min      1Q  Median      3Q     Max  
## -0.907  -0.907  -0.907   1.474   1.474  
## 
## Coefficients:
##             Estimate Std. Error z value            Pr(&gt;|z|)    
## (Intercept)  -0.6758     0.0132   -51.3 &lt;0.0000000000000002 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 33008  on 25820  degrees of freedom
## Residual deviance: 33008  on 25820  degrees of freedom
## AIC: 33010
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<pre><code>## 
## Call:
## glm(formula = suf.eh ~ age * sex * ethnicity, family = binomial, 
##     data = mydata)
## 
## Deviance Residuals: 
##    Min      1Q  Median      3Q     Max  
## -1.095  -0.913  -0.780   1.283   1.868  
## 
## Coefficients:
##                               Estimate Std. Error z value
## (Intercept)                    -0.6594     0.0207  -31.79
## ageold                         -0.7966     0.0563  -14.15
## sexmale                         0.4150     0.0337   12.32
## ethnicityMaori                  0.0639     0.0555    1.15
## ageold:sexmale                  0.0070     0.0852    0.08
## ageold:ethnicityMaori          -0.1599     0.1025   -1.56
## sexmale:ethnicityMaori         -0.0169     0.0819   -0.21
## ageold:sexmale:ethnicityMaori   0.0709     0.1470    0.48
##                                          Pr(&gt;|z|)    
## (Intercept)                   &lt;0.0000000000000002 ***
## ageold                        &lt;0.0000000000000002 ***
## sexmale                       &lt;0.0000000000000002 ***
## ethnicityMaori                               0.25    
## ageold:sexmale                               0.93    
## ageold:ethnicityMaori                        0.12    
## sexmale:ethnicityMaori                       0.84    
## ageold:sexmale:ethnicityMaori                0.63    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 33008  on 25820  degrees of freedom
## Residual deviance: 32136  on 25813  degrees of freedom
## AIC: 32152
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<pre><code>## Analysis of Deviance Table
## 
## Model 1: suf.eh ~ age * sex * ethnicity
## Model 2: suf.eh ~ age + sex + ethnicity + age:sex + age:ethnicity + sex:ethnicity
##   Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi)
## 1     25813      32136                     
## 2     25814      32136 -1   -0.233     0.63</code></pre>
<pre><code>## 
## Call:
## glm(formula = suf.eh ~ age + sex + ethnicity + age:sex + age:ethnicity + 
##     sex:ethnicity, family = binomial, data = mydata)
## 
## Deviance Residuals: 
##    Min      1Q  Median      3Q     Max  
## -1.099  -0.914  -0.784   1.284   1.861  
## 
## Coefficients:
##                        Estimate Std. Error z value            Pr(&gt;|z|)    
## (Intercept)            -0.65800    0.02053  -32.05 &lt;0.0000000000000002 ***
## ageold                 -0.80705    0.05211  -15.49 &lt;0.0000000000000002 ***
## sexmale                 0.41124    0.03277   12.55 &lt;0.0000000000000002 ***
## ethnicityMaori          0.05377    0.05142    1.05               0.296    
## ageold:sexmale          0.03083    0.06943    0.44               0.657    
## ageold:ethnicityMaori  -0.12538    0.07346   -1.71               0.088 .  
## sexmale:ethnicityMaori  0.00513    0.06800    0.08               0.940    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 33008  on 25820  degrees of freedom
## Residual deviance: 32136  on 25814  degrees of freedom
## AIC: 32150
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<pre><code>## Analysis of Deviance Table
## 
## Model 1: suf.eh ~ age + sex + ethnicity + age:sex + age:ethnicity + sex:ethnicity
## Model 2: suf.eh ~ age + sex + ethnicity + age:sex + age:ethnicity
##   Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi)
## 1     25814      32136                     
## 2     25815      32136 -1 -0.00569     0.94</code></pre>
<pre><code>## 
## Call:
## glm(formula = suf.eh ~ age + sex + ethnicity + age:sex + age:ethnicity, 
##     family = binomial, data = mydata)
## 
## Deviance Residuals: 
##    Min      1Q  Median      3Q     Max  
## -1.098  -0.913  -0.784   1.284   1.860  
## 
## Coefficients:
##                       Estimate Std. Error z value            Pr(&gt;|z|)    
## (Intercept)            -0.6583     0.0201  -32.82 &lt;0.0000000000000002 ***
## ageold                 -0.8077     0.0515  -15.69 &lt;0.0000000000000002 ***
## sexmale                 0.4121     0.0307   13.43 &lt;0.0000000000000002 ***
## ethnicityMaori          0.0561     0.0408    1.38               0.169    
## ageold:sexmale          0.0321     0.0674    0.48               0.634    
## ageold:ethnicityMaori  -0.1252     0.0734   -1.71               0.088 .  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 33008  on 25820  degrees of freedom
## Residual deviance: 32136  on 25815  degrees of freedom
## AIC: 32148
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<pre><code>## Analysis of Deviance Table
## 
## Model 1: suf.eh ~ age + sex + ethnicity + age:sex + age:ethnicity
## Model 2: suf.eh ~ age + sex + ethnicity + age:ethnicity
##   Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi)
## 1     25815      32136                     
## 2     25816      32136 -1   -0.226     0.63</code></pre>
<pre><code>## 
## Call:
## glm(formula = suf.eh ~ age + sex + ethnicity + age:ethnicity, 
##     family = binomial, data = mydata)
## 
## Deviance Residuals: 
##    Min      1Q  Median      3Q     Max  
## -1.099  -0.912  -0.779   1.282   1.854  
## 
## Coefficients:
##                       Estimate Std. Error z value            Pr(&gt;|z|)    
## (Intercept)            -0.6609     0.0194  -34.14 &lt;0.0000000000000002 ***
## ageold                 -0.7937     0.0423  -18.79 &lt;0.0000000000000002 ***
## sexmale                 0.4187     0.0273   15.32 &lt;0.0000000000000002 ***
## ethnicityMaori          0.0555     0.0408    1.36               0.174    
## ageold:ethnicityMaori  -0.1225     0.0732   -1.67               0.094 .  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 33008  on 25820  degrees of freedom
## Residual deviance: 32136  on 25816  degrees of freedom
## AIC: 32146
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<pre><code>## Analysis of Deviance Table
## 
## Model 1: suf.eh ~ age + sex + ethnicity + age:ethnicity
## Model 2: suf.eh ~ age + sex + ethnicity
##   Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi)  
## 1     25816      32136                       
## 2     25817      32139 -1    -2.81    0.094 .
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<pre><code>## 
## Call:
## glm(formula = suf.eh ~ age + sex + ethnicity, family = binomial, 
##     data = mydata)
## 
## Deviance Residuals: 
##    Min      1Q  Median      3Q     Max  
## -1.086  -0.915  -0.768   1.279   1.840  
## 
## Coefficients:
##                Estimate Std. Error z value            Pr(&gt;|z|)    
## (Intercept)     -0.6549     0.0190  -34.45 &lt;0.0000000000000002 ***
## ageold          -0.8350     0.0346  -24.11 &lt;0.0000000000000002 ***
## sexmale          0.4191     0.0273   15.34 &lt;0.0000000000000002 ***
## ethnicityMaori   0.0173     0.0339    0.51                0.61    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 33008  on 25820  degrees of freedom
## Residual deviance: 32139  on 25817  degrees of freedom
## AIC: 32147
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<pre><code>## Analysis of Deviance Table
## 
## Model 1: suf.eh ~ age + sex + ethnicity
## Model 2: suf.eh ~ age + sex
##   Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi)
## 1     25817      32139                     
## 2     25818      32140 -1   -0.261     0.61</code></pre>
<pre><code>## 
## Call:
## glm(formula = suf.eh ~ age + sex, family = binomial, data = mydata)
## 
## Deviance Residuals: 
##    Min      1Q  Median      3Q     Max  
## -1.081  -0.916  -0.770   1.278   1.837  
## 
## Coefficients:
##             Estimate Std. Error z value            Pr(&gt;|z|)    
## (Intercept)  -0.6525     0.0184   -35.4 &lt;0.0000000000000002 ***
## ageold       -0.8305     0.0335   -24.8 &lt;0.0000000000000002 ***
## sexmale       0.4201     0.0273    15.4 &lt;0.0000000000000002 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 33008  on 25820  degrees of freedom
## Residual deviance: 32140  on 25818  degrees of freedom
## AIC: 32146
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<pre><code>## Logistic Regression Model
##  
##  lrm(formula = suf.eh ~ age + sex, data = mydata, x = T, y = T, 
##      linear.predictors = T)
##  
##                         Model Likelihood    Discrimination    Rank Discrim.    
##                               Ratio Test           Indexes          Indexes    
##  Obs         25821    LR chi2     868.21    R2       0.046    C       0.602    
##   0          17114    d.f.             2    g        0.432    Dxy     0.203    
##   1           8707    Pr(&gt; chi2) &lt;0.0001    gr       1.541    gamma   0.302    
##  max |deriv| 3e-10                          gp       0.091    tau-a   0.091    
##                                             Brier    0.216                     
##  
##            Coef    S.E.   Wald Z Pr(&gt;|Z|)
##  Intercept -0.6525 0.0184 -35.39 &lt;0.0001 
##  age=old   -0.8305 0.0335 -24.78 &lt;0.0001 
##  sex=male   0.4201 0.0273  15.42 &lt;0.0001 
## </code></pre>
<pre><code>##                 Wald Statistics          Response: suf.eh 
## 
##  Factor     Chi-Square d.f. P     
##  age        614.0      1    &lt;.0001
##  sex        237.7      1    &lt;.0001
##  TOTAL      802.6      2    &lt;.0001</code></pre>
<pre><code>## 
##      Backwards Step-down - Original Model
## 
##  Deleted               Chi-Sq d.f. P      Residual d.f. P      AIC  
##  age * sex             0.01   1    0.9346 0.01     1    0.9346 -1.99
##  sex * ethnicity       0.05   1    0.8239 0.06     2    0.9723 -3.94
##  age * sex * ethnicity 0.41   1    0.5230 0.46     3    0.9267 -5.54
##  ethnicity             1.85   1    0.1735 2.32     4    0.6778 -5.68
##  age * ethnicity       1.21   1    0.2711 3.53     5    0.6192 -6.47
## 
## Approximate Estimates after Deleting Factors
## 
##              Coef    S.E. Wald Z P
## Intercept -0.6524 0.01843 -35.39 0
## age=old   -0.8301 0.03353 -24.76 0
## sex=male   0.4199 0.02725  15.41 0
## 
## Factors in Final Model
## 
## [1] age sex</code></pre>
<pre><code>##           index.orig training    test optimism index.corrected   n
## Dxy           0.2032   0.2042  0.2036   0.0006          0.2026 200
## R2            0.0458   0.0462  0.0457   0.0005          0.0454 200
## Intercept     0.0000   0.0000 -0.0022   0.0022         -0.0022 200
## Slope         1.0000   1.0000  0.9960   0.0040          0.9960 200
## Emax          0.0000   0.0000  0.0013   0.0013          0.0013 200
## D             0.0336   0.0338  0.0335   0.0003          0.0332 200
## U            -0.0001  -0.0001  0.0000  -0.0001          0.0000 200
## Q             0.0337   0.0339  0.0335   0.0004          0.0333 200
## B             0.2163   0.2162  0.2163  -0.0001          0.2164 200
## g             0.4323   0.4350  0.4327   0.0023          0.4301 200
## gp            0.0910   0.0914  0.0910   0.0004          0.0906 200
## 
## Factors Retained in Backwards Elimination
## 
##  age sex ethnicity age * sex age * ethnicity sex * ethnicity
##  *   *                                                      
##  *   *                                                      
##  *   *                       *                              
##  *   *                                                      
##  *   *                                                      
##  *   *                                                      
##  *   *                                                      
##  *   *                                                      
##  *   *                       *                              
##  *   *   *                   *                              
##  *   *                                                      
##  *   *                                                      
##  *   *                       *                              
##  *   *                                                      
##  *   *   *         *         *                              
##  *   *                                                      
##  *   *             *                                        
##  *   *                                                      
##  *   *                                                      
##  *   *                                                      
##  *   *                       *                              
##  *   *   *                   *                              
##  *   *                       *               *              
##  *   *             *                                        
##  *   *                                                      
##  *   *                                                      
##  *   *                                                      
##  *   *                       *                              
##  *   *                       *                              
##  *   *                                                      
##  *   *   *                   *                              
##  *   *   *                   *                              
##  *   *                                                      
##  *   *                                                      
##  *   *                                                      
##  *   *             *                                        
##  *   *                                                      
##  *   *                       *                              
##  *   *                                                      
##  *   *                       *                              
##  *   *                                                      
##  *   *                                                      
##  *   *                                       *              
##  *   *                                                      
##  *   *   *         *         *               *              
##  *   *                                                      
##  *   *                                                      
##  *   *   *         *         *               *              
##  *   *                                                      
##  *   *                       *               *              
##  *   *                                                      
##  *   *             *         *                              
##  *   *                                                      
##  *   *                       *                              
##  *   *   *         *         *                              
##  *   *                                       *              
##  *   *                                                      
##  *   *                       *                              
##  *   *                       *                              
##  *   *                       *                              
##  *   *                                                      
##  *   *                                                      
##  *   *                                                      
##  *   *                                                      
##  *   *                                                      
##  *   *                       *                              
##  *   *                                       *              
##  *   *                                                      
##  *   *                                                      
##  *   *   *         *         *               *              
##  *   *                                                      
##  *   *                                                      
##  *   *                       *                              
##  *   *                                                      
##  *   *                                                      
##  *   *                                                      
##  *   *                       *                              
##  *   *                       *                              
##  *   *                                                      
##  *   *                                                      
##  *   *                                                      
##  *   *   *                   *                              
##  *   *   *                   *               *              
##  *   *   *                   *                              
##  *   *                                                      
##  *   *                                       *              
##  *   *             *                                        
##  *   *                       *                              
##  *   *   *                   *                              
##  *   *   *                   *                              
##  *   *                                                      
##  *   *                                                      
##  *   *                       *                              
##  *   *                                                      
##  *   *                       *                              
##  *   *                                                      
##  *   *                                                      
##  *   *                       *                              
##  *   *                       *                              
##  *   *                                                      
##  *   *                                                      
##  *   *                                                      
##  *   *                                                      
##  *   *                                                      
##  *   *                       *                              
##  *   *                                                      
##  *   *                                                      
##  *   *   *                   *                              
##  *   *                                                      
##  *   *                                                      
##  *   *                                                      
##  *   *                                                      
##  *   *                                                      
##  *   *                                                      
##  *   *                                                      
##  *   *                                                      
##  *   *                                                      
##  *   *                                                      
##  *   *                                                      
##  *   *                                                      
##  *   *   *                                                  
##  *   *                                                      
##  *   *                                                      
##  *   *   *                   *                              
##  *   *                       *               *              
##  *   *   *                   *                              
##  *   *   *                   *                              
##  *   *                                                      
##  *   *   *                   *                              
##  *   *                                                      
##  *   *   *                   *                              
##  *   *                                                      
##  *   *                                                      
##  *   *                                                      
##  *   *                                                      
##  *   *                                                      
##  *   *                                                      
##  *   *                                                      
##  *   *                                                      
##  *   *                                                      
##  *   *                                                      
##  *   *                                                      
##  *   *                                                      
##  *   *   *                   *                              
##  *   *             *                                        
##  *   *                                                      
##  *   *                       *                              
##  *   *                                                      
##  *   *                       *                              
##  *   *                       *                              
##  *   *                                                      
##  *   *                                                      
##  *   *                                                      
##  *   *                       *                              
##  *   *                       *                              
##  *   *             *                                        
##  *   *                       *                              
##  *   *                                                      
##  *   *                                                      
##  *   *                                                      
##  *   *                                                      
##  *   *                                                      
##  *   *                       *                              
##  *   *                                       *              
##  *   *                                       *              
##  *   *                                                      
##  *   *                       *                              
##  *   *                                                      
##  *   *   *                   *                              
##  *   *                       *                              
##  *   *                                                      
##  *   *                                                      
##  *   *                                                      
##  *   *                                                      
##  *   *                                                      
##  *   *                                                      
##  *   *                                                      
##  *   *   *         *                                        
##  *   *   *                   *                              
##  *   *   *                   *                              
##  *   *                                                      
##  *   *                                                      
##  *   *   *                   *                              
##  *   *                       *               *              
##  *   *                                                      
##  *   *                                                      
##  *   *             *                                        
##  *   *                                                      
##  *   *                                                      
##  *   *                       *                              
##  *   *                                                      
##  *   *             *                                        
##  *   *                       *                              
##  *   *                                                      
##  *   *             *                                        
##  *   *                                                      
##  *   *                                                      
##  *   *                                                      
##  *   *                                                      
##  *   *   *         *         *               *              
##  age * sex * ethnicity
##                       
##                       
##                       
##                       
##                       
##                       
##                       
##                       
##  *                    
##                       
##                       
##                       
##                       
##                       
##  *                    
##                       
##                       
##                       
##                       
##                       
##                       
##                       
##                       
##  *                    
##                       
##                       
##                       
##  *                    
##                       
##                       
##                       
##                       
##                       
##                       
##                       
##                       
##                       
##                       
##                       
##                       
##                       
##                       
##  *                    
##                       
##  *                    
##                       
##                       
##  *                    
##                       
##                       
##                       
##  *                    
##                       
##  *                    
##                       
##                       
##                       
##  *                    
##                       
##                       
##                       
##                       
##                       
##                       
##                       
##                       
##                       
##                       
##                       
##  *                    
##                       
##                       
##                       
##                       
##                       
##                       
##                       
##                       
##                       
##                       
##                       
##                       
##  *                    
##  *                    
##                       
##                       
##  *                    
##                       
##                       
##                       
##                       
##                       
##                       
##                       
##                       
##                       
##                       
##                       
##                       
##                       
##                       
##                       
##                       
##                       
##                       
##                       
##                       
##                       
##                       
##                       
##                       
##                       
##                       
##                       
##                       
##                       
##                       
##                       
##                       
##                       
##                       
##                       
##                       
##                       
##                       
##                       
##                       
##                       
##                       
##                       
##                       
##                       
##                       
##                       
##                       
##                       
##  *                    
##                       
##                       
##                       
##  *                    
##                       
##                       
##                       
##                       
##                       
##                       
##                       
##  *                    
##  *                    
##                       
##                       
##                       
##                       
##                       
##                       
##                       
##                       
##                       
##                       
##                       
##                       
##                       
##                       
##                       
##                       
##                       
##                       
##                       
##                       
##                       
##                       
##                       
##                       
##                       
##                       
##                       
##  *                    
##                       
##                       
##                       
##  *                    
##  *                    
##                       
##                       
##                       
##  *                    
##                       
##                       
##                       
##                       
##                       
##  *                    
##                       
##                       
##                       
##                       
##                       
##                       
##  *                    
## 
## Frequencies of Numbers of Factors Retained
## 
##   2   3   4   5   6   7 
## 116  41  32   5   2   4</code></pre>
<pre><code>## 
## Best penalty:
## 
##  penalty    df
##      0.8 1.999
## 
##  penalty    df   aic   bic aic.c
##     0.00 2.000 864.2 847.9 864.2
##     0.05 2.000 864.2 847.9 864.2
##     0.10 2.000 864.2 847.9 864.2
##     0.15 2.000 864.2 847.9 864.2
##     0.20 2.000 864.2 847.9 864.2
##     0.25 2.000 864.2 847.9 864.2
##     0.30 2.000 864.2 847.9 864.2
##     0.35 2.000 864.2 847.9 864.2
##     0.40 2.000 864.2 847.9 864.2
##     0.45 2.000 864.2 847.9 864.2
##     0.50 2.000 864.2 847.9 864.2
##     0.55 1.999 864.2 847.9 864.2
##     0.60 1.999 864.2 847.9 864.2
##     0.65 1.999 864.2 847.9 864.2
##     0.70 1.999 864.2 847.9 864.2
##     0.75 1.999 864.2 847.9 864.2
##     0.80 1.999 864.2 847.9 864.2</code></pre>
<pre><code>## [1] 868.2</code></pre>
<pre><code>## [1] 2</code></pre>
<pre><code>## [1] 0</code></pre>
<pre><code>## Pseudo R^2 for logistic regression
## Hosmer and Lemeshow R^2   0.026 
## Cox and Snell R^2         0.033 
## Nagelkerke R^2            0.046</code></pre>
<pre><code>##               2.5 %  97.5 %
## (Intercept) -0.6887 -0.6164
## ageold      -0.8965 -0.7651
## sexmale      0.3667  0.4735</code></pre>
<pre><code>## (Intercept)      ageold     sexmale 
##      0.5207      0.4358      1.5221</code></pre>
<pre><code>##              2.5 % 97.5 %
## (Intercept) 0.5022 0.5399
## ageold      0.4080 0.4653
## sexmale     1.4430 1.6057</code></pre>
<pre><code>## Analysis of Deviance Table
## 
## Model 1: suf.eh ~ 1
## Model 2: suf.eh ~ age + sex
##   Resid. Df Resid. Dev Df Deviance            Pr(&gt;Chi)    
## 1     25820      33008                                    
## 2     25818      32140  2      868 &lt;0.0000000000000002 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<pre><code>##   file.speaker.id no.eh eh text.id spk.ref    sex   age ethnicity
## 1   &lt;S1A-001#1:F&gt;    95 56  S1A001       F female young    Pakeha
## 2   &lt;S1A-001#1:M&gt;    97 75  S1A001       M   male young    Pakeha
## 3   &lt;S1A-002#1:B&gt;    99 45  S1A002       B female young    Pakeha
## 4   &lt;S1A-002#1:Q&gt;    86 51  S1A002       Q female young    Pakeha
## 5   &lt;S1A-003#1:B&gt;    58 50  S1A003       B   male young    Pakeha
## 6   &lt;S1A-003#1:M&gt;   119 84  S1A003       M   male young    Pakeha</code></pre>
<pre><code>## [1] 66.28</code></pre>
<pre><code>## [1] 66.28</code></pre>
<p><img src="advancedstatz_files/figure-html/unnamed-chunk-47-3.png" width="672" /></p>
<pre><code>##  ageold sexmale 
##   1.005   1.005</code></pre>
<pre><code>##  ageold sexmale 
##  0.9952  0.9952</code></pre>
<pre><code>## [1] 1.005</code></pre>
<pre><code>##   file.speaker.id text.id spk.ref  sex   age ethnicity suf.eh    dfb.1_
## 1   &lt;S1A-001#1:M&gt;  S1A001       M male young    Pakeha      0 -0.001211
## 2   &lt;S1A-001#1:M&gt;  S1A001       M male young    Pakeha      1  0.001432
## 3   &lt;S1A-001#1:M&gt;  S1A001       M male young    Pakeha      0 -0.001211
## 4   &lt;S1A-001#1:M&gt;  S1A001       M male young    Pakeha      0 -0.001211
## 5   &lt;S1A-001#1:M&gt;  S1A001       M male young    Pakeha      1  0.001432
## 6   &lt;S1A-001#1:M&gt;  S1A001       M male young    Pakeha      1  0.001432
##    dfb.agld  dfb.sxml    dffit cov.r     cook.d       hat dfb.1_.1
## 1  0.003762 -0.007929 -0.01071     1 0.00003231 0.0001223    FALSE
## 2 -0.004449  0.009375  0.01266     1 0.00005142 0.0001223    FALSE
## 3  0.003762 -0.007929 -0.01071     1 0.00003231 0.0001223    FALSE
## 4  0.003762 -0.007929 -0.01071     1 0.00003231 0.0001223    FALSE
## 5 -0.004449  0.009375  0.01266     1 0.00005142 0.0001223    FALSE
## 6 -0.004449  0.009375  0.01266     1 0.00005142 0.0001223    FALSE
##   dfb.agld.1 dfb.sxml.1 dffit.1 cov.r.1 cook.d.1 hat.1
## 1      FALSE      FALSE   FALSE   FALSE    FALSE FALSE
## 2      FALSE      FALSE   FALSE   FALSE    FALSE FALSE
## 3      FALSE      FALSE   FALSE   FALSE    FALSE FALSE
## 4      FALSE      FALSE   FALSE   FALSE    FALSE FALSE
## 5      FALSE      FALSE   FALSE   FALSE    FALSE FALSE
## 6      FALSE      FALSE   FALSE   FALSE    FALSE FALSE</code></pre>
<pre><code>## [1] &quot;Sample size sufficient&quot;</code></pre>
<pre><code>##                             Estimate VIF OddsRatio CI(2.5%) CI(97.5%)
## (Intercept)                    -0.65          0.52      0.5      0.54
## ageold                         -0.83   1      0.44     0.41      0.47
## sexmale                         0.42   1      1.52     1.44      1.61
## Model statistics                                                     
## Number of cases in model                                             
## Observed misses                                                      
## Observed successes                                                   
## Null deviance                                                        
## Residual deviance                                                    
## R2 (Nagelkerke)                                                      
## R2 (Hosmer &amp; Lemeshow)                                               
## R2 (Cox &amp; Snell)                                                     
## C                                                                    
## Somers&#39; Dxy                                                          
## AIC                                                                  
## Prediction accuracy                                                  
## Model Likelihood Ratio Test                                          
##                                     Std. Error z value   Pr(&gt;|z|)
## (Intercept)                               0.02  -35.39          0
## ageold                                    0.03  -24.78          0
## sexmale                                   0.03   15.42          0
## Model statistics                                                 
## Number of cases in model                                         
## Observed misses                                               0 :
## Observed successes                                            1 :
## Null deviance                                                    
## Residual deviance                                                
## R2 (Nagelkerke)                                                  
## R2 (Hosmer &amp; Lemeshow)                                           
## R2 (Cox &amp; Snell)                                                 
## C                                                                
## Somers&#39; Dxy                                                      
## AIC                                                              
## Prediction accuracy                                              
## Model Likelihood Ratio Test Model L.R.: 868.21   df: 2 p-value: 0
##                                 Significance
## (Intercept)                      p &lt; .001***
## ageold                           p &lt; .001***
## sexmale                          p &lt; .001***
## Model statistics                       Value
## Number of cases in model               25821
## Observed misses                        17114
## Observed successes                      8707
## Null deviance                       33007.75
## Residual deviance                   32139.54
## R2 (Nagelkerke)                        0.046
## R2 (Hosmer &amp; Lemeshow)                 0.026
## R2 (Cox &amp; Snell)                       0.033
## C                                      0.602
## Somers&#39; Dxy                            0.203
## AIC                                 32145.54
## Prediction accuracy                   66.28%
## Model Likelihood Ratio Test sig: p &lt; .001***</code></pre>
<div id="model-fit-parameters" class="section level2">
<h2><span class="header-section-number">3.1</span> Model Fit Parameters</h2>
<div id="r2-hosmer-lemeshow" class="section level3">
<h3><span class="header-section-number">3.1.1</span> R2 (Hosmer &amp; Lemeshow)</h3>
<p>âRt is the proportional reduction in the absolute value of the log-likelihood measure and as such it is a measure of how much the badness of fit improves as a result of the inclusionof the predictor variables. It can vary between 0 (indicating that the predictors are useless at predicting the outcome variable) and 1 (indicating that the model predicts the outcome variable perfectly)â (<span class="citation">(A. Field, Miles, and Field 2012, 317)</span>).</p>
</div>
<div id="r2-cox-snell" class="section level3">
<h3><span class="header-section-number">3.1.2</span> R2 (Cox &amp; Snell)</h3>
<p>âCox and Snellâs R~s (1989) is based on the deviance of the model (-2LL(newÂ») and the deviance of the baseline model (-2LL(baseline), and the sample size, n [â¦]. However, this statistic never reaches its theoretical maximum of 1.</p>
</div>
<div id="r2-nagelkerke" class="section level3">
<h3><span class="header-section-number">3.1.3</span> R2 (Nagelkerke)</h3>
<p>Since R2 (Cox &amp; Snell) never reaches its theoretical maximum of 1, Nagelkerke (1991) suggested Nagelkerkeâs R^2. (Field, Miles &amp; Field 2012:317-318).</p>
</div>
<div id="somers-dxy" class="section level3">
<h3><span class="header-section-number">3.1.4</span> Somersâ Dxy</h3>
<p>Somersâ Dxy is a rank correlation between predicted probabilities and observed responses ranges between 0 (randomness) and 1 (perfect prediction). (cf. <span class="citation">(Baayen 2008, 204)</span>).</p>
</div>
<div id="c" class="section level3">
<h3><span class="header-section-number">3.1.5</span> C</h3>
<p>C is an index of concordance between the predicted probability and the observed response. When C takes the value 0.5, the predictions are random, when it is 1, prediction is perfect. A value above 0.8 indicates that the model may have some real predictive capacity (cf. <span class="citation">(Baayen 2008, 204)</span>).</p>
</div>
<div id="akaike-information-criteria-aic" class="section level3">
<h3><span class="header-section-number">3.1.6</span> Akaike information criteria (AIC)</h3>
<p>Akaike information criteria (AlC = -2LL + 2k) provide a value that reflects a ratio between the number of predictors in the model and the variance that is explained by these predictors. Changes in AIC can serve as a measure of whether the inclusion of a variable leads to a significant incerase in the amount of variance that is explained by the model. âYou can think of this as the price you pay for something: you get a better value of R2, but you pay a higher price, and was that higher price worth it? These information criteria help you to decide.model. The BIC is the same as the AIC but adjusts the penalty included in the AlC (i.e., 2k) by the number of cases: BlC = -2LL + 2k x log(n) in which n is the number of cases in the modelâ (<span class="citation">(A. Field, Miles, and Field 2012, 318)</span>).</p>
</div>
</div>
</div>
<div id="mixed-effects-binomial-logistic-regression" class="section level1">
<h1><span class="header-section-number">4</span> Mixed Effects Binomial Logistic Regression</h1>
<pre><code>##   file.speaker.id text.id spk.ref  sex   age ethnicity suf.eh
## 1   &lt;S1A-001#1:M&gt;  S1A001       M male young    Pakeha      0
## 2   &lt;S1A-001#1:M&gt;  S1A001       M male young    Pakeha      1
## 3   &lt;S1A-001#1:M&gt;  S1A001       M male young    Pakeha      0
## 4   &lt;S1A-001#1:M&gt;  S1A001       M male young    Pakeha      0
## 5   &lt;S1A-001#1:M&gt;  S1A001       M male young    Pakeha      1
## 6   &lt;S1A-001#1:M&gt;  S1A001       M male young    Pakeha      1</code></pre>
<pre><code>## &#39;data.frame&#39;:    25821 obs. of  7 variables:
##  $ file.speaker.id: chr  &quot;&lt;S1A-001#1:M&gt;&quot; &quot;&lt;S1A-001#1:M&gt;&quot; &quot;&lt;S1A-001#1:M&gt;&quot; &quot;&lt;S1A-001#1:M&gt;&quot; ...
##  $ text.id        : chr  &quot;S1A001&quot; &quot;S1A001&quot; &quot;S1A001&quot; &quot;S1A001&quot; ...
##  $ spk.ref        : chr  &quot;M&quot; &quot;M&quot; &quot;M&quot; &quot;M&quot; ...
##  $ sex            : Factor w/ 2 levels &quot;female&quot;,&quot;male&quot;: 2 2 2 2 2 2 2 2 2 2 ...
##  $ age            : Factor w/ 2 levels &quot;young&quot;,&quot;old&quot;: 1 1 1 1 1 1 1 1 1 1 ...
##  $ ethnicity      : Factor w/ 2 levels &quot;Pakeha&quot;,&quot;Maori&quot;: 1 1 1 1 1 1 1 1 1 1 ...
##  $ suf.eh         : num  0 1 0 0 1 1 0 0 0 1 ...</code></pre>
<pre><code>##  file.speaker.id      text.id            spk.ref              sex       
##  Length:25821       Length:25821       Length:25821       female:15852  
##  Class :character   Class :character   Class :character   male  : 9969  
##  Mode  :character   Mode  :character   Mode  :character                 
##                                                                         
##                                                                         
##                                                                         
##     age         ethnicity         suf.eh     
##  young:19150   Pakeha:20024   Min.   :0.000  
##  old  : 6671   Maori : 5797   1st Qu.:0.000  
##                               Median :0.000  
##                               Mean   :0.337  
##                               3rd Qu.:1.000  
##                               Max.   :1.000</code></pre>
<p><img src="advancedstatz_files/figure-html/unnamed-chunk-48-1.png" width="672" /><img src="advancedstatz_files/figure-html/unnamed-chunk-48-2.png" width="672" /></p>
<pre><code>## , ,  = female,  = Pakeha
## 
##    
##     young  old
##   0  6820 1930
##   1  3527  450
## 
## , ,  = male,  = Pakeha
## 
##    
##     young  old
##   0  3237 1125
##   1  2535  400
## 
## , ,  = female,  = Maori
## 
##    
##     young  old
##   0  1063 1218
##   1   586  258
## 
## , ,  = male,  = Maori
## 
##    
##     young  old
##   0   759  962
##   1   623  328</code></pre>
<div id="model-building" class="section level2">
<h2><span class="header-section-number">4.1</span> Model Building</h2>
<pre><code>## 
## Call:
## glm(formula = suf.eh ~ 1, family = binomial, data = mydata)
## 
## Deviance Residuals: 
##    Min      1Q  Median      3Q     Max  
## -0.907  -0.907  -0.907   1.474   1.474  
## 
## Coefficients:
##             Estimate Std. Error z value            Pr(&gt;|z|)    
## (Intercept)  -0.6758     0.0132   -51.3 &lt;0.0000000000000002 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 33008  on 25820  degrees of freedom
## Residual deviance: 33008  on 25820  degrees of freedom
## AIC: 33010
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<pre><code>## Logistic Regression Model
##  
##  lrm(formula = suf.eh ~ 1, data = mydata, x = T, y = T)
##  
##                        Model Likelihood    Discrimination    Rank Discrim.    
##                              Ratio Test           Indexes          Indexes    
##  Obs         25821    LR chi2      0.00    R2       0.000    C       0.500    
##   0          17114    d.f.            0    g        0.000    Dxy     0.000    
##   1           8707    Pr(&gt; chi2) 1.0000    gr       1.000    gamma   0.000    
##  max |deriv|     0                         gp       0.000    tau-a   0.000    
##                                            Brier    0.223                     
##  
##            Coef   
##  Intercept -0.6758
## </code></pre>
<pre><code>## Linear mixed model fit by REML [&#39;lmerMod&#39;]
## Formula: pptw ~ (1 | genre/region) + 1
##    Data: mydata
## REML criterion at convergence: 4489
## Random effects:
##  Groups       Name        Std.Dev.
##  region:genre (Intercept)  4.36   
##  genre        (Intercept) 12.54   
##  Residual                 15.02   
## Number of obs: 537, groups:  region:genre, 31; genre, 16
## Fixed Effects:
## (Intercept)  
##         134</code></pre>
<pre><code>## [1] 32479</code></pre>
<pre><code>## [1] 33010</code></pre>
<pre><code>## 
## Call:
## glm(formula = suf.eh ~ 1, family = binomial, data = mydata)
## 
## Deviance Residuals: 
##    Min      1Q  Median      3Q     Max  
## -0.907  -0.907  -0.907   1.474   1.474  
## 
## Coefficients:
##             Estimate Std. Error z value            Pr(&gt;|z|)    
## (Intercept)  -0.6758     0.0132   -51.3 &lt;0.0000000000000002 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 33008  on 25820  degrees of freedom
## Residual deviance: 33008  on 25820  degrees of freedom
## AIC: 33010
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<pre><code>## Generalized linear mixed model fit by maximum likelihood (Laplace
##   Approximation) [glmerMod]
##  Family: binomial  ( logit )
## Formula: suf.eh ~ (1 | file.speaker.id)
##    Data: mydata
## 
##      AIC      BIC   logLik deviance df.resid 
##    32479    32495   -16237    32475    25819 
## 
## Scaled residuals: 
##    Min     1Q Median     3Q    Max 
## -1.059 -0.741 -0.614  1.193  2.368 
## 
## Random effects:
##  Groups          Name        Variance Std.Dev.
##  file.speaker.id (Intercept) 0.158    0.398   
## Number of obs: 25821, groups:  file.speaker.id, 203
## 
## Fixed effects:
##             Estimate Std. Error z value            Pr(&gt;|z|)    
## (Intercept)  -0.6866     0.0315   -21.8 &lt;0.0000000000000002 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
</div>
<div id="model-fitting" class="section level2">
<h2><span class="header-section-number">4.2</span> Model Fitting</h2>
<pre><code>## Data: mydata
## Models:
## m0.glmer: suf.eh ~ 1 + (1 | file.speaker.id)
## m1.glmer: suf.eh ~ age + (1 | file.speaker.id)
## m2.glmer: suf.eh ~ age + sex + (1 | file.speaker.id)
## m3.glmer: suf.eh ~ age + sex + ethnicity + (1 | file.speaker.id)
## m4.glmer: suf.eh ~ age + sex + ethnicity + age:sex + (1 | file.speaker.id)
## m5.glmer: suf.eh ~ age + sex + ethnicity + age:sex + age:ethnicity + (1 | 
## m5.glmer:     file.speaker.id)
## m6.glmer: suf.eh ~ age + sex + ethnicity + age:sex + age:ethnicity + sex:ethnicity + 
## m6.glmer:     (1 | file.speaker.id)
## m7.glmer: suf.eh ~ age + sex + ethnicity + age:sex + age:ethnicity + sex:ethnicity + 
## m7.glmer:     age:sex:ethnicity + (1 | file.speaker.id)
##          Df   AIC   BIC logLik deviance  Chisq Chi Df          Pr(&gt;Chisq)
## m0.glmer  2 32479 32495 -16237    32475                                  
## m1.glmer  3 32302 32327 -16148    32296 178.65      1 &lt;0.0000000000000002
## m2.glmer  4 32148 32180 -16070    32140 156.50      1 &lt;0.0000000000000002
## m3.glmer  5 32149 32190 -16070    32139   0.26      1               0.609
## m4.glmer  6 32151 32200 -16070    32139   0.12      1               0.728
## m5.glmer  7 32150 32207 -16068    32136   2.91      1               0.088
## m6.glmer  8 32152 32218 -16068    32136   0.01      1               0.940
## m7.glmer  9 32154 32227 -16068    32136   0.23      1               0.629
##             
## m0.glmer    
## m1.glmer ***
## m2.glmer ***
## m3.glmer    
## m4.glmer    
## m5.glmer .  
## m6.glmer    
## m7.glmer    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<pre><code>## Data: mydata
## Models:
## m0.glmer: suf.eh ~ 1 + (1 | file.speaker.id)
## m1.glmer: suf.eh ~ age + (1 | file.speaker.id)
##          Df   AIC   BIC logLik deviance Chisq Chi Df          Pr(&gt;Chisq)
## m0.glmer  2 32479 32495 -16237    32475                                 
## m1.glmer  3 32302 32327 -16148    32296   179      1 &lt;0.0000000000000002
##             
## m0.glmer    
## m1.glmer ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<pre><code>## Data: mydata
## Models:
## m1.glmer: suf.eh ~ age + (1 | file.speaker.id)
## m2.glmer: suf.eh ~ age + sex + (1 | file.speaker.id)
##          Df   AIC   BIC logLik deviance Chisq Chi Df          Pr(&gt;Chisq)
## m1.glmer  3 32302 32327 -16148    32296                                 
## m2.glmer  4 32148 32180 -16070    32140   156      1 &lt;0.0000000000000002
##             
## m1.glmer    
## m2.glmer ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<pre><code>## Data: mydata
## Models:
## m2.glmer: suf.eh ~ age + sex + (1 | file.speaker.id)
## m3.glmer: suf.eh ~ age + sex + ethnicity + (1 | file.speaker.id)
##          Df   AIC   BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq)
## m2.glmer  4 32148 32180 -16070    32140                        
## m3.glmer  5 32149 32190 -16070    32139  0.26      1       0.61</code></pre>
<pre><code>## Data: mydata
## Models:
## m2.glmer: suf.eh ~ age + sex + (1 | file.speaker.id)
## m4.glmer: suf.eh ~ age + sex + ethnicity + age:sex + (1 | file.speaker.id)
##          Df   AIC   BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq)
## m2.glmer  4 32148 32180 -16070    32140                        
## m4.glmer  6 32151 32200 -16070    32139  0.38      2       0.83</code></pre>
<pre><code>## Data: mydata
## Models:
## m2.glmer: suf.eh ~ age + sex + (1 | file.speaker.id)
## m5.glmer: suf.eh ~ age + sex + ethnicity + age:sex + age:ethnicity + (1 | 
## m5.glmer:     file.speaker.id)
##          Df   AIC   BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq)
## m2.glmer  4 32148 32180 -16070    32140                        
## m5.glmer  7 32150 32207 -16068    32136  3.29      3       0.35</code></pre>
<pre><code>## Data: mydata
## Models:
## m2.glmer: suf.eh ~ age + sex + (1 | file.speaker.id)
## m6.glmer: suf.eh ~ age + sex + ethnicity + age:sex + age:ethnicity + sex:ethnicity + 
## m6.glmer:     (1 | file.speaker.id)
##          Df   AIC   BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq)
## m2.glmer  4 32148 32180 -16070    32140                        
## m6.glmer  8 32152 32218 -16068    32136   3.3      4       0.51</code></pre>
<pre><code>## Data: mydata
## Models:
## m2.glmer: suf.eh ~ age + sex + (1 | file.speaker.id)
## m7.glmer: suf.eh ~ age + sex + ethnicity + age:sex + age:ethnicity + sex:ethnicity + 
## m7.glmer:     age:sex:ethnicity + (1 | file.speaker.id)
##          Df   AIC   BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq)
## m2.glmer  4 32148 32180 -16070    32140                        
## m7.glmer  9 32154 32227 -16068    32136  3.53      5       0.62</code></pre>
<pre><code>## Data: mydata
## Models:
## m0.glmer: suf.eh ~ 1 + (1 | file.speaker.id)
## mlr.glmer: suf.eh ~ age + sex + (1 | file.speaker.id)
##           Df   AIC   BIC logLik deviance Chisq Chi Df          Pr(&gt;Chisq)
## m0.glmer   2 32479 32495 -16237    32475                                 
## mlr.glmer  4 32148 32180 -16070    32140   335      2 &lt;0.0000000000000002
##              
## m0.glmer     
## mlr.glmer ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<pre><code>## Generalized linear mixed model fit by maximum likelihood (Laplace
##   Approximation) [glmerMod]
##  Family: binomial  ( logit )
## Formula: suf.eh ~ age + sex + (1 | file.speaker.id)
##    Data: mydata
##      AIC      BIC   logLik deviance df.resid 
##    32148    32180   -16070    32140    25817 
## Random effects:
##  Groups          Name        Std.Dev.
##  file.speaker.id (Intercept) 0       
## Number of obs: 25821, groups:  file.speaker.id, 203
## Fixed Effects:
## (Intercept)       ageold      sexmale  
##      -0.652       -0.831        0.420</code></pre>
<pre><code>## Analysis of Variance Table
##     Df Sum Sq Mean Sq F value
## age  1    565     565     565
## sex  1    238     238     238</code></pre>
<pre><code>## Data: mydata
## Models:
## m0.glmer: suf.eh ~ 1 + (1 | file.speaker.id)
## m1.glmer: suf.eh ~ age + (1 | file.speaker.id)
##          Df   AIC   BIC logLik deviance Chisq Chi Df          Pr(&gt;Chisq)
## m0.glmer  2 32479 32495 -16237    32475                                 
## m1.glmer  3 32302 32327 -16148    32296   179      1 &lt;0.0000000000000002
##             
## m0.glmer    
## m1.glmer ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<pre><code>## Data: mydata
## Models:
## m1.glmer: suf.eh ~ age + (1 | file.speaker.id)
## m2.glmer: suf.eh ~ age + sex + (1 | file.speaker.id)
##          Df   AIC   BIC logLik deviance Chisq Chi Df          Pr(&gt;Chisq)
## m1.glmer  3 32302 32327 -16148    32296                                 
## m2.glmer  4 32148 32180 -16070    32140   156      1 &lt;0.0000000000000002
##             
## m1.glmer    
## m2.glmer ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
</div>
<div id="extracting-model-fit-parameters" class="section level2">
<h2><span class="header-section-number">4.3</span> Extracting Model Fit Parameters</h2>
<p>We now create a lmr object equivalent to the final minimal adequate model but without the random effect.</p>
<pre><code>## 
##  Pearson&#39;s product-moment correlation
## 
## data:  coef(mlr.lrm) and fixef(mlr.lmer)
## t = Inf, df = 1, p-value &lt;0.0000000000000002
## alternative hypothesis: true correlation is not equal to 0
## sample estimates:
## cor 
##   1</code></pre>
<pre><code>##          C        Dxy          n    Missing 
##     0.6181     0.2361 25821.0000     0.0000</code></pre>
</div>
<div id="model-diagnostics-1" class="section level2">
<h2><span class="header-section-number">4.4</span> Model Diagnostics</h2>
<pre class="r"><code># model diagnostics: plot fitted against residuals
plot(mlr.glmer)</code></pre>
<p><img src="advancedstatz_files/figure-html/unnamed-chunk-52-1.png" width="672" /></p>
<pre class="r"><code># plot residuals against fitted
plot(mlr.glmer, form = resid(., type = &quot;response&quot;) ~ fitted(.) | file.speaker.id, abline = 0, cex = .5,id = 0.05, adj = -0.3)</code></pre>
<p><img src="advancedstatz_files/figure-html/unnamed-chunk-53-1.png" width="672" /></p>
<pre class="r"><code># diagnostic plot: examining residuals (Pinheiro &amp; Bates 2000:175)
plot(mlr.glmer, file.speaker.id ~ resid(.), abline = 0 , cex = .5)</code></pre>
<p><img src="advancedstatz_files/figure-html/unnamed-chunk-54-1.png" width="672" /></p>
<pre class="r"><code># summarize final model
meblrm.summary(m0.glm, m1.glm, m0.glmer, mlr.glmer, dpvar=mydata$suf.eh)</code></pre>
<pre><code>##                                    Group(s) Variance Std. Dev.         
## Random Effect(s)            file.speaker.id        0         0         
## Fixed Effect(s)                    Estimate      VIF OddsRatio CI(2.5%)
## (Intercept)                           -0.65               0.52      0.5
## ageold                                -0.83        1      0.44     0.41
## sexmale                                0.42        1      1.52     1.44
## Model statistics                                                       
## Number of Groups                                                       
## Number of cases in model                                               
## Observed misses                                                        
## Observed successes                                                     
## Residual deviance                                                      
## R2 (Nagelkerke)                                                        
## R2 (Hosmer &amp; Lemeshow)                                                 
## R2 (Cox &amp; Snell)                                                       
## C                                                                      
## Somers&#39; Dxy                                                            
## AIC                                                                    
## BIC                                                                    
## Prediction accuracy                                                    
## Model Likelihood Ratio Test                                            
##                                               L.R. X2      DF         Pr
## Random Effect(s)                               533.06       1          0
## Fixed Effect(s)             CI(97.5%)      Std. Error z value   Pr(&gt;|z|)
## (Intercept)                      0.54            0.02  -35.39          0
## ageold                           0.47            0.03  -24.78          0
## sexmale                          1.61            0.03   15.42          0
## Model statistics                                                        
## Number of Groups                                                        
## Number of cases in model                                                
## Observed misses                                                         
## Observed successes                                                      
## Residual deviance                                                       
## R2 (Nagelkerke)                                                         
## R2 (Hosmer &amp; Lemeshow)                                                  
## R2 (Cox &amp; Snell)                                                        
## C                                                                       
## Somers&#39; Dxy                                                             
## AIC                                                                     
## BIC                                                                     
## Prediction accuracy                                                     
## Model Likelihood Ratio Test           L.R. X2: 868.21   DF: 3 p-value: 0
##                                 Significance
## Random Effect(s)                 p &lt; .001***
## Fixed Effect(s)                 Significance
## (Intercept)                      p &lt; .001***
## ageold                           p &lt; .001***
## sexmale                          p &lt; .001***
## Model statistics                       Value
## Number of Groups                         203
## Number of cases in model               25821
## Observed misses                        17114
## Observed successes                      8707
## Residual deviance                   32139.54
## R2 (Nagelkerke)                        0.046
## R2 (Hosmer &amp; Lemeshow)                 0.026
## R2 (Cox &amp; Snell)                       0.033
## C                                      0.602
## Somers&#39; Dxy                            0.203
## AIC                                 32147.54
## BIC                                 32180.18
## Prediction accuracy                   66.28%
## Model Likelihood Ratio Test sig: p &lt; .001***</code></pre>
</div>
</div>
<div id="conditional-inference-trees" class="section level1">
<h1><span class="header-section-number">5</span> Conditional Inference Trees</h1>
<pre><code>## &#39;data.frame&#39;:    314 obs. of  15 variables:
##  $ Age             : chr  &quot;26-40&quot; &quot;26-40&quot; &quot;26-40&quot; &quot;17-25&quot; ...
##  $ Adjective       : chr  &quot;good&quot; &quot;good&quot; &quot;good&quot; &quot;nice&quot; ...
##  $ FileSpeaker     : chr  &quot;&lt;S1A-001:1$B&gt;&quot; &quot;&lt;S1A-001:1$B&gt;&quot; &quot;&lt;S1A-001:1$B&gt;&quot; &quot;&lt;S1A-003:1$B&gt;&quot; ...
##  $ Function        : chr  &quot;Attributive&quot; &quot;Attributive&quot; &quot;Predicative&quot; &quot;Attributive&quot; ...
##  $ Priming         : chr  &quot;NoPrime&quot; &quot;NoPrime&quot; &quot;NoPrime&quot; &quot;NoPrime&quot; ...
##  $ Gender          : chr  &quot;Men&quot; &quot;Men&quot; &quot;Men&quot; &quot;Men&quot; ...
##  $ Occupation      : chr  &quot;AcademicManagerialProfessionals&quot; &quot;AcademicManagerialProfessionals&quot; &quot;AcademicManagerialProfessionals&quot; &quot;AcademicManagerialProfessionals&quot; ...
##  $ ConversationType: chr  &quot;SameSex&quot; &quot;SameSex&quot; &quot;SameSex&quot; &quot;SameSex&quot; ...
##  $ AudienceSize    : chr  &quot;MultipleInterlocutors&quot; &quot;MultipleInterlocutors&quot; &quot;MultipleInterlocutors&quot; &quot;Dyad&quot; ...
##  $ very            : int  0 0 0 1 0 1 0 1 0 0 ...
##  $ really          : int  0 0 0 0 0 0 0 0 1 1 ...
##  $ Freq            : num  27.848 27.848 27.848 7.293 0.617 ...
##  $ Gradabilty      : chr  &quot;NotGradable&quot; &quot;NotGradable&quot; &quot;NotGradable&quot; &quot;NotGradable&quot; ...
##  $ SemanticCategory: chr  &quot;Value&quot; &quot;Value&quot; &quot;Value&quot; &quot;HumanPropensity&quot; ...
##  $ Emotionality    : chr  &quot;PositiveEmotional&quot; &quot;PositiveEmotional&quot; &quot;PositiveEmotional&quot; &quot;NonEmotional&quot; ...</code></pre>
<pre><code>##  [1] &quot;Age&quot;              &quot;Adjective&quot;        &quot;Function&quot;        
##  [4] &quot;Priming&quot;          &quot;Gender&quot;           &quot;ConversationType&quot;
##  [7] &quot;AudienceSize&quot;     &quot;really&quot;           &quot;Freq&quot;            
## [10] &quot;Gradabilty&quot;       &quot;SemanticCategory&quot; &quot;Emotionality&quot;</code></pre>
<pre><code>## &#39;data.frame&#39;:    314 obs. of  12 variables:
##  $ Age             : Factor w/ 3 levels &quot;17-25&quot;,&quot;26-40&quot;,..: 2 2 2 1 3 3 3 3 1 1 ...
##  $ Adjective       : Factor w/ 6 levels &quot;bad&quot;,&quot;funny&quot;,..: 3 3 3 5 6 6 3 6 6 6 ...
##  $ Function        : Factor w/ 2 levels &quot;Attributive&quot;,..: 1 1 2 1 2 2 1 2 1 1 ...
##  $ Priming         : Factor w/ 2 levels &quot;NoPrime&quot;,&quot;Prime&quot;: 1 1 1 1 1 1 1 1 1 2 ...
##  $ Gender          : Factor w/ 2 levels &quot;Men&quot;,&quot;Women&quot;: 1 1 1 1 1 1 2 2 1 1 ...
##  $ ConversationType: Factor w/ 2 levels &quot;MixedSex&quot;,&quot;SameSex&quot;: 2 2 2 2 2 1 1 1 1 1 ...
##  $ AudienceSize    : Factor w/ 2 levels &quot;Dyad&quot;,&quot;MultipleInterlocutors&quot;: 2 2 2 1 1 2 2 2 2 2 ...
##  $ really          : Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 1 1 1 1 1 1 1 1 2 2 ...
##  $ Freq            : num  27.848 27.848 27.848 7.293 0.617 ...
##  $ Gradabilty      : Factor w/ 3 levels &quot;GradabilityUndetermined&quot;,..: 3 3 3 3 3 3 3 1 1 3 ...
##  $ SemanticCategory: Factor w/ 5 levels &quot;Dimension&quot;,&quot;HumanPropensity&quot;,..: 5 5 5 2 5 5 5 2 1 4 ...
##  $ Emotionality    : Factor w/ 3 levels &quot;NegativeEmotional&quot;,..: 3 3 3 2 2 3 3 1 2 2 ...</code></pre>
<pre><code>## png 
##   2</code></pre>
<pre><code>## [1] 100</code></pre>
<pre><code>## [1] 41.08</code></pre>
</div>
<div id="random-forests" class="section level1">
<h1><span class="header-section-number">6</span> Random Forests</h1>
<div id="example-1" class="section level2">
<h2><span class="header-section-number">6.1</span> Example 1:</h2>
<pre class="r"><code># prepare data
rfd &lt;- reallyaus
# convert really into a factor
rfd$really &lt;- as.factor(rfd$really)
# start with random forest
# set seed
set.seed(222)
# partition data for evaluating rf 
id &lt;- sample(2, nrow(rfd), replace = T, prob = c(.7, .3))
train &lt;- rfd[id == 1, ]
test &lt;- rfd[id == 2,]
# load library
library(randomForest)
# create initial model
reallyaus_rf1 &lt;- randomForest(really~., data = train)
# inspect model
print(reallyaus_rf1)</code></pre>
<pre><code>## 
## Call:
##  randomForest(formula = really ~ ., data = train) 
##                Type of random forest: classification
##                      Number of trees: 500
## No. of variables tried at each split: 3
## 
##         OOB estimate of  error rate: 41.47%
## Confusion matrix:
##    0  1 class.error
## 0 79 44      0.3577
## 1 46 48      0.4894</code></pre>
<pre class="r"><code># inspect attibutes
attributes(reallyaus_rf1)</code></pre>
<pre><code>## $names
##  [1] &quot;call&quot;            &quot;type&quot;            &quot;predicted&quot;      
##  [4] &quot;err.rate&quot;        &quot;confusion&quot;       &quot;votes&quot;          
##  [7] &quot;oob.times&quot;       &quot;classes&quot;         &quot;importance&quot;     
## [10] &quot;importanceSD&quot;    &quot;localImportance&quot; &quot;proximity&quot;      
## [13] &quot;ntree&quot;           &quot;mtry&quot;            &quot;forest&quot;         
## [16] &quot;y&quot;               &quot;test&quot;            &quot;inbag&quot;          
## [19] &quot;terms&quot;          
## 
## $class
## [1] &quot;randomForest.formula&quot; &quot;randomForest&quot;</code></pre>
<pre class="r"><code># start model evaluation
# install package
#source(&quot;https://bioconductor.org/biocLite.R&quot;); biocLite(); library(Biobase)
#install.packages(&quot;Biobase&quot;, repos=c(&quot;http://rstudio.org/_packages&quot;, &quot;http://cran.rstudio.com&quot;, 
#                                      &quot;http://cran.rstudio.com/&quot;, dependencies=TRUE))
#install.packages(&quot;dimRed&quot;, dependencies = TRUE)
#install.packages(&#39;caret&#39;, dependencies = TRUE)

# load caret library
library(caret) # because initially caret did not work, the libraries above had to be installed
# extract prediction for training data
ptrain1 &lt;- predict(reallyaus_rf1, train)
# inspect predictions
head(ptrain1); head(train$really)</code></pre>
<pre><code>## 2 3 4 7 8 9 
## 0 0 0 0 0 1 
## Levels: 0 1</code></pre>
<pre><code>## [1] 0 0 0 0 0 1
## Levels: 0 1</code></pre>
<pre class="r"><code># create confusionMatrix
confusionMatrix(ptrain1, train$really)</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction   0   1
##          0 116  13
##          1   7  81
##                                              
##                Accuracy : 0.908              
##                  95% CI : (0.861, 0.943)     
##     No Information Rate : 0.567              
##     P-Value [Acc &gt; NIR] : &lt;0.0000000000000002
##                                              
##                   Kappa : 0.811              
##  Mcnemar&#39;s Test P-Value : 0.264              
##                                              
##             Sensitivity : 0.943              
##             Specificity : 0.862              
##          Pos Pred Value : 0.899              
##          Neg Pred Value : 0.920              
##              Prevalence : 0.567              
##          Detection Rate : 0.535              
##    Detection Prevalence : 0.594              
##       Balanced Accuracy : 0.902              
##                                              
##        &#39;Positive&#39; Class : 0                  
## </code></pre>
<pre class="r"><code># extract prediction for test data
ptest1 &lt;- predict(reallyaus_rf1, test)
# create confusionMatrix
confusionMatrix(ptest1, test$really)</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction  0  1
##          0 44 16
##          1 18 19
##                                         
##                Accuracy : 0.649         
##                  95% CI : (0.546, 0.744)
##     No Information Rate : 0.639         
##     P-Value [Acc &gt; NIR] : 0.462         
##                                         
##                   Kappa : 0.249         
##  Mcnemar&#39;s Test P-Value : 0.864         
##                                         
##             Sensitivity : 0.710         
##             Specificity : 0.543         
##          Pos Pred Value : 0.733         
##          Neg Pred Value : 0.514         
##              Prevalence : 0.639         
##          Detection Rate : 0.454         
##    Detection Prevalence : 0.619         
##       Balanced Accuracy : 0.626         
##                                         
##        &#39;Positive&#39; Class : 0             
## </code></pre>
<pre class="r"><code># determine errorrate of random forest model
plot(reallyaus_rf1, main = &quot;&quot;)</code></pre>
<p><img src="advancedstatz_files/figure-html/unnamed-chunk-58-1.png" width="672" /></p>
<pre class="r"><code># tune model
reallyaus_rf2 &lt;- tuneRF(train[, !colnames(train)== &quot;really&quot;], train[, colnames(train)== &quot;really&quot;], 
                        stepFactor = .5, # for most values 6 appears to be optimal
                        plot = T,
                        ntreeTry = 200,
                        trace = T,
                        improve = .05
)</code></pre>
<pre><code>## mtry = 3  OOB error = 42.4% 
## Searching left ...
## mtry = 6     OOB error = 42.4% 
## 0 0.05 
## Searching right ...
## mtry = 1     OOB error = 43.32% 
## -0.02174 0.05</code></pre>
<p><img src="advancedstatz_files/figure-html/unnamed-chunk-59-1.png" width="672" /></p>
<pre class="r"><code># create improved model
reallyaus_rf2 &lt;- randomForest(really~., data = train, 
                              ntree = 200,
                              ntry = 6,
                              importance= T,
                              proximity = T)
# inspect model
print(reallyaus_rf2)</code></pre>
<pre><code>## 
## Call:
##  randomForest(formula = really ~ ., data = train, ntree = 200,      ntry = 6, importance = T, proximity = T) 
##                Type of random forest: classification
##                      Number of trees: 200
## No. of variables tried at each split: 3
## 
##         OOB estimate of  error rate: 40.09%
## Confusion matrix:
##    0  1 class.error
## 0 86 37      0.3008
## 1 50 44      0.5319</code></pre>
<pre class="r"><code># predict based on improved model
ptrain2 &lt;- predict(reallyaus_rf2, train)
# create confusionMatrix
confusionMatrix(ptrain2, train$really)</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction   0   1
##          0 114  15
##          1   9  79
##                                              
##                Accuracy : 0.889              
##                  95% CI : (0.84, 0.928)      
##     No Information Rate : 0.567              
##     P-Value [Acc &gt; NIR] : &lt;0.0000000000000002
##                                              
##                   Kappa : 0.773              
##  Mcnemar&#39;s Test P-Value : 0.307              
##                                              
##             Sensitivity : 0.927              
##             Specificity : 0.840              
##          Pos Pred Value : 0.884              
##          Neg Pred Value : 0.898              
##              Prevalence : 0.567              
##          Detection Rate : 0.525              
##    Detection Prevalence : 0.594              
##       Balanced Accuracy : 0.884              
##                                              
##        &#39;Positive&#39; Class : 0                  
## </code></pre>
<pre class="r"><code># extract prediction for test data
ptest2 &lt;- predict(reallyaus_rf2, test)
# create confusionMatrix
confusionMatrix(ptest2, test$really)</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction  0  1
##          0 45 17
##          1 17 18
##                                         
##                Accuracy : 0.649         
##                  95% CI : (0.546, 0.744)
##     No Information Rate : 0.639         
##     P-Value [Acc &gt; NIR] : 0.462         
##                                         
##                   Kappa : 0.24          
##  Mcnemar&#39;s Test P-Value : 1.000         
##                                         
##             Sensitivity : 0.726         
##             Specificity : 0.514         
##          Pos Pred Value : 0.726         
##          Neg Pred Value : 0.514         
##              Prevalence : 0.639         
##          Detection Rate : 0.464         
##    Detection Prevalence : 0.639         
##       Balanced Accuracy : 0.620         
##                                         
##        &#39;Positive&#39; Class : 0             
## </code></pre>
<pre class="r"><code># inspect number of nodes for trees
hist(treesize(reallyaus_rf2), main = &quot;&quot;, col = &quot;lightgray&quot;)</code></pre>
<p><img src="advancedstatz_files/figure-html/unnamed-chunk-59-2.png" width="672" /></p>
<pre class="r"><code># check variable importance
varImpPlot(reallyaus_rf2, main = &quot;&quot;, pch = 20) </code></pre>
<p><img src="advancedstatz_files/figure-html/unnamed-chunk-60-1.png" width="672" /></p>
<pre class="r"><code># left plot (Accuracy): how much accuracy decreases if factor is left out
# left plot (Gini/Pureness): how much more unpure (ambigious) the distributions become if fector is left out
# extract variable importance values
#importance(reallyaus_rf2)

#which variables have been used in the trees
varUsed(reallyaus_rf2)</code></pre>
<pre><code>##  [1]  817  826  894  745  672  902  831 2114  461 1129  853</code></pre>
<pre class="r"><code># partial dependence plot
partialPlot(reallyaus_rf2, train, Freq, 1)</code></pre>
<p><img src="advancedstatz_files/figure-html/unnamed-chunk-61-1.png" width="672" /></p>
<pre class="r"><code>partialPlot(reallyaus_rf2, train, ConversationType, 1)</code></pre>
<p><img src="advancedstatz_files/figure-html/unnamed-chunk-62-1.png" width="672" /></p>
<pre class="r"><code>partialPlot(reallyaus_rf2, train, Function, 1)</code></pre>
<p><img src="advancedstatz_files/figure-html/unnamed-chunk-63-1.png" width="672" /></p>
<pre class="r"><code>partialPlot(reallyaus_rf2, train, SemanticCategory, 1)</code></pre>
<p><img src="advancedstatz_files/figure-html/unnamed-chunk-64-1.png" width="672" /></p>
<pre class="r"><code>partialPlot(reallyaus_rf2, train, Gender, 1)</code></pre>
<p><img src="advancedstatz_files/figure-html/unnamed-chunk-65-1.png" width="672" /></p>
<pre class="r"><code># extract tree
getTree(reallyaus_rf2, 1, labelVar = T)</code></pre>
<pre><code>##    left daughter right daughter        split var split point status
## 1              2              3       Gradabilty      3.0000      1
## 2              4              5 SemanticCategory      1.0000      1
## 3              6              7          Priming      1.0000      1
## 4              8              9     AudienceSize      1.0000      1
## 5             10             11           Gender      1.0000      1
## 6             12             13        Adjective     22.0000      1
## 7             14             15              Age      1.0000      1
## 8              0              0             &lt;NA&gt;      0.0000     -1
## 9              0              0             &lt;NA&gt;      0.0000     -1
## 10             0              0             &lt;NA&gt;      0.0000     -1
## 11            16             17         Function      1.0000      1
## 12            18             19        Adjective      2.0000      1
## 13            20             21             Freq      1.2295      1
## 14            22             23     Emotionality      2.0000      1
## 15             0              0             &lt;NA&gt;      0.0000     -1
## 16             0              0             &lt;NA&gt;      0.0000     -1
## 17            24             25        Adjective      8.0000      1
## 18            26             27              Age      3.0000      1
## 19            28             29        Adjective      4.0000      1
## 20            30             31         Function      1.0000      1
## 21            32             33     AudienceSize      1.0000      1
## 22            34             35             Freq      2.8177      1
## 23            36             37             Freq     11.6575      1
## 24             0              0             &lt;NA&gt;      0.0000     -1
## 25            38             39       Gradabilty      1.0000      1
## 26             0              0             &lt;NA&gt;      0.0000     -1
## 27             0              0             &lt;NA&gt;      0.0000     -1
## 28            40             41              Age      1.0000      1
## 29            42             43 ConversationType      1.0000      1
## 30            44             45     Emotionality      1.0000      1
## 31            46             47     AudienceSize      1.0000      1
## 32            48             49              Age      2.0000      1
## 33            50             51 SemanticCategory      2.0000      1
## 34             0              0             &lt;NA&gt;      0.0000     -1
## 35            52             53             Freq      5.7459      1
## 36            54             55     Emotionality      1.0000      1
## 37            56             57     AudienceSize      1.0000      1
## 38             0              0             &lt;NA&gt;      0.0000     -1
## 39            58             59             Freq      0.6844      1
## 40            60             61 ConversationType      1.0000      1
## 41            62             63     AudienceSize      1.0000      1
## 42             0              0             &lt;NA&gt;      0.0000     -1
## 43            64             65     AudienceSize      1.0000      1
## 44            66             67           Gender      1.0000      1
## 45            68             69     Emotionality      2.0000      1
## 46            70             71             Freq      0.7032      1
## 47            72             73 SemanticCategory      3.0000      1
## 48            74             75 ConversationType      1.0000      1
## 49             0              0             &lt;NA&gt;      0.0000     -1
## 50            76             77     Emotionality      2.0000      1
## 51             0              0             &lt;NA&gt;      0.0000     -1
## 52             0              0             &lt;NA&gt;      0.0000     -1
## 53             0              0             &lt;NA&gt;      0.0000     -1
## 54            78             79     AudienceSize      1.0000      1
## 55             0              0             &lt;NA&gt;      0.0000     -1
## 56             0              0             &lt;NA&gt;      0.0000     -1
## 57             0              0             &lt;NA&gt;      0.0000     -1
## 58             0              0             &lt;NA&gt;      0.0000     -1
## 59             0              0             &lt;NA&gt;      0.0000     -1
## 60             0              0             &lt;NA&gt;      0.0000     -1
## 61             0              0             &lt;NA&gt;      0.0000     -1
## 62             0              0             &lt;NA&gt;      0.0000     -1
## 63             0              0             &lt;NA&gt;      0.0000     -1
## 64            80             81           Gender      1.0000      1
## 65             0              0             &lt;NA&gt;      0.0000     -1
## 66             0              0             &lt;NA&gt;      0.0000     -1
## 67             0              0             &lt;NA&gt;      0.0000     -1
## 68            82             83 ConversationType      1.0000      1
## 69             0              0             &lt;NA&gt;      0.0000     -1
## 70             0              0             &lt;NA&gt;      0.0000     -1
## 71            84             85              Age      1.0000      1
## 72             0              0             &lt;NA&gt;      0.0000     -1
## 73             0              0             &lt;NA&gt;      0.0000     -1
## 74             0              0             &lt;NA&gt;      0.0000     -1
## 75             0              0             &lt;NA&gt;      0.0000     -1
## 76             0              0             &lt;NA&gt;      0.0000     -1
## 77             0              0             &lt;NA&gt;      0.0000     -1
## 78             0              0             &lt;NA&gt;      0.0000     -1
## 79            86             87 SemanticCategory      2.0000      1
## 80            88             89         Function      1.0000      1
## 81             0              0             &lt;NA&gt;      0.0000     -1
## 82            90             91             Freq      0.9945      1
## 83             0              0             &lt;NA&gt;      0.0000     -1
## 84            92             93           Gender      1.0000      1
## 85            94             95 SemanticCategory      8.0000      1
## 86             0              0             &lt;NA&gt;      0.0000     -1
## 87            96             97             Freq      1.6575      1
## 88             0              0             &lt;NA&gt;      0.0000     -1
## 89             0              0             &lt;NA&gt;      0.0000     -1
## 90            98             99 SemanticCategory      1.0000      1
## 91             0              0             &lt;NA&gt;      0.0000     -1
## 92             0              0             &lt;NA&gt;      0.0000     -1
## 93             0              0             &lt;NA&gt;      0.0000     -1
## 94             0              0             &lt;NA&gt;      0.0000     -1
## 95             0              0             &lt;NA&gt;      0.0000     -1
## 96             0              0             &lt;NA&gt;      0.0000     -1
## 97             0              0             &lt;NA&gt;      0.0000     -1
## 98             0              0             &lt;NA&gt;      0.0000     -1
## 99             0              0             &lt;NA&gt;      0.0000     -1
##    prediction
## 1        &lt;NA&gt;
## 2        &lt;NA&gt;
## 3        &lt;NA&gt;
## 4        &lt;NA&gt;
## 5        &lt;NA&gt;
## 6        &lt;NA&gt;
## 7        &lt;NA&gt;
## 8           1
## 9           0
## 10          0
## 11       &lt;NA&gt;
## 12       &lt;NA&gt;
## 13       &lt;NA&gt;
## 14       &lt;NA&gt;
## 15          1
## 16          0
## 17       &lt;NA&gt;
## 18       &lt;NA&gt;
## 19       &lt;NA&gt;
## 20       &lt;NA&gt;
## 21       &lt;NA&gt;
## 22       &lt;NA&gt;
## 23       &lt;NA&gt;
## 24          0
## 25       &lt;NA&gt;
## 26          1
## 27          0
## 28       &lt;NA&gt;
## 29       &lt;NA&gt;
## 30       &lt;NA&gt;
## 31       &lt;NA&gt;
## 32       &lt;NA&gt;
## 33       &lt;NA&gt;
## 34          1
## 35       &lt;NA&gt;
## 36       &lt;NA&gt;
## 37       &lt;NA&gt;
## 38          0
## 39       &lt;NA&gt;
## 40       &lt;NA&gt;
## 41       &lt;NA&gt;
## 42          1
## 43       &lt;NA&gt;
## 44       &lt;NA&gt;
## 45       &lt;NA&gt;
## 46       &lt;NA&gt;
## 47       &lt;NA&gt;
## 48       &lt;NA&gt;
## 49          0
## 50       &lt;NA&gt;
## 51          0
## 52          0
## 53          1
## 54       &lt;NA&gt;
## 55          0
## 56          1
## 57          0
## 58          0
## 59          0
## 60          1
## 61          1
## 62          1
## 63          0
## 64       &lt;NA&gt;
## 65          1
## 66          0
## 67          1
## 68       &lt;NA&gt;
## 69          0
## 70          0
## 71       &lt;NA&gt;
## 72          1
## 73          0
## 74          0
## 75          1
## 76          1
## 77          0
## 78          0
## 79       &lt;NA&gt;
## 80       &lt;NA&gt;
## 81          0
## 82       &lt;NA&gt;
## 83          0
## 84       &lt;NA&gt;
## 85       &lt;NA&gt;
## 86          0
## 87       &lt;NA&gt;
## 88          0
## 89          0
## 90       &lt;NA&gt;
## 91          0
## 92          0
## 93          1
## 94          0
## 95          1
## 96          0
## 97          1
## 98          1
## 99          1</code></pre>
<pre class="r"><code># mds plot
MDSplot(reallyaus_rf2, test$really)</code></pre>
<p><img src="advancedstatz_files/figure-html/unnamed-chunk-66-1.png" width="672" /></p>
</div>
<div id="example-2" class="section level2">
<h2><span class="header-section-number">6.2</span> Example 2:</h2>
<pre><code>##              Age        Adjective         Function          Priming 
##            0.001            0.005            0.012            0.000 
##           Gender ConversationType     AudienceSize             Freq 
##            0.002            0.003            0.002            0.011 
##       Gradabilty SemanticCategory     Emotionality 
##            0.000            0.004            0.001</code></pre>
<p><img src="advancedstatz_files/figure-html/unnamed-chunk-67-1.png" width="672" /></p>
<pre class="r"><code># load library
library(Hmisc)
# evaluate random forst
reallyaus.rf.pred &lt;- unlist(treeresponse(reallyaus.rf))[c(FALSE,TRUE)]
somers2(reallyaus.rf.pred, as.numeric(rfd$really) - 1)</code></pre>
<pre><code>##        C      Dxy        n  Missing 
##   0.8069   0.6138 314.0000   0.0000</code></pre>
<pre class="r"><code>##     C         Dxy           n     Missing 
##0.8119422   0.6238843 314.0000000   0.0000000 </code></pre>
</div>
<div id="example-3" class="section level2">
<h2><span class="header-section-number">6.3</span> Example 3:</h2>
<pre class="r"><code>#                     RANDOM FOREST III
# load library
library(party)
# create data
randomforestdata &lt;- reallyaus

cf1 &lt;- cforest(really ~ . , data= randomforestdata, control=cforest_unbiased(mtry=2,ntree=100)) # fit the random forest
varimp(cf1) # get variable importance, based on mean decrease in accuracy</code></pre>
<pre><code>##              Age        Adjective         Function          Priming 
##        0.0013913        0.0169565        0.0082609        0.0008696 
##           Gender ConversationType     AudienceSize             Freq 
##        0.0053043        0.0046087        0.0040870        0.0253043 
##       Gradabilty SemanticCategory     Emotionality 
##        0.0031304        0.0032174        0.0097391</code></pre>
<pre class="r"><code>varimp(cf1, conditional=TRUE) # conditional=True, adjusts for correlations between predict</code></pre>
<pre><code>##              Age        Adjective         Function          Priming 
##       0.00034783       0.00834783       0.00582609      -0.00191304 
##           Gender ConversationType     AudienceSize             Freq 
##       0.00295652       0.00069565       0.00252174       0.01139130 
##       Gradabilty SemanticCategory     Emotionality 
##       0.00008696       0.00008696       0.00086957</code></pre>
<pre class="r"><code>varimpAUC(cf1)  # more robust towards class imbalance.</code></pre>
<pre><code>##              Age        Adjective         Function          Priming 
##        0.0056743        0.0319284        0.0116915        0.0006288 
##           Gender ConversationType     AudienceSize             Freq 
##        0.0108261        0.0076496        0.0093993        0.0415649 
##       Gradabilty SemanticCategory     Emotionality 
##        0.0085987        0.0060204        0.0077188</code></pre>
<pre class="r"><code>par(mar = c(5, 8, 4, 2) + 0.1)
plot(y = 1:length(varimpAUC(cf1)), x = varimpAUC(cf1)[order(varimpAUC(cf1))], 
     axes = F, ann = F, pch = 20, xlim = c(-0.01, 0.05), main = &quot;Predictor Importance&quot;)
axis(1, at = seq(-0.01, 0.05, 0.005), seq(-0.01, 0.05, 0.005))
axis(2, at = 1:length(varimpAUC(cf1)), names(varimpAUC(cf1))[order(varimpAUC(cf1))], las = 2)
grid()
box()</code></pre>
<p><img src="advancedstatz_files/figure-html/unnamed-chunk-71-1.png" width="672" /></p>
<pre class="r"><code>par(mar = c(5, 4, 4, 2) + 0.1)</code></pre>
</div>
</div>
<div id="boruta" class="section level1">
<h1><span class="header-section-number">7</span> Boruta</h1>
<pre class="r"><code># load library
library(Boruta)
# create dada for boruta
borutadata &lt;- reallyaus
# run 1
boruta.ampaus &lt;- Boruta(really~.,data=borutadata)
print(boruta.ampaus)</code></pre>
<pre><code>## Boruta performed 99 iterations in 6.58 secs.
##  3 attributes confirmed important: Adjective, Freq, Function;
##  3 attributes confirmed unimportant: Age, Priming,
## SemanticCategory;
##  5 tentative attributes left: AudienceSize, ConversationType,
## Emotionality, Gender, Gradabilty;</code></pre>
<pre class="r"><code>getConfirmedFormula(boruta.ampaus)</code></pre>
<pre><code>## really ~ Adjective + Function + Freq
## &lt;environment: 0x000000002b5a4938&gt;</code></pre>
<pre class="r"><code>plot(boruta.ampaus, cex = .5)</code></pre>
<p><img src="advancedstatz_files/figure-html/unnamed-chunk-73-1.png" width="672" /></p>
<pre class="r"><code>plotImpHistory(boruta.ampaus)</code></pre>
<p><img src="advancedstatz_files/figure-html/unnamed-chunk-74-1.png" width="672" /></p>
<pre class="r"><code># remove superfluous variables
borutadata$Emotionality &lt;- NULL
borutadata$Priming &lt;- NULL
borutadata$Age &lt;- NULL
borutadata$SemanticCategory &lt;- NULL
# run2
boruta.ampaus &lt;- Boruta(really~.,data=borutadata)
print(boruta.ampaus)</code></pre>
<pre><code>## Boruta performed 99 iterations in 5.875 secs.
##  4 attributes confirmed important: Adjective, Freq, Function,
## Gradabilty;
##  No attributes deemed unimportant.
##  3 tentative attributes left: AudienceSize, ConversationType,
## Gender;</code></pre>
<pre class="r"><code>getConfirmedFormula(boruta.ampaus)</code></pre>
<pre><code>## really ~ Adjective + Function + Freq + Gradabilty
## &lt;environment: 0x000000002e171c18&gt;</code></pre>
<pre class="r"><code>plot(boruta.ampaus, cex = .75)</code></pre>
<p><img src="advancedstatz_files/figure-html/unnamed-chunk-76-1.png" width="672" /></p>
<pre class="r"><code>plotImpHistory(boruta.ampaus)</code></pre>
<p><img src="advancedstatz_files/figure-html/unnamed-chunk-77-1.png" width="672" /></p>
<pre class="r"><code>getConfirmedFormula(boruta.ampaus)</code></pre>
<pre><code>## really ~ Adjective + Function + Freq + Gradabilty
## &lt;environment: 0x00000000370a11b0&gt;</code></pre>
<pre class="r"><code>par(mar = c(10, 8, 4, 2) + 0.1)
plot(boruta.ampaus, cex.axis=.75, las=2, xlab=&quot;&quot;, ylab = &quot;&quot;, cex = .75, 
     col = c(&quot;grey50&quot;, &quot;grey50&quot;, &quot;grey50&quot;,  &quot;grey50&quot;, &quot;grey50&quot;, &quot;grey50&quot;, &quot;grey50&quot;,&quot;grey90&quot;,&quot;grey90&quot;,&quot;grey90&quot;))
abline(v = 3.5, lty = &quot;dashed&quot;)
mtext(&quot;Predictors&quot;, 1, line = 8, at = 7, cex = 1)
mtext(&quot;Control&quot;, 1, line = 8, at = 2, cex = 1)
mtext(&quot;Importance&quot;, 2, line = 2.5, at = 2.5, cex = 1, las = 0)</code></pre>
<p><img src="advancedstatz_files/figure-html/unnamed-chunk-79-1.png" width="672" /></p>
<pre class="r"><code>par(mar = c(5, 4, 4, 2) + 0.1)</code></pre>
<pre class="r"><code>plotImpHistory(boruta.ampaus)</code></pre>
<p><img src="advancedstatz_files/figure-html/unnamed-chunk-80-1.png" width="672" /></p>
</div>
<div id="references" class="section level1 unnumbered">
<h1>References</h1>
<div id="refs" class="references">
<div id="ref-achen1982interpreting">
<p>Achen, Christopher H. 1982. <em>Interpreting and Using Regression</em>. Vol. 29. Sage.</p>
</div>
<div id="ref-baayen2008analyzing">
<p>Baayen, R Harald. 2008. <em>Analyzing Linguistic Data. a Practical Introduction to Statistics Using R</em>. Cambridge: Cambridge University press.</p>
</div>
<div id="ref-bortz2006statistik">
<p>Bortz, JÃ¼rgen. 2006. <em>Statistik: FÃ¼r Human-Und Sozialwissenschaftler</em>. Springer-Verlag.</p>
</div>
<div id="ref-bowerman1990linear">
<p>Bowerman, Bruce L, and Richard T OâConnell. 1990. <em>Linear Statistical Models: An Applied Approach</em>. Boston: PWS-Kent.</p>
</div>
<div id="ref-crawley2005statistics">
<p>Crawley, Michael J. 2005. <em>Statistics: An Introduction Using R. 2005</em>. Chichester, West Sussex: John Wiley &amp; Sons.</p>
</div>
<div id="ref-crawley2012r">
<p>âââ. 2012. <em>The R Book</em>. John Wiley &amp; Sons.</p>
</div>
<div id="ref-faraway2002practical">
<p>Faraway, Julian J. 2002. <em>Practical Regression and Anova Using R.</em> University of Bath.</p>
</div>
<div id="ref-field2012discovering">
<p>Field, Andy, Jeremy Miles, and Zoe Field. 2012. <em>Discovering Statistics Using R</em>. Sage.</p>
</div>
<div id="ref-green1991many">
<p>Green, Samuel B. 1991. âHow Many Subjects Does It Take to Do a Regression Analysis.â <em>Multivariate Behavioral Research</em> 26 (3). Taylor &amp; Francis: 499â510.</p>
</div>
<div id="ref-gries2009statistics">
<p>Gries, Stefan Th. 2009. <em>Statistics for Linguistics Using R: A Practical Introduction</em>. Berlin &amp; New York: Mouton de Gruyter.</p>
</div>
<div id="ref-menard1995applied">
<p>Menard, Scott. 1995. <em>Applied Logistic Regression Analysis: Sage University Series on Quantitative Applications in the Social Sciences</em>. Thousand Oaks, CA: Sage.</p>
</div>
<div id="ref-myers1990classical">
<p>Myers, Raymond H. 1990. <em>Classical and Modern Regression with Applications</em>. Vol. 2. Duxbury Press Belmont, CA.</p>
</div>
<div id="ref-szmrecsanyi2006morphosyntactic">
<p>Szmrecsanyi, Benedikt. 2006. <em>Morphosyntactic Persistence in Spoken English: A Corpus Study at the Intersection of Variationist Sociolinguistics, Psycholinguistics, and Discourse Analysis</em>. Berlin &amp; New York: Walter de Gruyter.</p>
</div>
<div id="ref-wilcox2009basic">
<p>Wilcox, Rand R. 2009. <em>Basic Statistics: Understanding Conventional Methods and Modern Insights</em>. Oxford University Press.</p>
</div>
<div id="ref-zuur2010protocol">
<p>Zuur, Alain F., Elena N. Ieno, and Chris S. Elphick. 2010. âA Protocol for Data Exploration to Avoid Common Statistical Problems.â <em>Methods in Ecology and Evolution</em> 1 (1): 3â14.</p>
</div>
</div>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
