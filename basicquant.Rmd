---
title: "Basics of Quantitative Reasoning"
author: "UQ SLC Digital Team"
date: "`r format(Sys.time(), '%Y-%m-%d')`"
output:
  bookdown::html_document2: default
bibliography: bibliography.bib
link-citations: yes
---
```{r uq1, echo=F, fig.cap="", message=FALSE, warning=FALSE, out.width='100%'}
knitr::include_graphics("images/uq1.jpg")
```

# Introduction

Science can be defined as a systematic enterprise that builds and organizes knowledge in the form of testable explanations and predictions about the universe [@wilson1999science 58]. One of the most fundamental concepts in that definition is the concept of testable explanations. Another name for such explanations is "hypothesis". Thus, Edward Wilson's definition of science can be rephrased (somewhat crudely) as the methodological testing of hypotheses. This goes to show that hypotheses are at the very heart of the scientific endeveor and, in the fowllowing, we will try to understand what hypotheses are, how to formulate them, and  what logic underpins hypothesis testing. To begin with, we will focus on a practical example to avoid talking merely about abstract ideas. The example we will look at is the English comparative construction. 

# Primer: Comparatives in English

In English, the comparative forms of adjectives can be formed according to two strategies: either synthetically/morphologically as in [(1)](#1ex) or analytically/periphrastically as in [(2)](#2ex).

As a general rule, the comparative of adjectives that have only one syllable are formed by using the morphological strategy while adjectives that have three or more syllables are formed using the periphrastic strategy. However, in some cases where adjectives consist of two syllables, speakers may choose which strategy they apply. In our example, we want to find out, how to proceed when trying to understand the reasons why a speakers chooses the first strategy in one  case and the second strategy in another.   


 (1) synthetic/morphological comparative {#1ex}  
proud $\rightarrow$ prouder


 (2) analytic/periphrastic comparative {#2ex}  
proud $\rightarrow$ more proud


To investigate this phenomenon more closely, we should first determine which variables or factors influence which comparative strategy a speaker uses. To answer whcih factors afffect the comparative choice, we need to have a look at the respective literature. In the literature on the English comparative constructions the follwoing influencing factors ahve been named:

**Length of the adjective**

Adjectives that consits of a single syllable tend to form the comparative form via the morphological strategy  as in (\ref{3ex}) while multisyllabic adjectives tend to form the comparative via the periphrastic strategy as in (\ref{4ex}). 


(3) synthetic/morphological comparative: cool $\rightarrow$ cooler  {#3ex}

(4) analytic/periphrastic comparative: attractive $\rightarrow$ more attractive {#4ex}

**Syntactic Function**

Adjectives in attributive position prefer the morphological strategy, while adjectives in predicative position prefer the the periphrastic strategy.

(5) "*The prouder boy of the two was smiling*." {#5ex}

(6) "*The boy to the left was more proud*." {#6ex}


**Ending**

Adjectives which end in –ly or -y prefer the morphological strategy.

**Subsequent *than***

If a *than* follows the comparative, then the adjective prefers the morphological strategy as in (\ref{7ex}).

(7) "*This joke is funnier than the other one*." {#7ex}

It helps to create an overview table for the variables that have been shown in the literature to significantly affect the choice of the comparative strategy. Both better and worse examples of such overview tables are shown in [@gries2009statistics 27-30]. To answer our example question, we have to define the variables in order to formulate a proper hypothesis in the next step.

An example for such a hypothesis would, for instance, be "*If an adjectives has only one syllable, then a typical native speaker will prefer the morphological variant*". The next question then is how to test such a hypothesis and which concepts underly hypothesis testing. And these questions and issues are addressed below.

# Hypotheses

Probabaly the most important task in empirical research is hypothesis testing. A proper scientific hypothesis is commonly - but not neccessarily - a general assumption in the form of a statement. Hypotheses are tested by comparing systematic observation with the predictions of the hypothesis. More specifically, in order to test hypothesis one seeks for observations which contradict and are at odds with the hypothesis. If we find such a counter example and we have determined that it is an accurate observation, then the hypothesis is falsified, i.e. is is not correct. 

If we proposed the hypothesis "Apples always fall down." and we find an example of an apple not falling down, then our hypothesis would be falsified. 

> Discussion Time!
>
> Can you thnk of cases where apples do not fall down? How would we have to modify our hypothesis to accommodate potential counter-examples?


The fact that hypothesis must be falsifiable is a defining feature of hypotheses and it means that for a statement to be a hypothesis, it must be falsifiable (which does not mean that it must be false!).

The for trying to falsifying rather than prooving or validating hypothesis, lies in the act that falsification is possible while providing proof for an emirical fact is impossible: If we make only one observation which refutes a hypothesis, the hypothesis is falsified. No matter how many evidence we have for that hypothesis, the hypothesis remains falsified. It is therefore impossible to proove an empirical hypothesis! There are, however, statements that cannot be disproven or falsified - either for technical reasons (\@ref(exh3)) or because they are subjectiv  (\@ref(exh4)).


(1) There are forms of life in the Andromeda galaxy. {#exh3}

(2) I like chocolate ice cream better than vanilla ice cream. {#exh4}


Statements that cannot be falsified are called "speculation". Speculation is nothing bad or somehting worthless - on the contrary! - but they simply fall outside of the realm of empirical science. Exacmples for the creativity and the usefulness of specualtion are, for instance, art, literture, music, and philosophy. 

Summing up, hypotheses can be defined as possessing at least four criteria:

* Hypotheses are falsifiable statements about empirical reality.

* Hypothesen are testable statments about the empirical world.

* Hypothese are unambigious.

* Hypotheses are inherently consistent. 


Universality cannot be considered a defining feature of hypotheses, because it is - striktly speaking - not neccessary. For instance, we could formulate the hypothesis that a certain archeological model is correct, if we find certain artefacts at a specific place in a certain layer of earth. This hypothesis relates to a a very specific singular event but it would still be a falsifiable and testatble statement (and thus a hypothesis). 

## Types of Hypotheses

On a very fiúndamental level, we can differentiate between null-hypotheses (H$_{0}$), that claim non-existence of either a state of being or a difference, and alternative or test-hypothesis (H$_{1}$) that claim or postulate the existence of of either a state of being or a difference. Among test-hypotheses, we can furthermore distinguish between undirected hypotheses which claim that one sample is different from another sample, and directed hypotheses which claim that a feature of one sample is bigger, smaller, more frequent, or less frequent, etc. Thus, a hypothesis that group A will perform better in an exam is a diercted testhypothesis while an undirected hypothesis would merely claim that they differ in their test restults. In cotrast, the null-hypothesis would claim that there is no difference between the groups in terms of their performance in that exam. 

An additional distinction among hypotheses is the difference between deterministic and probabilistic hypotheses. While we are dealing with a determinitic hypotheses in  (10) because it is a categorical claim, we are dealing with a probabilistic hypothesis in (11) bcause, here, the hypothesis simply claims that the likelihood of Y is higher if X is the case (but not neccessarily categorically).

(10) If the length of two words in an English phrase is different, then the shorter word will always preceed the longer word. {#exh1}

(11) If the length of two words in an English phrase is different, then it is more likely for the shorter word to preceed the longer word than vice versa. {#exh2}

## Why Testing The Null-Hypothesis?!

Although it is counter-intuitive, we do not actually test the test-hypothesis but we test the null-hypothesis. We will now have  acloser lok at how to formulate hypotheses and that formulating hypotheses is formulating expected outcomes/explanations in a formal descritption.  

* Nullhypothesis (H_{0}) 
Groups A and B do not differ systematically! ($\mu$A = $\mu$B)

* Testhypothesis (H_{1}a)
Groups A and B differ systematically! ($\mu$A $\neq$ $\mu$B; undirected)

* Testhypothesis (H_{1}b)
Group A has significantly better results/higher levels of x compared with group B. ($\mu$A $>$ $\mu$B; directed)

What does that mean and what are we testing? In non-technical terms, we test how likely it is that the results came about by accident. If the probability is high (p > .05) that the results happen to be random, then we do not discard the H_0_. If the likelihood is low (p < .05) that the results came about randomly, then we discard the H_0_ and assume the H_1_ instead! To better understand this logic, we will discuss probabilities and their role in quantitative research.


## Exercises

1. Which of the follwoing sentences are hypotheses? Briefly explain your results!

* Smoking could have negative effects on one's health.

* Alcohol is a gateway drug.

* If alcohol is a gateway drug, then it should be criminalized.

* If alcohol is a gateway drug but tabacco is not, then a significantly higher proportion of drug addicts have consumed alcohol compared with the proportion of drug addicts who have smoked before taking drugs.

* Alcohol is a gateway drug, when/if it is illegal.

* Colorless green ideas sleep furiously.

* Nightingales dream in Italian.


2. What four characteristics do hypotheses have?

3. Come up with (a) three directed hypotheses and (b) three undirected hypotheses.

4. Oftentimes, it is not that easy to differentiate between hypotheses and other types of statements. Find a partner and come with statemenst that are not hypotheses and discuss why these statements are not hypotheses.

5. Find a partner and come up with statements that can be classified as both hypotheses and non-hypotheses and be prepared to explain your reasoning to the group.

# Significance and Probability

Hypothesis testing fundamentally builds on probabilites - or more precisely probabilities of error which is an estimation for the likelihood of the H~0~ being true given the data. This type of probability is typically providde in the form of p-values. In a more prosaic (and also coarse-grained, imprecise manner), p-values are an estimate of how likely an outcome is a result of chance. We will delve a little deeper into probabilities and how they relate to hypothesis testing below.

## Significance Levels

Before consucting a study, it is adviasble to determine the so-called significance or $\alpha$ level. This $\alpha$ level of significance Signifikanzniveau gibt an, wie hoch bzw. niedrig der p-Wert sein darf ohne dass man davon ausgehen muss, das kein signifikanter Zusammenhang zwischen den untersuchten Variablen vorliegt. Es ist hierbei gebräuchlich zwischen drei Stufen des $\alpha$ Signifikanzniveaus unterscheiden:

* p < .001: *highly significant*  - indicated by three stars (***)

* p < .01: *very significant* - indicated by two stars (**)

* p < .05: *significant* - indicated by one star (*)

As we stated above, before we perform a test, we determine a value above which we reject the null hypothesis, the so-called significance level. It's usually 5%. If the error probability is less than 5% (p <. 05), we reject the null hypothesis. Conclusion: The relationship between the variables is statistically significant. It is importnat to note here that the H~1~ (or Test Hypothesis) is correct only because the null hypothesis can be rejected! Statistics can NEVER prove hypotheses but only reject Null Hypotheses which leads us to accept the H~1~ as preliminary accepted or not-yet-rejected. So all knowledge is preliminar in the empirical sciences.

## Probability

In the following, we will turn to probability and try to understand why probabilty is relevant for testing hypotheses. This is important at this point because statistics, and thus hypothesis testing, fundamentally builds upon probabilies and probability distributions. In order to understand how probability works, we will investaigte what happens when we flip a coin. The first question that we will be addressing is "*What is the probablility of getting three Heads when flipping a coin three times?*".

The probablility of getting three heads when flipping a coin three times is .5 to the power of 3: .5^3^ = .5 times .5 times .5 = .125. The probability of getting Heads twice when flipping the coin three times is .375. How do we know?


The probability of getting 3 heads in tree tosses is 12.5 percent:

.5^3^ = .5 * .5 * .5 = .125

The probability of getting 2 heads in tree tosses is 37.5 percent:

.125 + .125 + .125 = 0.375

But how do we know this? Well, have  alook at the table below.

***

```{r eval=T, echo=F, message=FALSE, warning=FALSE, paged.print=FALSE}
library(knitr)
numbers <- matrix(c("Head" , "Head" , "Head" , "3" , "0" , "0.125", "Head" , "Head" , "Tails" , "2" , "1" , "0.125", "Head" , "Tails" , "Head" , "2" , "1" , "0.125", "Tails" , "Head" , "Head" , "2" , "1" , "0.125", "Head" , "Tails" , "Tails" , "1" , "2" , "0.125", "Tails" , "Head" , "Tails" , "1" , "2" , "0.125", "Tails" , "Tails" , "Head" , "1" , "2" , "0.125", "Tails" , "Tails" , "Tails" , "0" , "3" , "0.125"), byrow = T, nrow = 8)
colnames(numbers) <- c("1^st^ Toss","2^nd^ Toss","3^rd^ Toss","Heads", "Tails", "Probabilty")
kable(numbers, caption = "Probabilities of Heads and Tails in 3 Coin Tosses.", align=c(rep('c',times=4)))
```

 ***

Given this table, we are in fact, in a position to calculate the probability of getting 100 heads in 100 coin tosses because we can simply fill in the numbers in the formulas used above:  .5^100^ = 7.888609 * 10^-31^

Okay, let us make a bet..

* If head shows, I win a dollar.

* If tails shows, you win a dollar.

But given that you know I am cheeky bastard, you do not trust me and calim that I will cheat. But how will you know that I cheat? At which point can you claim that the result is so unlikely that you are (scientifically backed) allowed to claim that I cheat and have manipulated the coin?

```{r normal2, echo=F, message=FALSE, warning=FALSE}
Type <- c(rep("NormalCoin", 500000), 
            rep("ManipulatedCoin", 500000))
Tosser <- rep(paste("Tosser", 1:500000, sep = ""), 2) 
Frequency <- c(rnorm(500000, 50, 3),
               rnorm(500000, 53, 3))
particletable <- data.frame(Type, Tosser, Frequency)
library(ggplot2)
ggplot(particletable,aes(x=Frequency, fill = Type))+
  geom_histogram(aes(y=..density..), alpha=.2, , position = "identity")+
  theme_bw()+
  labs(y = "Density", x = "Distribution of <heads> in coin tosses with a normal coin and a manipulated coin.") + 
#  geom_density(alpha=.2) +  
#  theme(legend.position = "none") +         # surpress legend
  scale_fill_manual(values = c('gray10', 'gray50')) # define colours
```


So before we actually start with the coin tossing, you  operationalize your hypothesis: 

* H~0~: The author (I) is not cheating (heads shows just as often as tails).

* H~1~: The author (I) is cheating (heads shows so often that the probability of the author not cheating is lower than 5 percent)

We now toss the coin and head shows twice. The question now is whther head showing twice is lower than 5 percent. 

We toss the coin 3 times. Head shows twice. How likely is it that I do not cheat and head falls more than twice anyway? (In other words, what is the probability p that I win twice or more and not cheat?) If you set the significance level at .05, could you then accuse me of being a cheater? 

As you can see in the fourth column, there are three options that lead to heads showing twice (rows 2, 3, and 4). If we add these up (0.125 + 0.125 + 0.125 = 0.375). Also, we need to add the case where head shows 3 times which is another .125 (0.375 + 0.125 = .5), then we find out that the probabilty of heads showing at least twice in three coin tosses is 50 percent and thus 10 times more than the 5-percent threshold that we set initially. Therefore, you cannot claim that I cheated. 

```{r echo=F, eval = T, message=FALSE, warning=FALSE}
probdata1 <- matrix(c(0.125, 0.375, 0.375, 0.125), byrow = T, nrow = 1)
colnames(probdata1) <- c("0 Heads", "1 Time Head", "2 Times Heads", "3 Times Heads")
kable(probdata1, caption = "", align=c(rep('c',times=4)))
```

```{r echo=F, eval = T, message=FALSE, warning=FALSE}
probdata1 <- matrix(c(0.125, 0.375, 0.375, 0.125), byrow = T, nrow = 1)
colnames(probdata1) <- c("0 Heads", "1 Head", "2 Heads", "3 Heads")
x <- barplot(probdata1, ylim = c(0, 0.5), col = "lightgrey", main = "Probabilities of Frequency Counts of Heads in 3 Coin Tosses.")
text(seq(0.7, 4.3, 1.2), probdata1+0.05, probdata1)
```

Calculating the probabilities for three coin tosses is still managable manually but is there an easier way to calculate probabilities? A handier way is have a computer caluculate probabilities and the code below shows how to do that in `R` - a very powerful and flexible programming environment that has been designed for quantitative analysis (but `R` can, in fact, do much more - this website, for instance, is programmed in `R`).


```{r echo=T, eval = T, message=FALSE, warning=FALSE}
# probabilies of  0, 1, 2 and 3 times head in 3 coin tosses
dbinom(0:3, 3, 0.5)
```

```{r echo=T, eval = T, message=FALSE, warning=FALSE}
# probabilies of  2 or 3 times head in 3 coin tosses
sum(dbinom(2:3, 3, 0.5))
```


```{r echo=T, eval = T, message=FALSE, warning=FALSE}
# probabily of  100 times head in 100 coin tosses
dbinom(100, 100, 0.5)
```


```{r echo=T, eval = T, message=FALSE, warning=FALSE}
# probabily of  58 to a 100 times head in 100 coin tosses
sum(dbinom(58:100, 100, 0.5))
```


```{r echo=T, eval = T, message=FALSE, warning=FALSE}
# probabily of  59 to a 100 times head in 100 coin tosses
sum(dbinom(59:100, 100, 0.5))
```


```{r echo=T, eval = T, message=FALSE, warning=FALSE}
# at which point does the probability of getting head 
# dip below 5 percent in 100 coin tosses?
qbinom(0.05, 100, 0.5, lower.tail=FALSE)
```

```{r echo=F, eval = F, message=FALSE, warning=FALSE}
# at which point does the probability of getting head 
# dip below 5 percent in 100 coin tosses?
qnorm(0.05, lower.tail=TRUE)
```

In our example, we dealing with a directed hypothesis and not with an undirected hypothesis because we claimed in our H~1~ that I was cheating and would get more heads than would be expected by chance. For this reason, the test we use is one-tailed. When dealing with undirected hypotheses, you simply claim that the outcome is either higher or lower - in other words the test is two-tailed as you do not know in which direction the effect will manifest itself. 

To understand this a more thoroughly, we will consider tossing a coin not merely 3 but 100 times. The Figure below shows the probabilities for the number of heads showing when we toss a coin 100 from 0 occurrences to 100 occurrences.

```{r echo=F, eval = T, message=FALSE, warning=FALSE}
# load packages
library(grDevices)
# set up data
p100 <- dbinom(0:100, 100, 0.5)
w100 <- c(0:100)
wtb <- rbind(w100, p100)
colnames(wtb) <- c(0:100)
rownames(wtb) <- c("anzahl (kopf)", "prob")
xseq <- seq(0.7, 120.7, 1.2)
xseq <- xseq[c(1, 20, 40, 60, 80, 100)]
# start plotting
barplot(wtb[2,], ylim = c(0, 0.1), ylab = "Probability", xlab = "Frequency Count (Head)",
  axes = F, axisnames = F, col = "lightgrey")
axis(1, at= xseq, labels= c("0", "20", "40", "60", "80", "100"))
axis(2, at= seq(0.0, 0.1, 0.05), labels= c("0.00", "0.05", "0.10"), cex = .75)
box()
```

The next Figure shows at which number of heads the cumulative probabilities dip below 5 percent for two-tailed hypotheses. According to the graph, if head shows up to 40 or more often than 60 times, the cumulative probability dips below 5 perent. Applied to our initial bet, you could thus claim that I ceated if head shows less than 41 times or more than 60 times (if out hypothesis were two-tailed - which it is not). 

```{r echo=F, eval = T, message=FALSE, warning=FALSE}
# summ probs
sp <- cumsum(wtb[2,])
ttsp <- as.vector(unlist(sapply(sp, function(x){
  x <- ifelse(x <= 0.025, TRUE,
    ifelse(x >= 0.975, TRUE, FALSE))
    })))
barplot(wtb[2,], ylim = c(0, 0.1), ylab = "Probability", xlab = "Frequency Count (Head)",
  axes = F, axisnames = F, , col = ifelse(ttsp == T, "red", "lightgrey"))
axis(1, at= xseq, labels= c("0", "20", "40", "60", "80", "100"))
axis(2, at= seq(0.0, 0.1, 0.05), labels= c("0.00", "0.05", "0.10"), cex = .75)
box()
#grid()
text(75, 0.06, expression(paste(mu^1 !=  mu^2, sep = "")))
```

The Figure below shows at which point the probability of heads showing dips below 5 percent for one-tailed hypotheses. Thus, according to the Figure below, if we toss a coin 100 times and head shows 59 or more often, then you are justified in claiming that I cheated.

```{r echo=F, eval = T, message=FALSE, warning=FALSE}
ttsp <- as.vector(unlist(sapply(sp, function(x){
  x <- ifelse(x <= 0.95, TRUE, FALSE)
    })))
barplot(wtb[2,], ylim = c(0, 0.1), ylab = "Probability", xlab = "Frequency Count (Head)",
  axes = F, axisnames = F, col = ifelse(ttsp == T, "lightgrey", "red"))
axis(1, at= xseq, labels= c("0", "20", "40", "60", "80", "100"))
axis(2, at= seq(0.0, 0.1, 0.05), labels= c("0.00", "0.05", "0.10"), cex = .75)
box()
#grid()
text(75, 0.06, expression(paste(mu^1 >=  mu^2, sep = "")))
```


When compaing the two Figures above, it is notable that the number at which you can claim I cheated differs according to whether the H~1~ as one- or two-tailed. When formulating a one-tailed hypothesis, then the number is lower compared with the the number at which you can reject the H~0~ if your H~1~ is two-tailed. This is actually th reason for why it is  preferable to formulare more precise, one-tailed hypotheses ( as then, it is easier for the data to be sufficient to reject the H~0~). 


## The Normal Distribution

It is important to note here that the above described calculation of probabilities does not work for numeric variables that are intervall-scaled. The reason for this is that it is not possible to calculate the probabilities for all possible outcomes of a reaction time experiment. In such cases, we rely on distribution (typically the normal distribution) in order to detremine how likely or probable a certain outcome is. When relying on distributions, we determine whether a certain values falls within or outside of the area of a distribution that accounts for 5 percent of the entire area of the distribution - if it falls within the area that accounts for less tha  5 percent of the total area, then the resul is called statistically significant (see the normal distribution below).


```{r echo=F, eval = T, message=FALSE, warning=FALSE}
# load packages
library(grDevices)
library(graphics)
### normal distribution: Mu=0, Sigma=1: Standard normal
library(graphics)
plot(dnorm, -4, 4, axes = F, xlab = "Standard Deviations", ylab = "Probability")
axis(1, at = seq(-4, 4, 1), labels = seq(-4, 4, 1))
axis(2, at= c(0.0, 0.2, 0.4), labels= c("0.0", "0.2", "0.4"))
text(3, 0.2, expression(paste(mu, "=0, ", sigma^2, "=1", sep = "")))
box()
lines(c(0, 0), c(-1, 0.5), col = "lightgrey")
lines(c(-0.675, 0.675), c(rep(0.3, 2)), col="red", lty=2)
text(0, 0.35, "50.0%")
lines(c(-1.96,1.96), c(rep(0.2, 2)), col="blue", lty=2)
text(0, 0.25, "95%")
lines(c(-2.576,2.576), c(rep(0.1, 2)), col="green", lty=2)
text(0, 0.15, "99%")
lines(c(-0.675, -0.675), c(-1, 0.5), col = "red")
lines(c(0.675, 0.675), c(-1, 0.5), col = "red")
lines(c(-1.96, -1.96), c(-1, 0.5), col = "blue")
lines(c(1.96, 1.96), c(-1, 0.5), col = "blue")
lines(c(-2.576, -2.576), c(-1, 0.5), col = "green")
lines(c(2.576, 2.576), c(-1, 0.5), col = "green")
text(-2.9, 0.025, "-2.576", cex = .75)
text(-1.6, 0.025, "-1.96", cex = .75)
text(-1, 0.025, "-0.675", cex = .75)
text(2.9, 0.025, "2.576", cex = .75)
text(1.7, 0.025, "1.96", cex = .75)
text(1, 0.025, "0.675", cex = .75)
```

The normal distribution (or Gaussian curve or Gaussian distribution) shown in the Figure above has certain characteristics that can be derived mathematically. Some of these characteristcs relate to the area of certain sections of that distribution and the mean, median, and mode are identical (and are at the value of the highest point of the normal distribution).

In addition, 50 percent of the total area under the curve are to left and 50 percent of the right of the mean value. Furthermore, 68 percent of the area are within -1 and +1 standard devaitions from the mean; 95 percent of the area lie between -2 and +2 standard devaitions from the mean;  99.7 percent of the area lie between -3 and +3 standard devaitions from the mean. 

In addition, 5 percent of the area lie outside -1.96 and +1.96 standard devaitions from the mean (if these areas are combined) (see the Figure below).


```{r echo=F, eval = T, message=FALSE, warning=FALSE}
plot(dnorm, -4, 4, axes = F, xlab = "Standard Deviations", ylab = "Probability")
axis(1, at = seq(-4, 4, 1), labels = seq(-4, 4, 1))
axis(2, at= c(0.0, 0.2, 0.4), labels= c("0.0", "0.2", "0.4"))
text(3, 0.2, expression(paste(mu, "=0, ", sigma^2, "=1", sep = "")))
box()
lines(c(1.96, 1.96), c(0, 0.5), col="lightgrey", lty=2)
text(3, 0.15, "2.5%")
text(3, 0.085, "(sd=1.96)")
lines(c(-1.96, -1.96), c(0, 0.5), col="lightgrey", lty=2)
text(-3, 0.15, "2.5%")
text(-3, 0.085, "(sd=-1.96)")
```

Finally, 5 percent of the area lies beyond +1.68 standard devaitions from the mean (see the Figure below).    

```{r echo=F, eval = T, message=FALSE, warning=FALSE}
plot(dnorm, -4, 4, axes = F, xlab = "Standard Deviations", ylab = "Probability")
axis(1, at = seq(-4, 4, 1), labels = seq(-4, 4, 1))
axis(2, at= c(0.0, 0.2, 0.4), labels= c("0.0", "0.2", "0.4"))
text(3, 0.2, expression(paste(mu, "=0, ", sigma^2, "=1", sep = "")))
box()
lines(c(1.64, 1.64), c(0, 0.5), col="lightgrey", lty=2)
text(3, 0.15, "5% (sd=1.64)")
```

These properties are extremely useful when determining the likelihood of values or outcomes that reflect certain intervall-scalled variables. 

> Exercises
>
> Create a table with the possible outcomes and probabilities of 4 coin tosses (you can consider the table showing the outcomes of three coin tossse above as a guideline).

```{r echo=F, eval = F, message=FALSE, warning=FALSE}
sum(dbinom(4, 7, 0.5))
```

> How likely is it for heads to show exactly 3 times when tossing a coin 7 times?

```{r echo=F, eval = F, message=FALSE, warning=FALSE}
sum(dbinom(3, 7, 0.5))
```

>  How likely is it for heads to show exactly 2 or 5 times when tossing a coin 7 times?

```{r echo=F, eval = F, message=FALSE, warning=FALSE}
sum(dbinom(c(2, 5), 7, 0.5))
```

> How likely is it for heads to show 5 or more times when tossing a coin 7 times?

```{r echo=F, eval = F, message=FALSE, warning=FALSE}
sum(dbinom(5:7, 7, 0.5))
```

>  How likely is it for heads to show between 3 and 6 times when tossing a coin 7 times?

```{r echo=F, eval = F, message=FALSE, warning=FALSE}
sum(dbinom(3:6, 7, 0.5))
```

# Alpha and Beta Errors

One practice that unfortunetely still very frequent, which is a very serious problem in data analysis, and which has led to the development of multivariate techniques is the increase of the error rates in multiple or repeated testing. 

We have stated before that we usually assume a significance level of 5%. However, this also means that, on average, every 20^th^ test result, which has a significance value of .05, is misinterpreted because, on average, one out of 20 results shown to be significant *is actually not caused by a real effect but the result of normally distributed probabilities and a fixed significance level*. If we perform several tests, the probability that we obtain a significant result for soemthing which is, in fact, not significant adds up and incerases exponetntially. Indeed, even with only four tests the likelihood of a significant result in the test - altought there is in actually no difference - is 18.5%! This incerase in error rates can by easily calculated with formula below.


\begin{equation}

1 - .95^{n} = error

\label{eq:inflatederrors}

\end{equation}

\begin{equation}

1 - .95^{4} = 1 - 0.814 = 0.185

\label{eq:inflatederrorsbsp}

\end{equation}

We will return to this later, but first we will look at different types of errors.

One differentiates between $\alpha$- (or alpha-) and $\beta$ (or beta-) errors. $\alpha$ errors represent a situation in which a test reports a significant effect althought there is no effect in empirical reality. $\beta$ errors reflect a situation in which a test does not report a significant effect although there is one in empirical reality (see Table below).

```{r echo=F, message=FALSE, warning=FALSE}
#install.packages("knitr")      # install library (remove # to activate)
#install.packages("kableExtra") # install library (remove # to activate)
library(knitr)
library(kableExtra)
errortb <- matrix(c("", "alpha-error" , "beta-error", ""), byrow = F, nrow = 2)
colnames(errortb) <- c("Correlation", "No Correlation")
rownames(errortb) <- c("**Correlation**", "**No Correlation**")
library(knitr)                 # activate library for tabulating
kable(errortb, caption = "Alpha- and beta-errors.", booktabs = T)%>%
  kable_styling("striped", "bordered") %>%
  add_header_above(c("Reality" = 1, "Test" = 2))
```

Regarding the difference between $\alpha$ and $\beta$ errors, it can be said that $\beta$ errors are generally to be preferred, as they merely state that, based on the data, it can not be assumed that X or Y is the case, while $\alpha$ errors do not false statements become part of recognized knowledge. As a rule of thumb, more conservative and conservative behavior is less problematic in terms of science theory, and thus $\alpha$ rather than $\beta$ errors should be avoided.

Now that we have clarified what types of errors exist and that errors accumulate, we will examine a related concept: *Independence*.

# Independence

If errors would always add up, then statistics would not be possible, since every new test would have to take all previous tests into account. This is obviously absurd and cannot be the case. The question now arises about what determines, if errors accumulate or not? The answer is called *independence*.

If tests are independent of one another, then their errors do not accumulate. However, if they are related to each other, then the errors add up. A word of caution is in order here as the concept of independence in statistics has a different meaning from everyday use: in statistics, independence means the independence of hypotheses. In the case of specifications of more general hypotheses, the specified hypotheses are not independent of the general hypothesis and are not independent of the other specified hypotheses. In other words, if we test several specific hypotheses in a more generalized hypothesis, then the hypotheses are not strictly independent and cannot be treated that way. If we formulate two hypotheses that are not conceptually linked or one hypothesis is not derived from the other, then these hypotheses are independent. 

We encounter a related phenomenon when dealing with extensions of $\chi$^2^ test: the reason why we could not calculate the ordinary Pearson's $\chi$^2^ test when focusuing on subtables derived from larger tables is that the data of represented in the subtable is not independent from the other obsrevations summarized in the larger table. 

When we performed the ordinary Pearson's $\chi$^2^ test we tested whether emotion metaphors are realized differently across registers. The subsequent test built on the results of that frist test and zoomed in on a specified hypothesis (namely that two specific types of metaphors would differ in two specific registers). Therefore, we are dealing with two hypotheses, the second hypothesis being a specification of the first hypothesis. This means that the hypotheses were related and not independent. Consequently, the errors would have added up if we had not considered that not only the part table was extracted from the data, but we wanted to test a part table of a larger table.

A second and perhaps more important feature of independence is that independent variables must not be correlated. If they are, however, this is called *multicollinearity* (more on this when we look at multiple regression).

# Corrections

Now that we know about error accumilation and issues of independence, how can we test multiple hypotheses (simultaneously)? One option is to use multivariate methods, as we will see in the section on "Advanced Statistics". Another option is to incorporate corrections to ensure that the $\alpha$-level remains at 5% even with repeated or multiple testing.

The best known and probably the most widely used correction is the *Bonferroni* correction, where the $\alpha$-level is divided by the number of tests. For example, if we perform 4 tests, then the $\alpha$-level is lowered to .05 / 4 = .0125 so that the $\alpha$-level of the four tests returns to the usual 5% level. The disadvantage of this correction is that it is more conservative and therefore leads to a relatively high $\beta$-error rate.

Other common corrections are the Holm and the Benjamini-Hochberg corrections. However, we will not discuss them here. The interested reader is referred to @field2012discovering, pp. 429-430. 

# What To Do With Non-Normal Data

We have now a basic understanding of the normal distribution and its characterists. The normal distribution is relevant in statistics because tests build on assumptions that involve the distributions of variables or residuals. If the data is not distributed normally but the test you want to use assumes and therefore requires normality, then there is a problem. In this section, we will have a look at ways to "nromalize" data. In this context, normalization refers to transformations of data that cause the data to exhibit different distributional characteristsics (e.g. being more normal).

Let us have a look at an example to clarify what is meant here and what can be done when dealing with non-normal data. In a first step, we will create some data and test whether it is "nromal" or not. The  data represents the frequency of discourse particles differs across two corpora  that represent the speech of the same fifty speakers (see the Figure below).

```{r normal1, echo=F, message=FALSE, warning=FALSE}
Corpus <- c(rep("Corpus1", 50), 
            rep("Corpus2", 50))
Speaker <- rep(paste("Speaker", 1:50, sep = ""), 2) 
Frequency <- c(rnorm(50, 15, 3),
               rnorm(40, 15, 3),
               rnorm(10, 30, 3))
particletable <- data.frame(Corpus, Speaker, Frequency)
library(ggplot2)
ggplot(particletable,aes(x=Frequency, fill = Corpus))+
  geom_histogram(aes(y=..density..))+
  facet_grid(~Corpus)+
  theme_bw()+
  geom_density(alpha=.5) +  
  theme(legend.position = "none") +         # surpress legend
  scale_fill_manual(values = c('indianred4', 'gold')) # define colours
```

In a first setp, we test wehther the frequencies of the discourse particles are distributed normally in the two corpora. The test we use here is called the Shapiro-Wilk test and it determines the likelihood with which a given sample is drawn from normally distributed population. If the Shapiro-Wilk test reports a significant result, then this means that the  samle is significantly non-normal und unlikely to be drawn from nromally distributed popualtion. 

```{r testnromality, echo=F, eval = T, message=FALSE, warning=FALSE}
# test for normality
# anmerkungen
c1 <- particletable$Frequency[particletable$Corpus == "Corpus1"]
c2 <- particletable$Frequency[particletable$Corpus == "Corpus2"]
shapiro.test(c1) # p > .05 = normal
shapiro.test(c2) # p < .05 = non-normal
```

Accordign to the Shapiro-Wilk test, the distribution in Corpus 1 is normal while the distribution in Corpus 2 is non-normal. It important to mention here that the Shapiro-Wilk test is inexact for small samples (N < 60) but to rigorous for large samples (N > 200).

One issue that we face now that we have confirmed that the distribution in Corpus 2 is non-normal relates to the description of the data. If data are non-normal, then the usual descriptive statistics (mena, median, stadard deviattion) are not really appropriate because they either assume normality (mena and standard deviation) or they are unfit to report non-normal structure). Therefore, when dealing with non-normal data, it is common to use alternative descriptive statistics.

```{r summarizenromality, echo=F, eval = T, message=FALSE, warning=FALSE}
summary(c2)                       # summary for non-normal data

quantile(c2, c(0.25, 0.5, 0.75))  # quantiles

range(c2)                         # extract range (lowest/highest value)

mad(c2, constant = 1)             # median absolute deviation (median distance from median)

IQR(c2)                           # interquartile range
```


```{r createnormality, echo=F, eval = T, message=FALSE, warning=FALSE}

# möglichkeit 1
mydata <- read.delim("data/data01.txt", sep="\t", header = T)

# inspect data
head(mydata)
```

```{r plotnonnormal, echo=F, eval = T, message=FALSE, warning=FALSE}

# 4 plots in one window (2 rows and 2 columns)
par(mfrow=c(2,2))
# plot word.count (histogram)
hist(mydata$word.count)

# plot word.count (density plot)
plot(density(mydata$word.count))

# plot word.count (quantile-quantile plot)
qqnorm(mydata$word.count)

# add line showing normal distribution)
qqline(mydata$word.count)

# boxplot of data
boxplot(mydata$word.count) # dots in boxplot show outliers

# 1 plot per window
par(mfrow=c(1,1))
```

One way to deal with non-normal data is to transform the data. Here are selected rules of thumb for data transformations that are particularly useful when dealing with language data.

* If the data is moderatly positively skewed, the best option is to take the square root of the dependent varianble: sqrt(DepVar). 
* If the data is moderatly negatively skewed, the best option is to square the dependent varianble:   (DepVar)^2.
* If the data is J shaped, the best option is to transform the depenedent variable in the follwoing way:
1/(DepVar + 1).

```{r testnonnormal, echo=F, eval = T, message=FALSE, warning=FALSE}


# WARNING: data is positively skewed!
# test for normality (confirm non-normality)
shapiro.test(mydata$word.count)
```

```{r boxplotwords, echo=F, eval = T, message=FALSE, warning=FALSE}
# remove outliers
# find outliers (boxplot)
boxplot(mydata$word.count) # dots in boxplot show outliers
```

```{r outlierswords, echo=F, eval = T, message=FALSE, warning=FALSE}

# find outliers (z-value based)
wc <- scale(mydata$word.count, center = TRUE, scale = TRUE)
# remove values that exceed 1.96 (95%) sds
nrow(mydata)

mydataWoOutliers <- mydata[-c(which(wc >= 1.96 | wc <= -1.96)),]

nrow(mydataWoOutliers)

```

```{r boxplotwords2, echo=F, eval = T, message=FALSE, warning=FALSE}


boxplot(mydataWoOutliers$word.count)
```

```{r diagnosticplots, echo=F, eval = T, message=FALSE, warning=FALSE}

# transform variable (add 1 to value to avoid values of 0 (log(0) = -inf) and then log them
mydata$logwc <- log1p(mydata$word.count) # log (values +1)
mydata$sqrtwc <- sqrt(mydata$word.count) # take square root of data
# inspect logged data
par(mfrow=c(2,2))            # 4 plots in one window (2 rows and 2 columns)
hist(mydata$logwc)           # plot word.count (histogram)
plot(density(mydata$logwc))  # plot word.count (density plot)
qqnorm(mydata$logwc)         # plot word.count (quantile-quantile plot)
qqline(mydata$logwc)         # add line showing normal distribution)
boxplot(mydata$logwc)        # boxplot of data, dots in boxplot show outliers
par(mfrow=c(1,1))            # 1 plot per window
```

```{r retestnormality, echo=F, eval = T, message=FALSE, warning=FALSE}

# test for normality
shapiro.test(mydata$logwc)  # logging not appropriate here
```

```{r plotnormality, echo=F, eval = T, message=FALSE, warning=FALSE}

# inspect square root transformed data
par(mfrow=c(2,2))            # 4 plots in one window (2 rows and 2 columns)
hist(mydata$sqrtwc)          # plot word.count (histogram)
plot(density(mydata$sqrtwc)) # plot word.count (density plot)
qqnorm(mydata$sqrtwc)        # plot word.count (quantile-quantile plot)
qqline(mydata$sqrtwc)        # add line showing normal distribution)
boxplot(mydata$sqrtwc)       # boxplot of data, dots in boxplot show outliers
par(mfrow=c(1,1))            # 1 plot per window
```

```{r retestnormality2, echo=F, eval = T, message=FALSE, warning=FALSE}


# test for normality
shapiro.test(mydata$sqrtwc)  # square root transforming appropriate here



data01 <- mydata # save data fro later use

```

# Bayes Theorem

Bayes theorem is a different approach to statistics that does not rely on the normal distribution but on conditional probabilities. 

To exemplify how Bayes theorem works, we will use a simple example. The example involves the probability of being diagnosed with cancer given the probability of the test being wrong.

Imagine the probability of having cancer is 1 percent. Among 1,000 people, 10 would actually have cancer. However, the test that is used to determine if a person has cancer has an error rate of 10 percent. This means that the test is wrong is 10 percent of cases. If 1000 people took the test, this means that 10 out of these 1000 people would be expected to have cancer but the test would inidcate that 100 people had cancer.

It is notable that Bayes theorem takes into account that there is a difference between an actual fact (a person having cancer) and the result of a test (th etest reports that a person has cancer). This means that in application of Bayes therem, we consider the chance of false results.

```{r eval=T, echo=F, message=FALSE, warning=FALSE, paged.print=FALSE}
library(knitr)
bayestable <- matrix(c("Correct","False positive","False negative","Correct"), byrow = T, nrow = 2)
colnames(bayestable) <- c("Reality: Cancer", "Reality: NoCancer")
rownames(bayestable) <- c("Test: Cancer", "Test: NoCancer")
kable(bayestable, caption = "", align=c(rep('c',times=4)))
```

Now, applied to our example, this would mean the following.

```{r eval=T, echo=F, message=FALSE, warning=FALSE, paged.print=FALSE}
library(knitr)
bayestable <- matrix(c("90%","10.9%","10%","89.1%"), byrow = T, nrow = 2)
colnames(bayestable) <- c("Reality: Cancer", "Reality: NoCancer")
rownames(bayestable) <- c("Test: Cancer", "Test: NoCancer")
kable(bayestable, caption = "", align=c(rep('c',times=4)))
```

The table above can be read as follows:

* 1% of people have cancer
* If the patient actually has cancer, the patient is in the first column. The test will detect cancer with a 90% accuracy, i.e. the test reports that the patient has cancer in 90% of cases. But in 10% of cases, the test will report "no cancer"" although the patient does, in fact, have cancer.
* If the patient does not have cancer, the patient is in the second column. There’s a 10.9% chance that the test will report that you have cancer, and a 89.1% chance that the test will report a negative result.

Imagine now, that the test reports that a patient has cancer. What is probability that the patient actually has cancer?

Because the test reports cancer, the patient is in the top row of the table. However, it could be a true positive or a false positive. The chances of a true positive (the patient has in deed cancer and the tests reports it) are = 1% \times 90% = .09%. The chances of a false positive (the patient does not have cancer but the test reports that the patient has cancer) are = 99% \times 9.6% = 0.081 (8%). 


# References
