---
title: "Basic Inferential Statistics"
author: "UQ SLC Digital Team"
date: "`r format(Sys.time(), '%Y-%m-%d')`"
output: html_document
---
```{r uq1, echo=F, fig.cap="", message=FALSE, warning=FALSE, out.width='100%'}
knitr::include_graphics("images/uq1.jpg")
```

# Introduction
This tutorial introduces basic statistical techniques from inferential statistics for hypothesis testing. The first part of this section focuses on basic non-parametric  tests such as the chi-square family of tests while the second part introduces simple regression models and discusses their underlying logic. 

# Non-Parametric Tests
This section focuses on test that do not require the data to be distributed normally. Tests that do not rewuire normal data are referred to as *non-parametric tests* (test that require the data to be distributed normally are analogously called *parametric tests*). We focus on non-parametric tests first as this family of test in frequently used in linguistics. In the later part of this section, we will focus on regression modelling where assumptions of about the data become more important. 

## Pearsons's Chi-Square Test

To explore how chi-square tests (or Pearsons's chi-square tests, chi-squared tests, or simply $\chi^{2}$ tests) work, we will focus on a practicla example. In this example, we will test whether speakers of American English (AmE) and speakers of British English (BrE) differ in their use of the near-synonyms *sort of* and *kind of* as in "*He's sort of stupid*" and "*He's kind of stupid*". As a first step, we formulate the hypothesis that we want to test (H$_{1}$) and its Nullhypothesis (H$_{0}$). The Alternativ- or Testhypothesis reads:

H$_{1}$: Speakers of AmE and BrE differ with respect to their preference for *sort of* und *kind of*.

while the Nullhypothesis (H$_{0}$) states 

H$_{0}$ Speakers of AmE and BrE do not differ with respect to their preference for *sort of* und *kind of*.

The H$_{0}$ claims the non-existence of something (which is the more conservative position) and in our example the non-existence of a correlation between variety of English and the use of  *sort of* und *kind of*. The question now arises what has to be the case in order to reject the H$_{0}$ in favor of the H$_{1}$.

To answer this question, we require information about the probability of error, i.e. the probability that the H$_{0}$ does indeed hold for the entire population. Before performing the chi-square test, we follow the convetion that the required significance level is 5 percent. In other words, we will reject the H$_{0}$ if the likelyhood for the H$_{0}$ being true is less than 5 percent given the distribution of the data. In that case, i.e. in case that the likelihood for the H$_{0}$ being true is less than 5 percent, we consider the result of the  chi-square test as  statistically significant. This means that the observed distribution makes it very unlikey that there is no correlation between the variety of English and the use of  *sort of* and *kind of*.

Let us now assume that we have performed a search for *sort of* and *kind of* in two corpora representing American and British English and that we have obtained the following freqeuncies:

```{r echo=F, message=FALSE, warning=FALSE}
library(knitr)
chidata <- matrix(c(181, 655, 177, 67), nrow = 2, byrow = F)
# add column and row names
colnames(chidata) <- c("BrE", "AmE")
rownames(chidata) <- c("kindof", "sortof")
kable(chidata, caption = "Observed frequencies of *sort of* and *kind of* in American and British English")
```

In a  first step, we now have to caluculate the row and column sums of our table.

I had the same question. I have tried all solutions provided above and none of them worked... But I have found a solution that works for me, and hopefully for others too.


```{r echo = F, results = 'asis'}
library(knitr)
chidata_extended <- matrix(c(181, 177, 358, 655, 67, 722, 836, 244, 1080), nrow = 3, byrow = F)
# add column and row names
colnames(chidata_extended) <- c("BrE", "AmE", "Total")
rownames(chidata_extended) <- c("kindof", "sortof", "Total")
kable(chidata_extended, caption = "Observed frequencies of *sort of* and *kind of* in American and British English with row and column totals")
```

Next, we calculate, the values that would have expected if there was no correlation between variety of English and the use of  *sort of* and *kind of*. In order to get these "expected" freqeuncies, we apply the equation below to all cells in our table.

$\frac{Column total*Row total}{Overall total}$

In our example this means that for the cell with [+]BrE [+]kindof we get:

$\frac{836*358}{1080} = \frac{299288}{1080} = 277.1185$

For the entire table this means we get the following expected values:

```{r echo = F, results = 'asis'}
library(knitr)
chidata_expected <- matrix(c(277.1185, 80.88148, 358, 558.8815,163.11852, 722, 836, 244, 1080), nrow = 3, byrow = F)
# add column and row names
colnames(chidata_expected) <- c("BrE", "AmE", "Total")
rownames(chidata_expected) <- c("kindof", "sortof", "Total")
kable(chidata_expected, caption = "Expected frequencies of *sort of* and *kind of* in American and British English with row and column totals")
```


In a next step, we calculate the contribution of each cell to the overall $\chi^{2}$ value ($\chi^{2}$ contribution). To get $\chi^{2}$ contribution for each cell, we apply the equation below to each cell.

$\frac{(observed – expected)^{2}}{expected}$

In our example this means that for the cell with [+]BrE [+]kindof we get:

$\frac{(181 – 277.1185)^{2}}{277.1185} = \frac{-96.1185^{2}}{277.1185} = \frac{9238.766}{277.1185} = 33.33868$


For the entire table this means we get the following $\chi^{2}$ values:

```{r echo = F, results = 'asis'}
library(knitr)
chidata_chi <- matrix(c(33.33869, 114.22602, 147.5647, 16.53082, 56.63839, 73.16921, 49.86951, 170.8644, 220.7339), nrow = 3, byrow = F)
# add column and row names
colnames(chidata_chi) <- c("BrE", "AmE", "Total")
rownames(chidata_chi) <- c("kindof", "sortof", "Total")
kable(chidata_chi, caption = "Chi values of *sort of* and *kind of* in American and British English with row and column totals")
```

The sum of $\chi^{2}$ contributions in our example is 220.7339. To see if this value is staistically significant, we need to calculate the degrees of freedom because the $\chi$ distribution differes across degrees of freedom.  Degrees of freedom are calculated accroding to the equation below.

$DF = (rows -1) * (columns – 1) = (2-1) * (2-1) = 1 * 1 = 1$

In a last step, we check whether the $\chi^{2}$ value that we have calculated is higher thana critical value (in which case the correlation in our table is significant). Degrees of freedom are relevcvenat here because the critical values is dependent upon the degrees of freedom: the more degrees of freedom, the higher the critical value, i.e. the harder it is to breach ethe level of significance.

Since theer is only 1 degree of freedom in our case, we need to consider only the first column in the table of critical values below.


```{r echo = F, results = 'asis'}
library(knitr)
critval <- matrix(c(1, 3.84, 6.64, 10.83, 2, 5.99, 9.21, 13.82, 3, 7.82, 11.35, 16.27, 4, 9.49, 13.28, 18.47, 5, 11.07, 15.09, 20.52), ncol = 4, byrow = T)
# add column names
colnames(critval) <- c("DF", "p<.05", "p<.01", "p<.001")
kable(critval, caption = "Critical chi values for 1 to 5 degrees of freedom")
```


Since the $\chi^{2}$ value that we have calculated is much higher than the critical value provided for p<.05, we can reject the $H_{0}$ and may now claim that speakers of AmE and BrE differ with respect to their preference for *sort of* und *kind of*.


Before we summarize the results, we will calculate the effect size which is a measure for how strong the correleations are.

### Effect Sizes in Chi-Square 

Effect sizes are important because they correlations may be highly significant but the effect between  variables can be extremely weak. The effect size is therefore a measure how strong the correlation or the explanatory and predictive power between variables is.

The effect size measure for $\chi^{2}$ tests can be either the $\phi$-coeffizient (phi-coeffizient) or Cramer's $\phi$ (Cramer's phi). The $\phi$-coeffizient is used when dealing with 2x2 tables while Cramer's $\phi$ is used when dealing with tables with more than 4 cells. The $\phi$ coeffizient can be calculated by using the equation below (N = overall sample size).

$\phi = \sqrt{\frac{\chi^{2}}{N}}$

In our case, this means:

$\phi = \sqrt{\frac{220.7339}{1080}} = \sqrt{0.2043832} = 0.4520876$

The $\phi$ coefficient varies between 0 (no effect) and 1 (perfect correlation). Für die Einteilung in schwache, moderate und starke Effekte kann man der Einteilung für $\omega$ (kleines Omega) folgen, sodass man bei Werten von .1 von schwacher, um einen Wert bei 0.3 von moderater und ab .5 von einer starken Effektstärke sprechen kann \citep[266]{buehner2009statistik}. Wir haben es in diesem Beispiel also mit einem mittleren Effekt oder Zusammenhang zu tun.

### Chi-Square in R

Befroe we summarize the results, we will see how to perform a chi-square test in `R`. In addition to what we have done above, we will also visualize the data. To begin with, we will have a look ate the data set (which is the same data we have used above).

```{r echo=T, warning=F, message=F}
chidata              # inspect data
```

We will now visualize the data with an association. Bars above the dashed line indicate that a feauture combination occurs more frequently than expected by chance. The width of the bars indicates the frequency of the feature combination. 

```{r echo=F, warning=F, message=F}
assocplot(as.matrix(chidata))   # association plot
```
 
 The fact that the bars are distributed complimentarily (top left red and below bar; top right black above bar; bottom left black above bar; bottom right red below bar) indicates that the use of "sort of" and "kind of" differs across AmE and BrE. We will check whether the mosaicplot confirms this impression.
 
```{r echo=T, warning=F, message=F}
mosaicplot(chidata, shade = TRUE, type = "pearson", main = "")  # mosaic plot
```

The color contarst in the mosaicplot substantiate the impression that the two varieties of English differ significantly. To ascertain whether teh differences are statistically significant, we can now apply the chi-square test.

```{r echo=T, warning=F, message=F}
chisq.test(chidata, corr = F)  # perform chi square test
```

The results reported by `R` are identical to the results we derived by hand and confirm that BrE and AmE differ significantly in their use of "sort of" and "kind of". In a next setp, we calculate the effect size.

```{r echo=T, warning=F, message=F}
# calculate effect size
sqrt(chisq.test(chidata, corr = F)$statistic / sum(chidata) * (min(dim(chidata))-1))
```

The phi coefficient of .45 shows that variety of English correlates moderately with the use of "sort of" and "kind of". We will now summarize the results.

### Summarizing Chi-Square Results

The results of our analysis can be summaried as follows: A $\chi$^{2}-Test confirms a highly significant correlation of moderate size between the variety of English and the use of the near-synonymous hedges *sort of* and *kind of* ($\chi$^{2} = 220.73, df = 1, p $<$ .001***, $\phi$ = .452).

### Requirements of Chi-Square
Chi-square tests depend on certain requirements that, if violated, negativly affect the reliabilty of the results of the test. To provide reliable results, 80 percent of cells in a table to which the chi-square test is applied have to have expected values of 5 or higher and at most 20 percent of expected values can be smaller than 5 (vgl. [@bortz1990verteilungsfreie] 98). In addition, none of the expected values can be smaller than 1 (vgl. [@bortz1990verteilungsfreie] 136) because then, the estimation, which relies on the $\chi$^{2}-distribution, becomes too imprecise to allow meaningful inferences ([@cochran1954somemethods]).  

If these requierments are violated, then the *Fisher's Exact Test* is more reliable and offers the additional advantage that these tests can also be applied to data that represent very small sample sizes. When applying the Fisher's Exact Test, the probabilities for all possibles outcomes are calculated and the summed probability for the observed or more extreme results are determined. If this sum of probabilities exceeds five percent, then the result is deemed statistically significant.

## Extensions of Chi-Square
In the following, we will have a look at tests and methods that can be used if the requirements for ordinary (Pearson's) chi-square tests are violated and their use would be inaapropriate

### The Yates-Correction
If all requirements for ordinary chi-square tests are acceptable and only the sample size is the issue, then applying a so-called *Yates-correction* may be appropriate. This type of correction is  used in cases where the overall sample size lies inbetween 60 and 15 cases ([@bortz1990verteilungsfreie] 91). The difference between the ordinary chi-square and a Yates-corrected chi-square lies in the fact that the Yates-corrected chi-square is calculated according to the equation below.

$\frac{(|observed – expected|-0.5)^{2}}{expected}$

According to this formula, we would get the values shown below rather than the values tabulated above. It is important to note here that this is only a demonstration because a Yates-Correction would actually be inapproprate as our sample size exceeds 60 cases. 

```{r echo=F, warning=F, message=F}
library(knitr)
critval <- matrix(c("kind of", 32.9927, 113.0407, 146.0335, "sort of", 16.3593, 56.0507, 72.4100, "Total", 49.3520, 169.0914, 218.4434), ncol = 4, byrow = T)
# add column names
colnames(critval) <- c("Variant", "BrE", "AmE", "Total")
kable(critval, caption = "Corrected chi-square values for sort of and kind of in BrE and AmE")
```

If the Yates-correction were applied, then this results in a slightly lower $\chi$^{2}-value abd thus to more conservative results compared with the traditional test according to Pearsons.

### Chi-Square within 2*k Tables

Although the chi-sqaure test is widely used, it is often used inappropriately. This is especially the case when chi-square tests are applied to data representing tables with more than two rows and more than two columns. It is important to note that applying the common Pearsons' chi-square test to subtables of a larger table is inappropriate because, in such cases, a modified variant of Pearsons' chi-square test is warranted. We will go through two examples that represnt different scenarios where we are dealing with subsamples of larger tables and a modified version of the chi-sqaure test should be used rather than Pearsons' chi-square. 

In this first example, we are dealing with a table consisting of two columns and multiple rows, a so-called 2\*k table (two-by-k table). In order to test if a feature combination, that is represented by a row in the 2\*k table, is significantly more common compared with other feature combineations, we need to implement the chi-sqaure equation from [@bortz1990verteilungsfreie](126-127). 

In this example, we want to find out whether soft and hard X-rays differ in their effect on grasshopper larva. The question is whether the larva reach or do not reach a certain life cicle depending on whtehre they are exposed to soft X-rays, hard X-rays, light , or beta rays. The dat for this example is provided below.

```{r echo = F, results = 'asis'}
library(knitr)
critval <- matrix(c("X-ray soft",  21, 14, 35, "X-ray hard", 18, 13, 31, "Beta-rays", 24, 12, 36, "Light", 13, 30, 43, "Total", 76, 69, 145), ncol = 4, byrow = T)
# add column names
colnames(critval) <- c("", "Mitosis not reached", "Mitosis reached", "Total")
kable(critval, caption = "Data adapted from Bortz (1990: 126)")
```


If we would apply an ordinary chi-square test, we would ignore that all data were collected together and using only a subsample would ignore the data set of which the subsample is part of. In other words, te subsampel is not independent from the other  data (as it represents a subsetion of the whiole data set). However, for exemplary reasons, we will apply an ordinary chi-square test first and then compare its results to results prvided by the correct version of the chi-square test. Ina  first step, we create a table with all the data.

```{r echo = T, message=FALSE, warning=FALSE}
# create tdata
wholetable <- matrix(c(21, 14, 18, 13, 24, 12, 13, 30), byrow = T, nrow = 4)
colnames(wholetable) <- c("reached", "notreached")           # add column names
rownames(wholetable) <- c("rsoft", "rhard", "beta", "light") # add row names
wholetable                                                   # inspect data
```

Now, we extract the subsample from the data.

```{r echo = T, message=FALSE, warning=FALSE}
subtable <- wholetable[1:2,] # extract subtable
subtable                     # inspect subtable
```

Next, we apply the ordinary chi-square test to the subsample. 

```{r echo = T, message=FALSE, warning=FALSE}
# simple x2-test
chisq.test(subtable, corr = F)
```

Finally, we perform the correct chi-square test.

```{r echo = T, message=FALSE, warning=FALSE}
# load function for correct chi-square
source("rscripts/x2.2k.r") 
x2.2k(wholetable, 1, 2)
```

Below is a table comparing the results of the two chi-square tests.

```{r echo = T, message=FALSE, warning=FALSE}
library(knitr)
critval <- matrix(c("chi-squared", 0.0255, 0.025, "p-value", 0.8732, 0.8744), ncol = 3, byrow = T)
# add column names
colnames(critval) <- c("", "chi-square" , "chi-square in 2*k-tables")
kable(critval, caption = "Table adapted from Bortz (1990: 126)")
```

The comparison shows that, in this example, the results of the two tests are very similar but this may not alwyas be the case.


### Chi-Square within z*k Tables

Eine weitere Anwendung, bei der der $\chi$^{2}-Test häufig inkorrekter Weise verwendet wird, ist das Testen von Teilen von Tabellen mit mehr als zwei Zeilen und mehr als zwei Spalten, d.h. z*k-Tabellen (z: Zeile, k: Kolumne). Ein Beispiel wird in \citep{gries2014frequency} besprochen, der auch das \verb!R!-Skript für die korrekte Version des $\chi^{2}$-Test erstellt hat.

Laden wir zuerst die Daten, die in dem Beispiel von \citep[9]{gries2014frequency} besprochen werden, in dem es darum geht, ob sich nicht nur Register und EMOTION-Metaphern unterscheiden, sondern auch darum, ob sich gesprochene Konversationen von Fiktion im Gebrach von EMOTION IST LICHT und EMOTION IST EINE NATURKRAFT unterscheiden.

```{r echo=T, message=FALSE, warning=FALSE}
# create table
wholetable <- matrix(c(8, 31, 44, 36, 5, 14, 25, 38, 4, 22, 17, 12, 8, 11, 16, 24), ncol=4)
attr(wholetable, "dimnames")<-list(Register=c("acad", "spoken", "fiction", "new"),
Metaphor = c("Heated fluid", "Light", "NatForce", "Other"))
```

Es ergibt sich aus diesen Daten Tabelle (\ref{tab:chisub}).

```{r echo = F, results = 'asis'}
library(knitr)
critval <- matrix(c("acad", 8, 5, 4, 8, "spoken", 31, 14, 22, 11, "fiction", 44, 25, 17, 16, "new", 36, 38, 12, 24), ncol = 5, byrow = T)
# add column names
colnames(critval) <- c("Register", "Heated fluid", "Light", "NatForce", "Other")
kable(critval, caption = "Table adapted from Gries (2014: 9)")
```

Würden wir einen normalen (hier inkorrekten) $\chi$^{2}-Test verwenden, so würde sich ergeben, dass sich gesprochene Konversationen nicht signifikant von Fiktion im Gebrach von EMOTION IST LICHT und EMOTION IST EINE NATURKRAFT unterscheiden ($\chi^{2}$=3.3016, df=1, p$=$.069, $\phi$ = .2057).

```{r echo = T, message=FALSE, warning=FALSE}
# create table
subtable <- matrix(c(14, 25, 22, 17), ncol=2)
chisq.results <- chisq.test(subtable, correct=FALSE) # WRONG!
phi.coefficient = sqrt(chisq.results$statistic / sum(subtable) * (min(dim(subtable))-1))
chisq.results
phi.coefficient
```

Die korrekte Analyse berücksichtigt hierbei, dass es sich um eine Untertabelle handelt, die nicht unabhängig von der Gesamttabelle ist. Dies bedeutet, dass die korrekte Analyse die gesamte Anzahl der Fälle, sowie die Gesamtzeilen- und Gesamtspaltensummen zu berücksichtigen sind [vgl.@bortz1990verteilungsfreie 144-148].

Um die korrekte Analyse durchführen zu können, muss die Analyse von Funktion von [@bortz1990verteilungsfreie 144-148] implementiert oder direkt die Funktion von Gries eingelesen und auf die Teiltabelle angewandt werden.

```{r echo = T, message=FALSE, warning=FALSE}
# load function for chi square test for subtables
source("rscripts/sub.table.r") 
# apply test
results <- sub.table(wholetable, 2:3, 2:3, out="short")
# inspect results
results
```


The results show that the difference is, in fact, statistically significant  ($\chi^{2}$=3.864, df=1, p=.049*).


### Chi-Square Excercises 

1. Imagine you are interested in whether older or younger speakers tend to refer to themselves linguistically. The underlying hypothessi is that - contrary to common belief - older people are more narcicisstic comapred with younger people. Given this research question, perform a chi-square test and summarize the results on the data below. 

```{r echo = F, message=FALSE, warning=FALSE}
library(knitr)
critval <- matrix(c("Young", 61, 43, 104, "Old", 42, 36, 78, "Total", 103, 79, 182), ncol = 4, byrow = T)
# add column names
colnames(critval) <- c("", "1SGPN", "PN without 1SG", "Total")
kable(critval, caption = "Table adapted from Gries (2014: 9)")
```



2. Imagine you are interested in whether young men or young women exhibit a preference for the word *whatever* because you have made the unsystemtic, anectdotal observation that young men use this word more frequently than young women. Given this research question, perform a chi-square test and summarize the results on the data below.

```{r echo = F, results = 'asis'}
library(knitr)
critval <- matrix(c("whatever", 17, 55, 71, "other words", 345128, 916552, 1261680, "Total", 345145, 916607, 1261752), ncol = 4, byrow = T)
# add column names
colnames(critval) <- c("", "YoungMales", "YoungFemales", "Total")
kable(critval, caption = "Observed frequency with row- and column totals for the use of *whatever* by male and female speakers.")
```

3. Find a partner and discuss the relationship between significance and effect size. Then, go and find another partner and sicuss probelms that may arise when testing the frequency of certain words compared with the overall ferqeuncy of words ina corpus. 

## Other Non-Parametric Tests

If the dependend variable is neither nominal nor categorical, but ordinal (that is if the dependent variable represnets an order factor such as ranks), using a chi-square test is not warranted. In such cases it is adviasble to apply tests that are designed to handle ordinal data. In the following, we will therefore briefly touch on bivariate tests that can handle ordinal dependent variables.

### Mann-Whitney U Test

It is a rather freuent case that numeric depend variables are transformed or converted into ordinal variables because the distribution of residuals do not allow the application of a linear regression. Because we are dealing with ordinal data, the application of a chi-square test is unwarranted and we need to use another test. In such cases, the Mann-Whitney U Test, which may be used even if the groups under investigation represent identical participants that are tested under two conditions, irepresents a viable alternative. If we are dealing with independent groups, i.e. groups that do not consist of the same subjects, and the dependent variable is ordinal, then the Mann-Whitney U Test is implemented as follows.

```{r echo=TRUE, warning=F, message=F}
# y ist aV und numerisch, x ist uV und ein binaerer faktor
#wilcox.test(Ranks ~ Groups)

# y1 ist aV, y2 ist uV und beide sind numerisch
#wilcox.test(y1, y2)
```

%Hat man es mit abhängigen Gruppen zu tun, muss man das Argument \verb!paired! gleich \verb!TRUE! setzen (siehe unten).

```{r echo=TRUE, warning=F, message=F}
# dependent 2-group Wilcoxon Signed Rank Test
#wilcox.test(y1,y2,paired=TRUE) # where y1 and y2 are numeric
```

### Kruskal Wallis Test
The Kruskal Wallis Test is a type of Analysis of Variance which can handle ordinal data. 

```{r echo=TRUE, warning=F, message=F}
#y <- c(15, 13, 10, 8, 37, 23, 31, 52, 11, 17)
#x <- c("A", "A", "A", "A", "A", "B", "B", "B", "B", "B")
# Kruskal Wallis Test One Way Anova by Ranks
#kruskal.test(y~x) # where y1 is numeric and x is a factor
```



```{r echo=T, message=FALSE, warning=FALSE}
# Randomized Block Design - Friedman Test
# friedman.test(y~x|z)
# where y are the data values, x is a grouping factor
# and z is a blocking factor
```

### Excercises for Other Non-Parametric Tests


# Simple Linear Regression
 
This chapter focuses on a very widely used statistical method which is called regression. Regressions are used when we try to understand how independent variables correlate with a dependent or outcome variable. So if you want to investigate how certain criteria affect an outcome, then a regression is the way to go. We will have a look at two simple examples to understand what the concepts underlying a regression mean and how a regression works. After that, we will turn to a more compley case and we will implement a multiple linear regression. The `R`-Code, that we will use, is based on [@field2012discovering].

## Introduction

Although the basic logic underlying regressions is identical to the conceptual underpinnings of analysis of variance (ANOVA), a related method, sociolinguistists have traditionally favoured regression analysis in their studes while ANOVAs have been the method of choice in psycholinguistics. The preference for either method is grounded in historical happenstances and the culture of these subdisciplines rather than in methodological reasoning.

A minor difference between regressions and ANOVA lies in the fact that regressions are based on the $t$-distribution while ANOVAs use the $F$-distribution (however, the $F$-value is merely t^2^). Both  $t$- and $F$-values report on the ratio between explained and unexplained variance.

The idea behind regression analysis is expressed formaly in [equation](#eq:slm) and can best be described graphically: Imagine drawing a line through points in a scatterplot (Grafik \ref{fig:scat1} left panel).

$f_{(x)} = \alpha + \beta_{1}x_{i} + \epsilon$ {#eq:slm}

Regressions aim to find that line which has the minimal summed distance between points and the line (Grafik \ref{fig:scat1} center panel). Technically speaking, the aim of a regression is to find the line with the minimal deviance or the line with the minimal sum of residuals (variance) (Grafik \ref{fig:scat1} right panel). This means that the sum of the length of the lines between the points and the lines should be minimal. The slope of the line is called *coefficient* and the point where the line crosses the y-axis is called the *intercept*.

```{r echo=T, message=FALSE, warning=FALSE}
# Scatterplot with lines from teh regression line to the dots
x <- c(173, 169, 176, 166, 161, 164, 160, 158, 180, 187)
y <- c(80, 68, 72, 75, 70, 65, 62, 60, 85, 92) # plot scatterplot and the regression line
mod1 <- lm(y ~ x)

par(mfrow=c(1, 3)) # two plots in two columns in one window
# ein plot pro fenster
plot(x, y, xlim=c(min(x)-5, max(x)+5), ylim=c(min(y)-10, max(y)+10))
plot(x, y, xlim=c(min(x)-5, max(x)+5), ylim=c(min(y)-10, max(y)+10))
abline(mod1, lwd=2)
plot(x, y, xlim=c(min(x)-5, max(x)+5), ylim=c(min(y)-10, max(y)+10))
abline(mod1, lwd=2)

# calculate residuals and predicted values
res <- signif(residuals(mod1), 5)
pre <- predict(mod1) # plot distances between points and the regression line
segments(x, y, x, pre, col="red")
# add labels (res values) to points
library(calibrate)
textxy(x, y, round(res,1), cex=1)
par(mfrow=c(1, 1)) # restore original graphics parameters
```

A word about standard errors (SE) is in order here because most commonly used statistics programs will provide SE values when reportigng regression models. The SE is a measure that tells us how much the coefficients were to vary if the same regression were applied to many samples from the same population. A relatively small SE value therefore indicates that the coefficients will ermain very stable if the same regression model is fitted to many different samples with identical parameters. In contrast, a large SE tells you that the model is volatile and not very stable or reliable as the coefficients vary substantially if the model is applied to many samples.  

## Example 1: Preposition Use across Real-Time

We will now turn to our first example. In this example, we will investiagte whether the frequency of prepositions has changed from Middle English to Late Modern English. The reasoning behind this example is that Old English was highly sythetic compared with Present-Day English which comparatively analytic. In other words, while Old English speakers used case to indicate syntactic relations, speakers of Present-Day English use word order and prepositions to indicate syntactic relationships. This means that the loss of case had to be compensated by different strategies and maybe these strategies continued to develop and incerase in frequency even after the change from synthetic to analytic had been mostly accomplished. And this prolonged change in compensatory strategies is what this example will focus on. 

The analysis is based on data extrcted from the *Penn Corpora of Historical English* (see http://www.ling.upenn.edu/hist-corpora/), that consistss of 603 texts written between 1125 and 1900. In preparation of this example, all elements that were part-of-speech tagged as prepositions were extracted from the PennCorpora. 

Then, the relative frequencies (per 1,000 words) of prepositions per text were calculated. This frequency of prepositions per 1,000 words represents our dependent variable. In a next step, the date when each letter had been written was extracted. The resulting two vectors were combined into a table which thus contained for each text, when it was written (independent variable) and its relative freqeuncy of prepositions (dependent or outcome variable).

A regression analysis will follow the steps described below: 
Die folgende Ablauf ist typisch für Regressionsanalysen.

1. Extraction and processing of the  data
2. Data visualization
3. Applying the regression analysis to the data
4. Diagnosing the regression model and checking whether or not basic model assumptions have been violated.

### Simple Linear Regression in `R`

In a first step, we prepare our session by cleaning the work space, installing libraries that we need for the analysis, and loading the libraries and fucntions.

```{r echo=T, message=FALSE, warning=FALSE}
# clean workspace
rm(list=ls(all=T))
# installing neccessary or helpful packages/libraries (if you have not sone so previously)
# (remove the # sign to activate the code)
#install.packages("QuantPsyc")
#install.packages("car")
# initialise packages/libraries
library(QuantPsyc)
library(car)
library(ggplot2)
# function which allows us to plot several plots in one window
source("rscripts/multiplot_ggplot2.r")
# function for summarizing simple linear regression models
source("rscripts/slr.summary.tb.r") 
```

After perparing our session, we can now load and inspect the data to get a first implression of its properties.

```{r echo=T, message=FALSE, warning=FALSE}
# load data
slrdata <- read.delim("data/slrdata.txt", header = TRUE)
# attach data
attach(slrdata)
# unnoetige spalten entfernen
slrdata <- as.data.frame(cbind(datems, pptw))
# spaltennamen hinzufuegen
colnames(slrdata) <- c("year", "prep.ptw")
# entfernen unvollstaendiger datenpunkte
slrdata <- slrdata[!is.na(slrdata$year) == T, ]
# erste zeilen des datensatzes betrachten
head(slrdata)

# struktur des datensatzes betrachten
str(slrdata)

# eigenschaften des datensatzes betrachten
summary(slrdata)
```

Inspecting the data is very important because it can happen that a data set may not load completely or that variables which should be numeric have been converted in charater variables. If unchecked, then such hidden data issues could go unnoticed and cause much trouble once you realize that the data you have been working with is different from what you had in mind.

We will now plot the data to get a more thorough impression of the structure of the data.

```{r echo=T, message=FALSE, warning=FALSE}
# visualisieren der daten
ggplot(slrdata, aes(year, prep.ptw)) +
geom_point() +
labs(x = "Year") +
labs(y = "Prepositions per 1,000 words") +
geom_smooth()
```


```{r echo=T, message=FALSE, warning=FALSE}
ggplot(slrdata, aes(year, prep.ptw)) +
geom_point() +
labs(x = "Year") +
labs(y = "Prepositions per 1,000 words") +
geom_smooth(method = "lm") # with linear model smoothing!
```

Before beginning with the regression analysis, we will scale the year. We scale by subtracting each values from the mean of year. This can be useful when dealing with numeric variables becasue if we did not scale year, we would get estaimated values for year 0 (a year when English did not even exist yet). If a variable is scaled, the regression provides estimates of the model refer to the mean of that numeric variable. In other words, scaling can eb very helpful, especially with respect to the interpretation of the results that regression models report.


```{r echo=T, message=FALSE, warning=FALSE}
slrdata$prep.ptw <- slrdata$prep.ptw - mean(slrdata$prep.ptw) # scaling date
```


Wir beginnen nun mit der Regression, indem wir eine erste Regression rechnen und anschließend deren Ergebnisse und diagnostische Plots betrachten.

```{r echo=T, message=FALSE, warning=FALSE}
# create initial model
prep.lm <- lm(prep.ptw ~ year, data = slrdata)
# inspect results
summary(prep.lm)

# plot model: 3 plots per row in one window
par(mfrow = c(1, 3))
plot(resid(prep.lm))
plot(rstandard(prep.lm))
plot(rstudent(prep.lm))
par(mfrow = c(1, 1)) # restroe default parameters
```

Die linke Grafik zeigt die Residuen des Modells (d.h. die Unterscheide zwischen den beobachteten und den durch das Modell vorhergesagten Werten). Das Problem bei diesem Plot ist, dass die Residuen nicht standardisiert sind und man sie so nicht mit den Residuen anderer Modelle vergleichen kann. Um diesen Mangel zu beheben standardisiert man die Residuen, indem man die Residuen durch deren Standartabweichung dividiert, und plottet sie gegen die beobachteten Werte (mittlerer Plot). Auf diese Weise erhält man nicht nur standardisierte Residuen, sondern die Werte der Residuen sind nun zu z-Werten geworden und man kann die z-Verteilung nutzen, um problematische Punkte zu finden. Es gibt drei Daumenregeln bezüglich des Findens problematischer Datenpunkte aufgrund von Residuen ([@field2012discovering] 268-269):

1. Punkte mit extremen Werten, d.h. Werten $\ge$ 3 (um genau zu sein, Werten $\ge$ 3.29), sollten aus den Daten entfernt werden.
2. Falls mehr als 1% der Datenpunkte Werte $\ge$ 2.5 haben (2.58 um genau zu sein), dann sind die Fehler unseres Models zu groß.
3. Falls mehr als 5% der Datenpunkte Werte $\ge$ 2 haben (1.96 um genau zu sein), dann sind die Fehler unseres Models ebenfalls zu groß.


Die rechte Grafik zeigt die *studentized Residuen*, d.h. die angepassten vorhergesagten Werte jedes Datenpunkts werden durch den Standardfehler der Residuen dividiert. Auf diesem Weg ist es möglich die Student's t-Verteilung zu nutzen, um unser Model zu diagnostizieren.

Angepasste vorhergesagte Werte sind ebenfalls Residuen, aber einer besonderen Art: Das Model wird ohne einen Datenpunkt gerechnet und dann genutzt um diesen Datenpunkt vorherzusagen. Der Unterschied zwischen dem beobachteten Datenpunkt und dem vorhergesagten Datenpunkt wird dann angepasster vorhergesagter Wert genannt. Zusammenfassend kann gesagt werden, dass studentized Residuen sehr nützlich dahingehend sind, dass Sie einflussreiche Datenpunkte erkennen lassen.

Die Plots zeigen, dass es zwei potentiell problematische Datenpunkte gibt (die Punkte ganz oben und ganz unten). diese zwei Punkte setzten sich deutlich von den anderen Punkten ab und können demnach Ausreißer (outlier) darstellen. Wir werden später testen, ob diese punkte entfernt werden müssen.

Wir werden nun weitere modelldiagnostische Grafiken generieren.

```{r echo=T, message=FALSE, warning=FALSE}
# generiere eine 2x2 matrize diagnostischer grafiken
par(mfrow = c(2, 2))
plot(prep.lm)
par(mfrow = c(1, 1))
```

Die diagnostischen Grafiken sehen sehr gut aus und wir werden im Folgenden erklären warum. Die Grafik im oberen linken Panel ist nützlich, um (a) Oulier zu finden oder (b) die Korrelation zwischen Residuen und vorhergesagten Werten zu bestimmen: Wenn ein Trend in der Linie oder den Punkten sichtbar wird (bspw. ein aufsteigender Trend oder eine Zickzacklinie), dann hätte unser Model ein Problem und wir müssten wahrscheinlich Datenpunkte entfernen.

Die Grafik im oberen rechten Panel zeigt an, ob die Residuen normal verteilt sind (was wünschenswert ist), ober ob die Residuen nicht einer Normalverteilung folgen. Liegen die Punkte auf der Linie, so folgen die Residuen einer Normalverteilung. Wenn die Punkte beispielsweise am oberen und unteren Ende nicht auf der Linie liegen, so zeigt dies, dass das Model kleine und große Werte nicht gut vorhersagt und daher nicht gut auf die Daten angepasst ist.

Die Grafik im unteren linken Panel gibt Aufschluss über Homoskedastizität. Homoskedastizität bedeutet, dass die Varianz der Residuen konstant bleibt und nicht mit dem Wert der unabhängigen Variable korrelieren. In unproblematischen Fällen zeigt die Grafik eine flache Linie. Liegt eine Trend in der Linie vor, so haben wir es mit Heteroskedastizität, also mit einer Korrelation zwischen unabhängigen Variablen und den Residuen, zu tun, die für Regressionen sehr problematisch ist.

Die Grafik im unteren rechten Panel zeigt problematische einflussreiche Datenpunkte, die die Regression überproportional beeinflussen (dies sollte nicht der Fall sein). Falls solche einflussreichen Datenpunkte vorliegen, so sollten diese entweder (a) gewichtet werden (robuste Regression) oder (b) entfernt werden. Die Grafik zeigt die Cookdistanz, welche zeigt, wie sich die Regression verändert, wenn ein Model ohne diesen Datenpunkt gerechnet wird. Die Cookdistanz zeigt also, welchen Einfluss ein Datenpunkt auf die Regression als ganzes hat. Datenpunkte, die eine Cookdistanz $\ge$ 1 haben sind problematisch \citep[269]{field2012discovering}.

Die sogenannte Leverage ist ebenso ein Maß, das anzeigt, wie stark ein Datenpunkt die Genauigkeit der Regression beeinflusst. Leveragewerte liegen zwischen 0 (kein Einfluss) und 1 (starker Einfluss: suboptimal!). Um zu testen, ob ein spezifischer Datenpunkt einen hohen Leveragewert besitzt, muss man einen Cut-Off-Punkt berechnen, der anzeigt, ob die Leverage zu stark oder noch akzeptabel ist. Folgende zwei Formeln werden hierzu genutzt:

$\frac{3(k + 1)}{n}$

oder

$\frac{2(k + 1)}{n}$

Wir werden im Kontext der multiplen linearen regression genauer auf Leverage eingehen und nun nur noch eine Überblicktabelle der Ergebnisse der Regression generieren.

```{r echo=T, message=FALSE, warning=FALSE}
# tabulate results
slr.summary(prep.lm)
```

Typischer weise werden die Ergebnisse von Regressionen in solchen Tabellen wiedergegeben, da diese aller wichtigen Kennzahlen der Modellgüte und die Signifikanz wie auch die Stärke der Effekte beinhalten. Die Tabelle ist hier noch einmal abgebildet.

\begin{table}[H]
\begin{minipage}{\textwidth}
\begin{center}
\begin{footnotesize}
\begin{tabular}{l cccccc r}
\hline
& Estimate & Std. $\beta$ & Pearson's r & SE & t-value & Pr$(>|t|)$ & Sig.\\
\hline
(Intercept) & --27.72 & & & 10.86 & --2.55 & 0.011 & p$<$.05* \\
year & 0.02 & 0.1039 & 0.1 & 0.01 & 2.56 & 0.0107 & p$<$.05* \\
\hline
Model statistics & & & & & & & Value \\
\hline
Number of cases & & & & & & & 603 \\
Residual SE (601 DF) & & & & & & & 21.11 \\
Multiple $R^{2}$ & & & & & & & 0.0108 \\
Adjusted $R^{2}$ & & & & & & & 0.0091 \\
F-statistic (1, 601) & & & & & & & 6.55 \\
Model p-value & & & & & & & 0.0107 \\
\hline
\end{tabular}
\end{footnotesize}
\caption{Zusammenfassung der Ergebnisse der einfachen linearen Regression}
\label{tab:slr1}
\end{center}
\end{minipage}
\end{table}

Zusätzlich sollten die Ergebnisse von einfachen linearen Regressionen schriftlich in etwa wie folgt zusammengefasst werden:\\[.2cm]

\noindent Eine einfache lineare Regression wurde auf die Daten angepasst. Eine visuelle Begutachtung der modelldiagostischen Grafiken zeigten keine problematischen Datenpunkte (Ausreißer) oder überproportional einflussreiche Datenpunkte an und wiesen auf einen guten Modellfit hin. Das finale lineare Regressionsmodell basiert auf 603 Datenpunkten und korreliert hoch signifikant mit den Daten ($R^{2}$: 0.0108, F-Statistik (1, 601): 6.553, p-Wert: 0.0107\*) und bestätigt eine signifikante positive Korrelation zwischen dem Jahr in dem der Text geschrieben wurde und der relativen Häufigkeit von Präpositionen in den Texten nach (Koeffizient: .02, Std. $\beta$: 0.1039, SE: 0.01, t-Wert: 2.560, p-Wert: .0107*).

## Example 2: Teaching Styles

Im vorhergehenden Beispiel haben wir es mit zwei numerischen Variablen zu tun gehabt, während es sich in dem folgenden Beispiel um eine kategoriale und eine numerische abhängige Variable handelt. Die Eigenschaft, dass Regressionen mit sehr verschiedenen Variablenarten umgehen können, macht Regressionen zu einer weit verbreiteten und robusten Analysemethode.

In diesem Beispiel haben wir es mit zwei Gruppen von Schülern zu tun, die zufällig einer Gruppe zugewiesen wurden und unterschiedlichen Lehrmethoden ausgesetzt waren. Beide Gruppen unterziehen sich im Anschluss an die Lehreinheit einem Sprachlerntest mit einer Höchstpunktzahl von 20 Punkten. Die Schüler der ersten Gruppen haben folgende Punktzahlen erreicht:

* Gruppe A: 15, 12, 11, 18, 15, 15, 9, 19, 14, 13, 11, 12, 18, 15, 16, 14, 16, 17, 15, 17, 13, 14, 13, 15, 17, 19, 17, 18, 16, 14 (mean: 14,93)

Die Schüler der zweiten Gruppe haben diese Punktzahlen erreicht.

* Gruppe B: 11, 16, 14, 18, 6, 8, 9, 14, 12, 12, 10, 15, 12, 9, 13, 16, 17, 12, 8, 7, 15, 5, 14, 13, 13, 12, 11, 13, 11, 7 (mean: 11,77)

Unsere Frage ist nun, ob Gruppe A wirklich besser ist oder ob das Ergebnis Zufall ist?

Gehen wir nun dazu über, die Regression in `R` zu implementieren. Wie im vorherigen Beispiel leeren wir den gegenwärtigen Workspace, installieren und initialisieren/aktivieren notwendige Pakete und laden zusätzliche Funktionen.

```{r echo=T, message=FALSE, warning=FALSE}
# entfernen aller objekte aus dem aktuellen workspace
rm(list=ls(all=T))
# installieren der notwendigen pakete
# (falls nicht schon geschehen)
# (um die befehle zu aktivieren # entfernen)
#install.packages("QuantPsyc")
#install.packages("car")
# pakete initialisieren
library(QuantPsyc)
library(car)
library(ggplot2)
source("rscripts/multiplot_ggplot2.r") # mehrere ggplots in einem fenster
source("rscripts/slr.summary.tb.r") # funktion zum erstellen von summary tabellen
```


Nachdem die notwendigen Spezifikationen durchgeführt wurden, werden wir nun unser Datenset generieren und es anschließend betrachten.

```{r echo=T, message=FALSE, warning=FALSE}
# einladen der daten
g1 <- c(15, 12, 11, 18, 15, 15, 9, 19, 14, 13, 11, 12, 18, 15, 16, 14, 16, 17, 15, 17, 13, 14, 13, 15, 17, 19, 17, 18, 16, 14)
g2 <- c(11, 16, 14, 18, 6, 8, 9, 14, 12, 12, 10, 15, 12, 9, 13, 16, 17, 12, 8, 7, 15, 5, 14, 13, 13, 12, 11, 13, 11, 7)
g <- c(rep("A", length(g1)), rep("B", length(g2)))
sprtestdata <- data.frame(g, c(g1, g2))
# spaltennamen hinzufuegen
colnames(sprtestdata) <- c("gruppe", "punkte")
# erste zeilen des datensatzes betrachten
head(sprtestdata)

# struktur des datensatzes betrachten
str(sprtestdata)

# eigenschaften des datensatzes betrachten
summary(sprtestdata)
```

Nun stellen wir die Daten grafisch dar. In diesem Fall bietet sich ein Boxplot zur Visualisierung an.

```{r boxplot, fig.cap="Darstellung der Sprachtestdaten"}
# erstelle boxplot
boxplot(punkte ~ gruppe,
data = sprtestdata, # the data we want to display
main = "", # you could specify a title here
ylab = "Punkte", # titel der y-achse
ylim = c(0, 20), # grenzen der y-achse festlegen
xlab = c("Gruppen"), # titel der x-achse
notch = T, # notches einfuegen
col = c("lightgreen", "lightblue")) # box einfaerben
# text darstellen
text(1:2,
c(4.0, 4.0),
cex = 0.85,
labels = paste("mean\n",
c(round(as.vector(by(sprtestdata$punkte, sprtestdata$gruppe, mean))[1], 2),
round(as.vector(by(sprtestdata$punkte, sprtestdata$gruppe, mean))[2], 2),
sep = "")))
rug(jitter(sprtestdata$punkte),
side=4)
grid()
box()
```

Die Daten weisen darauf hin, dass Gruppe A signifikant besser abgeschnitten hat als Gruppe B. Wir werden diesen Eindruck dadurch testen, dass wir im nächsten Schritt das Regressionsmodell und erstellen die modelldiagnostischen Grafiken generieren.

```{r echo=T, message=FALSE, warning=FALSE}
# Simples Lineares Regressionsmodel erstellen
sprtest.lm <- lm(punkte ~ gruppe, data = sprtestdata)
# ergebnisse betrachten
summary(sprtest.lm)

# graphik parameter setzen: 3 plots in einer reihe
par(mfrow = c(1, 3))
plot(resid(sprtest.lm))
plot(rstandard(sprtest.lm))
plot(rstudent(sprtest.lm))
par(mfrow = c(1, 1)) # wiederherstellen der originalparameter
```

\begin{figure}[H]
\centering
\includegraphics[width=1\textwidth]{images/sprtest1.png}\\[.25cm]
\caption{Diagnostische Grafiken der Sprachtestdaten}
\label{fig:sprtest1}
\end{figure}

Die Grafiken weisen nicht auf Ausreißer oder andere Probleme hin und wir können daher mit weiteren diagnostischen Grafiken fortfahren.

```{r echo=T, message=FALSE, warning=FALSE}
# generiere eine 2x2 matrize diagnostischer grafiken
par(mfrow = c(2, 2))
plot(sprtest.lm)
par(mfrow = c(1, 1))
```

\begin{figure}[H]
\centering
\includegraphics[width=1\textwidth]{images/sprtest2.png}\\[.25cm]
\caption{Weitere diagnostische Grafiken der Sprachtestdaten}
\label{fig:prep3}
\end{figure}

Auch diese Grafiken weisen auf keine Probleme hin. In diesem Fall können die Daten im nächsten Schritt zusammengefasst werden.

```{r echo=T, message=FALSE, warning=FALSE}
# ergebnisse tabellieren
slr.summary(sprtest.lm)
```

\begin{table}[H]
\begin{minipage}{\textwidth}
\begin{center}
\begin{footnotesize}
\begin{tabular}{l cccccc r}
\hline
& Estimate & Std. $\beta$ & Pearson's r & SE & t-value & Pr$(>|t|)$ & Sig.\\
\hline
(Intercept) & 14.93 & & & 0.53 & 27.94 & 0 & p$<$.001*** \\
GruppeB & -3.17 & -0.4819 & 0.48 & 0.76 & -4.19 & 0 & p$<$.001*** \\
\hline
Model statistics & & & & & & & Value \\
\hline
Number of cases & & & & & & & 60 \\
Residual SE (601 DF) & & & & & & & 2.93 \\
Multiple $R^{2}$ & & & & & & & 0.2322 \\
Adjusted $R^{2}$ & & & & & & & 0.219 \\
F-statistic (1, 58) & & & & & & & 17.55 \\
Model p-value & & & & & & & p$<$.001*** \\
\hline
\end{tabular}
\end{footnotesize}
\caption{Zusammenfassung der Ergebnisse der einfachen linearen Regression}
\label{tab:slr1}
\end{center}
\end{minipage}
\end{table}

Die Ergebnisse dieser einfachen linearen Regressionen können wie folgt zusammengefasst werden:\\[.2cm]

Eine einfache lineare Regression wurde auf die Daten angepasst. Eine visuelle Begutachtung der modelldiagostischen Grafiken zeigten keine problematischen Datenpunkte (Ausreißer) oder überproportional einflussreiche Datenpunkte an und wiesen auf einen guten Modellfit hin. Das finale lineare Regressionsmodell basiert auf 60 Datenpunkten und korreliert hoch signifikant mit den Daten ($R^{2}$: 0.2322, $F$-Statistik (1, 58): 2.93, p-Wert $<$.001\*\*\*) und bestätigt, dass Gruppe A signifikant besser bei dem Sprachlerntest abgeschnitten hat als Gruppe B (Koeffizient: -3.17, Std. $\beta$: -0.4819, SE: 0.48, t-Wert: -4.19, p-Wert $<$.001***).

# References


