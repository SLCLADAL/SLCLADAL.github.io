---
title: "Basic Inferential Statistics"
author: "UQ SLC Digital Team"
date: "`r format(Sys.time(), '%Y-%m-%d')`"
output:
  bookdown::html_document2: default
bibliography: bibliography.bib
link-citations: yes
---
```{r uq1, echo=F, fig.cap="", message=FALSE, warning=FALSE, out.width='100%'}
knitr::include_graphics("images/uq1.jpg")
```

# Introduction
This tutorial introduces basic statistical techniques from inferential statistics for hypothesis testing. The first part of this section focuses on basic non-parametric  tests such as Fisher's Exact test while the second part introduces parametric tests such as the t-test. 

Non-Parametric Tests do not require the data to be distributed normally. Tests that do not rewuire normal data are referred to as *non-parametric tests* (test that require the data to be distributed normally are analogously called *parametric tests*). We focus on non-parametric tests first as this family of test in frequently used in linguistics. In the later part of this section, we will focus on regression modelling where assumptions of about the data become more important. 

## Fisher's Exact Test

Fisher's Exact test is very useful because it does not rely on distributional assumptions relying on normality. Instead, Fisher's Exact Test calculates the probabilities of all possible outcomes and uses these to determine significance. To understand how a Fisher's Exact test, we will use a very simple example. 

Imagine, you observe that an exchange student from Germany has written an essay and uses the phrase "very cool" seven times and the phrase "truely cool" once. This strikes you as odd (not because of the register but) because you would rather say "really uncool". In other words, the co-occurrence of very plus cool seems non-native-like to you. So you have a look at a corpus to check which adverbs are used to modify "cool". You find that "cool" is modifed by an adverb in 40 cases and the distribution is shown below.

```{r fisherdata, echo=TRUE, warning=F, message=F}
library(knitr)                 # activate library for tabulating
# generate data
coolmatrix <- matrix(c("truely", "extremely", "so","really", "massively", "very", "Total", 5, 6, 8, 8, 5, 8, 40), ncol = 2, byrow = F)
colnames(coolmatrix) <- c("Combination", "Frequency")
kable(coolmatrix, caption = "Co-occurrence of cool with adverbs.")

```

You now want to find out whether the student diverges in their modification of "cool" from how native speakers would modify "cool". 

The way to do this is to calculate the probability of getting "cool" seven times together with "very" plus "very" being modified by "truely" once given the frequencies of combinations in the corpus.  The probability of getting "very" plus "cool" is 8 in 40 for the first instance, 7 in 39 for the second instance, 6 in 38 for the third instance, 5 in 37 for the fourth instance, 4 in 36 in the fifth instance, 3 in 35 in the sixth instance, and 2 in 34 in the seveneth instance. In addition, we need to add the probability of getting "truely cool" one which is 5 in 33.

\begin{equation}

p_{7very1truely} = \frac{8}{40} \times \frac{7}{39} \times \frac{6}{38} \times \frac{5}{37} \times \frac{4}{36} \times \frac{3}{35} \times \frac{2}{34} \times \frac{5}{33} 

= .2 \times 0.1794872 \times 0.1578947 \times 0.1351351 \times 0.1111111 \times 0.08571429 \times 0.05882353 \times 0.1515152 = 

= 0.0000006501554

\end{equation}

This probability is very low, but we also need to consider the probabilities of getting "truely cool" not at the very end (where we only have 33 combinations left) but at the very beginning and in all other positions. For getting "truely cool" first, we would calculate the probability as shown below:

\begin{equation}

p_{1+7} = \frac{5}{40}   \times  \frac{8}{39} \times \frac{7}{38} \times \frac{6}{37} \times \frac{5}{36} \times \frac{4}{35} \times \frac{3}{34} \times \frac{2}{33}

= 0.125 \times 0.2051282 \times 0.1842105 \times 0.1621622 \times 0.1388889 \times 0.1142857 \times 0.08823529 \times 0.06060606

= 0.0000006501554

\end{equation}

This measn that the combined probabilities of getting7 instances of "very cool" and one instance of "truely cool" given the distribution in the corpus is 0.000005201243 (8 \times 0.0000006501554). This measn that the probability is really low. But is it so low that we can say that the language learner differs significantly fro expert users?




```{r fisher, echo=TRUE, warning=F, message=F}
# generate data
Rank <- c(1,3,5,6,8,9,10,11,17,19, 2,4,7,12,13,14,15,16,18,20)
Groups <- c(rep("LanguageFamily1", 10), rep("LanguageFamily2", 10))
fisher.test(x, y) 
```



## Ranking and Sign-Based Non-Parametric Tests

If the depended variable is neither nominal nor categorical, but ordinal (that is if the dependent variable represents an order factor such as ranks), using a chi-square test is not warranted. In such cases it is advisable to apply tests that are designed to handle ordinal data. In the following, we will therefore briefly touch on bivariate tests that can handle ordinal dependent variables.

## Mann-Whitney U Test

It is a rather frequent case that numeric depend variables are transformed or converted into ordinal variables because the distribution of residuals does not allow the application of a linear regression. Because we are dealing with ordinal data, the application of a chi-square test is unwarranted and we need to use another test. In such cases, the Mann-Whitney U Test, which may be used even if the groups under investigation represent identical participants that are tested under two conditions, represents a viable alternative. If we are dealing with independent groups, i.e. groups that do not consist of the same subjects, and the dependent variable is ordinal, then the Wilcox Test or the Mann-Whitney U Test can be implemented.

```{r echo=TRUE, warning=F, message=F}
# generate data
Rank <- c(1,3,5,6,8,9,10,11,17,19, 2,4,7,12,13,14,15,16,18,20)
Groups <- c(rep("LanguageFamily1", 10), rep("LanguageFamily2", 10))
wilcox.test(Rank ~ Groups) # Rank is ordinal and Groups is a binary factor
```

```{r echo=F, eval = F, warning=F, message=F}
#install.packages("VGAM")       # install VGAM package (remove # to activate)
#library(VGAM)
# generate non-nirmal skewed numeric data
#r=.1 
#Frequency=rsnorm(100,0,2,4)
#NormalReaction=rsnorm(100,0,2,4)
#ReactionTimes=Frequency*r+NormalReaction*sqrt(1-r^2)
#wilcox.test(ReactionTimes, Frequency) # both variables are numeric but non-normal
```

If you are dealing with dependent groups, you have to set the argument "paired" to "TRUE" as in the dependent 2-group Wilcoxon Signed Rank Test below. In this example, y1 and y2 are both numeric.

```{r echo=TRUE, warning=F, message=F}
#wilcox.test(y1,y2,paired=TRUE) # where y1 and y2 are numeric
```

## Kruskal Wallis Test

The Kruskal Wallis Test is a type of "Analysis of Variance" or ANOVA. For this reason, the Kruskal Wallis Test is also referd to as a "one-way Anova by ranks" which can not only handle numeric but also ordinal data. In the example below, y is numeric and x is a factor.

```{r echo=TRUE, warning=F, message=F}
y <- c(15, 13, 10, 8, 37, 23, 31, 52, 11, 17)
x <- c("A", "A", "A", "A", "A", "B", "B", "B", "B", "B")
#kruskal.test(y~x) 
```

## The Friedman Test

The Friedman Test is also called a randomized block design and it is used where y are the data values, x is a grouping factor and and z is a blocking factor.

```{r echo=T, message=FALSE, warning=FALSE}
# friedman.test(y~x|z)
```

>
> Exercises
>

# References


