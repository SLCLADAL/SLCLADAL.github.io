---
title: "Basic Inferential Statistics using R"
author: "Martin Schweinberger"
date: "`r format(Sys.time(), '%Y-%m-%d')`"
output:
  bookdown::html_document2: 
    includes:
      in_header: GoogleAnalytics.html
bibliography: bibliography.bib
link-citations: yes
---

<!--html_preserve-->
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-130562131-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-130562131-1');
</script>
<!--/html_preserve-->

```{r uq1, echo=F, fig.cap="", message=FALSE, warning=FALSE, out.width='100%'}
knitr::include_graphics("https://slcladal.github.io/images/uq1.jpg")
```

# Introduction{-}

This tutorial introduces the basic statistical techniques for inferential statistics for hypothesis testing with R. The R-markdown document of this tutorial  can be downloaded [here](https://slcladal.github.io/rscripts/basicstatz.Rmd). The first part of this tutorial focuses on basic non-parametric tests such as Fisher's Exact test, the second part focuses on the $\chi$^2^ family of tests, and the third part focuses on simple linear regression. 

## Preparation and session set up{-}

This tutorial is based on R. If you have not installed R or are new to it, you will find an introduction to and more information how to use R [here](https://slcladal.github.io/intror.html). For this tutorials, we need to install certain *packages* from an R *library* so that the scripts shown below are executed without errors. Before turning to the code below, please install the packages by running the code below this paragraph. If you have already installed the packages mentioned below, then you can skip ahead and ignore this section. To install the necessary packages, simply run the following code - it may take some time (between 1 and 5 minutes to install all of the libraries so you do not need to worry if it takes some time).

```{r prep1, echo=T, eval = F, message=FALSE, warning=FALSE}
# install packages
install.packages(c("tidyverse", "VGAM", "fGarch",  "cfa", "gridExtra", 
                   "calibrate", "car",  "QuantPsyc", "DT"))
```

Once you have installed R, RStudio, and once you have also initiated the session by executing the code shown above, you are good to go.


# Fisher's Exact Test

Non-Parametric Tests do not require the data (or the errors of the dependent variable, to be more precise) to be distributed normally. Tests that do not require normal data are referred to as *non-parametric tests* (tests that require the data to be distributed normally are analogously called *parametric tests*). We focus on non-parametric tests first, as this family of test in frequently used in linguistics. In the later part of this section, we will focus on regression modeling where assumptions of about the data become more important.

Fisher's Exact test is very useful because it does not rely on distributional assumptions relying on normality. Instead, Fisher's Exact Test calculates the probabilities of all possible outcomes and uses these to determine significance. To understand how a Fisher's Exact test, we will use a very simple example. 

Imagine you are interested in adjective modification and you want to find out if *very* and *truly* differ in their collocational preferences. So you extract all instances of *cool*, all instances of *very*, and all instances of *truly* from a corpus. Now that you have gathered this data, you want to test if *truly* and *very* differ with respect to their preference to co-occur with *cool*. Accordingly, you tabulate the results and get the following table.

```{r fisher1, echo=F, warning=F, message=F}
# load packages
library(DT)
library(knitr)
library(kableExtra)
library(tidyverse)
library(tidyr)
# generate data
coolmatrix <- matrix(c("truly", "very", 5, 17, 40, 41), ncol = 3, byrow = F)
colnames(coolmatrix) <- c("Adverb", "with cool", "with other adjectives")
# inspect data
coolmatrix %>%
  kable(caption = "") %>%
  kableExtra::kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), full_width = F)
```

To perform a Fisher's Exact test, we first create a table with these results and then use the `fisher.test` function to perform the Fisher's Exact Test to see if *very* and *truly* differ in their preference to co-occur with *cool* (as shown below). The null hypothesis is that there is no difference between the adverbs.


```{r fisher2, eval = T, echo= T, warning=F, message=F}
# create table
coolmx <- matrix(
  c(5, 17, 40, 41),
  nrow = 2, # number of rows of the table
  # def. dimension names
  dimnames = list(
    Adverbs = c("truly", "very"),
    Adjectives = c("cool", "other adjective"))
)
# perform test
fisher.test(coolmx)
```

The results of the Fisher's Exact test show that the p-value is lower than .05, which means we reject the null hypothesis, and we are therefore justified in assuming that *very* and *truly* differ in their collocational preferences to co-occur with *cool*.

# Selected Non-Parametric Tests

Non-parametric tests are used when we the dependent variable is ordinal (it represents ranks). This is often the case when numeric variables have to be transformed into ordinal variables because the numeric variable is distributed non-normally (well, actually, it is not the distribution of the dependent variable that matters but the distribution of the errors). Thus, when the dependent variable is non-normal, it is advisable to apply tests that are designed to handle ordinal data. In the following, we will therefore briefly touch on bi-variate tests that can handle ordinal dependent variables.

## Mann-Whitney U-Test {-}

It is a rather frequent case that numeric depend variables are transformed or converted into ordinal variables because the distribution of residuals does not allow the application of a linear regression. Because we are dealing with ordinal data, the application of a chi-square test is unwarranted and we need to use another test. In such cases, the Mann-Whitney U-test (also called Wilcoxon rank sum-test) can be used. 

The Mann-Whitney U-test can also be used if the groups under investigation represent identical participants that are tested under two conditions.

Imagine we wanted to determine if two language families differed with respect to the size of their phoneme inventories. You have already ranked the inventory sizes and would now like to now if language family correlates with inventory size. To answer this question, you create the table shown below.  

```{r mwu1, warning=F, message=F}
# create table
Rank <- c(1,3,5,6,8,9,10,11,17,19, 2,4,7,12,13,14,15,16,18,20)
LanguageFamily <- c(rep("Kovati", 10), rep("Urudi", 10))
lftb <- data.frame(LanguageFamily, Rank)
```

```{r mwu1b, echo=F, warning=F, message=F}
lftb %>%
  kable(caption = "") %>%
  kableExtra::kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), full_width = F)
```

We will also briefly inspect the data visually using a box plot.

```{r mwu2, eval = T, echo=T, warning=F, message=F}
boxplot(Rank ~ LanguageFamily, col = c("orange", "darkgrey"), main = "", data = lftb)
```

To use Mann-Whitney U test, the dependent variable (Rank) must be ordinal and independent variable (Group) must be a binary factor. We briefly check this by inspecting the structure of the data.

```{r mwu3, eval = T, echo=T, warning=F, message=F}
# perform test
str(lftb)
```

As the variables are what we need them to be, we can now perform the Mann-Whitney U test on the table. The null hypothesis is that there is no difference between the 2 groups.

```{r mwu4, warning=F, message=F}
# perform test
wilcox.test(lftb$Rank ~ lftb$LanguageFamily) 
```

Since the p-value is greater than 0.05, we fail to reject the null hypothesis. The results of the Wilcoxon rank sum test tell us that the two language families do not differ significantly with respect to their phoneme inventory size.

The Wilcoxon rank sum test can also be used with continuity correction. A continuity correction is necessary when both variables represent numeric values that are non-normal. In the following example, we want to test if the reaction time for identifying a word as real is correlated with its token frequency.

For this example, we generate data is deliberately non-normal.

```{r mwu5, echo=F, warning=F, message=F}
# activate packages
library(fGarch)
library(gridExtra)
# generate non-normal skewed numeric data
r <- .1 
frequency <- rsnorm(100,0,2,4)
normal_reaction <- rsnorm(100,0,2,4)
reaction_times <- frequency*r+normal_reaction*sqrt(1-r^2)
# combine into data frame
wxdata <- data.frame(frequency, normal_reaction, reaction_times)
# plot data
p1 <- ggplot(wxdata, aes(frequency)) + # define data
  geom_density(fill = "orange", alpha = .2) + # define plot type (density)
  theme_bw() +                                # black + white background
  labs(y="Density", x = "Frequency") +        # axes titles
  coord_cartesian(ylim = c(0, .5),            # define y-axis coordinates
                  xlim = c(-5, 10))           # define x-axis coordinates
p2 <- ggplot(wxdata, aes(reaction_times)) +
  geom_density(fill = "lightgray", alpha = .2) +
  theme_bw() +
  labs(y="Density", x = "Reaction Time") +
  coord_cartesian(ylim = c(0, .5), xlim = c(-5, 10))
grid.arrange(p1, p2, nrow = 1)             # 2 plots in one window
```

Both variables are skewed (non-normally distributed) but we can use the Wilcoxon rank sum test with continuity correction which takes the skewness into account. The null hypothesis is that there is no difference between the 2 groups.

```{r mwu6, echo=T, eval = T, warning=F, message=F}
# perform test
wilcox.test(reaction_times, frequency) 
```

The p-value is greater than 0.05, therefore we cannot reject the null hypothesis. There is no statistically significant evidence to assume the groups are different.

When performing the Wilcoxon rank sum test with data that represent the same individuals that were tested under two condition, i.e. if the samples are dependent, then the argument "paired" has to be "TRUE". 

In this example, the same individuals had to read tongue twisters when they were sober and when they were intoxicated. A Wilcoxon signed rank test with continuity correction is used to test if the number of errors that occur when reading tongue twisters correlates with being sober/intoxicated. Again, we create fictitious data.

```{r mwu7, echo=T, eval = T, warning=F, message=F}
# create data
sober <- sample(0:9, 15, replace = T)
intoxicated <-  sample(3:12, 15, replace = T) 
# tabulate data
intoxtb <- data.frame(sober, intoxicated) 
```

```{r mwu7b, echo=F, warning=F, message=F}
intoxtb %>%
  kable(caption = "") %>%
  kableExtra::kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), full_width = F)
```


Now, we briefly plot the data. 

```{r mwu8, eval = T, echo=T, warning=F, message=F}
intoxtb2 <- data.frame(c(rep("sober", nrow(intoxtb)),
                         rep("intoxicated", nrow(intoxtb))),
                       c(intoxtb$sober, intoxtb$intoxicated)) %>%
  dplyr::rename(State = 1,
                Errors = 2)
ggplot(intoxtb2, aes(State, Errors)) +
  geom_boxplot(fill = c("orange", "darkgrey"), width=0.5) +
  labs(y = "Number of errors", x = "State") +
  theme_bw()
```

The boxes indicate a significant difference. Finally, we perform the Wilcoxon signed rank test with continuity correction. The null hypothesis is that the two groups are the same.

```{r mwu9, echo=T, eval = T, warning=F, message=F}
# perform test
wilcox.test(intoxtb$sober, intoxtb$intoxicated, paired=T) 
```

The p-value is lower than 0.05 (rejecting the null hypothesis) which means that the number of errors when reading tongue twisters is affected by one's state (sober/intoxicated) - at least in this fictitious example.

## Kruskal-Wallis Rank Sum Test{-}

The Kruskal-Wallis rank sum test is a type of ANOVA (Analysis of Variance). For this reason, the Kruskal Wallis Test is also referred to as a "one-way Anova by ranks" which can handle numeric and ordinal data. 

In the example below, *uhm* represents the number of filled pauses in a short 5 minute interview while speaker represents whether the speaker was a native speaker or a learner of English. As before, the data is generated and thus artificial.

```{r kwt1, warning=F, message=F}
# create data
uhms <- c(15, 13, 10, 8, 37, 23, 31, 52, 11, 17)
speaker <- c(rep("Learner", 5), rep("NativeSpeaker", 5))
# create table
uhmtb <- data.frame(speaker, uhms)
```

```{r kwt1b, echo=F, warning=F, message=F}
uhmtb %>%
  kable(caption = "") %>%
  kableExtra::kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), full_width = F)
```

Now, we briefly plot the data. 

```{r kwt2, eval = T, echo=T, warning=F, message=F}
ggplot(uhmtb, aes(speaker, uhms)) +
  geom_boxplot(fill = c("orange", "darkgrey")) +
  theme_bw() +
  labs(x = "Speaker type", y = "Errors")
```

Now, we test for statistical significance. The null hypothesis is that there is no difference between the groups.

```{r kwt3, echo=T, eval = T, warning=F, message=F}
kruskal.test(uhmtb$speaker~uhmtb$uhms) 
```

The p-value is greater than 0.05, therefore we fail to reject the null hypothesis. The Kruskal-Wallis test does not report a significant difference for the number of *uhms* produced by native speakers and learners of English in the fictitious data. 

## The Friedman Rank Sum Test{-}

The Friedman rank sum test is also called a randomized block design and it is used when the correlation between a numeric dependent variable, a grouping factor and a blocking factor is tested. The Friedman rank sum test assumes that each combination of the grouping factor (Gender) and the blocking factor (Age) occur only once. Thus, imagine that the values of *uhms* represent the means of the respective groups.

```{r ft1, echo=T, eval = T, warning=F, message=F}
# create data
uhms <- c(7.2, 9.1, 14.6, 13.8)
Gender <- c("Female", "Male", "Female", "Male")
Age <- c("Young", "Young", "Old", "Old")
# create table
uhmtb2 <- data.frame(Gender, Age, uhms)
```

```{r ft1b, echo=F, warning=F, message=F}
uhmtb2 %>%
  kable(caption = "") %>%
  kableExtra::kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), full_width = F)
```

We now perform the Friedman rank sum test.

```{r ft2, echo=T, eval = T, warning=F, message=F}
friedman.test(uhms ~ Age | Gender, data = uhmtb2)
```

In our example, age does not affect the use of filled pauses even if we control for gender as the p-value is higher than .05.

# (Pearsons's) Chi-Square Test

One of the most frequently used statistical test in linguistics is the $\chi$^2^ test (or Pearsons's chi-square test, chi-squared test, or chi-square test). We will use a simple, practical example to explore how this test works. In this example, we will test whether speakers of American English (AmE) and speakers of British English (BrE) differ in their use of the near-synonyms *sort of* and *kind of* as in "*He's sort of stupid*" and "*He's kind of stupid*". As a first step, we formulate the hypothesis that we want to test (H~1~) and its Nullhypothesis (H~0~). The Alternative- or Test Hypothesis reads:

H~1~: Speakers of AmE and BrE differ with respect to their preference for *sort of* und *kind of*.

while the Null Hypothesis (H~0~) states 

H~0~: Speakers of AmE and BrE do not differ with respect to their preference for *sort of* und *kind of*.

The H~0~ claims the non-existence of something (which is the more conservative position) and in our example the non-existence of a correlation between variety of English and the use of *sort of* und *kind of*. The question now arises what has to be the case in order to reject the H~0~ in favour of the H~1~.

To answer this question, we require information about the probability of error, i.e. the probability that the H~0~ does indeed hold for the entire population. Before performing the chi-square test, we follow the convention that the required significance level is 5 percent. In other words, we will reject the H~0~ if the likelihood for the H$_{0}$ being true is less than 5 percent given the distribution of the data. In that case, i.e. in case that the likelihood for the H~0~ being true is less than 5 percent, we consider the result of the  chi-square test as  statistically significant. This means that the observed distribution makes it very unlikely that there is no correlation between the variety of English and the use of *sort of* and *kind of*.

Let us now assume that we have performed a search for *sort of* and *kind of* in two corpora representing American and British English and that we have obtained the following frequencies:

```{r echo=F, message=FALSE, warning=FALSE}
chidata <- matrix(c(181, 655, 177, 67), nrow = 2, byrow = T)
# add column and row names
colnames(chidata) <- c("BrE", "AmE")
rownames(chidata) <- c("kindof", "sortof")
# inspect data
chidata %>%
  kable(caption = "") %>%
  kableExtra::kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), full_width = F)
```


In a first step, we now have to calculate the row and column sums of our table.

```{r echo = F, results = 'asis'}
chidata_extended <- matrix(c(181, 177, 358, 655, 67, 722, 836, 244, 1080), nrow = 3, byrow = F)
# add column and row names
colnames(chidata_extended) <- c("BrE", "AmE", "Total")
rownames(chidata_extended) <- c("kindof", "sortof", "Total")
# inspect data
chidata_extended %>%
  kable(caption = "") %>%
  kableExtra::kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), full_width = F)
```

Next, we calculate, the values that would have expected if there was no correlation between variety of English and the use of *sort of* and *kind of*. In order to get these "expected" frequencies, we apply the equation below to all cells in our table.

$\frac{Column total*Row total}{Overall total}$

In our example this means that for the cell with [+]BrE [+]kindof we get:

$\frac{836*358}{1080} = \frac{299288}{1080} = 277.1185$

For the entire table this means we get the following expected values:

```{r echo = F, results = 'asis'}
chidata_expected <- matrix(c(277.1185, 80.88148, 358, 558.8815,163.11852, 722, 836, 244, 1080), nrow = 3, byrow = F)
# add column and row names
colnames(chidata_expected) <- c("BrE", "AmE", "Total")
rownames(chidata_expected) <- c("kindof", "sortof", "Total")
# inspect data
chidata_expected %>%
  kable(caption = "") %>%
  kableExtra::kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), full_width = F)
```


In a next step, we calculate the contribution of each cell to the overall $\chi$^2^ value ($\chi$^2^ contribution). To get $\chi$^2^ contribution for each cell, we apply the equation below to each cell.

$\frac{(observed – expected)^{2}}{expected}$

In our example this means that for the cell with [+]BrE [+]kindof we get:

$\frac{(181 – 277.1185)^{2}}{277.1185} = \frac{-96.1185^{2}}{277.1185} = \frac{9238.766}{277.1185} = 33.33868$


For the entire table this means we get the following $\chi^{2}$ values:

```{r echo = F, results = 'asis'}
chidata_chi <- matrix(c(33.33869, 114.22602, 147.5647, 16.53082, 56.63839, 73.16921, 49.86951, 170.8644, 220.7339), nrow = 3, byrow = F)
# add column and row names
colnames(chidata_chi) <- c("BrE", "AmE", "Total")
rownames(chidata_chi) <- c("kindof", "sortof", "Total")
# inspect data
chidata_chi %>%
  kable(caption = "") %>%
  kableExtra::kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), full_width = F)
```

The sum of $\chi$^2^ contributions in our example is 220.7339. To see if this value is statistically significant, we need to calculate the degrees of freedom because the $\chi$ distribution differs across degrees of freedom.  Degrees of freedom are calculated according to the equation below.

$DF = (rows -1) * (columns – 1) = (2-1) * (2-1) = 1 * 1 = 1$

In a last step, we check whether the $\chi$^2^ value that we have calculated is higher than a critical value (in which case the correlation in our table is significant). Degrees of freedom are relevant here because the critical values are dependent upon the degrees of freedom: the more degrees of freedom, the higher the critical value, i.e. the harder it is to breach the level of significance.

Since there is only 1 degree of freedom in our case, we need to consider only the first column in the table of critical values below.


```{r echo = F, results = 'asis'}
critval <- matrix(c(1, 3.84, 6.64, 10.83, 2, 5.99, 9.21, 13.82, 3, 7.82, 11.35, 16.27, 4, 9.49, 13.28, 18.47, 5, 11.07, 15.09, 20.52), ncol = 4, byrow = T)
# add column names
colnames(critval) <- c("DF", "p<.05", "p<.01", "p<.001")
# inspect data
critval %>%
  kable(caption = "") %>%
  kableExtra::kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), full_width = F)
```


Since the $\chi$^2^ value that we have calculated is much higher than the critical value provided for p<.05, we can reject the H~0~ and may now claim that speakers of AmE and BrE differ with respect to their preference for *sort of* und *kind of*.


Before we summarize the results, we will calculate the effect size which is a measure for how strong the correlations are.

## Effect Sizes in Chi-Square{-}

Effect sizes are important because they correlations may be highly significant but the effect between variables can be extremely weak. The effect size is therefore a measure how strong the correlation or the explanatory and predictive power between variables is.

The effect size measure for $\chi$^2^ tests can be either the $\phi$-coefficient (phi-coefficient) or Cramer's $\phi$ (Cramer's phi). The $\phi$-coefficient is used when dealing with 2x2 tables while Cramer's $\phi$ is used when dealing with tables with more than 4 cells. The $\phi$ coefficient can be calculated by using the equation below (N = overall sample size).

$\phi = \sqrt{\frac{\chi^{2}}{N}}$

In our case, this means:

$\phi = \sqrt{\frac{220.7339}{1080}} = \sqrt{0.2043832} = 0.4520876$

The $\phi$ coefficient varies between 0 (no effect) and 1 (perfect correlation). For the division into weak, moderate and strong effects one can follow the division for $\omega$ (small omega), so that with values beginning with .1 represent weak, values between 0.3 and .5 represent moderate and values above .5 represent strong effects [@buehner2009statistik, pp. 266]. So, in this example we are dealing with a medium-sized effect/correlation.

## Chi-Square in R{-}

Before we summarize the results, we will see how to perform a chi-square test in R. In addition to what we have done above, we will also visualize the data. To begin with, we will have a look at the data set (which is the same data we have used above).

```{r chi_01_05, message=FALSE, warning=FALSE}
# inspect data
chidata %>%
  kable(caption = "") %>%
  kableExtra::kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), full_width = F)
```

We will now visualize the data with an association. Bars above the dashed line indicate that a feature combination occurs more frequently than expected by chance. The width of the bars indicates the frequency of the feature combination. 

```{r echo=F, warning=F, message=F}
assocplot(as.matrix(chidata))   # association plot
```
 
The fact that the bars are distributed complimentarily (top left red and below bar; top right black above bar; bottom left black above bar; bottom right red below bar) indicates that the use of "sort of" and "kind of" differs across AmE and BrE. We will check whether the mosaic plot confirms this impression.
 
```{r echo=T, warning=F, message=F}
mosaicplot(chidata, shade = TRUE, type = "pearson", main = "")  # mosaic plot
```

The colour contrasts in the mosaic plot substantiate the impression that the two varieties of English differ significantly. To ascertain whether the differences are statistically significant, we can now apply the chi-square test.

```{r echo=T, warning=F, message=F}
chisq.test(chidata, corr = F)  # perform chi square test
```

The results reported by `R` are identical to the results we derived by hand and confirm that BrE and AmE differ significantly in their use of "sort of" and "kind of". In a next step, we calculate the effect size.

```{r echo=T, warning=F, message=F}
# calculate effect size
sqrt(chisq.test(chidata, corr = F)$statistic / sum(chidata) * (min(dim(chidata))-1))
```

The phi coefficient of .45 shows that variety of English correlates moderately with the use of "sort of" and "kind of". We will now summarize the results.

## Summarizing Chi-Square Results{-}

The results of our analysis can be summarised as follows: A $\chi$^2^-test confirms a highly significant correlation of moderate size between the variety of English and the use of the near-synonymous hedges *sort of* and *kind of* ($\chi$^2^ = 220.73, df = 1, p < .001***, $\phi$ = .452).

## Requirements of Chi-Square{-
}
Chi-square tests depend on certain requirements that, if violated, negatively affect the reliability of the results of the test. To provide reliable results, 80 percent of cells in a table to which the chi-square test is applied have to have expected values of 5 or higher and at most 20 percent of expected values can be smaller than 5 [see @bortz109verteilungsfreie, pp.  98]. In addition, none of the expected values can be smaller than 1 [see @bortz109verteilungsfreie, pp. 136] because then, the estimation, which relies on the $\chi$^2^-distribution, becomes too imprecise to allow meaningful inferences [@cochran1954somemethods].  

If these requirements are violated, then the *Fisher's Exact Test* is more reliable and offers the additional advantage that these tests can also be applied to data that represent very small sample sizes. When applying the Fisher's Exact Test, the probabilities for all possible outcomes are calculated and the summed probability for the observed or more extreme results are determined. If this sum of probabilities exceeds five percent, then the result is deemed statistically significant.

## Chi-Square Exercises{-}

1. Imagine you are interested in whether older or younger speakers tend to refer to themselves linguistically. The underlying hypothesis is that - contrary to common belief - older people are more narcissistic compared with younger people. Given this research question, perform a chi-square test and summarize the results on the data below. 

```{r echo = F, message=FALSE, warning=FALSE}
critval <- matrix(c("Young", 61, 43, 104, "Old", 42, 36, 78, "Total", 103, 79, 182), ncol = 4, byrow = T)
# add column names
colnames(critval) <- c("", "1SGPN", "PN without 1SG", "Total")
# inspect data
critval %>%
  kable(caption = "") %>%
  kableExtra::kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), full_width = F)
```

2. Imagine you are interested in whether young men or young women exhibit a preference for the word *whatever* because you have made the unsystematic, anecdotal observation that young men use this word more frequently than young women. Given this research question, perform a chi-square test and summarize the results on the data below.

```{r whatevertb, echo=F, message=FALSE, warning=FALSE}
critval <- matrix(c("whatever", 17, 55, 71, "other words", 345128, 916552, 1261680, "Total", 345145, 916607, 1261752), ncol = 4, byrow = T)
# add column names
colnames(critval) <- c("", "YoungMales", "YoungFemales", "Total")
# inspect data
critval %>%
  kable(caption = "") %>%
  kableExtra::kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), full_width = F)
```

3. Find a partner and discuss the relationship between significance and effect size. Then, go and find another partner and discuss problems that may arise when testing the frequency of certain words compared with the overall frequency of words in a corpus. 


# Extensions of Chi-Square

In the following, we will have a look at tests and methods that can be used if the requirements for ordinary (Pearson's) chi-square tests are violated and their use would be inappropriate

## The Yates-Correction{-}

If all requirements for ordinary chi-square tests are acceptable and only the sample size is the issue, then applying a so-called *Yates-correction* may be appropriate. This type of correction is applied in cases where the overall sample size lies in-between 60 and 15 cases [@bortz109verteilungsfreie, pp. 91]. The difference between the ordinary chi-square and a Yates-corrected chi-square lies in the fact that the Yates-corrected chi-square is calculated according to the equation below.

$\frac{(|observed – expected|-0.5)^{2}}{expected}$

According to this formula, we would get the values shown below rather than the values tabulated above. It is important to note here that this is only a demonstration because a Yates-Correction would actually be inappropriate as our sample size exceeds 60 cases. 

```{r echo=F, warning=F, message=F}
critval <- matrix(c("kind of", 32.9927, 113.0407, 146.0335, "sort of", 16.3593, 56.0507, 72.4100, "Total", 49.3520, 169.0914, 218.4434), ncol = 4, byrow = T)
# add column names
colnames(critval) <- c("Variant", "BrE", "AmE", "Total")
# inspect data
critval %>%
  kable(caption = "") %>%
  kableExtra::kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), full_width = F)
```

If the Yates-correction were applied, then this results in a slightly lower $\chi$^2^-value and thus in more conservative results compared with the traditional test according to Pearson.

## Chi-Square within 2-by-k tables{-}

Although the $\chi$^2^-test is widely used, it is often used inappropriately. This is especially the case when chi-square tests are applied to data representing tables with more than two rows and more than two columns. It is important to note that applying the common Pearson’s' chi-square test to sub-tables of a larger table is inappropriate because, in such cases, a modified variant of Pearson’s' chi-square test is warranted. We will go through two examples that represent different scenarios where we are dealing with subsamples of larger tables and a modified version of the $\chi$^2^-test should be used rather than Pearson’s' chi-square. 

In this first example, we are dealing with a table consisting of two columns and multiple rows, a so-called 2\*k table (two-by-k table). In order to test if a feature combination, that is represented by a row in the 2\*k table, is significantly more common compared with other feature combinations, we need to implement the $\chi$^2^-equation from [@bortz109verteilungsfreie, pp. 126-127]. 

In this example, we want to find out whether soft and hard X-rays differ in their effect on grasshopper larva. The question is whether the larva reach or do not reach a certain life cycle depending on whether they are exposed to soft X-rays, hard X-rays, light, or beta rays. The data for this example is provided below.

```{r echo = F, results = 'asis'}
critval <- matrix(c("X-ray soft",  21, 14, 35, "X-ray hard", 18, 13, 31, "Beta-rays", 24, 12, 36, "Light", 13, 30, 43, "Total", 76, 69, 145), ncol = 4, byrow = T)
# add column names
colnames(critval) <- c("", "Mitosis not reached", "Mitosis reached", "Total")



# inspect data
critval %>%
  kable(caption = "Adapted from Bortz (1990:126).") %>%
  kableExtra::kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), full_width = F)
```


If we would apply an ordinary chi-square test, we would ignore that all data were collected together and using only a subsample would ignore the data set of which the subsample is part of. In other words, the subsample is not independent from the other data (as it represents a subsection of the whole data set). However, for exemplary reasons, we will apply an ordinary chi-square test first and then compare its results to results provided by the correct version of the chi-square test. In a first step, we create a table with all the data.

```{r echo = T, message=FALSE, warning=FALSE}
# create data
wholetable <- matrix(c(21, 14, 18, 13, 24, 12, 13, 30), byrow = T, nrow = 4)
colnames(wholetable) <- c("reached", "notreached")           # add column names
rownames(wholetable) <- c("rsoft", "rhard", "beta", "light") # add row names
```

```{r echo = F, message=FALSE, warning=FALSE}
# inspect data
wholetable %>%
  kable(caption = "") %>%
  kableExtra::kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), full_width = F)
```

Now, we extract the subsample from the data.

```{r echo = T, message=FALSE, warning=FALSE}
subtable <- wholetable[1:2,] # extract subtable
```


```{r echo = F, message=FALSE, warning=FALSE}
# inspect data
subtable %>%
  kable(caption = "") %>%
  kableExtra::kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), full_width = F)
```

Next, we apply the ordinary chi-square test to the subsample. 

```{r echo = T, message=FALSE, warning=FALSE}
# simple x2-test
chisq.test(subtable, corr = F)
```

Finally, we perform the correct chi-square test.

```{r echo = T, message=FALSE, warning=FALSE}
# load function for correct chi-square
source("https://slcladal.github.io/rscripts/x2.2k.r") 
x2.2k(wholetable, 1, 2)
```

Below is a table comparing the results of the two chi-square tests.

```{r echo = F, message=FALSE, warning=FALSE}
critval <- matrix(c("chi-squared", 0.0255, 0.025, "p-value", 0.8732, 0.8744), ncol = 3, byrow = T)
# add column names
colnames(critval) <- c("", "chi-square" , "chi-square in 2*k-tables")
# inspect data
critval %>%
  kable(caption = "Adapted from Bortz (1990: 126).") %>%
  kableExtra::kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), full_width = F)
```

The comparison shows that, in this example, the results of the two tests are very similar but this may not always be the case.


## Chi-Square within z-by-k tables{-}

Another application in which the $\chi$^2^ test is often applied incorrectly is when ordinary Parsons’s $\chi$^2^ tests are used to test portions of tables with more than two rows and more than two columns, that is z*k tables (z: row, k: column). An example is discussed by @gries2014frequency who also wrote the `R` Script for the correct version of the $\chi$^2^ test.

Let's first load the data discussed in the example of @gries2014frequency 9. The example deals with metaphors across registers. Based on a larger table, a $\chi$^2^ confirmed that registers differ with respect to the frequency of EMOTION metaphors. The more refined question is whether the use of the metaphors EMOTION IS LIGHT and EMOTION IS A FORCE OF NATURE differs between spoken conversation and fiction.

```{r echo=T, message=FALSE, warning=FALSE}
# create table
wholetable <- matrix(c(8, 31, 44, 36, 5, 14, 25, 38, 4, 22, 17, 12, 8, 11, 16, 24), ncol=4)
attr(wholetable, "dimnames")<-list(Register=c("acad", "spoken", "fiction", "new"),
Metaphor = c("Heated fluid", "Light", "NatForce", "Other"))
```

Based on the table above, we can extract the following subtable.

```{r echo = F, results = 'asis'}
critval <- matrix(c("acad", 8, 5, 4, 8, "spoken", 31, 14, 22, 11, "fiction", 44, 25, 17, 16, "new", 36, 38, 12, 24), ncol = 5, byrow = T)
# add column names
colnames(critval) <- c("Register", "Heated fluid", "Light", "NatForce", "Other")
# inspect data
critval %>%
  kable(caption = "Adapted from Gries (2014: 9).") %>%
  kableExtra::kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), full_width = F)
```

If we used an ordinary Pearson’s  $\chi$^2^ test (the use of which would be inappropriate here), it would reveal that spoken conversations do not differ significantly from fiction in their use of EMOTION IS LIGHT and EMOTION IS A FORCE OF NATURE ($\chi$^2^=3.3016, df=1, p=.069, $\phi$=.2057).

```{r echo = T, message=FALSE, warning=FALSE}
# create table
subtable <- matrix(c(14, 25, 22, 17), ncol=2)
chisq.results <- chisq.test(subtable, correct=FALSE) # WRONG!
phi.coefficient = sqrt(chisq.results$statistic / sum(subtable) * (min(dim(subtable))-1))
chisq.results
phi.coefficient
```

The correct analysis takes into account that it is a subtable that is not independent of the overall table. This means that the correct analysis should take into account the total number of cases, as well as the row and column totals [vgl. @bortz109verteilungsfreie, pp. 144-148].

In order to perform the correct analysis, we must either implement the equation proposed in @bortz109verteilungsfreie or read in the function written by @gries2014frequency and apply it to the subtable.

```{r echo = T, message=FALSE, warning=FALSE}
# load function for chi square test for subtables
source("https://slcladal.github.io/rscripts/sub.table.r") 
# apply test
results <- sub.table(wholetable, 2:3, 2:3, out="short")
# inspect results
```


The results show that the difference is, in fact, statistically significant ($\chi^{2}$=3.864, df=1, p=.0

## Configural Frequency Analysis (CFA){-}

```{r cfa1, message=FALSE, warning=FALSE}
# load packages
library(cfa)
# load data
cfadata <- read.delim("https://slcladal.github.io/data/cfadata.txt", 
                      header = T, sep = "\t")
```

```{r cfa1b, echo=F, message=F, warning=F}
# inspect data
cfadata %>%
  head(10) %>%
  kable(caption = "First 10 rows of cfadata.") %>%
  kableExtra::kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), full_width = F)
```


In a next step, we define the configurations and separate them from the counts.

```{r cfa2, message=FALSE, warning=FALSE}
# define configurations
configs <- cfadata %>%
  dplyr::select(Variety, Age, Gender, Class)
# define counts
counts <- cfadata$Frequency
```

Now that configurations and counts are separated, we can perform the configural frequency analysis.

```{r cfa3, message=FALSE, warning=FALSE}
# perform cfa
cfa(configs,counts)
```

## Hierarchical Configural Frequency Analysis (HCFA){-}

A hierarchical alternative to CFA is Hierarchical Configural Frequency Analysis (HCFA). In contrast to CFA, in HCFA, the data is assumed to be nested! We begin by defining the configurations and separate them from the counts.

```{r hcfa2, message=FALSE, warning=FALSE}
# define configurations
configs <- cfadata %>%
  dplyr::select(Variety, Age, Gender, Class)
# define counts
counts <- cfadata$Frequency
```

Now that configurations and counts are separated, we can perform the hierarchical configural frequency analysis.

```{r hcfa3, message=FALSE, warning=FALSE}
# perform cfa
hcfa(configs,counts)
```

According to the HCFA, only a single configuration (Variety : Age : Class) is significant (X2 = 12.21, p = .016). 

# t-Test

T-tests are used very widely and they determine if the means of two groups are significantly different. As such, t-tests are used when we have a normally distributed (or parametric), numeric dependent variable and a nominal predictor variable.

We now load some data that we can apply a t-test to. The data represents scores on a proficiency test of native speakers and learners of English. We want to use a t-test to determine if the native speakers and learners differ in their proficiency.

```{r t1, message=F, warning=F}
# load data
tdata <- read.delim("https://slcladal.github.io/data/data03.txt", 
                      header = T, sep = "\t") %>%
  dplyr::rename(NativeSpeakers = 1,
                Learners = 2) %>%
  tidyr::gather(Group, Score, NativeSpeakers:Learners) %>%
  dplyr::mutate(Group = factor(Group))
```

```{r t2, echo = F, message=F, warning=F}
# inspect data
tdata %>%
  head(10) %>%
  kable(caption = "First 10 rows of tdata.") %>%
  kableExtra::kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), full_width = F)
```

We now apply a t-test to the data.

```{r}
t.test(Score ~ Group, data = tdata)
```

As the p-value is higher than .05, we cannot reject the H-0- and we thus have to conclude that our evidence does not suffice to say that learners and Native Speakers differ in their proficiency.

# Simple Linear Regression
 
Simple linear regression is a very useful and widely used test to determine if predictors correlate with a numeric dependent variable. (Simple) Linear regression outperforms alternatives such as t-tests because it not only determines significance and provides effect sizes but but it also provides model fit statistics (for example, R^2^) and allows model diagnostics. For an elaborate tutorial on (simple) linear regression and how to implement a linear regression in R, have a look at [this tutorial](https://slcladal.github.io/regression.html#11_Simple_Linear_Regression).


# Citation & Session Info {-}

Schweinberger, Martin. `r format(Sys.time(), '%Y')`. *Basic Inferential Statistics with R*. Brisbane: The University of Queensland. url: https://slcladal.github.io/basicstatzchi.html (Version `r format(Sys.time(), '%Y.%m.%d')`).

```
@manual{schweinberger`r format(Sys.time(), '%Y')`basicstatz,
  author = {Schweinberger, Martin},
  title = {Basic Inferential Statistics using R},
  note = {https://slcladal.github.io/basicstatzchi.html},
  year = {`r format(Sys.time(), '%Y')`},
  organization = "The University of Queensland, School of Languages and Cultures},
  address = {Brisbane},
  edition = {`r format(Sys.time(), '%Y.%m.%d')`}
}
```

```{r fin}
sessionInfo()
```

***

[Back to top](#introduction)

[Back to HOME](https://slcladal.github.io/index.html)

***


# References{-}


