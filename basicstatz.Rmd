---
title: "Basic Inferential Statistics with R"
author: "UQ SLC Digital Team"
date: "`r format(Sys.time(), '%Y-%m-%d')`"
output:
  bookdown::html_document2: default
bibliography: bibliography.bib
link-citations: yes
---
```{r uq1, echo=F, fig.cap="", message=FALSE, warning=FALSE, out.width='100%'}
knitr::include_graphics("images/uq1.jpg")
```

# Introduction

This tutorial introduces the basic statistical techniques for inferential statistics for hypothesis testing with R. The entire code for the tutorial  can be downloaded [here](https://slcladal.github.io/rscripts/basicstatzrscript.r). The first part of this tutorial focuses on basic non-parametric tests such as Fisher's Exact test while the second part introduces parametric tests such as the t-test. 

Non-Parametric Tests do not require the data (or the errors of the dependent variable, to be more precise) to be distributed normally. Tests that do not require normal data are referred to as *non-parametric tests* (tests that require the data to be distributed normally are analogously called *parametric tests*). We focus on non-parametric tests first, as this family of test in frequently used in linguistics. In the later part of this section, we will focus on regression modelling where assumptions of about the data become more important. 

**Preparation and session set up**

As all visualizations in this tutorial rely on R, it is necessary to install R and RStudio. If these programs (or, in the case of R, environments) are not already installed on your machine, please search for them in your favorite search engine and add the term "download". Open any of the first few links and follow the installation instructions (they are easy to follow, do not require any specifications, and are pretty much self-explanatory).

In addition, certain *packages* need to be installed from an R *library* so that the scripts shown below are executed without errors. Before turning to the code below, please install the packages by running the code below this paragraph. If you have already installed the packages mentioned below, then you can skip ahead ignore this section. To install the necessary packages, simply run the following code - it may take some time (between 1 and 5 minutes to install all of the libraries so you do not need to worry if it takes some time).

```{r prep1, echo=T, eval = F, message=FALSE, warning=FALSE}
# install libraries
install.packages(c("stringr", "VGAM", "fGarch"))
```

Once you have installed R, R-Studio, and have also initiated the session by executing the code shown above, you are good to go.

# Fisher's Exact Test

Fisher's Exact test is very useful because it does not rely on distributional assumptions relying on normality. Instead, Fisher's Exact Test calculates the probabilities of all possible outcomes and uses these to determine significance. To understand how a Fisher's Exact test, we will use a very simple example. 

Imagine you are interested in adjective modification and you want to find out if "very" and "truly" differ in their collocational preferences. So you extract all instances of "cool", all instances of "very", and all instances of "truly" from a corpus. Now that you have gathered this data, you want to test if "truly" and "very" differ with respect to their preference to co-occur with "cool". Accordingly, you tabulate the results and get the following table.

```{r fisher1, echo=TRUE, eval = T, warning=F, message=F}
library(knitr)                 # activate library for tabulating
# generate data
coolmatrix <- matrix(c("truly", "very", 5, 17, 40, 41), ncol = 3, byrow = F)
colnames(coolmatrix) <- c("Adverb", "with cool", "with other adjectives")
kable(coolmatrix, caption = "Co-occurrence of cool with adverbs.")
```

To perform a Fisher's Exact test, we first need to create a table with these results in "R".

```{r fisher2, eval = T, echo= T, warning=F, message=F}
# create table
coolmx <- matrix(
  c(5, 17, 40, 41),
  nrow = 2, # number of rows of the table
  # def. dimension names
  dimnames = list(
    Adverbs = c("truly", "very"),
    Adjectives = c("cool", "other adjective"))
)
# inspect resulting table
coolmx
```

Once we have the created a matrix with these numbers, we can perform the Fisher's Exact test to see if "very" and "truly" differ in their preference to co-occur with "cool". The null hypothesis is that there is no difference between the adverbs.

```{r fisher3, eval = T, echo=TRUE, warning=F, message=F}
# perform test
fisher.test(coolmx)
```

The results of the Fisher's Exact test show that the p-value is lower than .05, which means we reject the null hypothesis, and we are therefore justified in assuming that "very" and "truly" differ in their collocational preferences to co-occur with "cool."

# Ranking and Sign-Based Non-Parametric Tests

If the depended variable is neither nominal nor categorical, but ordinal (that is if the dependent variable represents an order factor such as ranks), using a chi-square test is not warranted. In such cases it is advisable to apply tests that are designed to handle ordinal data. In the following, we will therefore briefly touch on bi-variate tests that can handle ordinal dependent variables.

# Mann-Whitney U-Test / Wilcoxon Rank Sum-Test

It is a rather frequent case that numeric depend variables are transformed or converted into ordinal variables because the distribution of residuals does not allow the application of a linear regression. Because we are dealing with ordinal data, the application of a chi-square test is unwarranted and we need to use another test. In such cases, the Mann-Whitney U-test (also called Wilcoxon rank sum-test) can be used. 
The Mann-Whitney U-test can also be used if the groups under investigation represent identical participants that are tested under two conditions.

Imagine we wanted to determine if two language families differed with respect to the size of their phoneme inventories. You have already ranked the inventory sizes and would now like to now if language family correlates with inventory size. To answer this question, you create the table shown below.  

```{r mwu1, eval = T, echo=T, warning=F, message=F}
# create table
rank <- c(1,3,5,6,8,9,10,11,17,19, 2,4,7,12,13,14,15,16,18,20)
groups <- c(rep("Lang.Fam.1", 10), rep("Lang.Fam.2", 10))
language_family_tb <- data.frame(groups, rank)
# inspect table
language_family_tb
```

We will also briefly inspect the data visually using a box plot.

```{r mwu2, eval = T, echo=T, warning=F, message=F}
# inspect data
boxplot(rank ~ groups, col = c("orange", "darkgrey"), main = "", data = language_family_tb)
```

To use Mann-Whitney U test, the dependent variable (Rank) must be ordinal and independent variable (Group) must be a binary factor. We briefly check this by inspecting the structure of the data.

```{r mwu3, eval = T, echo=T, warning=F, message=F}
# perform test
str(language_family_tb)
```

As the variables are what we need them to be, we can now perform the Mann-Whitney U test on the table. The null hypothesis is that there is no difference between the 2 groups.

```{r mwu4, eval = T, echo=T, warning=F, message=F}
# perform test
wilcox.test(language_family_tb$rank ~ language_family_tb$groups) 
```

Since the p-value is greater than 0.05, we fail to reject the null hypothesis. The results of the Wilcoxon rank sum test tell us that the two language families do not differ significantly with respect to their phoneme inventory size.

The Wilcoxon rank sum test can also be used with continuity correction. A continuity correction is necessary when both variables represent numeric values that are non-normal. In the following example, we want to test if the reaction time for identifying a word as real is correlated with its token frequency.

For this example, we generate data is deliberately non-normal.

```{r mwu5, echo=T, eval = T, warning=F, message=F}
# load librarieslibrary(VGAM)
library(fGarch)
# generate non-normal skewed numeric data
r <- .1 
frequency <- rsnorm(100,0,2,4)
normal_reaction <- rsnorm(100,0,2,4)
reaction_times <- frequency*r+normal_reaction*sqrt(1-r^2)
# inspect data
par(mfrow=c(1,2))
boxplot(frequency, col = "orange", main = "Frequency")
boxplot(reaction_times, col = "darkgrey", main = "Reaction Times")
par(mfrow=c(1,1))
```

Both variables are skewed but we can use the Wilcoxon rank sum test with continuity correction which takes the skewness into account. The null hypothesis is that there is no difference between the 2 groups.

```{r mwu6, echo=T, eval = T, warning=F, message=F}
# perform test
wilcox.test(reaction_times, frequency) 
```

The p-value is greater than 0.05, therefore we fail to reject the null hypothesis. There is no statistically significant evidence to assume the groups are different.

When performing the Wilcoxon rank sum test with data that represent the same individuals that were tested under two condition, i.e. if the samples are dependent, then the argument "paired" has to be "TRUE". 

In this example, the same individuals had to read tongue twisters when they were sober and when they were intoxicated. A Wilcoxon signed rank test with continuity correction is used to test if the number of errors that occur when reading tongue twisters correlates with being sober/intoxicated. Again, we create fictitious data.

```{r mwu7, echo=T, eval = T, warning=F, message=F}
# create data
sober <- sample(0:9, 15, replace = T)
intoxicated <-  sample(3:12, 15, replace = T) 
# tabulate data
intoxtb <- data.frame(sober, intoxicated) 
# inspect data
str(intoxtb)
```

Now, we briefly plot the data. 

```{r mwu8, eval = T, echo=T, warning=F, message=F}
# plot data
boxplot(sober, intoxicated, col = c("orange", "darkgrey"), main = "", ylab = "Number of errors", xlab = "State", data = intoxtb)
```

The boxes indicate a significant difference. Finally, we perform the Wilcoxon signed rank test with continuity correction. The null hypothesis is that the two groups are the same.

```{r mwu9, echo=T, eval = T, warning=F, message=F}
# perform test
wilcox.test(intoxtb$sober, intoxtb$intoxicated, paired=T) 
```

The p-value is lower than 0.05 (rejecting the null hypothesis) which means that the number of errors when reading tongue twisters is affected by one's state (sober/intoxicated) - at least in this fictitious example.

# Kruskal-Wallis Rank Sum Test

The Kruskal-Wallis rank sum test is a type of ANOVA (Analysis of Variance). For this reason, the Kruskal Wallis Test is also referred to as a "one-way Anova by ranks" which can handle numeric and ordinal data. 

In the example below, *uhm* represents the number of filled pauses in a short 5 minute interview while speaker represents whether the speaker was a native speaker or a learner of English. As before, the data is generated and thus artificial.

```{r kwt1, echo=T, eval = T, warning=F, message=F}
# create data
uhms <- c(15, 13, 10, 8, 37, 23, 31, 52, 11, 17)
speaker <- c(rep("Learner", 5), rep("NativeSpeaker", 5))
# create table
uhmtb <- data.frame(speaker, uhms)
# inspect table
str(uhmtb)
```

Now, we briefly plot the data. 

```{r kwt2, eval = T, echo=T, warning=F, message=F}
# inspect data
boxplot(uhms ~ speaker, col = c("orange", "darkgrey"), main = "", ylab = "Number of errors", xlab = "State", data = uhmtb)
```

Now, we test for statistical significance. The null hypothesis is that there is no difference between the groups.

```{r kwt3, echo=T, eval = T, warning=F, message=F}
kruskal.test(uhmtb$speaker~uhmtb$uhms) 
```

The p-value is greater than 0.05, therefore we fail to reject the null hypothesis. The Kruskal-Wallis test does not report a significant difference for the number of "uhms" produced by native speakers and learners of English in the fictitious data. 

# The Friedman Rank Sum Test

The Friedman rank sum test is also called a randomized block design and it is used when the correlation between a numeric dependent variable, a grouping factor and a blocking factor is tested. The Friedman rank sum test assumes that each combination of the grouping factor (Gender) and the blocking factor (Age) occur only once. Thus, imagine that the values of "Uhms" represent the means of the respective groups.

```{r ft1, echo=T, eval = T, warning=F, message=F}
# create data
uhms <- c(7.2, 9.1, 14.6, 13.8) #sample means
gender <- c("Female", "Male", "Female", "Male")
age <- c("Young", "Young", "Old", "Old")
# create table
uhmtb2 <- data.frame(gender, age, uhms)
# inspect table
uhmtb2
```



```{r ft2, echo=T, eval = T, warning=F, message=F}
# perform Friedman test
friedman.test(uhms ~ age | gender, data = uhmtb2)
```

In our example, age does not affect the use of filled pauses even if we control for gender as the p-value is higher than .05.

# How to cite this tutorial {-}

Schweinberger, Martin. 2020. *Basic Inferential Statistics with R*. Brisbane: The University of Queensland. url: https://slcladal.github.io/basicstatz.html.

# References {-}

