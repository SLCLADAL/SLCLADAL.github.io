---
title: "Basic Inferential Statistics"
author: "UQ SLC Digital Team"
date: "`r format(Sys.time(), '%Y-%m-%d')`"
output: html_document
bibliography: bibliography.bib
---
```{r uq1, echo=F, fig.cap="", message=FALSE, warning=FALSE, out.width='100%'}
knitr::include_graphics("images/uq1.jpg")
```

# Introduction
This tutorial introduces basic statistical techniques from inferential statistics for hypothesis testing. The first part of this section focuses on basic non-parametric  tests such as the chi-square family of tests while the second part introduces simple regression models and discusses their underlying logic. 

# Non-Parametric Tests
This section focuses on test that do not require the data to be distributed normally. Tests that do not rewuire normal data are referred to as *non-parametric tests* (test that require the data to be distributed normally are analogously called *parametric tests*). We focus on non-parametric tests first as this family of test in frequently used in linguistics. In the later part of this section, we will focus on regression modelling where assumptions of about the data become more important. 

## Pearsons's Chi-Square Test

To explore how chi-square tests (or Pearsons's chi-square tests, chi-squared tests, or simply $\chi$^2^ tests) work, we will focus on a practical example. In this example, we will test whether speakers of American English (AmE) and speakers of British English (BrE) differ in their use of the near-synonyms *sort of* and *kind of* as in "*He's sort of stupid*" and "*He's kind of stupid*". As a first step, we formulate the hypothesis that we want to test (H~1~) and its Nullhypothesis (H~0~). The Alternative- or Test Hypothesis reads:

H~1~: Speakers of AmE and BrE differ with respect to their preference for *sort of* und *kind of*.

while the Null Hypothesis (H~0~) states 

H~0~: Speakers of AmE and BrE do not differ with respect to their preference for *sort of* und *kind of*.

The H~0~ claims the non-existence of something (which is the more conservative position) and in our example the non-existence of a correlation between variety of English and the use of *sort of* und *kind of*. The question now arises what has to be the case in order to reject the H~0~ in favour of the H~1~.

To answer this question, we require information about the probability of error, i.e. the probability that the H~0~ does indeed hold for the entire population. Before performing the chi-square test, we follow the convention that the required significance level is 5 percent. In other words, we will reject the H~0~ if the likelihood for the H$_{0}$ being true is less than 5 percent given the distribution of the data. In that case, i.e. in case that the likelihood for the H~0~ being true is less than 5 percent, we consider the result of the  chi-square test as  statistically significant. This means that the observed distribution makes it very unlikely that there is no correlation between the variety of English and the use of *sort of* and *kind of*.

Let us now assume that we have performed a search for *sort of* and *kind of* in two corpora representing American and British English and that we have obtained the following frequencies:

```{r echo=F, message=FALSE, warning=FALSE}
library(knitr)
chidata <- matrix(c(181, 655, 177, 67), nrow = 2, byrow = F)
# add column and row names
colnames(chidata) <- c("BrE", "AmE")
rownames(chidata) <- c("kindof", "sortof")
kable(chidata, caption = "Observed frequencies of *sort of* and *kind of* in American and British English")
```

In a first step, we now have to calculate the row and column sums of our table.

```{r echo = F, results = 'asis'}
library(knitr)
chidata_extended <- matrix(c(181, 177, 358, 655, 67, 722, 836, 244, 1080), nrow = 3, byrow = F)
# add column and row names
colnames(chidata_extended) <- c("BrE", "AmE", "Total")
rownames(chidata_extended) <- c("kindof", "sortof", "Total")
kable(chidata_extended, caption = "Observed frequencies of *sort of* and *kind of* in American and British English with row and column totals")
```

Next, we calculate, the values that would have expected if there was no correlation between variety of English and the use of *sort of* and *kind of*. In order to get these "expected" frequencies, we apply the equation below to all cells in our table.

$\frac{Column total*Row total}{Overall total}$

In our example this means that for the cell with [+]BrE [+]kindof we get:

$\frac{836*358}{1080} = \frac{299288}{1080} = 277.1185$

For the entire table this means we get the following expected values:

```{r echo = F, results = 'asis'}
library(knitr)
chidata_expected <- matrix(c(277.1185, 80.88148, 358, 558.8815,163.11852, 722, 836, 244, 1080), nrow = 3, byrow = F)
# add column and row names
colnames(chidata_expected) <- c("BrE", "AmE", "Total")
rownames(chidata_expected) <- c("kindof", "sortof", "Total")
kable(chidata_expected, caption = "Expected frequencies of *sort of* and *kind of* in American and British English with row and column totals")
```


In a next step, we calculate the contribution of each cell to the overall $\chi$^2^ value ($\chi$^2^ contribution). To get $\chi$^2^ contribution for each cell, we apply the equation below to each cell.

$\frac{(observed – expected)^{2}}{expected}$

In our example this means that for the cell with [+]BrE [+]kindof we get:

$\frac{(181 – 277.1185)^{2}}{277.1185} = \frac{-96.1185^{2}}{277.1185} = \frac{9238.766}{277.1185} = 33.33868$


For the entire table this means we get the following $\chi^{2}$ values:

```{r echo = F, results = 'asis'}
library(knitr)
chidata_chi <- matrix(c(33.33869, 114.22602, 147.5647, 16.53082, 56.63839, 73.16921, 49.86951, 170.8644, 220.7339), nrow = 3, byrow = F)
# add column and row names
colnames(chidata_chi) <- c("BrE", "AmE", "Total")
rownames(chidata_chi) <- c("kindof", "sortof", "Total")
kable(chidata_chi, caption = "Chi values of *sort of* and *kind of* in American and British English with row and column totals")
```

The sum of $\chi$^2^ contributions in our example is 220.7339. To see if this value is statistically significant, we need to calculate the degrees of freedom because the $\chi$ distribution differs across degrees of freedom.  Degrees of freedom are calculated according to the equation below.

$DF = (rows -1) * (columns – 1) = (2-1) * (2-1) = 1 * 1 = 1$

In a last step, we check whether the $\chi$^2^ value that we have calculated is higher than a critical value (in which case the correlation in our table is significant). Degrees of freedom are relevant here because the critical values are dependent upon the degrees of freedom: the more degrees of freedom, the higher the critical value, i.e. the harder it is to breach the level of significance.

Since there is only 1 degree of freedom in our case, we need to consider only the first column in the table of critical values below.


```{r echo = F, results = 'asis'}
library(knitr)
critval <- matrix(c(1, 3.84, 6.64, 10.83, 2, 5.99, 9.21, 13.82, 3, 7.82, 11.35, 16.27, 4, 9.49, 13.28, 18.47, 5, 11.07, 15.09, 20.52), ncol = 4, byrow = T)
# add column names
colnames(critval) <- c("DF", "p<.05", "p<.01", "p<.001")
kable(critval, caption = "Critical chi values for 1 to 5 degrees of freedom")
```


Since the $\chi$^2^ value that we have calculated is much higher than the critical value provided for p<.05, we can reject the H~0~ and may now claim that speakers of AmE and BrE differ with respect to their preference for *sort of* und *kind of*.


Before we summarize the results, we will calculate the effect size which is a measure for how strong the correlations are.

### Effect Sizes in Chi-Square 

Effect sizes are important because they correlations may be highly significant but the effect between variables can be extremely weak. The effect size is therefore a measure how strong the correlation or the explanatory and predictive power between variables is.

The effect size measure for $\chi$^2^ tests can be either the $\phi$-coefficient (phi-coefficient) or Cramer's $\phi$ (Cramer's phi). The $\phi$-coefficient is used when dealing with 2x2 tables while Cramer's $\phi$ is used when dealing with tables with more than 4 cells. The $\phi$ coefficient can be calculated by using the equation below (N = overall sample size).

$\phi = \sqrt{\frac{\chi^{2}}{N}}$

In our case, this means:

$\phi = \sqrt{\frac{220.7339}{1080}} = \sqrt{0.2043832} = 0.4520876$

The $\phi$ coefficient varies between 0 (no effect) and 1 (perfect correlation). For the division into weak, moderate and strong effects one can follow the division for $\omega$ (small omega), so that with values beginning with .1 represent weak, values between 0.3 and .5 represent moderate and values above .5 represent strong effects [@buehner2009statistik, pp. 266]. So, in this example we are dealing with a medium-sized effect/correlation.

### Chi-Square in R

Before we summarize the results, we will see how to perform a chi-square test in `R`. In addition to what we have done above, we will also visualize the data. To begin with, we will have a look at the data set (which is the same data we have used above).

```{r echo=T, warning=F, message=F}
chidata              # inspect data
```

We will now visualize the data with an association. Bars above the dashed line indicate that a feature combination occurs more frequently than expected by chance. The width of the bars indicates the frequency of the feature combination. 

```{r echo=F, warning=F, message=F}
assocplot(as.matrix(chidata))   # association plot
```
 
The fact that the bars are distributed complimentarily (top left red and below bar; top right black above bar; bottom left black above bar; bottom right red below bar) indicates that the use of "sort of" and "kind of" differs across AmE and BrE. We will check whether the mosaic plot confirms this impression.
 
```{r echo=T, warning=F, message=F}
mosaicplot(chidata, shade = TRUE, type = "pearson", main = "")  # mosaic plot
```

The colour contrasts in the mosaic plot substantiate the impression that the two varieties of English differ significantly. To ascertain whether the differences are statistically significant, we can now apply the chi-square test.

```{r echo=T, warning=F, message=F}
chisq.test(chidata, corr = F)  # perform chi square test
```

The results reported by `R` are identical to the results we derived by hand and confirm that BrE and AmE differ significantly in their use of "sort of" and "kind of". In a next step, we calculate the effect size.

```{r echo=T, warning=F, message=F}
# calculate effect size
sqrt(chisq.test(chidata, corr = F)$statistic / sum(chidata) * (min(dim(chidata))-1))
```

The phi coefficient of .45 shows that variety of English correlates moderately with the use of "sort of" and "kind of". We will now summarize the results.

### Summarizing Chi-Square Results

The results of our analysis can be summarised as follows: A $\chi$^2^-test confirms a highly significant correlation of moderate size between the variety of English and the use of the near-synonymous hedges *sort of* and *kind of* ($\chi$^2^ = 220.73, df = 1, p < .001***, $\phi$ = .452).

### Requirements of Chi-Square
Chi-square tests depend on certain requirements that, if violated, negatively affect the reliability of the results of the test. To provide reliable results, 80 percent of cells in a table to which the chi-square test is applied have to have expected values of 5 or higher and at most 20 percent of expected values can be smaller than 5 [see @bortz1990verteilungsfreie, pp.  98]. In addition, none of the expected values can be smaller than 1 [see @bortz1990verteilungsfreie, pp. 136] because then, the estimation, which relies on the $\chi$^2^-distribution, becomes too imprecise to allow meaningful inferences [@cochran1954somemethods].  

If these requirements are violated, then the *Fisher's Exact Test* is more reliable and offers the additional advantage that these tests can also be applied to data that represent very small sample sizes. When applying the Fisher's Exact Test, the probabilities for all possible outcomes are calculated and the summed probability for the observed or more extreme results are determined. If this sum of probabilities exceeds five percent, then the result is deemed statistically significant.

## Extensions of Chi-Square
In the following, we will have a look at tests and methods that can be used if the requirements for ordinary (Pearson's) chi-square tests are violated and their use would be inappropriate

### The Yates-Correction
If all requirements for ordinary chi-square tests are acceptable and only the sample size is the issue, then applying a so-called *Yates-correction* may be appropriate. This type of correction is applied in cases where the overall sample size lies in-between 60 and 15 cases ([@bortz1990verteilungsfreie] 91). The difference between the ordinary chi-square and a Yates-corrected chi-square lies in the fact that the Yates-corrected chi-square is calculated according to the equation below.

$\frac{(|observed – expected|-0.5)^{2}}{expected}$

According to this formula, we would get the values shown below rather than the values tabulated above. It is important to note here that this is only a demonstration because a Yates-Correction would actually be inappropriate as our sample size exceeds 60 cases. 

```{r echo=F, warning=F, message=F}
library(knitr)
critval <- matrix(c("kind of", 32.9927, 113.0407, 146.0335, "sort of", 16.3593, 56.0507, 72.4100, "Total", 49.3520, 169.0914, 218.4434), ncol = 4, byrow = T)
# add column names
colnames(critval) <- c("Variant", "BrE", "AmE", "Total")
kable(critval, caption = "Corrected chi-square values for sort of and kind of in BrE and AmE")
```

If the Yates-correction were applied, then this results in a slightly lower $\chi$^2^-value and thus in more conservative results compared with the traditional test according to Pearson.

### Chi-Square within 2*k Tables

Although the $\chi$^2^-test is widely used, it is often used inappropriately. This is especially the case when chi-square tests are applied to data representing tables with more than two rows and more than two columns. It is important to note that applying the common Pearson’s' chi-square test to sub-tables of a larger table is inappropriate because, in such cases, a modified variant of Pearson’s' chi-square test is warranted. We will go through two examples that represent different scenarios where we are dealing with subsamples of larger tables and a modified version of the $\chi$^2^-test should be used rather than Pearson’s' chi-square. 

In this first example, we are dealing with a table consisting of two columns and multiple rows, a so-called 2\*k table (two-by-k table). In order to test if a feature combination, that is represented by a row in the 2\*k table, is significantly more common compared with other feature combinations, we need to implement the $\chi$^2^-equation from [@bortz1990verteilungsfreie, pp. 126-127]. 

In this example, we want to find out whether soft and hard X-rays differ in their effect on grasshopper larva. The question is whether the larva reach or do not reach a certain life cycle depending on whether they are exposed to soft X-rays, hard X-rays, light, or beta rays. The data for this example is provided below.

```{r echo = F, results = 'asis'}
library(knitr)
critval <- matrix(c("X-ray soft",  21, 14, 35, "X-ray hard", 18, 13, 31, "Beta-rays", 24, 12, 36, "Light", 13, 30, 43, "Total", 76, 69, 145), ncol = 4, byrow = T)
# add column names
colnames(critval) <- c("", "Mitosis not reached", "Mitosis reached", "Total")
kable(critval, caption = "Data adapted from Bortz (1990: 126)")
```


If we would apply an ordinary chi-square test, we would ignore that all data were collected together and using only a subsample would ignore the data set of which the subsample is part of. In other words, the subsample is not independent from the other data (as it represents a subsection of the whole data set). However, for exemplary reasons, we will apply an ordinary chi-square test first and then compare its results to results provided by the correct version of the chi-square test. In a first step, we create a table with all the data.

```{r echo = T, message=FALSE, warning=FALSE}
# create tdata
wholetable <- matrix(c(21, 14, 18, 13, 24, 12, 13, 30), byrow = T, nrow = 4)
colnames(wholetable) <- c("reached", "notreached")           # add column names
rownames(wholetable) <- c("rsoft", "rhard", "beta", "light") # add row names
wholetable                                                   # inspect data
```

Now, we extract the subsample from the data.

```{r echo = T, message=FALSE, warning=FALSE}
subtable <- wholetable[1:2,] # extract subtable
subtable                     # inspect subtable
```

Next, we apply the ordinary chi-square test to the subsample. 

```{r echo = T, message=FALSE, warning=FALSE}
# simple x2-test
chisq.test(subtable, corr = F)
```

Finally, we perform the correct chi-square test.

```{r echo = T, message=FALSE, warning=FALSE}
# load function for correct chi-square
source("rscripts/x2.2k.r") 
x2.2k(wholetable, 1, 2)
```

Below is a table comparing the results of the two chi-square tests.

```{r echo = T, message=FALSE, warning=FALSE}
library(knitr)
critval <- matrix(c("chi-squared", 0.0255, 0.025, "p-value", 0.8732, 0.8744), ncol = 3, byrow = T)
# add column names
colnames(critval) <- c("", "chi-square" , "chi-square in 2*k-tables")
kable(critval, caption = "Table adapted from Bortz (1990: 126)")
```

The comparison shows that, in this example, the results of the two tests are very similar but this may not always be the case.


### Chi-Square within z*k Tables

Another application in which the $\chi$^2^ test is often applied incorrectly is when ordinary Parsons’s $\chi$^2^ tests are used to test portions of tables with more than two rows and more than two columns, that is z*k tables (z: row, k: column). An example is discussed by @gries2014frequency who also wrote the `R` Script for the correct version of the $\chi$^2^ test.

Let's first load the data discussed in the example of @gries2014frequency 9. The example deals with metaphors across registers. Based on a larger table, a $\chi$^2^ confirmed that registers differ with respect to the frequency of EMOTION metaphors. The more refined question is whether the use of the metaphors EMOTION IS LIGHT and EMOTION IS A FORCE OF NATURE differs between spoken conversation and fiction.

```{r echo=T, message=FALSE, warning=FALSE}
# create table
wholetable <- matrix(c(8, 31, 44, 36, 5, 14, 25, 38, 4, 22, 17, 12, 8, 11, 16, 24), ncol=4)
attr(wholetable, "dimnames")<-list(Register=c("acad", "spoken", "fiction", "new"),
Metaphor = c("Heated fluid", "Light", "NatForce", "Other"))
```

Based on the table above, we can extract the following subtable.

```{r echo = F, results = 'asis'}
library(knitr)
critval <- matrix(c("acad", 8, 5, 4, 8, "spoken", 31, 14, 22, 11, "fiction", 44, 25, 17, 16, "new", 36, 38, 12, 24), ncol = 5, byrow = T)
# add column names
colnames(critval) <- c("Register", "Heated fluid", "Light", "NatForce", "Other")
kable(critval, caption = "Table adapted from Gries (2014: 9)")
```

If we used an ordinary Pearson’s  $\chi$^2^ test (the use of which would be inappropriate here), it would reveal that spoken conversations do not differ significantly from fiction in their use of EMOTION IS LIGHT and EMOTION IS A FORCE OF NATURE ($\chi$^2^=3.3016, df=1, p=.069, $\phi$=.2057).

```{r echo = T, message=FALSE, warning=FALSE}
# create table
subtable <- matrix(c(14, 25, 22, 17), ncol=2)
chisq.results <- chisq.test(subtable, correct=FALSE) # WRONG!
phi.coefficient = sqrt(chisq.results$statistic / sum(subtable) * (min(dim(subtable))-1))
chisq.results
phi.coefficient
```

The correct analysis takes into account that it is a subtable that is not independent of the overall table. This means that the correct analysis should take into account the total number of cases, as well as the row and column totals [vgl.@bortz1990verteilungsfreie 144-148].

In order to perform the correct analysis, we must either implement the equation proposed in @bortz1990verteilungsfreie 144-148 or read in the function written by @gries2014frequency and apply it to the subtable.

```{r echo = T, message=FALSE, warning=FALSE}
# load function for chi square test for subtables
source("rscripts/sub.table.r") 
# apply test
results <- sub.table(wholetable, 2:3, 2:3, out="short")
# inspect results
results
```


The results show that the difference is, in fact, statistically significant ($\chi^{2}$=3.864, df=1, p=.049*).


### Chi-Square Exercises 

1. Imagine you are interested in whether older or younger speakers tend to refer to themselves linguistically. The underlying hypothesis is that - contrary to common belief - older people are more narcissistic compared with younger people. Given this research question, perform a chi-square test and summarize the results on the data below. 

```{r echo = F, message=FALSE, warning=FALSE}
library(knitr)
critval <- matrix(c("Young", 61, 43, 104, "Old", 42, 36, 78, "Total", 103, 79, 182), ncol = 4, byrow = T)
# add column names
colnames(critval) <- c("", "1SGPN", "PN without 1SG", "Total")
kable(critval, caption = "Table adapted from Gries (2014: 9)")
```



2. Imagine you are interested in whether young men or young women exhibit a preference for the word *whatever* because you have made the unsystematic, anecdotal observation that young men use this word more frequently than young women. Given this research question, perform a chi-square test and summarize the results on the data below.

```{r whatevertb, echo=F, message=FALSE, warning=FALSE}
#install.packages("knitr")      # install library (remove # to activate)
#install.packages("kableExtra") # install library (remove # to activate)
library(knitr)
library(kableExtra)
critval <- matrix(c("whatever", 17, 55, 71, "other words", 345128, 916552, 1261680, "Total", 345145, 916607, 1261752), ncol = 4, byrow = T)
# add column names
colnames(critval) <- c("", "YoungMales", "YoungFemales", "Total")
kable(critval, caption = "Observed frequency with row- and column totals for the use of *whatever* by male and female speakers.") %>%
  kable_styling(bootstrap_options = "striped", full_width = T, position = "left")
```

3. Find a partner and discuss the relationship between significance and effect size. Then, go and find another partner and discuss problems that may arise when testing the frequency of certain words compared with the overall frequency of words in a corpus. 

## Other Non-Parametric Tests

If the depended variable is neither nominal nor categorical, but ordinal (that is if the dependent variable represents an order factor such as ranks), using a chi-square test is not warranted. In such cases it is advisable to apply tests that are designed to handle ordinal data. In the following, we will therefore briefly touch on bivariate tests that can handle ordinal dependent variables.

### Mann-Whitney U Test

It is a rather frequent case that numeric depend variables are transformed or converted into ordinal variables because the distribution of residuals does not allow the application of a linear regression. Because we are dealing with ordinal data, the application of a chi-square test is unwarranted and we need to use another test. In such cases, the Mann-Whitney U Test, which may be used even if the groups under investigation represent identical participants that are tested under two conditions, represents a viable alternative. If we are dealing with independent groups, i.e. groups that do not consist of the same subjects, and the dependent variable is ordinal, then the Wilcox Test or the Mann-Whitney U Test can be implemented.

```{r echo=TRUE, warning=F, message=F}
# generate data
Rank <- c(1,3,5,6,8,9,10,11,17,19, 2,4,7,12,13,14,15,16,18,20)
Groups <- c(rep("LanguageFamily1", 10), rep("LanguageFamily2", 10))
wilcox.test(Rank ~ Groups) # Rank is ordinal and Groups is a binary factor
```

```{r echo=F, eval = F, warning=F, message=F}
#install.packages("VGAM")       # install VGAM package (remove # to activate)
#library(VGAM)
# generate non-nirmal skewed numeric data
#r=.1 
#Frequency=rsnorm(100,0,2,4)
#NormalReaction=rsnorm(100,0,2,4)
#ReactionTimes=Frequency*r+NormalReaction*sqrt(1-r^2)
#wilcox.test(ReactionTimes, Frequency) # both variables are numeric but non-normal
```

If you are dealing with dependent groups, you have to set the argument `Paired` to `TRUE` (see below).

```{r echo=TRUE, warning=F, message=F}
# dependent 2-group Wilcoxon Signed Rank Test
#wilcox.test(y1,y2,paired=TRUE) # where y1 and y2 are numeric
```

### Kruskal Wallis Test

The Kruskal Wallis Test is a type of Analysis of Variance which can handle ordinal data. 

```{r echo=TRUE, warning=F, message=F}
#y <- c(15, 13, 10, 8, 37, 23, 31, 52, 11, 17)
#x <- c("A", "A", "A", "A", "A", "B", "B", "B", "B", "B")
# Kruskal Wallis Test One Way Anova by Ranks
#kruskal.test(y~x) # where y1 is numeric and x is a factor
```



```{r echo=T, message=FALSE, warning=FALSE}
# Randomized Block Design - Friedman Test
# friedman.test(y~x|z)
# where y are the data values, x is a grouping factor
# and z is a blocking factor
```

> Exercises (Non-Parametric Testing)


# Simple Linear Regression
 
This chapter focuses on a very widely used statistical method which is called regression. Regressions are used when we try to understand how independent variables correlate with a dependent or outcome variable. So, if you want to investigate how certain criteria affect an outcome, then a regression is the way to go. We will have a look at two simple examples to understand what the concepts underlying a regression mean and how a regression works. After that, we will turn to a more complex case and we will implement a multiple linear regression. The `R`-Code, that we will use, is based on [@field2012discovering].

## Introduction

Although the basic logic underlying regressions is identical to the conceptual underpinnings of analysis of variance (ANOVA), a related method, sociolinguistists have traditionally favoured regression analysis in their studies while ANOVAs have been the method of choice in psycholinguistics. The preference for either method is grounded in historical happenstances and the culture of these subdisciplines rather than in methodological reasoning.

A minor difference between regressions and ANOVA lies in the fact that regressions are based on the $t$-distribution while ANOVAs use the $F$-distribution (however, the $F$-value is merely t^2^). Both  $t$- and $F$-values report on the ratio between explained and unexplained variance.

The idea behind regression analysis is expressed formally in [equation](#eq:slm) and can best be described graphically: Imagine drawing a line through points in a scatterplot (Grafik \ref{fig:scat1} left panel).

$f_{(x)} = \alpha + \beta_{1}x_{i} + \epsilon$ {#eq:slm}

Regressions aim to find that line which has the minimal summed distance between points and the line (Figure \ref{fig:scat1} centre panel). Technically speaking, the aim of a regression is to find the line with the minimal deviance or the line with the minimal sum of residuals (variance) (Figure \ref{fig:scat1} right panel). This means that the sum of the length of the lines between the points and the lines should be minimal. The slope of the line is called *coefficient* and the point where the line crosses the y-axis is called the *intercept*.

```{r echo=T, message=FALSE, warning=FALSE}
# Scatterplot with lines from the regression line to the dots
x <- c(173, 169, 176, 166, 161, 164, 160, 158, 180, 187)
y <- c(80, 68, 72, 75, 70, 65, 62, 60, 85, 92) # plot scatterplot and the regression line
mod1 <- lm(y ~ x)

par(mfrow=c(1, 3)) # plot window: 1plot/row and 3 plots/column
plot(x, y, xlim=c(min(x)-5, max(x)+5), ylim=c(min(y)-10, max(y)+10))
plot(x, y, xlim=c(min(x)-5, max(x)+5), ylim=c(min(y)-10, max(y)+10))
abline(mod1, lwd=2)
plot(x, y, xlim=c(min(x)-5, max(x)+5), ylim=c(min(y)-10, max(y)+10))
abline(mod1, lwd=2)

# calculate residuals and predicted values
res <- signif(residuals(mod1), 5)
pre <- predict(mod1) # plot distances between points and the regression line
segments(x, y, x, pre, col="red")
# add labels (res values) to points
library(calibrate)
textxy(x, y, round(res,1), cex=1)
par(mfrow=c(1, 1)) # restore original graphics parameters
```

A word about standard errors (SE) is in order here because most commonly used statistics programs will provide SE values when reporting regression models. The SE is a measure that tells us how much the coefficients were to vary if the same regression were applied to many samples from the same population. A relatively small SE value therefore indicates that the coefficients will remain very stable if the same regression model is fitted to many different samples with identical parameters. In contrast, a large SE tells you that the model is volatile and not very stable or reliable as the coefficients vary substantially if the model is applied to many samples.  

## Example 1: Preposition Use across Real-Time

We will now turn to our first example. In this example, we will investigate whether the frequency of prepositions has changed from Middle English to Late Modern English. The reasoning behind this example is that Old English was highly synthetic compared with Present-Day English which comparatively analytic. In other words, while Old English speakers used case to indicate syntactic relations, speakers of Present-Day English use word order and prepositions to indicate syntactic relationships. This means that the loss of case had to be compensated by different strategies and maybe these strategies continued to develop and increase in frequency even after the change from synthetic to analytic had been mostly accomplished. And this prolonged change in compensatory strategies is what this example will focus on. 

The analysis is based on data extracted from the *Penn Corpora of Historical English* (see http://www.ling.upenn.edu/hist-corpora/), that consists of 603 texts written between 1125 and 1900. In preparation of this example, all elements that were part-of-speech tagged as prepositions were extracted from the PennCorpora. 

Then, the relative frequencies (per 1,000 words) of prepositions per text were calculated. This frequency of prepositions per 1,000 words represents our dependent variable. In a next step, the date when each letter had been written was extracted. The resulting two vectors were combined into a table which thus contained for each text, when it was written (independent variable) and its relative frequency of prepositions (dependent or outcome variable).

A regression analysis will follow the steps described below: 
1. Extraction and processing of the data
2. Data visualization
3. Applying the regression analysis to the data
4. Diagnosing the regression model and checking whether or not basic model assumptions have been violated.

### Simple Linear Regression in `R`

In a first step, we prepare our session by cleaning the work space, installing libraries that we need for the analysis, and loading the libraries and functions.

```{r echo=T, message=FALSE, warning=FALSE}
rm(list=ls(all=T))             # clean workspace
#install.packages("car")       # install car package (remove # to activate)
#install.packages("ggplot2")   # install ggplot2 package (remove # to activate)
#install.packages("QuantPsyc") # install QuantPsyc package (remove # to activate)
library(car)                   # activate car package
library(ggplot2)               # activate ggplot2 package
library(QuantPsyc)             # activate QuantPsyc package
source("rscripts/multiplot_ggplot2.r") # load multiplot function
source("rscripts/slr.summary.tb.r")    # load regression summary function
```

After preparing our session, we can now load and inspect the data to get a first impression of its properties.

```{r echo=T, message=FALSE, warning=FALSE}
slrdata <- read.delim("data/slrdata.txt", header = TRUE) # load data
attach(slrdata)                                          # attach data
slrdata <- as.data.frame(cbind(datems, pptw))  # remove superfluous columns
colnames(slrdata) <- c("year", "prep.ptw")     # add column names
slrdata <- slrdata[!is.na(slrdata$year) == T,] # remove NAs from data
str(slrdata)                                   # inspect structure of the data
```

```{r echo=F, message=FALSE, warning=FALSE}
library(knitr)                 # activate library for tabulating
kable(head(slrdata), caption = "First six lines of the data set.")
```

Inspecting the data is very important because it can happen that a data set may not load completely or that variables which should be numeric have been converted in character variables. If unchecked, then such hidden data issues could go unnoticed and cause much trouble once you realize that the data you have been working with is different from what you had in mind.

We will now plot the data to get a more thorough impression of the structure of the data.

```{r echo=T, message=FALSE, warning=FALSE}
ggplot(slrdata, aes(year, prep.ptw)) +
  geom_point() +
  theme_bw() +
  labs(x = "Year") +
  labs(y = "Prepositions per 1,000 words") +
  geom_smooth()
```


```{r echo=T, message=FALSE, warning=FALSE}
ggplot(slrdata, aes(year, prep.ptw)) +
  geom_point() +
  theme_bw() +
  labs(x = "Year") +
  labs(y = "Prepositions per 1,000 words") +
  geom_smooth(method = "lm") # with linear model smoothing!
```

Before beginning with the regression analysis, we will scale the year. We scale by subtracting each value from the mean of year. This can be useful when dealing with numeric variables because if we did not scale year, we would get estimated values for year 0 (a year when English did not even exist yet). If a variable is scaled, the regression provides estimates of the model refer to the mean of that numeric variable. In other words, scaling can eb very helpful, especially with respect to the interpretation of the results that regression models report.


```{r echo=T, message=FALSE, warning=FALSE}
slrdata$prep.ptw <- slrdata$prep.ptw - mean(slrdata$prep.ptw) # scaling date
```

We will now begin the regression analysis by generating a first regression model. Once the regression model is generated, we will inspect its results and check if certain mathematical assumptions have been violated or whether the data contains outliers. diagnostic plots.

```{r echo=T, message=FALSE, warning=FALSE}
# create initial model
prep.lm <- lm(prep.ptw ~ year, data = slrdata)
# inspect results
summary(prep.lm)

# plot model: 3 plots per row in one window
par(mfrow = c(1, 3))
plot(resid(prep.lm))
plot(rstandard(prep.lm))
plot(rstudent(prep.lm))
par(mfrow = c(1, 1)) # restore default parameters
```

The left graph shows the residuals of the model (i.e., the differences between the observed and the values predicted by the regression model). The problem with this plot is that the residuals are not standardized and so they cannot be compared to the residuals of other models. To remedy this deficiency, residuals are normalized by dividing the residuals by their standard deviation. Then, the normalized residuals can be plotted against the observed values (centre panel). In this way, not only are standardized residuals obtained, but the values of the residuals are transformed into z-values, and one can use the z-distribution to find problematic data points. There are three rules of thumb regarding finding problematic data points through standardized residuals [@ field2012discovering 268-269]:

* Points with values higher than 3.29 should be removed from the data.
*  If more than 1% of the data points have values higher than 2.58, then the error rate of our model is too high.
*  If more than 5% of the data points have values greater than 1.96, then the error rate of our model is too high.


The right panel shows the * studentized residuals* (adjusted predicted values: each data point is divided by the standard error of the residuals). In this way, it is possible to use Student's t-distribution to diagnose our model.

Adjusted predicted values are residuals of a special kind: the model is calculated without a data point and then used to predict this data point. The difference between the observed data point and its predicted value is then called the adjusted predicted value. In summary, studentized residuals are very useful because they allow us to identify influential data points.

The plots show that there are two potentially problematic data points (the top-most and bottom-most point). These two points are clearly different from the other data points and may therefore be outliers. We will test later if these points need to be removed.

We will now generate more diagnostic plots.

```{r echo=T, message=FALSE, warning=FALSE}
par(mfrow = c(2, 2)) # plot window: 2 plots/row, 2 plots/column
plot(prep.lm)        # generate diagnostic plots
par(mfrow = c(1, 1)) # restore normal plot window
```

The diagnostic plots are very positive and we will go through why this is so for each panel. The graph in the upper left panel is useful for finding outliers or for determining the correlation between residuals and predicted values: when a trend becomes visible in the line or points (e.g., a rising trend or a zigzag line), then this would indicate that the model would be problematic (in such cases, it can help to remove data points that are too influential (outliers)).

The graphic in the upper right panel indicates whether the residuals are normally distributed (which is desirable) or whether the residuals do not follow a normal distribution. If the points lie on the line, the residuals follow a normal distribution. For example, if the points are not on the line at the top and bottom, it shows that the model does not predict small and large values well and that it therefore does not have a good fit.

The graphic in the lower left panel provides information about *homoscedasticity*. Homoscedasticity means that the variance of the residuals remains constant and does not correlate with any independent variable. In unproblematic cases, the graphic shows a flat line. If there is a trend in the line, we are dealing with heteroscedasticity, that is, a correlation between independent variables and the residuals, which is very problematic for regressions.

The graph in the lower right panel shows problematic influential data points that disproportionately affect the regression (this would be problematic). If such influential data points are present, they should be either weighted (one could generate a robust rather than a simple linear regression) or they must be removed. The graph displays Cook's distance, which shows how the regression changes when a model without this data point is calculated. The cook distance thus shows the influence a data point has on the regression as a whole. Data points that have a Cook's distance value greater than 1 are problematic [@field2012discovering, pp. 269].

The so-called leverage is also a measure that indicates how strongly a data point affects the accuracy of the regression. Leverage values range between 0 (no influence) and 1 (strong influence: suboptimal!). To test whether a specific data point has a high leverage value, we calculate a cut-off point that indicates whether the leverage is too strong or still acceptable. The following two formulas are used for this:

\begin{equation}

\frac{3(k + 1)}{n}

\end{equation}

or

\begin{equation}

\frac{2(k + 1)}{n}

\end{equation}

We will look more closely at leverage in the context of multiple linear regression and will therefore end the current analysis by summarizing the results of the regression analysis in a table.

```{r echo=T, message=FALSE, warning=FALSE}
slrtb <- slr.summary(prep.lm)  # tabulate results
library(knitr)                 # activate library for tabulating
kable(slrtb, caption = "Results of a simple linear regression analysis.")
```

Typically, the results of regression analyses are presented in such tables as they include all important measures of model quality and significance, as well as the magnitude of the effects. 
In addition, the results of simple linear regressions should be summarized in writing. An example of how the results of a regression analysis can be written up is provided below.

A simple linear regression has been fitted to the data. A visual assessment of the model diagnostic graphics did not indicate any problematic data points (outliers) or disproportionately influential data points and pointed to a good model fit. The final linear regression model is based on 603 data points and correlates highly significantly with the data (R^2^: 0.0108, F-statistic (1, 601): 6,553, p-value: 0.0107\*) and confirms a significant and positive correlation between the year in which the text was written and the relative frequency of prepositions  (coefficient: .02, standardized $\beta$: 0.1039, SE: 0.01, t-value: 2.560, p-value:. 0107\*).

## Example 2: Teaching Styles

In the previous example, we dealt with two numeric variables, while the following example deals with a categorical independent variable and a numeric dependent variable. The ability for regressions to handle very different types of variables makes regressions a widely used and robust method of analysis.

In this example, we are dealing with two groups of students that have been randomly assigned to groups which are exposed to different teaching methods. Both groups undergo a language learning test after the lesson with a maximum score of 20 points. 

The students of the first group (group A) have reached the following scores:

* Group A: 15, 12, 11, 18, 15, 15, 9, 19, 14, 13, 11, 12, 18, 15, 16, 14, 16, 17, 15, 17, 13, 14, 13, 15, 17, 19, 17, 18, 16, 14 (mean: 14,93)

The students of the second group (group B) have reached the following scores:

* Group B: 11, 16, 14, 18, 6, 8, 9, 14, 12, 12, 10, 15, 12, 9, 13, 16, 17, 12, 8, 7, 15, 5, 14, 13, 13, 12, 11, 13, 11, 7 (mean: 11,77)

Our question is whether group A has performed significantly better than group B which would indicate that the teaching method to which group A was exposed works better than the teaching method to which group B was exposed.


Let's move on to implementing the regression in `R`. As in the previous example we empty the current workspace, install and activate necessary packages and load additional functions.

```{r echo=T, message=FALSE, warning=FALSE}
rm(list=ls(all=T))             # clean workspace
#install.packages("car")       # install car package (remove # to activate)
#install.packages("ggplot2")   # install ggplot2 package (remove # to activate)
#install.packages("QuantPsyc") # install QuantPsyc package (remove # to activate)
library(car)                   # activate car package
library(ggplot2)               # activate ggplot2 package
library(QuantPsyc)             # activate QuantPsyc package
source("rscripts/multiplot_ggplot2.r") # load multiplot function
source("rscripts/slr.summary.tb.r")    # load regression summary function
```

After the necessary specifications have been carried out, we generate the data set and inspect its structure.

```{r echo=T, message=FALSE, warning=FALSE}
# generate data
g1 <- c(15, 12, 11, 18, 15, 15, 9, 19, 14, 13, 11, 12, 18, 15, 16, 14, 16, 17, 15, 17, 13, 14, 13, 15, 17, 19, 17, 18, 16, 14)
g2 <- c(11, 16, 14, 18, 6, 8, 9, 14, 12, 12, 10, 15, 12, 9, 13, 16, 17, 12, 8, 7, 15, 5, 14, 13, 13, 12, 11, 13, 11, 7)
g <- c(rep("A", length(g1)), rep("B", length(g2))) # generate a grouping factor
sprtestdata <- data.frame(g, c(g1, g2))            # combine into a data frame
colnames(sprtestdata) <- c("group", "score")       # add column names
library(knitr)                                     # load library
kable(head(sprtestdata), caption = "First six line of the data set for testing differences between groups using a simple linear regression analysis.")
```

Now, we graphically display the data. In this case, a boxplot represents a good way to visualize the data.

```{r boxplot, fig.cap="Darstellung der Sprachtestdaten"}
boxplot(score ~ group,               # create box plot
data = sprtestdata,                  # the data we want to display
main = "",                           # empty title
ylab = "Score",                      # y-axis label
ylim = c(0, 20),                     # y-axis rage
xlab = c("Group"),                   # x-axis albel
notch = T,                           # show notches 
col = c("lightgreen", "lightblue"))  # define colours
text(1:2,                            # add text
c(4.0, 4.0),                         # y-axis position
cex = 0.85,                          # font size
labels = paste("mean\n",
c(round(as.vector(by(sprtestdata$score, sprtestdata$group, mean))[1], 2),
round(as.vector(by(sprtestdata$score, sprtestdata$group, mean))[2], 2),
sep = "")))
rug(jitter(sprtestdata$score),
side=4)
box()
```

The data indicate that group A did significantly better than group B. We will test this impression by generating the regression model and creating the model diagnostic graphics in the next step.

```{r echo=T, message=FALSE, warning=FALSE}
sprtest.lm <- lm(score ~ group, data = sprtestdata) # generate regression model
summary(sprtest.lm)                                 # inspect results
```


```{r echo=T, message=FALSE, warning=FALSE}
par(mfrow = c(1, 3))        # plot window: 1 plot/row, 3 plots/column
plot(resid(sprtest.lm))     # generate diagnostic plot
plot(rstandard(sprtest.lm)) # generate diagnostic plot
plot(rstudent(sprtest.lm))  # generate diagnostic plot
par(mfrow = c(1, 1))        # restore normal plot window
```

\caption{Diagnostic plots for the regression analysis that test the effect of language learning methods.}

The graphics do not indicate outliers or other issues, so we can continue with more diagnostic graphics.

```{r echo=T, message=FALSE, warning=FALSE}
par(mfrow = c(2, 2)) # generate a plot window with 2x2 panels
plot(sprtest.lm)     # generate diagnostic plots
par(mfrow = c(1, 1)) # restore normal plot window
```

\caption{Additional diagnostic plots for the regression analysis that test the effect of language learning methods.}


These graphics also show no problems. In this case, the data can be summarized in the next step.

```{r echo=T, message=FALSE, warning=FALSE}
# ergebnisse tabellieren
slrtb2 <- slr.summary(sprtest.lm)
library(knitr)
kable(slrtb2, caption = "Results of a simple linear regression analysis.")
```

The results of this second simple linear regressions can be summarized as follows: 

A simple linear regression was fitted to the data. A visual assessment of the model diagnostic graphics did not indicate any problematic data points (outliers) or disproportionately influential data points and pointed to a good model fit. The final linear regression model is based on 60 data points and correlates highly significantly with the data (R^2^: 0.2322, F statistic (1, 58): 2.93, p-value <. 001\*\*\*) and confirms that group A scored significantly better on the language learning test than group B (coefficient: -3.17, standardized $\beta$: -0.4819, SE: 0.48, t-value: -4.19, p-value <. 001 \*\*\*).

>
> Exercise Time!
>
> Load the data set called mblrdata by executing the following command in R:

```{r echo=T, message=FALSE, warning=FALSE}
slrdata2 <- read.delim("data/mlrdata.txt", header = TRUE) # load data
attach(slrdata2)                                 # attach data
slrdata2$attraction <- NULL                      # remove superfluous columns
str(slrdata2)                                    # inspect structure of the data
```
>
> Perform a regression analysis that analyses if the money spend on a present for someone (money) depends on the receiver's relationship status (status).
>

# References


