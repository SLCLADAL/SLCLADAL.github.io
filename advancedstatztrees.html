<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />


<meta name="author" content="UQ SLC Digital Team" />

<meta name="date" content="2019-07-09" />

<title>Tree-Based Models</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cosmo.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<link href="site_libs/font-awesome-5.0.13/css/fa-svg-with-js.css" rel="stylesheet" />
<script src="site_libs/font-awesome-5.0.13/js/fontawesome-all.min.js"></script>
<script src="site_libs/font-awesome-5.0.13/js/fa-v4-shims.min.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>


</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}

.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>


<div class="container-fluid main-container">

<!-- tabsets -->
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->




<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_').toLowerCase();
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}


.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
  padding-left: 25px;
  text-indent: 0;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>

<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">LADAL</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    <span class="fa fa-play-circle"></span>
     
    Basics
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li class="dropdown-header">Basics</li>
    <li>
      <a href="introquant.html">Introduction To Quantitative Reasoning</a>
    </li>
    <li>
      <a href="basicquant.html">Basic Concepts In Quantitative Reasoning</a>
    </li>
    <li class="divider"></li>
    <li class="dropdown-header">Research Designs</li>
    <li>
      <a href="researchdesigns.html">Overview</a>
    </li>
    <li>
      <a href="corpling.html">Corpus Linguistics</a>
    </li>
    <li>
      <a href="experiments.html">Experimental Designs</a>
    </li>
    <li>
      <a href="ca.html">Conversation Analysis</a>
    </li>
    <li>
      <a href="acoustic.html">Acoustic Analysis</a>
    </li>
    <li class="divider"></li>
    <li class="dropdown-header">Data Collection</li>
    <li>
      <a href="introdatacollection.html">Introduction</a>
    </li>
    <li>
      <a href="fieldwork.html">Field Work</a>
    </li>
    <li>
      <a href="interviews.html">Interviews</a>
    </li>
    <li>
      <a href="questionnaires.html">Questionnaires and Surveys</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    <span class="fa fa-gear"></span>
     
    Data Processing
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li class="dropdown-header">Data Processing</li>
    <li>
      <a href="intror.html">Basics: Getting started with R</a>
    </li>
    <li>
      <a href="introloading.html">Loading and saving data</a>
    </li>
    <li>
      <a href="introtables.html">Tabulating data</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    <span class="fa fa-bar-chart"></span>
     
    Visualization
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li class="dropdown-header">Visualization</li>
    <li>
      <a href="basicgraphs.html">Basic Visualization Techniques</a>
    </li>
    <li>
      <a href="advancedgraphs.html">Advanced Visualization Techniques</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    <span class="fa fa-eye"></span>
     
    Statistics
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li class="dropdown-header">Statistics</li>
    <li>
      <a href="descriptivestatz.html">Descriptive Statistics</a>
    </li>
    <li class="divider"></li>
    <li class="dropdown-header">Basic Interential Statistics</li>
    <li>
      <a href="basicstatz.html">Basic Inferential Tests</a>
    </li>
    <li>
      <a href="basicstatzchi.html">The Chi-Square Family</a>
    </li>
    <li>
      <a href="basicstatzregression.html">Simple Linear Regression</a>
    </li>
    <li class="divider"></li>
    <li class="dropdown-header">Advanced Interential Statistics</li>
    <li>
      <a href="advancedstatzregressions.html">Regression Analysis</a>
    </li>
    <li>
      <a href="advancedstatztrees.html">Tree-Based Models</a>
    </li>
    <li>
      <a href="groupingstatz.html">Agglomerative Procedures</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    <span class="fa fa-bars"></span>
     
    Text Analysis/Corpus Linguistics
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li class="dropdown-header">Text Analysis</li>
    <li>
      <a href="textanalysis.html">Introduction</a>
    </li>
    <li>
      <a href="webcrawling.html">Web Crawling</a>
    </li>
    <li>
      <a href="network.html">Network Analysis</a>
    </li>
    <li>
      <a href="topic.html">Topic Modeling</a>
    </li>
    <li>
      <a href="classification.html">Classification</a>
    </li>
    <li>
      <a href="tagging.html">Tagging and Parsing</a>
    </li>
    <li class="divider"></li>
    <li class="dropdown-header">Corpus Linguistics</li>
    <li>
      <a href="corplingr.html">Corpus Linguistics in R</a>
    </li>
    <li>
      <a href="corplingantconcexcel.html">Corpus Linguistics with AntConc, TextPad and Excel</a>
    </li>
    <li>
      <a href="available.html">Available Software</a>
    </li>
  </ul>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="about.html">
    <span class="fa fa-info"></span>
     
    Contact
  </a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Tree-Based Models</h1>
<h4 class="author"><em>UQ SLC Digital Team</em></h4>
<h4 class="date"><em>2019-07-09</em></h4>

</div>


<p><img src="images/uq1.jpg" width="100%" /></p>
<div id="conditional-inference-trees" class="section level1">
<h1><span class="header-section-number">1</span> Conditional Inference Trees</h1>
<p>This section deals with tree-structure models, the most basic type is a conditional inference tree. Like random forests, conditional inference trees are non-parametric and thus do not rely on distributional requirements. In addition, tree-structure models are very useful because they can deal with different types of variables. Tree-structure models are particularly interesting for linguists because they can handle moderate sample sizes and many high-order interactions better then regression models. The tree structure represents recursive partitioning of the data to minimize residual deviance.</p>
<p>Before we implement a conditional inference tree in R, we will have a look at how it works. We will do this in more detail here as random forests and Boruta anlyses are extensions of conditional inference trees and are therefore based on the same concepts.</p>
<p>Below is a conditional inference tree of the iris-data set. All conditional inference trees answer a simple question, namely “How do we classify elements based on the given predictors?”. The answer that conditional inference trees provide is the classification of the elements based on the levels of the predictors. If predictors are not significant or all elements can be classified correctly without them (i.e. if these predictors are unneccessary), then they are not included in the decision tree. The conditional inference tree shows that the best predictor for discourse <em>like</em> use is age as it is the highest node. Among young speakers, those with high status use like more compared with speakers of lower social status. Among old speakers, women use discourse <em>like</em> more than men.</p>
<pre class="r"><code>library(partykit)              # activate partykit library
library(dplyr)                 # activate dplyr library
options(stringsAsFactors = T)  # set options: do not convert strings
citdata &lt;- read.delim(&quot;https://slcladal.github.io/data/treedata.txt&quot;, header = T, sep = &quot;\t&quot;)
set.seed(111)        # set.seed
# apply bonferroni correction (1 minus alpha multiplied by n of predictors)
control = ctree_control(mincriterion = 1-(.05*ncol(citdata)-1))
# create initial conditional inference tree model
citd.ctree &lt;- ctree(LikeUser ~ Age + Gender + Status,
                    data = citdata)
plot(citd.ctree, gp = gpar(fontsize = 8)) # plot final ctree</code></pre>
<p><img src="advancedstatztrees_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
<p>Before going through how this conditional decision tree is generated, let us first go over some basic concepts. The top of the decision tree is called “root” or “root node”, the categories at the end of branches are called “leaves” or “leaf nodes”. Nodes that are in-between the root and leaves are called “internal nodes” or just “nodes”. The root node has only arrows or lines pointing away from it, internal nodes have lines going to and from them, while leaf nodes only have lines pointing towards them.</p>
<p>Let us now go over the process by which the decision tree above is generated. In our example, we want to predict whether a person makes use of discourse <em>like</em> given their age, gender, and social status. The first six lines of this ficticious data set are displayed in the table below:</p>
<table>
<caption>Example data set to illustrate conditional inference trees.</caption>
<thead>
<tr class="header">
<th align="left">Age</th>
<th align="left">Gender</th>
<th align="left">Status</th>
<th align="left">LikeUser</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">15-40</td>
<td align="left">female</td>
<td align="left">high</td>
<td align="left">no</td>
</tr>
<tr class="even">
<td align="left">15-40</td>
<td align="left">female</td>
<td align="left">high</td>
<td align="left">no</td>
</tr>
<tr class="odd">
<td align="left">15-40</td>
<td align="left">male</td>
<td align="left">high</td>
<td align="left">no</td>
</tr>
<tr class="even">
<td align="left">41-80</td>
<td align="left">female</td>
<td align="left">low</td>
<td align="left">yes</td>
</tr>
<tr class="odd">
<td align="left">41-80</td>
<td align="left">male</td>
<td align="left">high</td>
<td align="left">no</td>
</tr>
<tr class="even">
<td align="left">41-80</td>
<td align="left">male</td>
<td align="left">low</td>
<td align="left">no</td>
</tr>
</tbody>
</table>
<p>In a first step, we initialize our R session by activating relevant libraries, setting options and setting a seed. Then, we load the data into R.</p>
<pre class="r"><code>library(partykit)              # activate partykit library
library(dplyr)                 # activate dplyr library
options(stringsAsFactors = T)  # set options: do not convert strings
set.seed(111)                  # set.seed
citdata &lt;- read.delim(&quot;https://slcladal.github.io/data/treedata.txt&quot;, header = T, sep = &quot;\t&quot;)</code></pre>
<p>After initializing the R session, it needs to be determined, what the root of the decision tree should be. This means that we have to determine which of the variables represents the root node. In order to do so, we tabulate for each variable level, how many speakers of that level have used discourse <em>like</em> (LikeUsers) and how many have not used discourse <em>like</em> (NonLikeUsers).</p>
<pre class="r"><code># tabulate data
table(citdata$LikeUser, citdata$Gender)</code></pre>
<pre><code>##      
##       female male
##   no      43   75
##   yes     91   42</code></pre>
<pre class="r"><code>table(citdata$LikeUser, citdata$Age)</code></pre>
<pre><code>##      
##       15-40 41-80
##   no     34    84
##   yes    92    41</code></pre>
<pre class="r"><code>table(citdata$LikeUser, citdata$Status)</code></pre>
<pre><code>##      
##       high low
##   no    33  85
##   yes   73  60</code></pre>
<p>None of the predictors is perfect (the perdictors are therefore refered to as “impure”). To determine which variable is the root, we will caluculate the degree of “impurity” for each variable - the variable which has the lowest impurity value will be the root.</p>
<p>The most common measure of impurity in the conext of conditional inference trees is called “Gini”. For each level we apply the following equation to determine the impuruty value:</p>
<span class="math display">\[\begin{equation}

G_{x} = 1 - ( p_{1} )^{2} - ( p_{0} )^{2}

\end{equation}\]</span>
<p>For the node for men, this would mean the following:</p>
<span class="math display">\[\begin{equation}

G_{men} = 1-(\frac{42} {42+75})^{2} - (\frac{75} {42+75})^{2} = 0.4602235

\end{equation}\]</span>
<p>For women, we caculate G or Gini as follows:</p>
<span class="math display">\[\begin{equation}

G_{women} = 1-(\frac{91} {91+43})^{2} - (\frac{43} {91+43})^{2} = 0.4358432

\end{equation}\]</span>
<p>To calculate the Gini value of Gender, we need to calculate the weighted average leaf node impurity (weighted because the number of speakers is different in each group). We calculate the weighted avergage leaf node impurity using the forluar below.</p>
<span class="math display">\[\begin{equation}

G_{Gender} = \frac{N_{men}} {N_{Total}} \times G_{men} +  \frac{N_{women}} {N_{Total}} \times G_{women}

G_{Gender} = \frac{159} {303} \times 0.4602235 +  \frac{144} {303} \times 0.4358432 = 0.4611915

\end{equation}\]</span>
<p>In R, we would do the calculations like below.</p>
<pre class="r"><code># Gini: men
gini_men &lt;- 1-(42/(42+75))^2 - (75/(42+75))^2
# Gini: women
gini_women &lt;- 1-(91/(91+43))^2 - (43/(91+43))^2
# Weighted Average of Gini: Gender
gini_gender &lt;- 42/(42+75)* gini_men +  91/(91+43) * gini_women
gini_gender</code></pre>
<pre><code>## [1] 0.4611915</code></pre>
<p>We would now have to calculate the Gini values for all predictors in our data set. See below for the how to perform the calculations in R:</p>
<pre class="r"><code>gini_young &lt;- 1-(92/(92+34))^2 - (34/(92+34))^2  # Gini: young
gini_old &lt;- 1-(41/(41+84))^2 - (84/(41+84))^2    # Gini: old
# Weighted Average of Gini: Gender
gini_age &lt;- 92/(92+34)* gini_young +  41/(41+84) * gini_old
gini_age</code></pre>
<pre><code>## [1] 0.4323148</code></pre>
<pre class="r"><code>### Status
gini_high &lt;- 1-(73/(33+73))^2 - (33/(33+73))^2   # Gini: high
gini_low &lt;- 1-(60/(60+85))^2 - (85/(60+85))^2    # Gini: low
# Weighted Average of Gini: Status
gini_status &lt;- 73/(33+73)* gini_high +  60/(60+85) * gini_low
gini_status</code></pre>
<pre><code>## [1] 0.4960521</code></pre>
<span class="math display">\[\begin{equation} 

G_{Gender} = 0.4611915

G_{Age} = 0.4323148

G_{Status} = 0.4960521

\end{equation}\]</span>
<p>Age has the lowest Gini impurity value and therefore separates LikeUsers from NonLikeUsers better than Gender or Social Status. Therefore, age will be the root of our tree.</p>
<p><img src="advancedstatztrees_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<p>In a next step, we need to find out which of the remaining variables best separates the speakers who use discourse <em>like</em> from those that do not under the first node. In order to do so, we calculate the Gini values for Gender and SocialStatus for the “15-40” node.</p>
<pre class="r"><code>### 2nd node: gender
young &lt;- citdata[citdata$Age == &quot;15-40&quot;,]
tbyounggender &lt;- table(young$LikeUser, young$Gender)
tbyounggender</code></pre>
<pre><code>##      
##       female male
##   no      17   17
##   yes     58   34</code></pre>
<pre class="r"><code># Gini: men
gini_youngmen &lt;- 1-(tbyounggender[2,2]/sum(tbyounggender[,2]))^2 - (tbyounggender[1,2]/sum(tbyounggender[,2]))^2
# Gini: women
gini_youngwomen &lt;- 1-(tbyounggender[2,1]/sum(tbyounggender[,1]))^2 - (tbyounggender[1,1]/sum(tbyounggender[,1]))^2
# Weighted Average of Gini: Gender
gini_younggender &lt;- sum(tbyounggender[,2])/sum(tbyounggender)* gini_youngmen +  sum(tbyounggender[,1])/sum(tbyounggender) * gini_youngwomen
gini_younggender</code></pre>
<pre><code>## [1] 0.3885714</code></pre>
<pre class="r"><code>### 2nd node: status
young &lt;- citdata[citdata$Age == &quot;15-40&quot;,]
tbyoungstatus &lt;- table(young$LikeUser, young$Status)
tbyoungstatus</code></pre>
<pre><code>##      
##       high low
##   no    11  23
##   yes   57  35</code></pre>
<pre class="r"><code># Gini: low
gini_younglow &lt;- 1-(tbyoungstatus[2,2]/sum(tbyoungstatus[,2]))^2 - (tbyoungstatus[1,2]/sum(tbyoungstatus[,2]))^2
# Gini: high
gini_younghigh &lt;- 1-(tbyoungstatus[2,1]/sum(tbyoungstatus[,1]))^2 - (tbyoungstatus[1,1]/sum(tbyoungstatus[,1]))^2
# Weighted Average of Gini: Gender
gini_youngstatus &lt;- sum(tbyoungstatus[,2])/sum(tbyoungstatus)* gini_younglow +  sum(tbyoungstatus[,1])/sum(tbyoungstatus) * gini_younghigh
gini_youngstatus</code></pre>
<pre><code>## [1] 0.3666651</code></pre>
<p>As “Status” has a lower Gini value, we have determined “Status” as the next split. Next, we check whether we need to include another split by “Gender”.</p>
<pre class="r"><code>### 3nd node: gender
younghigh &lt;- citdata %&gt;%
  filter(Age == &quot;15-40&quot;) %&gt;%
  filter(Status == &quot;high&quot;)
tbyounghighgender &lt;- table(younghigh$LikeUser, younghigh$Gender)
tbyounghighgender</code></pre>
<pre><code>##      
##       female male
##   no       4    7
##   yes     37   20</code></pre>
<pre class="r"><code># Gini: men
gini_younghighmen &lt;- 1-(tbyounghighgender[2,2]/sum(tbyounghighgender[,2]))^2 - (tbyounghighgender[1,2]/sum(tbyounghighgender[,2]))^2
# Gini: women
gini_younghighwomen &lt;- 1-(tbyounghighgender[2,1]/sum(tbyounghighgender[,1]))^2 - (tbyounghighgender[1,1]/sum(tbyounghighgender[,1]))^2
# Weighted Average of Gini: Gender
gini_younghighgender &lt;- sum(tbyounghighgender[,2])/sum(tbyounghighgender)* gini_younghighmen +  sum(tbyounghighgender[,1])/sum(tbyounghighgender) * gini_younghighwomen
gini_younghighgender</code></pre>
<pre><code>## [1] 0.2586747</code></pre>
<pre class="r"><code># compare with younghigh
gini_younghigh</code></pre>
<pre><code>## [1] 0.2711938</code></pre>
<p>Because the Gini value of Status:high among young speakers is lower than the Gini value that results if we include “Gender”, we do not include another split. This means that the left branch is finished.</p>
<p>Now, we focus on the right branch. Again, we determine whether to include a split for “Gender” or for “Status”. As a first step, we calculate the Gini value for “Gender.</p>
<pre class="r"><code>### 3rd node: gender
old &lt;- citdata[citdata$Age == &quot;41-80&quot;,]
tboldgender &lt;- table(old$LikeUser, old$Gender)
tboldgender</code></pre>
<pre><code>##      
##       female male
##   no      26   58
##   yes     33    8</code></pre>
<pre class="r"><code># Gini: men
gini_oldmen &lt;- 1-(tboldgender[2,2]/sum(tboldgender[,2]))^2 - (tboldgender[1,2]/sum(tboldgender[,2]))^2
# Gini: women
gini_oldwomen &lt;- 1-(tboldgender[2,1]/sum(tboldgender[,1]))^2 - (tboldgender[2,1]/sum(tboldgender[,1]))^2
# Weighted Average of Gini: Gender
gini_oldgender &lt;- sum(tboldgender[,2])/sum(tboldgender)* gini_oldmen +  sum(tboldgender[,1])/sum(tboldgender) * gini_oldwomen
gini_oldgender</code></pre>
<pre><code>## [1] 0.2891628</code></pre>
<p>Now, we calclate the Gini value for Status.</p>
<pre class="r"><code>### 3rd node: status
tboldstatus &lt;- table(old$LikeUser, old$Status)
tboldstatus</code></pre>
<pre><code>##      
##       high low
##   no    22  62
##   yes   16  25</code></pre>
<pre class="r"><code># Gini: low
gini_oldlow &lt;- 1-(tboldstatus[2,2]/sum(tboldstatus[,2]))^2 - (tboldstatus[2,1]/sum(tboldstatus[,2]))^2
# Gini: high
gini_oldhigh &lt;- 1-(tboldstatus[2,1]/sum(tboldstatus[,1]))^2 - (tboldstatus[2,1]/sum(tboldstatus[,1]))^2
# Weighted Average of Gini: Gender
gini_oldstatus &lt;- sum(tboldstatus[,2])/sum(tboldstatus)* gini_oldlow +  sum(tboldstatus[,1])/sum(tboldstatus) * gini_oldhigh
gini_oldstatus</code></pre>
<pre><code>## [1] 0.811199</code></pre>
<pre class="r"><code># compare with gini gender
gini_oldgender</code></pre>
<pre><code>## [1] 0.2891628</code></pre>
<p>Since the Gini value for “Gender” is lower, we include a “Gender” node.</p>
<p>Now, we determine, wehther we also need to include a split for “Status”.</p>
<pre class="r"><code>### 3nd node: gender
oldfemale &lt;- citdata %&gt;%
  filter(Age == &quot;41-80&quot;) %&gt;%
  filter(Gender == &quot;female&quot;)
tboldfemalestatus &lt;- table(oldfemale$LikeUser, oldfemale$Status)
tboldfemalestatus</code></pre>
<pre><code>##      
##       high low
##   no     5  21
##   yes   11  22</code></pre>
<pre class="r"><code># Gini: low
gini_oldfemalelow &lt;- 1-(tboldfemalestatus[2,2]/sum(tboldfemalestatus[,2]))^2 - (tboldfemalestatus[1,2]/sum(tboldfemalestatus[,2]))^2
# Gini: high
gini_oldfemalehigh &lt;- 1-(tboldfemalestatus[2,1]/sum(tboldfemalestatus[,1]))^2 - (tboldfemalestatus[1,1]/sum(tboldfemalestatus[,1]))^2
# Weighted Average of Gini: Status
gini_oldgenderstatus &lt;- sum(tboldfemalestatus[,2])/sum(tboldfemalestatus)* gini_oldfemalelow +  sum(tboldfemalestatus[,1])/sum(tboldfemalestatus) * gini_oldfemalehigh
gini_oldgenderstatus   # gini of status for old plus female</code></pre>
<pre><code>## [1] 0.4807351</code></pre>
<pre class="r"><code>gini_oldgender         # compare with younghigh</code></pre>
<pre><code>## [1] 0.2891628</code></pre>
<p>Because the Gini value for Status is higher than the value for Gender, we do not include another split. This means that our conditional inference tree is finished.</p>
<p>Below you see, how to implement a conditional inference tree in R. First, we prepare the session by activating the libraries that we will need (and installing them in case we have not done so before). Then, we set the options and read in the data.</p>
<pre class="r"><code>#install.packages(Rling)       # install Rling library (remove # to activate)
library(Rling)                 # activate Rling library
library(partykit)              # activate partykit library
library(dplyr)                 # activate dplyr library
options(stringsAsFactors = T)  # set options: do not convert strings
options(scipen = 999)          # set options: supress math. notation
options(max.prAmplified=10000) # set options
# load data
citdata &lt;- read.delim(&quot;data/treedata.txt&quot;, header = T, sep = &quot;\t&quot;)
head(citdata)</code></pre>
<pre><code>##     Age Gender Status LikeUser
## 1 15-40 female   high       no
## 2 15-40 female   high       no
## 3 15-40   male   high       no
## 4 41-80 female    low      yes
## 5 41-80   male   high       no
## 6 41-80   male    low       no</code></pre>
<pre class="r"><code>set.seed(111)        # set.seed
# apply bonferroni correction (1 minus alpha multiplied by n of predictors)
control = ctree_control(mincriterion = 1-(.05*ncol(citdata)-1))
# create initial conditional inference tree model
citd.ctree &lt;- ctree(LikeUser ~ Age + Gender + Status,
                    data = citdata)
plot(citd.ctree, gp = gpar(fontsize = 8)) # plot final ctree</code></pre>
<p><img src="advancedstatztrees_files/figure-html/unnamed-chunk-16-1.png" width="672" /></p>
<p>In addition to simply plotting the conditional inference tree, we can also check its accuracy. In a first step, we calculate the percentage of correct predictions made by our tree.</p>
<pre class="r"><code># test prediction accuracy
ptb &lt;- table(predict(citd.ctree), citdata$LikeUser)
(((ptb[1]+ptb[4])+(ptb[2]+ptb[3]))/sum(table(predict(citd.ctree), citdata$LikeUser)))*100</code></pre>
<pre><code>## [1] 100</code></pre>
<p>Next, we calculate the baseline accuracy, so see how much better the conditional decision tree performs compared with a chance distribution.</p>
<pre class="r"><code># determine baseline
(table(citdata$LikeUser)[[2]]/sum(table(citdata$LikeUser)))*100</code></pre>
<pre><code>## [1] 52.98805</code></pre>
<div id="splitting-numeric-ordinal-and-true-categorical-variables" class="section level2">
<h2><span class="header-section-number">1.1</span> Splitting numeric, ordinal, and true categorical variables</h2>
<p>While it is rather straight forward to calculate the Gini values for categorical variables, it may not seem quite as apparent how to calculate splits for numeric or ordinal variables. To illustrate how the algorithm works on such variables, consider the example data set shown below.</p>
<table>
<caption>Example data set to illustrate numeric splits in conditional inference trees.</caption>
<thead>
<tr class="header">
<th align="right">Age</th>
<th align="left">LikeUser</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">15</td>
<td align="left">yes</td>
</tr>
<tr class="even">
<td align="right">37</td>
<td align="left">no</td>
</tr>
<tr class="odd">
<td align="right">63</td>
<td align="left">no</td>
</tr>
<tr class="even">
<td align="right">42</td>
<td align="left">yes</td>
</tr>
<tr class="odd">
<td align="right">22</td>
<td align="left">yes</td>
</tr>
<tr class="even">
<td align="right">27</td>
<td align="left">yes</td>
</tr>
</tbody>
</table>
<p>In a first step, we order the numeric variable so that we arrive at the follwoing table.</p>
<table>
<caption>Ordered data set to illustrate numeric splits in conditional inference trees.</caption>
<thead>
<tr class="header">
<th align="right">Age</th>
<th align="left">LikeUser</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">15</td>
<td align="left">yes</td>
</tr>
<tr class="even">
<td align="right">22</td>
<td align="left">yes</td>
</tr>
<tr class="odd">
<td align="right">27</td>
<td align="left">yes</td>
</tr>
<tr class="even">
<td align="right">37</td>
<td align="left">no</td>
</tr>
<tr class="odd">
<td align="right">42</td>
<td align="left">yes</td>
</tr>
<tr class="even">
<td align="right">63</td>
<td align="left">no</td>
</tr>
</tbody>
</table>
<p>Next, we calculate the means for each level of “Age”.</p>
<table>
<caption>Ordered data set with mean age levels to illustrate numeric splits in conditional inference trees.</caption>
<thead>
<tr class="header">
<th align="right">Age</th>
<th align="left">LikeUser</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">15.0</td>
<td align="left">yes</td>
</tr>
<tr class="even">
<td align="right">18.5</td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="right">22.0</td>
<td align="left">yes</td>
</tr>
<tr class="even">
<td align="right">24.5</td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="right">27.0</td>
<td align="left">yes</td>
</tr>
<tr class="even">
<td align="right">32.0</td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="right">37.0</td>
<td align="left">no</td>
</tr>
<tr class="even">
<td align="right">39.5</td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="right">42.0</td>
<td align="left">yes</td>
</tr>
<tr class="even">
<td align="right">52.5</td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="right">63.0</td>
<td align="left">no</td>
</tr>
</tbody>
</table>
<p>Now, we calculate the Gini values for each average level of age.</p>
<span class="math display">\[\begin{equation}

G_{x} = 1 - ( p_{1} )^{2} - ( p_{0} )^{2}

\end{equation}\]</span>
<p>For an age smaller than 18.5 this would mean:</p>
<span class="math display">\[\begin{equation}

G_{youngerthan18.5} = 1-(\frac{1} {1+0})^{2} - (\frac{0} {1+0})^{2} = 0.0

\end{equation}\]</span>
<p>For an age greater than 18.5, we caculate G or Gini as follows:</p>
<span class="math display">\[\begin{equation}

G_{olerthan18.5} = 1-(\frac{2} {2+3})^{2} - (\frac{3} {2+3})^{2} = 0.48

\end{equation}\]</span>
<p>Now, we calculate the Gini for that split as we have done above.</p>
<span class="math display">\[\begin{equation}

G_{split18.5} = \frac{N_{youngerthan18.5}} {N_{Total}} \times G_{youngerthan18.5} +  \frac{N_{olderthan18.5}} {N_{Total}} \times G_{olderthan18.5}

G_{split18.5} = \frac{1} {6} \times 0.0 +  \frac{5} {6} \times 0.48 = 0.4

\end{equation}\]</span>
<p>We now have to calculate the Gini values for all possible age splits which yields the follwoing results:</p>
<table>
<caption>Possible age splits and their associated Gini values.</caption>
<thead>
<tr class="header">
<th align="right">AgeSplit</th>
<th align="right">Gini</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">18.5</td>
<td align="right">0.400</td>
</tr>
<tr class="even">
<td align="right">24.5</td>
<td align="right">0.500</td>
</tr>
<tr class="odd">
<td align="right">32.0</td>
<td align="right">0.444</td>
</tr>
<tr class="even">
<td align="right">39.5</td>
<td align="right">0.410</td>
</tr>
<tr class="odd">
<td align="right">52.5</td>
<td align="right">0.267</td>
</tr>
</tbody>
</table>
<p>The split at 52.5 years of age has the lowest Gini value. Accordingly, we would split the data between speakers who are younger than 52.5 and speakers who are older than 52.5 years of age. The lowest Gini value for any age split would also be the Gini value that would be compared to other variables.</p>
<p>The same procedure that we have used to determine potential splits for a numeric variable would apply to an ordinal variable with only two differences:</p>
<ul>
<li>The Gini values are calculated for the actual levels and not the means between variable levels.</li>
<li>The Gini value is nor calculated for the lowest and highest level as the calculation of the Gini values is impissble for extreme values. Extreme levels can, therefore, not serve as a potential split locations.</li>
</ul>
<p>When dealing with categorical variables with more than two levels, the situation is slightly more complex as we would also have to calculate the Gini values for combinations of variable levels. While the calculations are, in principle, analogous to the ones performed for binary of nominal categorical variables, we would also have to check if combinations would lead to improved splits. For instance, imagine we have a variable with categories “A”, “B”, and “C”. In such cases we would not only have to calculate the Gini scores for “A”, “B”, and “C” but also for “A plus B”, “A plus C”, and “B plus C”. Note that we ignore the combination “A plus B plus C” as this combination would include all other portential combinations.</p>
</div>
<div id="problems-of-conditional-inference-trees" class="section level2">
<h2><span class="header-section-number">1.2</span> Problems of Conditional Inference Trees</h2>
<p>Conditional Inference Trees are very intuitive, multivariate, non-parametric, they do not reuire large datasets and they are easy to implement. However, they are prone to overfitting and do not perform well why they are applied to new data. An extension which remedies these problems is to grow many varied trees - this is called a Random Forest Analysis and will will have a look at how Random Forests work and how to implement them in R in the next section.</p>
</div>
</div>
<div id="random-forests" class="section level1">
<h1><span class="header-section-number">2</span> Random Forests</h1>
<p>Random Forests are an extension of Conditional Inference Trees. Like Conditional Inference Trees, Random Forests represent a multivariate, non-parametric partitioning method that is particularly useful when dealing with relatively small sample sizes and many predictors (including interactions). Random Forests outperform CITs in that they are substantially less prone to overfitting and they perform much better when applied to new data.</p>
<p><strong>Bootstrapped Data</strong></p>
<p>Random Forests do not work on one-and-the-same data set (as CITs do) but in Random Forest analyses, many samples (with replacement) are drawn from the original data set. This generation of new data set based on an existing data set is called “bootstrapping”. Bootstrapping allows us to produce many trees based on variations of the original data set rather than dealing with only a single, fixed data set that would produce only a single tree. Therefore, because the data is different each time, the individual CITs are also different.</p>
<p>Imagine, we are dealing with a very small data set to which we want to apply a Random Forest Analysis. The original data set is displayed below.</p>
<table>
<caption>Example data set to illustrate Random Forests.</caption>
<thead>
<tr class="header">
<th align="right">ID</th>
<th align="left">Age</th>
<th align="left">Gender</th>
<th align="left">Status</th>
<th align="left">LikeUser</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">1</td>
<td align="left">15-40</td>
<td align="left">female</td>
<td align="left">high</td>
<td align="left">no</td>
</tr>
<tr class="even">
<td align="right">2</td>
<td align="left">15-40</td>
<td align="left">female</td>
<td align="left">high</td>
<td align="left">no</td>
</tr>
<tr class="odd">
<td align="right">3</td>
<td align="left">15-40</td>
<td align="left">male</td>
<td align="left">high</td>
<td align="left">no</td>
</tr>
<tr class="even">
<td align="right">4</td>
<td align="left">41-80</td>
<td align="left">female</td>
<td align="left">low</td>
<td align="left">yes</td>
</tr>
<tr class="odd">
<td align="right">5</td>
<td align="left">41-80</td>
<td align="left">male</td>
<td align="left">high</td>
<td align="left">no</td>
</tr>
<tr class="even">
<td align="right">6</td>
<td align="left">41-80</td>
<td align="left">male</td>
<td align="left">low</td>
<td align="left">no</td>
</tr>
</tbody>
</table>
<p>We now draw a sample from this data set and receive the following data set.</p>
<table>
<caption>Bootstrapped data set to illustrate Random Forests.</caption>
<thead>
<tr class="header">
<th align="right">ID</th>
<th align="left">Age</th>
<th align="left">Gender</th>
<th align="left">Status</th>
<th align="left">LikeUser</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">6</td>
<td align="left">41-80</td>
<td align="left">male</td>
<td align="left">low</td>
<td align="left">no</td>
</tr>
<tr class="even">
<td align="right">3</td>
<td align="left">15-40</td>
<td align="left">male</td>
<td align="left">high</td>
<td align="left">no</td>
</tr>
<tr class="odd">
<td align="right">4</td>
<td align="left">41-80</td>
<td align="left">female</td>
<td align="left">low</td>
<td align="left">yes</td>
</tr>
<tr class="even">
<td align="right">1</td>
<td align="left">15-40</td>
<td align="left">female</td>
<td align="left">high</td>
<td align="left">no</td>
</tr>
<tr class="odd">
<td align="right">2</td>
<td align="left">15-40</td>
<td align="left">female</td>
<td align="left">high</td>
<td align="left">no</td>
</tr>
<tr class="even">
<td align="right">2</td>
<td align="left">15-40</td>
<td align="left">female</td>
<td align="left">high</td>
<td align="left">no</td>
</tr>
</tbody>
</table>
<p>As you can see, the bootstrapped data contains the second row twice while the fifth row is missing.</p>
<p><strong>Out-Of-Bag data</strong></p>
<p>Because the data is reshuffeled for every new tree, a part of the data (on average about 30%) remains unused fro a given tree. The dat that is not used is called Out-Of-Bag data or OOB. The OOB is importnat because the quality of teh overall performance of the random forest can be assessed by applying the resulting tree-model to the data that it was not fit to. The quality of that tree is then measured in the OOB error, which is the error rate of the respective tree if applied to the OOB data.</p>
<p><strong>Random Variable Selection</strong></p>
<p>Random Forests also differ from simple CITs in that at each step, not all possible variables are considered for a node, but only a subset. For example, we have a data set with five predicting independent variables and one dependent variable. When generating a CIT, all possible variables (variables that do not represent a node further up in the tree) are consiederd as splitting candidates. In Random Forests, only a fixed number (typically the square-root of the number of independent variables) are considered as candidates for a node. So at each potential split, a fixed number of randomly selected variables is considered potential node candidates.</p>
<div id="random-forests-in-r-heart-disease" class="section level2">
<h2><span class="header-section-number">2.1</span> Random Forests in R: Heart Disease</h2>
<p>This section is adapted from <a href="https://statquest.org/2018/02/26/statquest-random-forests-in-r/" class="uri">https://statquest.org/2018/02/26/statquest-random-forests-in-r/</a>. We start the procedure by activating the packages that we will need for the analysis.</p>
<pre class="r"><code>library(ggplot2)       # activate ggplot2 library for plotting
library(cowplot)       # activate cowplot library for modifying ggplot2
library(randomForest)  # activate randomForest library for random forests</code></pre>
<p>The data used in this demo comes from the UCI machine learning repository ( <a href="http://archive.ics.uci.edu/ml/index.php" class="uri">http://archive.ics.uci.edu/ml/index.php</a>). Specifically, this is the heart disease data set ( <a href="http://archive.ics.uci.edu/ml/datasets/Heart+Disease" class="uri">http://archive.ics.uci.edu/ml/datasets/Heart+Disease</a>).</p>
<pre class="r"><code># define path to data
url &lt;- &quot;http://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data&quot;
# load data 
data &lt;- read.csv(url, header=FALSE)
# inspect data
head(data) </code></pre>
<pre><code>##   V1 V2 V3  V4  V5 V6 V7  V8 V9 V10 V11 V12 V13 V14
## 1 63  1  1 145 233  1  2 150  0 2.3   3 0.0 6.0   0
## 2 67  1  4 160 286  0  2 108  1 1.5   2 3.0 3.0   2
## 3 67  1  4 120 229  0  2 129  1 2.6   2 2.0 7.0   1
## 4 37  1  3 130 250  0  0 187  0 3.5   3 0.0 3.0   0
## 5 41  0  2 130 204  0  2 172  0 1.4   1 0.0 3.0   0
## 6 56  1  2 120 236  0  0 178  0 0.8   1 0.0 3.0   0</code></pre>
<p>Because the data does not have variable names, we have to add variable names manually. In additon, we need to rReformat the data so that it is easy to use and interpreted correctly by randomForest.</p>
<pre class="r"><code>colnames(data) &lt;- c(
  &quot;age&quot;,
  &quot;sex&quot;,# 0 = female, 1 = male
  &quot;cp&quot;, # chest pain
          # 1 = typical angina,
          # 2 = atypical angina,
          # 3 = non-anginal pain,
          # 4 = asymptomatic
  &quot;trestbps&quot;, # resting blood pressure (in mm Hg)
  &quot;chol&quot;, # serum cholestoral in mg/dl
  &quot;fbs&quot;,  # fasting blood sugar greater than 120 mg/dl, 1 = TRUE, 0 = FALSE
  &quot;restecg&quot;, # resting electrocardiographic results
          # 1 = normal
          # 2 = having ST-T wave abnormality
          # 3 = showing probable or definite left ventricular hypertrophy
  &quot;thalach&quot;, # maximum heart rate achieved
  &quot;exang&quot;,   # exercise induced angina, 1 = yes, 0 = no
  &quot;oldpeak&quot;, # ST depression induced by exercise relative to rest
  &quot;slope&quot;, # the slope of the peak exercise ST segment
          # 1 = upsloping
          # 2 = flat
          # 3 = downsloping
  &quot;ca&quot;, # number of major vessels (0-3) colored by fluoroscopy
  &quot;thal&quot;, # this is short of thalium heart scan
          # 3 = normal (no cold spots)
          # 6 = fixed defect (cold spots during rest and exercise)
          # 7 = reversible defect (when cold spots only appear during exercise)
  &quot;hd&quot; # (the predicted attribute) - diagnosis of heart disease
          # 0 if less than or equal to 50% diameter narrowing
          # 1 if greater than 50% diameter narrowing
  )
head(data) # now we have data and column names</code></pre>
<pre><code>##   age sex cp trestbps chol fbs restecg thalach exang oldpeak slope  ca
## 1  63   1  1      145  233   1       2     150     0     2.3     3 0.0
## 2  67   1  4      160  286   0       2     108     1     1.5     2 3.0
## 3  67   1  4      120  229   0       2     129     1     2.6     2 2.0
## 4  37   1  3      130  250   0       0     187     0     3.5     3 0.0
## 5  41   0  2      130  204   0       2     172     0     1.4     1 0.0
## 6  56   1  2      120  236   0       0     178     0     0.8     1 0.0
##   thal hd
## 1  6.0  0
## 2  3.0  2
## 3  7.0  1
## 4  3.0  0
## 5  3.0  0
## 6  3.0  0</code></pre>
<pre class="r"><code>str(data) </code></pre>
<pre><code>## &#39;data.frame&#39;:    303 obs. of  14 variables:
##  $ age     : num  63 67 67 37 41 56 62 57 63 53 ...
##  $ sex     : num  1 1 1 1 0 1 0 0 1 1 ...
##  $ cp      : num  1 4 4 3 2 2 4 4 4 4 ...
##  $ trestbps: num  145 160 120 130 130 120 140 120 130 140 ...
##  $ chol    : num  233 286 229 250 204 236 268 354 254 203 ...
##  $ fbs     : num  1 0 0 0 0 0 0 0 0 1 ...
##  $ restecg : num  2 2 2 0 2 0 2 0 2 2 ...
##  $ thalach : num  150 108 129 187 172 178 160 163 147 155 ...
##  $ exang   : num  0 1 1 0 0 0 0 1 0 1 ...
##  $ oldpeak : num  2.3 1.5 2.6 3.5 1.4 0.8 3.6 0.6 1.4 3.1 ...
##  $ slope   : num  3 2 2 3 1 1 3 1 2 3 ...
##  $ ca      : Factor w/ 5 levels &quot;?&quot;,&quot;0.0&quot;,&quot;1.0&quot;,..: 2 5 4 2 2 2 4 2 3 2 ...
##  $ thal    : Factor w/ 4 levels &quot;?&quot;,&quot;3.0&quot;,&quot;6.0&quot;,..: 3 2 4 2 2 2 2 2 4 4 ...
##  $ hd      : int  0 2 1 0 0 0 3 0 2 1 ...</code></pre>
<p>This shows that we need to tell R which columns contain factors and it also shows us that there are some missing values For example, there are “?”s in the dataset. First, replace “?”s with NAs. And then, we add factors for variables that are factors and clean up the factors that had missing data…</p>
<pre class="r"><code>data[data == &quot;?&quot;] &lt;- NA
data[data$sex == 0,]$sex &lt;- &quot;F&quot;
data[data$sex == 1,]$sex &lt;- &quot;M&quot;
data$sex &lt;- as.factor(data$sex)
data$cp &lt;- as.factor(data$cp)
data$fbs &lt;- as.factor(data$fbs)
data$restecg &lt;- as.factor(data$restecg)
data$exang &lt;- as.factor(data$exang)
data$slope &lt;- as.factor(data$slope)
data$ca &lt;- as.integer(data$ca) </code></pre>
<p>Since “ca” had “?”s in it (which we have since converted to NAs) R thinks that the levels for the factor are strings, but we know they are integers, so we’ll first convert the strings to integiers…</p>
<pre class="r"><code>data$ca &lt;- as.factor(data$ca)  # ...then convert the integers to factor levels
data$thal &lt;- as.integer(data$thal) # &quot;thal&quot; also had &quot;?&quot;s in it.
data$thal &lt;- as.factor(data$thal)
data$hd &lt;- ifelse(test=data$hd == 0, yes=&quot;Healthy&quot;, no=&quot;Unhealthy&quot;)
data$hd &lt;- as.factor(data$hd) # Now convert to a factor
str(data) </code></pre>
<pre><code>## &#39;data.frame&#39;:    303 obs. of  14 variables:
##  $ age     : num  63 67 67 37 41 56 62 57 63 53 ...
##  $ sex     : Factor w/ 2 levels &quot;F&quot;,&quot;M&quot;: 2 2 2 2 1 2 1 1 2 2 ...
##  $ cp      : Factor w/ 4 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;: 1 4 4 3 2 2 4 4 4 4 ...
##  $ trestbps: num  145 160 120 130 130 120 140 120 130 140 ...
##  $ chol    : num  233 286 229 250 204 236 268 354 254 203 ...
##  $ fbs     : Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 2 1 1 1 1 1 1 1 1 2 ...
##  $ restecg : Factor w/ 3 levels &quot;0&quot;,&quot;1&quot;,&quot;2&quot;: 3 3 3 1 3 1 3 1 3 3 ...
##  $ thalach : num  150 108 129 187 172 178 160 163 147 155 ...
##  $ exang   : Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 1 2 2 1 1 1 1 2 1 2 ...
##  $ oldpeak : num  2.3 1.5 2.6 3.5 1.4 0.8 3.6 0.6 1.4 3.1 ...
##  $ slope   : Factor w/ 3 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;: 3 2 2 3 1 1 3 1 2 3 ...
##  $ ca      : Factor w/ 4 levels &quot;2&quot;,&quot;3&quot;,&quot;4&quot;,&quot;5&quot;: 1 4 3 1 1 1 3 1 2 1 ...
##  $ thal    : Factor w/ 3 levels &quot;2&quot;,&quot;3&quot;,&quot;4&quot;: 2 1 3 1 1 1 1 1 3 3 ...
##  $ hd      : Factor w/ 2 levels &quot;Healthy&quot;,&quot;Unhealthy&quot;: 1 2 2 1 1 1 2 1 2 2 ...</code></pre>
<p>This shows that the correct columns are factors and we’ve replaced “?”s with NAs because “?” no longer appears in the list of factors for “ca” and “thal”.</p>
<p>Note that for most machine learning methods, you need to divide the data manually into a “training” set and a “test” set. This allows you to train the method using the training data, and then test it on data it was not originally trained on.</p>
<p>In contrast, Random Forests split the data into “training” and “test” sets for you. This is because Random Forests use bootstrapped data, and thus, not every sample is used to build every tree. The “training” dataset is the bootstrapped data and the “test” dataset is the remaining samples. The remaining samples are called the “Out-Of-Bag” (OOB) data.</p>
<pre class="r"><code>set.seed(42)
## impute any missing values in the training set using proximities
data.imputed &lt;- rfImpute(hd ~ ., data = data, iter=6)</code></pre>
<pre><code>## ntree      OOB      1      2
##   300:  17.49% 12.80% 23.02%
## ntree      OOB      1      2
##   300:  16.83% 14.02% 20.14%
## ntree      OOB      1      2
##   300:  17.82% 13.41% 23.02%
## ntree      OOB      1      2
##   300:  17.49% 14.02% 21.58%
## ntree      OOB      1      2
##   300:  17.16% 12.80% 22.30%
## ntree      OOB      1      2
##   300:  18.15% 14.63% 22.30%</code></pre>
<p>The argument “iter” refers to the number of iterations to run. Breiman says 4 to 6 iterations is usually good enough. With this dataset, when we set iter=6, OOB-error bounces around between 17% and 18%. When we set iter=20, we get values a little better and a little worse, so doing more iterations doesn’t improve the situation.</p>
<p>Also, if you really want to micromanage how rfImpute(), you can change the number of trees it makes (the default is 300) and the number of variables that it will consider at each step.</p>
<p>Now we are ready to build a random forest. If the thing we’re trying to predict (in this case it is whether or not someone has heart disease) is a continuous number (i.e. “weight” or “height”), then by default, randomForest() will set “mtry”, the number of variables to consider at each step, to the total number of variables divided by 3 (rounded down), or to 1 (if the division results in a value less than 1). If the thing we’re trying to predict is a “factor” (i.e. either “yes/no” or “ranked”), then randomForest() will set mtry to the square root of the number of variables (rounded down to the next integer value).</p>
<p>In this example, “hd”, the thing we are trying to predict, is a factor and there are 13 variables. So by default, randomForest() will set mtry = sqrt(13) = 3.6 rounded down = 3. Also, by default random forest generates 500 trees (NOTE: rfImpute() only generates 300 tress by default)</p>
<pre class="r"><code>model &lt;- randomForest(hd ~ ., data=data.imputed, proximity=TRUE)
model </code></pre>
<pre><code>## 
## Call:
##  randomForest(formula = hd ~ ., data = data.imputed, proximity = TRUE) 
##                Type of random forest: classification
##                      Number of trees: 500
## No. of variables tried at each split: 3
## 
##         OOB estimate of  error rate: 16.83%
## Confusion matrix:
##           Healthy Unhealthy class.error
## Healthy       142        22   0.1341463
## Unhealthy      29       110   0.2086331</code></pre>
<p>The model gives us an overview of the call, along with the OOB error rate for the forest with ntree trees (in this case ntree=500 by default). In additon, the output provides a confusion matrix for the forest with ntree trees. The confusion matrix is laid out like this:</p>
<table>
<caption>Confusion matrix for the random forest for Heart Disease data.</caption>
<tbody>
<tr class="odd">
<td></td>
<td align="left">Healthy</td>
<td align="left">Unhealthy</td>
</tr>
<tr class="even">
<td>Healthy</td>
<td align="left">Number of healthy people correctly called healthy by the forest.</td>
<td align="left">Number of healthy people incorectly called unhealthy by the forest</td>
</tr>
<tr class="odd">
<td>Unhealthy</td>
<td align="left">Number of unhealthy people incorrectly called healthy by the forest</td>
<td align="left">Number of unhealthy people correctly called unhealthy by the forest</td>
</tr>
</tbody>
</table>
<p>Now, we check to see if the random forest is actually big enough… Up to a point, the more trees in the forest, the better. You can tell when you’ve made enough when the OOB no longer improves.</p>
<pre class="r"><code>oob.error.data &lt;- data.frame(
  Trees=rep(1:nrow(model$err.rate), times=3),
  Type=rep(c(&quot;OOB&quot;, &quot;Healthy&quot;, &quot;Unhealthy&quot;), each=nrow(model$err.rate)),
  Error=c(model$err.rate[,&quot;OOB&quot;],
    model$err.rate[,&quot;Healthy&quot;],
    model$err.rate[,&quot;Unhealthy&quot;]))
 
ggplot(data=oob.error.data, aes(x=Trees, y=Error)) +
  geom_line(aes(color=Type))</code></pre>
<p><img src="advancedstatztrees_files/figure-html/unnamed-chunk-35-1.png" width="672" /></p>
<pre class="r"><code># ggsave(&quot;oob_error_rate_500_trees.pdf&quot;)</code></pre>
<p>The blue line shows the error rate specifically for calling “Unheathly” patients that are OOB. The green line shows the overall OOB error rate. The red line shows the error rate specifically for calling “Healthy” patients that are OOB.</p>
<p>After building a random forest with 500 tress, the graph does not make it clear that the OOB-error has settled on a value or, if we added more trees, it would continue to decrease. So we do the whole thing again, but this time add more trees.</p>
<pre class="r"><code>model &lt;- randomForest(hd ~ ., data=data.imputed, ntree=1000, proximity=TRUE)
model</code></pre>
<pre><code>## 
## Call:
##  randomForest(formula = hd ~ ., data = data.imputed, ntree = 1000,      proximity = TRUE) 
##                Type of random forest: classification
##                      Number of trees: 1000
## No. of variables tried at each split: 3
## 
##         OOB estimate of  error rate: 17.16%
## Confusion matrix:
##           Healthy Unhealthy class.error
## Healthy       141        23   0.1402439
## Unhealthy      29       110   0.2086331</code></pre>
<pre class="r"><code>oob.error.data &lt;- data.frame(
  Trees=rep(1:nrow(model$err.rate), times=3),
  Type=rep(c(&quot;OOB&quot;, &quot;Healthy&quot;, &quot;Unhealthy&quot;), each=nrow(model$err.rate)),
  Error=c(model$err.rate[,&quot;OOB&quot;],
    model$err.rate[,&quot;Healthy&quot;],
    model$err.rate[,&quot;Unhealthy&quot;]))
 
ggplot(data=oob.error.data, aes(x=Trees, y=Error)) +
  geom_line(aes(color=Type))</code></pre>
<p><img src="advancedstatztrees_files/figure-html/unnamed-chunk-37-1.png" width="672" /></p>
<pre class="r"><code># ggsave(&quot;oob_error_rate_1000_trees.pdf&quot;)</code></pre>
<p>After building a random forest with 1,000 trees, we get the same OOB-error 16.5% and we can see convergence in the graph. So we could have gotten away with only 500 trees, but we wouldn’t have been sure that number was enough.</p>
<p>If we want to compare this random forest to others with different values for mtry (to control how many variables are considered at each step)…</p>
<pre class="r"><code>oob.values &lt;- vector(length=10)
for(i in 1:10) {
  temp.model &lt;- randomForest(hd ~ ., data=data.imputed, mtry=i, ntree=1000)
  oob.values[i] &lt;- temp.model$err.rate[nrow(temp.model$err.rate),1]
}
oob.values</code></pre>
<pre><code>##  [1] 0.1749175 0.1782178 0.1716172 0.1782178 0.1683168 0.1881188 0.1815182
##  [8] 0.1980198 0.1782178 0.1848185</code></pre>
<p>The lowest value is when mtry=3, so the default setting was the best. Now let’s create an MDS-plot to show how the samples are related to each other.</p>
<p>Start by converting the proximity matrix into a distance matrix.</p>
<pre class="r"><code>distance.matrix &lt;- dist(1-model$proximity)
mds.stuff &lt;- cmdscale(distance.matrix, eig=TRUE, x.ret=TRUE)
## calculate the percentage of variation that each MDS axis accounts for...
mds.var.per &lt;- round(mds.stuff$eig/sum(mds.stuff$eig)*100, 1)
## now make a fancy looking plot that shows the MDS axes and the variation:
mds.values &lt;- mds.stuff$points
mds.data &lt;- data.frame(Sample=rownames(mds.values),
  X=mds.values[,1],
  Y=mds.values[,2],
  Status=data.imputed$hd)
 
ggplot(data=mds.data, aes(x=X, y=Y, label=Sample)) +
  geom_text(aes(color=Status)) +
  theme_bw() +
  xlab(paste(&quot;MDS1 - &quot;, mds.var.per[1], &quot;%&quot;, sep=&quot;&quot;)) +
  ylab(paste(&quot;MDS2 - &quot;, mds.var.per[2], &quot;%&quot;, sep=&quot;&quot;)) +
  ggtitle(&quot;MDS plot using (1 - Random Forest Proximities)&quot;)</code></pre>
<p><img src="advancedstatztrees_files/figure-html/unnamed-chunk-39-1.png" width="672" /></p>
<p>As a final step, we plot the effect sizes of the variables as the mean of decrease in Gini values.</p>
<pre class="r"><code>varImpPlot(model, main = &quot;&quot;, pch = 20) </code></pre>
<p><img src="advancedstatztrees_files/figure-html/unnamed-chunk-40-1.png" width="672" /></p>
</div>
<div id="random-forests-in-r-really" class="section level2">
<h2><span class="header-section-number">2.2</span> Random Forests in R: Really</h2>
<p>We now use a similar but slightly more elaborate method of going through a random forest analysis. Again, we start the procedure by activating libraries that we will need and by loading the data and setting a seed. A seed allows us to replicate this analysis.</p>
<pre class="r"><code>library(knitr)         # activate knitr library
library(dplyr)         # activate dplyr library
library(randomForest)  # activate randomForest library
rfd &lt;- read.delim(&quot;data/treedata.txt&quot;, header = T, sep = &quot;\t&quot;)
set.seed(222)                       # set seed</code></pre>
<p>After the data preparation, we can now start with the random forest analysis and, in a first step, we partition the data in order to be able to evaluate the precision of the random forest model later on. We partition the data into a training and a test set.</p>
<pre class="r"><code>id &lt;- sample(2, nrow(rfd), replace = T, prob = c(.7, .3))
train &lt;- rfd[id == 1, ]
test &lt;- rfd[id == 2,]</code></pre>
<p>We now create an initial random forest model and inspect its parameters.</p>
<pre class="r"><code>like_rf1 &lt;- randomForest(LikeUser~., data = train)
print(like_rf1) # inspect model</code></pre>
<pre><code>## 
## Call:
##  randomForest(formula = LikeUser ~ ., data = train) 
##                Type of random forest: classification
##                      Number of trees: 500
## No. of variables tried at each split: 1
## 
##         OOB estimate of  error rate: 27.59%
## Confusion matrix:
##     no yes class.error
## no  40  37   0.4805195
## yes 11  86   0.1134021</code></pre>
<p>We can also inspect attibutes of the random forest model.</p>
<pre class="r"><code>attributes(like_rf1)</code></pre>
<pre><code>## $names
##  [1] &quot;call&quot;            &quot;type&quot;            &quot;predicted&quot;      
##  [4] &quot;err.rate&quot;        &quot;confusion&quot;       &quot;votes&quot;          
##  [7] &quot;oob.times&quot;       &quot;classes&quot;         &quot;importance&quot;     
## [10] &quot;importanceSD&quot;    &quot;localImportance&quot; &quot;proximity&quot;      
## [13] &quot;ntree&quot;           &quot;mtry&quot;            &quot;forest&quot;         
## [16] &quot;y&quot;               &quot;test&quot;            &quot;inbag&quot;          
## [19] &quot;terms&quot;          
## 
## $class
## [1] &quot;randomForest.formula&quot; &quot;randomForest&quot;</code></pre>
<p>After running an initial random forest model, we will now start with the model evaluation. To evaluate the model, we will have to load additional libraries.</p>
<pre class="r"><code># install package
#source(&quot;https://bioconductor.org/biocLite.R&quot;); biocLite(); library(Biobase)
#install.packages(&quot;Biobase&quot;, repos=c(&quot;http://rstudio.org/_packages&quot;, &quot;http://cran.rstudio.com&quot;, 
#                                      &quot;http://cran.rstudio.com/&quot;, dependencies=TRUE))
#install.packages(&quot;dimRed&quot;, dependencies = TRUE)
#install.packages(&#39;caret&#39;, dependencies = TRUE)
# load caret library
library(caret) # because initially caret did not work, the libraries above had to be installed
ptrain1 &lt;- predict(like_rf1, train) # extract prediction for training data
head(ptrain1); head(train$LikeUser)         # inspect predictions</code></pre>
<pre><code>##   2   3   4   7   8   9 
## yes yes yes yes yes  no 
## Levels: no yes</code></pre>
<pre><code>## [1] no  no  yes yes no  no 
## Levels: no yes</code></pre>
<p>Now, we generate a confusion matrix for the training data, extract the prediction for test data, and create confusion matrix for the test data.</p>
<pre class="r"><code>confusionMatrix(ptrain1, train$LikeUser)  </code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction no yes
##        no  40   4
##        yes 37  93
##                                           
##                Accuracy : 0.7644          
##                  95% CI : (0.6942, 0.8253)
##     No Information Rate : 0.5575          
##     P-Value [Acc &gt; NIR] : 0.00000001119   
##                                           
##                   Kappa : 0.5004          
##  Mcnemar&#39;s Test P-Value : 0.00000058060   
##                                           
##             Sensitivity : 0.5195          
##             Specificity : 0.9588          
##          Pos Pred Value : 0.9091          
##          Neg Pred Value : 0.7154          
##              Prevalence : 0.4425          
##          Detection Rate : 0.2299          
##    Detection Prevalence : 0.2529          
##       Balanced Accuracy : 0.7391          
##                                           
##        &#39;Positive&#39; Class : no              
## </code></pre>
<pre class="r"><code>ptest1 &lt;- predict(like_rf1, test)
confusionMatrix(ptest1, test$LikeUser)</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction no yes
##        no  18   4
##        yes 23  32
##                                           
##                Accuracy : 0.6494          
##                  95% CI : (0.5322, 0.7547)
##     No Information Rate : 0.5325          
##     P-Value [Acc &gt; NIR] : 0.025323        
##                                           
##                   Kappa : 0.3177          
##  Mcnemar&#39;s Test P-Value : 0.000532        
##                                           
##             Sensitivity : 0.4390          
##             Specificity : 0.8889          
##          Pos Pred Value : 0.8182          
##          Neg Pred Value : 0.5818          
##              Prevalence : 0.5325          
##          Detection Rate : 0.2338          
##    Detection Prevalence : 0.2857          
##       Balanced Accuracy : 0.6640          
##                                           
##        &#39;Positive&#39; Class : no              
## </code></pre>
<p>Now, we determine error rate of random forest model.</p>
<pre class="r"><code>plot(like_rf1, main = &quot;&quot;)</code></pre>
<p><img src="advancedstatztrees_files/figure-html/unnamed-chunk-47-1.png" width="672" /></p>
<p>After going through the model evaluation, we can now tune the ransom forest model.</p>
<pre class="r"><code>like_rf2 &lt;- tuneRF(train[, !colnames(train)== &quot;LikeUser&quot;], 
                        train[, colnames(train)== &quot;LikeUser&quot;], 
                        stepFactor = 3, # for most values 3 appears to be optimal
                        plot = T,
                        ntreeTry = 200,
                        trace = T,
                        improve = .05
)</code></pre>
<pre><code>## mtry = 1  OOB error = 25.86% 
## Searching left ...
## Searching right ...
## mtry = 3     OOB error = 23.56% 
## 0.08888889 0.05</code></pre>
<p><img src="advancedstatztrees_files/figure-html/unnamed-chunk-48-1.png" width="672" /></p>
<p>Again, we create an improved model and inspect its properties.</p>
<pre class="r"><code>like_rf2 &lt;- randomForest(LikeUser~., data = train, 
                              ntree = 200,
                              ntry = 6,
                              importance= T,
                              proximity = T)
print(like_rf2)   # inspect model</code></pre>
<pre><code>## 
## Call:
##  randomForest(formula = LikeUser ~ ., data = train, ntree = 200,      ntry = 6, importance = T, proximity = T) 
##                Type of random forest: classification
##                      Number of trees: 200
## No. of variables tried at each split: 1
## 
##         OOB estimate of  error rate: 24.14%
## Confusion matrix:
##     no yes class.error
## no  40  37  0.48051948
## yes  5  92  0.05154639</code></pre>
<p>As before, we extract the predictions based on the improved model and create a confusion matrix.</p>
<pre class="r"><code>ptrain2 &lt;- predict(like_rf2, train)
confusionMatrix(ptrain2, train$LikeUser)</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction no yes
##        no  40   4
##        yes 37  93
##                                           
##                Accuracy : 0.7644          
##                  95% CI : (0.6942, 0.8253)
##     No Information Rate : 0.5575          
##     P-Value [Acc &gt; NIR] : 0.00000001119   
##                                           
##                   Kappa : 0.5004          
##  Mcnemar&#39;s Test P-Value : 0.00000058060   
##                                           
##             Sensitivity : 0.5195          
##             Specificity : 0.9588          
##          Pos Pred Value : 0.9091          
##          Neg Pred Value : 0.7154          
##              Prevalence : 0.4425          
##          Detection Rate : 0.2299          
##    Detection Prevalence : 0.2529          
##       Balanced Accuracy : 0.7391          
##                                           
##        &#39;Positive&#39; Class : no              
## </code></pre>
<p>We will now extract predictions for test data and create a confusion matrix.</p>
<pre class="r"><code>ptest2 &lt;- predict(like_rf2, test)
confusionMatrix(ptest2, test$LikeUser)</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction no yes
##        no  18   4
##        yes 23  32
##                                           
##                Accuracy : 0.6494          
##                  95% CI : (0.5322, 0.7547)
##     No Information Rate : 0.5325          
##     P-Value [Acc &gt; NIR] : 0.025323        
##                                           
##                   Kappa : 0.3177          
##  Mcnemar&#39;s Test P-Value : 0.000532        
##                                           
##             Sensitivity : 0.4390          
##             Specificity : 0.8889          
##          Pos Pred Value : 0.8182          
##          Neg Pred Value : 0.5818          
##              Prevalence : 0.5325          
##          Detection Rate : 0.2338          
##    Detection Prevalence : 0.2857          
##       Balanced Accuracy : 0.6640          
##                                           
##        &#39;Positive&#39; Class : no              
## </code></pre>
<p>Now, we inspect number of nodes per tree in the forest.</p>
<pre class="r"><code>hist(treesize(like_rf2), main = &quot;&quot;, col = &quot;lightgray&quot;)</code></pre>
<p><img src="advancedstatztrees_files/figure-html/unnamed-chunk-52-1.png" width="672" /></p>
<p>We now move on and determine the effect size by checking variable importance.</p>
<pre class="r"><code>varImpPlot(like_rf2, main = &quot;&quot;, pch = 20) </code></pre>
<p><img src="advancedstatztrees_files/figure-html/unnamed-chunk-53-1.png" width="672" /></p>
<p>The left plot shows the accuracy. More precisely, it shows how much accuracy decreases if factor is left out. The right plot shows the impurity of the nodes of the model based on the predictors. In other words, it shows how much more unpure (ambiguous) the distributions become if factor is left out extract variable importance values.</p>
<pre class="r"><code>importance(like_rf2)</code></pre>
<pre><code>##               no       yes MeanDecreaseAccuracy MeanDecreaseGini
## Age    14.099255 12.421169            14.586045         8.184826
## Gender 14.606837 14.386844            15.423353         8.799358
## Status  4.539962  3.110163             4.601034         3.468382</code></pre>
<p>Which variables have been used in the trees?</p>
<pre class="r"><code>varUsed(like_rf2)</code></pre>
<pre><code>## [1] 215 217 215</code></pre>
<p>We can also show the effect size in partial dependence plots.</p>
<pre class="r"><code>partialPlot(like_rf2, train, Age)</code></pre>
<p><img src="advancedstatztrees_files/figure-html/unnamed-chunk-56-1.png" width="672" /></p>
<p>To extract a specific tree. we can use the following command.</p>
<pre class="r"><code>getTree(like_rf2, 1, labelVar = T)</code></pre>
<pre><code>##   left daughter right daughter split var split point status prediction
## 1             2              3    Status           1      1       &lt;NA&gt;
## 2             4              5       Age           1      1       &lt;NA&gt;
## 3             0              0      &lt;NA&gt;           0     -1         no
## 4             0              0      &lt;NA&gt;           0     -1        yes
## 5             0              0      &lt;NA&gt;           0     -1         no</code></pre>
<p>Finally, a multidimensional scaling plot shows patternings in the data.</p>
<pre class="r"><code>MDSplot(like_rf2, test$LikeUser)</code></pre>
<p><img src="advancedstatztrees_files/figure-html/unnamed-chunk-58-1.png" width="672" /></p>
</div>
<div id="random-forests-in-r-really-again" class="section level2">
<h2><span class="header-section-number">2.3</span> Random Forests in R: Really again</h2>
<pre class="r"><code># detach partykit
#detach(&quot;package:partykit&quot;, unload=TRUE)
# load package party
library(party)
# set seed
set.seed(333)
# create initial model
like.rf &lt;- cforest(LikeUser ~ Age + Gender + Status,
                        data = rfd, controls = cforest_unbiased(ntree = 50, mtry = 3))
# determine importance of factors
like.varimp &lt;- varimp(like.rf, conditional = T)
round(like.varimp, 3)</code></pre>
<pre><code>##    Age Gender Status 
##  0.090  0.068 -0.001</code></pre>
<pre class="r"><code># plot result
dotchart(sort(like.varimp), pch = 20, main = &quot;Conditional importance of variables&quot;)</code></pre>
<p><img src="advancedstatztrees_files/figure-html/unnamed-chunk-60-1.png" width="672" /></p>
<pre class="r"><code># load library
library(Hmisc)
# evaluate random forst
like.rf.pred &lt;- unlist(treeresponse(like.rf))[c(FALSE,TRUE)]
somers2(like.rf.pred, as.numeric(rfd$LikeUser) - 1)</code></pre>
<pre><code>##           C         Dxy           n     Missing 
##   0.8059768   0.6119536 251.0000000   0.0000000</code></pre>
</div>
<div id="random-forests-in-r-really-1" class="section level2">
<h2><span class="header-section-number">2.4</span> Random Forests in R: Really?</h2>
<pre class="r"><code># load library
library(party)
cf1 &lt;- cforest(LikeUser ~ . , data= rfd, control=cforest_unbiased(mtry=2,ntree=100)) # fit the random forest
varimp(cf1) # get variable importance, based on mean decrease in accuracy</code></pre>
<pre><code>##         Age      Gender      Status 
## 0.125543478 0.065652174 0.001304348</code></pre>
<pre class="r"><code># conditional=True, adjusts for correlations between predict
varimp(cf1, conditional=TRUE) </code></pre>
<pre><code>##         Age      Gender      Status 
##  0.09543478  0.05934783 -0.00326087</code></pre>
<pre class="r"><code>varimpAUC(cf1)  # more robust towards class imbalance.</code></pre>
<pre><code>##        Age     Gender     Status 
## 0.14008569 0.06914335 0.03530152</code></pre>
<pre class="r"><code>par(mar = c(5, 8, 4, 2) + 0.1)
plot(y = 1:length(varimpAUC(cf1)), x = varimpAUC(cf1)[order(varimpAUC(cf1))], 
     axes = F, ann = F, pch = 20, xlim = c(-0.01, 0.2), main = &quot;Predictor Importance&quot;)
axis(1, at = seq(-0.01, 0.2, 0.05), seq(-0.01, 0.2, 0.05))
axis(2, at = 1:length(varimpAUC(cf1)), names(varimpAUC(cf1))[order(varimpAUC(cf1))], las = 2)
grid()
box()</code></pre>
<p><img src="advancedstatztrees_files/figure-html/unnamed-chunk-65-1.png" width="672" /></p>
<pre class="r"><code>par(mar = c(5, 4, 4, 2) + 0.1)</code></pre>
</div>
</div>
<div id="boruta" class="section level1">
<h1><span class="header-section-number">3</span> Boruta</h1>
<p>Boruta (Kursa &amp; Rudnicki 2010, 2018) is a variable selection procedure and it represents an extension of random forest analyses (cf. Breiman 2001; Tagliamonte &amp; Baayen 2012). The name “Boruta” is derived from a demon in Slavic mythology who dwelled in pine forests. Boruta is an alternative to regression modelling that is better equipped to handle small data sets because it uses a distributional approach during which hundreds of (random) forests are grown from permutated data sets. This means that Boruta outperforms random forest analyses because it does not provide merely a single value for each predictor but a distribution of values leading to higher reliability.</p>
<p>The Boruta analysis consists out of five steps.</p>
<ul>
<li><p>In a first step, the Boruta algorithm copies the data set and adds randomness to the data by (re-)shuffling data points and thereby creating randomized variables. These randomized variables are referred to as shadow features.</p></li>
<li><p>Secondly, a random forest classifier is trained on the extended data set.</p></li>
<li><p>In a third step, a feature importance measure (Mean Decrease Accuracy represented by z-scores) is calculated to determine the relative importance of all predictors (both original or real variables and the randomized shadow features).</p></li>
<li><p>In the next step, it is checked at each iteration of the process whether a real predictor has a higher importance compared with the best shadow feature. The algorithm keeps track of the performance of the original variables by storing whether they outperformed the best shadow feature or not in a vector.</p></li>
<li><p>In the fifth step, predictors that did not outperform the best shadow feature are removed and the process continues without them. After a set number of iterations, or if all the variables have been either confirmed as outperforming the best shadow feature, the algorithm stops.</p></li>
</ul>
<p>Despite its obvious advantages of Boruta over random forest analyses and regression modelling, it can neither handle multicollinearity not hierarchical data structures where data points are nested or grouped by a given predictor (as is the case in the present analysis as data points are grouped by adjective type). As Boruta is a variable selection procedure, it is also limited in the sense that it provides information on which predictors to include and how good these predictors are (compared to the shadow variables) while it is neither able to take hierarchical data structure into account, nor does it provide information about how one level of a factor compares to other factors. In other words, Boruta shows that a predictor is relevant and how strong it is but it does not provide information on how the likelihood of an outcome being used differs between variable levels, for instance between men and women.</p>
</div>
<div id="boruta-in-r" class="section level1">
<h1><span class="header-section-number">4</span> Boruta in R</h1>
<p>We begin by preparing the session. In a first step, we load the necessary library and the data.</p>
<pre class="r"><code>#install.packages(Boruta)       # install Boruta library (remove # to activate)
library(Boruta)                # activate Boruta library
options(stringsAsFactors = T)  # set options: do not convert strings
options(scipen = 999)          # set options: supress math. notation
options(max.prAmplified=10000) # set options
# load data
borutadata &lt;- read.delim(&quot;data/treedata.txt&quot;, header = T, sep = &quot;\t&quot;)
head(borutadata)</code></pre>
<pre><code>##     Age Gender Status LikeUser
## 1 15-40 female   high       no
## 2 15-40 female   high       no
## 3 15-40   male   high       no
## 4 41-80 female    low      yes
## 5 41-80   male   high       no
## 6 41-80   male    low       no</code></pre>
<p>Now that we have loaded the data, we will start our initial analysis.</p>
<pre class="r"><code># initial run
boruta1 &lt;- Boruta(LikeUser~.,data=borutadata)
print(boruta1)</code></pre>
<pre><code>## Boruta performed 9 iterations in 0.3752592 secs.
##  3 attributes confirmed important: Age, Gender, Status;
##  No attributes deemed unimportant.</code></pre>
<pre class="r"><code>getConfirmedFormula(boruta1)</code></pre>
<pre><code>## LikeUser ~ Age + Gender + Status
## &lt;environment: 0x0000000034774638&gt;</code></pre>
<pre class="r"><code>plot(boruta1, cex = .5)</code></pre>
<p><img src="advancedstatztrees_files/figure-html/unnamed-chunk-68-1.png" width="672" /></p>
<p>We now have a look at the importance history to see if any variables can be removed from the analysis.</p>
<pre class="r"><code>plotImpHistory(boruta1)</code></pre>
<p><img src="advancedstatztrees_files/figure-html/unnamed-chunk-69-1.png" width="672" /></p>
<p>After inspecting the importance history, we see that our predictors performed consistently better than chance and the Boruta parameters tehrefore do not need to be modified and we also do not need to remove insignificant variables which do not consistently perform better than the shadow variable. If perdictors do perform worse than the best shadow variable, then these variables should be excludde and the Boruta analysis should be re-run on the dataset that does no longer contain the superfluous variable.</p>
<p>Now, we print the confirmed formular to check which predictors should be included in the model.</p>
<pre class="r"><code>getConfirmedFormula(boruta1)</code></pre>
<pre><code>## LikeUser ~ Age + Gender + Status
## &lt;environment: 0x000000003db9aaf8&gt;</code></pre>
<p>As a last step, we customize the results plot of the Boruta analysis.</p>
<pre class="r"><code>par(mar = c(8, 8, 4, 2) + 0.1)
plot(boruta1, cex.axis=.75, las=2, xlab=&quot;&quot;, ylab = &quot;&quot;, cex = .75, 
     col = c(&quot;grey50&quot;, &quot;grey50&quot;, &quot;grey50&quot;,&quot;grey90&quot;,&quot;grey90&quot;,&quot;grey90&quot;))
abline(v = 3.5, lty = &quot;dashed&quot;)
mtext(&quot;Predictors&quot;, 1, line = 6, at = 5, cex = 1)
mtext(&quot;Control&quot;, 1, line = 6, at = 2, cex = 1)
mtext(&quot;Importance&quot;, 2, line = 2.5, at = 20, cex = 1, las = 0)</code></pre>
<p><img src="advancedstatztrees_files/figure-html/unnamed-chunk-71-1.png" width="672" /></p>
<pre class="r"><code>par(mar = c(5, 4, 4, 2) + 0.1)</code></pre>
</div>
<div id="references" class="section level1">
<h1><span class="header-section-number">5</span> References</h1>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
