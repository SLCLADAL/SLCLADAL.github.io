<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />


<meta name="author" content="UQ SLC Digital Team" />

<meta name="date" content="2020-04-09" />

<title>Tree-Based Models</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cosmo.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<link href="site_libs/font-awesome-5.1.0/css/all.css" rel="stylesheet" />
<link href="site_libs/font-awesome-5.1.0/css/v4-shims.css" rel="stylesheet" />

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>




<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #ffffff;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">LADAL</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    <span class="fa fa-play-circle"></span>
     
    Basics
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li class="dropdown-header">Basics</li>
    <li>
      <a href="introcomputer.html">General Tips on Computering</a>
    </li>
    <li class="divider"></li>
    <li>
      <a href="introquant.html">Introduction To Quantitative Reasoning</a>
    </li>
    <li>
      <a href="basicquant.html">Basic Concepts In Quantitative Reasoning</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    <span class="fa fa-gear"></span>
     
    Data Processing
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li class="dropdown-header">Data Processing</li>
    <li>
      <a href="intror.html">Basics: Getting started with R</a>
    </li>
    <li>
      <a href="introloading.html">Loading and saving data</a>
    </li>
    <li>
      <a href="stringprocessing.html">String processing</a>
    </li>
    <li>
      <a href="regularexpressions.html">Regular expressions</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    <span class="fa fa-bar-chart"></span>
     
    Visualization
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li class="dropdown-header">Visualization</li>
    <li>
      <a href="basicgraphs.html">Visualizing Data with R</a>
    </li>
    <li>
      <a href="maps.html">Geo-Spatial Data Visualization in R</a>
    </li>
    <li>
      <a href="motion.html">Motion Charts in R</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    <span class="fa fa-eye"></span>
     
    Statistics
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li class="dropdown-header">Statistics</li>
    <li>
      <a href="descriptivestatz.html">Descriptive Statistics</a>
    </li>
    <li class="divider"></li>
    <li class="dropdown-header">Basic Interential Statistics</li>
    <li>
      <a href="basicstatz.html">Basic Inferential Tests</a>
    </li>
    <li>
      <a href="basicstatzchi.html">The Chi-Square Family</a>
    </li>
    <li>
      <a href="basicstatzregression.html">Simple Linear Regression</a>
    </li>
    <li class="divider"></li>
    <li class="dropdown-header">Advanced Interential Statistics</li>
    <li>
      <a href="fixedregressions.html">Fixed-Effects Regression Models</a>
    </li>
    <li>
      <a href="mixedregressions.html">Mixed-Effects Regression Models</a>
    </li>
    <li>
      <a href="advancedstatztrees.html">Tree-Based Models</a>
    </li>
    <li>
      <a href="groupingstatz.html">Classification</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    <span class="fa fa-bars"></span>
     
    Text Analysis/Corpus Linguistics
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li class="dropdown-header">Text Analysis</li>
    <li>
      <a href="textanalysis.html">Introduction</a>
    </li>
    <li>
      <a href="webcrawling.html">Web Crawling</a>
    </li>
    <li>
      <a href="network.html">Network Analysis</a>
    </li>
    <li>
      <a href="topicmodels.html">Topic Modeling</a>
    </li>
    <li>
      <a href="classification.html">Classification</a>
    </li>
    <li>
      <a href="tagging.html">Tagging and Parsing</a>
    </li>
    <li class="divider"></li>
    <li class="dropdown-header">Corpus Linguistics</li>
    <li>
      <a href="corplingr.html">Corpus Linguistics in R</a>
    </li>
  </ul>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="about.html">
    <span class="fa fa-info"></span>
     
    Contact
  </a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Tree-Based Models</h1>
<h4 class="author">UQ SLC Digital Team</h4>
<h4 class="date">2020-04-09</h4>

</div>


<p><img src="images/uq1.jpg" width="100%" /></p>
<div id="introduction" class="section level1">
<h1><span class="header-section-number">1</span> Introduction</h1>
<p>This tutorial focuses on tree-based models and their implementation in R. The entire code for the sections below can be downloaded <a href="https://slcladal.github.io/rscripts/advancedstatztreesrscript.r">here</a>.</p>
</div>
<div id="preparation-and-session-set-up" class="section level1">
<h1><span class="header-section-number">2</span> Preparation and session set up</h1>
<p>As all calculations and visualizations rely on R, it is necessary to install R and RStudio. If these programs (or, in the case of R, environments) are not already installed on your machine, please search for them in your favourite search engine and add the term “download”. Open any of the first few links and follow the installation instructions (they are easy to follow, do not require any specifications, and are pretty much self-explanatory).</p>
<p>In addition, certain <em>packages</em> need to be installed from an R <em>library</em> so that the scripts shown below are executed without errors. Before turning to the code below, please install the packages by running the code below this paragraph. If you have already installed the packages mentioned below, then you can skip ahead ignore this section. To install the necessary packages, simply run the following code - it may take some time (between 1 and 5 minutes to install all of the libraries so you do not need to worry if it takes some time).</p>
<pre class="r"><code># clean current workspace
rm(list=ls(all=T))
# set options
options(stringsAsFactors = F)         # no automatic data transformation
options(&quot;scipen&quot; = 100, &quot;digits&quot; = 4) # supress math annotation
# install libraries
install.packages(c(&quot;Boruta&quot;, &quot;caret&quot;, &quot;cowplot&quot;, &quot;dplyr&quot;, 
                   &quot;ggplot2&quot;, &quot;Gmisc&quot;, &quot;grid&quot;, &quot;Hmisc&quot;, 
                   &quot;knitr&quot;, &quot;party&quot;, &quot;partykit&quot;, &quot;randomForest&quot;, 
                   &quot;Rling&quot;))</code></pre>
<p>Once you have installed R, R-Studio, and have also initiated the session by executing the code shown above, you are good to go.</p>
</div>
<div id="conditional-inference-trees" class="section level1">
<h1><span class="header-section-number">3</span> Conditional Inference Trees</h1>
<p>This section deals with tree-structure models, the most basic type is a conditional inference tree. Like random forests, conditional inference trees are non-parametric and thus do not rely on distributional requirements (or at least on fewer). The tree structure represents recursive partitioning of the data to minimize residual deviance. There are several advantages to using tree-based models:</p>
<ul>
<li>Tree-structure models are very useful because they can deal with different types of variables and provide a very good understanding of the structure in the data.</li>
<li>Tree-structure models are particularly interesting for linguists because they can handle moderate sample sizes and many high-order interactions better then regression models.</li>
<li>Tree-based models can be used as variable-selection procedure which informs about which variables have any sort of significant relationship with the dependent variable and can thereby inform model fitting.</li>
</ul>
<p>Before we implement a conditional inference tree in R, we will have a look at how it works. We will do this in more detail here as random forests and Boruta analyses are extensions of conditional inference trees and are therefore based on the same concepts.</p>
<p>Below is a conditional inference tree which shows how and what factors contribute to the use of discourse like. All conditional inference trees answer a simple question, namely “How do we classify elements based on the given predictors?”. The answer that conditional inference trees provide is the classification of the elements based on the levels of the predictors. If predictors are not significant or all elements can be classified correctly without them (i.e. if these predictors are unnecessary), then they are not included in the decision tree (or in the model fitting process). The conditional inference tree shows that the best predictor for discourse <em>like</em> use is age as it is the highest node. Among young speakers, those with high status use like more compared with speakers of lower social status. Among old speakers, women use discourse <em>like</em> more than men.</p>
<p><img src="advancedstatztrees_files/figure-html/cit1-1.png" width="672" /></p>
<p>Before going through how this conditional decision tree is generated, let us first go over some basic concepts. The top of the decision tree is called “root” or “root node”, the categories at the end of branches are called “leaves” or “leaf nodes”. Nodes that are in-between the root and leaves are called “internal nodes” or just “nodes”. The root node has only arrows or lines pointing away from it, internal nodes have lines going to and from them, while leaf nodes only have lines pointing towards them.</p>
<p>Let us now go over the process by which the decision tree above is generated. In our example, we want to predict whether a person makes use of discourse <em>like</em> given their age, gender, and social status. The first six lines of this fictitious data set are displayed in the table below:</p>
<table>
<caption>Example data set to illustrate conditional inference trees.</caption>
<thead>
<tr class="header">
<th align="left">Age</th>
<th align="left">Gender</th>
<th align="left">Status</th>
<th align="left">LikeUser</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">15-40</td>
<td align="left">female</td>
<td align="left">high</td>
<td align="left">no</td>
</tr>
<tr class="even">
<td align="left">15-40</td>
<td align="left">female</td>
<td align="left">high</td>
<td align="left">no</td>
</tr>
<tr class="odd">
<td align="left">15-40</td>
<td align="left">male</td>
<td align="left">high</td>
<td align="left">no</td>
</tr>
<tr class="even">
<td align="left">41-80</td>
<td align="left">female</td>
<td align="left">low</td>
<td align="left">yes</td>
</tr>
<tr class="odd">
<td align="left">41-80</td>
<td align="left">male</td>
<td align="left">high</td>
<td align="left">no</td>
</tr>
<tr class="even">
<td align="left">41-80</td>
<td align="left">male</td>
<td align="left">low</td>
<td align="left">no</td>
</tr>
</tbody>
</table>
<p>In a first step, we initialize our R session by activating relevant libraries, setting options and setting a seed. Then, we load the data into R.</p>
<pre class="r"><code># activate libraries
library(partykit)              
library(dplyr)  
library(grid)
library(Gmisc) 
library(Rling) 
library(ggplot2)       
library(cowplot)       
library(randomForest)
library(party)
library(Hmisc)
library(Boruta) 
library(RCurl)
# to install the caret library, it was necessary to go through the installation 
# process below - once caret is installed once, you do not need to go through 
# these steps again
# install caret library
#source(&quot;https://bioconductor.org/biocLite.R&quot;); biocLite(); library(Biobase)
#install.packages(&quot;Biobase&quot;, repos=c(&quot;http://rstudio.org/_packages&quot;, &quot;http://cran.rstudio.com&quot;, 
#                                      &quot;http://cran.rstudio.com/&quot;, dependencies=TRUE))
#install.packages(&quot;dimRed&quot;, dependencies = TRUE)
#install.packages(&#39;caret&#39;, dependencies = TRUE)
# activate caret library
library(caret) 
# set options
options(stringsAsFactors = F)
options(scipen = 999)
options(max.print=10000)
# load data
citdata &lt;- read.delim(&quot;https://raw.githubusercontent.com/MartinSchweinberger/coedlss2019materials/master/datatables/treedata.txt&quot;, header = T, sep = &quot;\t&quot;)
# inspect data
head(citdata); str(citdata)</code></pre>
<pre><code>##     Age Gender Status LikeUser
## 1 15-40 female   high       no
## 2 15-40 female   high       no
## 3 15-40   male   high       no
## 4 41-80 female    low      yes
## 5 41-80   male   high       no
## 6 41-80   male    low       no</code></pre>
<pre><code>## &#39;data.frame&#39;:    251 obs. of  4 variables:
##  $ Age     : chr  &quot;15-40&quot; &quot;15-40&quot; &quot;15-40&quot; &quot;41-80&quot; ...
##  $ Gender  : chr  &quot;female&quot; &quot;female&quot; &quot;male&quot; &quot;female&quot; ...
##  $ Status  : chr  &quot;high&quot; &quot;high&quot; &quot;high&quot; &quot;low&quot; ...
##  $ LikeUser: chr  &quot;no&quot; &quot;no&quot; &quot;no&quot; &quot;yes&quot; ...</code></pre>
<p>The “str” functions output tells us that the data is not factorized yet. As tree-based models require either numeric or factorized data, we factorize the “character” variables in our data.</p>
<pre class="r"><code># factorize variables (cit require factors instead of character vectors)
fcts &lt;- c(&quot;Age&quot;, &quot;Gender&quot;, &quot;Status&quot;, &quot;LikeUser&quot;)
citdata[fcts] &lt;- lapply(citdata[fcts], factor)
# inspect data
str(citdata)</code></pre>
<pre><code>## &#39;data.frame&#39;:    251 obs. of  4 variables:
##  $ Age     : Factor w/ 2 levels &quot;15-40&quot;,&quot;41-80&quot;: 1 1 1 2 2 2 2 1 2 2 ...
##  $ Gender  : Factor w/ 2 levels &quot;female&quot;,&quot;male&quot;: 1 1 2 1 2 2 1 2 2 2 ...
##  $ Status  : Factor w/ 2 levels &quot;high&quot;,&quot;low&quot;: 1 1 1 2 1 2 2 1 2 2 ...
##  $ LikeUser: Factor w/ 2 levels &quot;no&quot;,&quot;yes&quot;: 1 1 1 2 1 1 2 1 1 1 ...</code></pre>
<p>The data now consists of factors which two levels each. After initializing the R session, it needs to be determined, what the root of the decision tree should be. This means that we have to determine which of the variables represents the root node. In order to do so, we tabulate for each variable level, how many speakers of that level have used discourse <em>like</em> (LikeUsers) and how many have not used discourse <em>like</em> (NonLikeUsers).</p>
<pre class="r"><code># tabulate data
table(citdata$LikeUser, citdata$Gender)</code></pre>
<pre><code>##      
##       female male
##   no      43   75
##   yes     91   42</code></pre>
<pre class="r"><code>table(citdata$LikeUser, citdata$Age)</code></pre>
<pre><code>##      
##       15-40 41-80
##   no     34    84
##   yes    92    41</code></pre>
<pre class="r"><code>table(citdata$LikeUser, citdata$Status)</code></pre>
<pre><code>##      
##       high low
##   no    33  85
##   yes   73  60</code></pre>
<p>None of the predictors is perfect (the predictors are therefore referred to as “impure”). To determine which variable is the root, we will calculate the degree of “impurity” for each variable - the variable which has the lowest impurity value will be the root.</p>
<p>The most common measure of impurity in the context of conditional inference trees is called “Gini”. The Gini value or gini index was introduced by Corrado Gini as a measure for income inequality. In our case we seek to maximize inequality of distributions of leave nodes which is why the gini index is useful for tree based models. For each level we apply the following equation to determine the gini impurity value:</p>
<p><span class="math display">\[\begin{equation}

G_{x} = 1 - ( p_{1} )^{2} - ( p_{0} )^{2}

\end{equation}\]</span></p>
<p>For the node for men, this would mean the following:</p>
<p><span class="math display">\[\begin{equation}

G_{men} = 1-(\frac{42} {42+75})^{2} - (\frac{75} {42+75})^{2} = 0.4602235

\end{equation}\]</span></p>
<p>For women, we calculate G or Gini as follows:</p>
<p><span class="math display">\[\begin{equation}

G_{women} = 1-(\frac{91} {91+43})^{2} - (\frac{43} {91+43})^{2} = 0.4358432

\end{equation}\]</span></p>
<p>To calculate the Gini value of Gender, we need to calculate the weighted average leaf node impurity (weighted because the number of speakers is different in each group). We calculate the weighted average leaf node impurity using the equation below.</p>
<p><span class="math display">\[\begin{equation}

G_{Gender} = \frac{N_{men}} {N_{Total}} \times G_{men} +  \frac{N_{women}} {N_{Total}} \times G_{women}

G_{Gender} = \frac{159} {303} \times 0.4602235 +  \frac{144} {303} \times 0.4358432 = 0.4611915

\end{equation}\]</span></p>
<p>Before we do these calculations in R, we will just briefly revisit the gender distribution and perform a <span class="math inline">\(\chi\)</span><sup>2</sup>-test to see whether testing gender makes sense in the first place.</p>
<pre class="r"><code># GENDER
# re-inspect gender distribution
tblikegender &lt;- table(citdata$LikeUser, citdata$Gender)
tblikegender</code></pre>
<pre><code>##      
##       female male
##   no      43   75
##   yes     91   42</code></pre>
<pre class="r"><code>chisq.test(tblikegender) # sig difference: data can be split!</code></pre>
<pre><code>## 
##  Pearson&#39;s Chi-squared test with Yates&#39; continuity correction
## 
## data:  tblikegender
## X-squared = 24.428, df = 1, p-value = 0.0000007714</code></pre>
<p>The <span class="math inline">\(\chi\)</span><sup>2</sup>-test shows that testing if we should split by gender does make sense as there are significant differences between men and women. We will now perform the gini-calculation for gender (see below).</p>
<pre class="r"><code># calculate Gini for men
gini_men &lt;- 1-(42/(42+75))^2 - (75/(42+75))^2
# calculate Gini for women
gini_women &lt;- 1-(91/(91+43))^2 - (43/(91+43))^2
# calculate weighted average of Gini for Gender
gini_gender &lt;- 42/(42+75)* gini_men +  91/(91+43) * gini_women
gini_gender</code></pre>
<pre><code>## [1] 0.4611915</code></pre>
<p>The gini for gender is .461. In a next step, we revisit the age distribution and also perform a <span class="math inline">\(\chi\)</span><sup>2</sup>-test.</p>
<pre class="r"><code># re-inspect age distribution
tblikeage &lt;- table(citdata$LikeUser, citdata$Age)
tblikeage</code></pre>
<pre><code>##      
##       15-40 41-80
##   no     34    84
##   yes    92    41</code></pre>
<pre class="r"><code>chisq.test(tblikeage) # sig difference: data can be split!</code></pre>
<pre><code>## 
##  Pearson&#39;s Chi-squared test with Yates&#39; continuity correction
## 
## data:  tblikeage
## X-squared = 39.141, df = 1, p-value = 0.0000000003943</code></pre>
<p>As the <span class="math inline">\(\chi\)</span><sup>2</sup>-test returns a significant result, we continue to calculate the gini value for age.</p>
<pre class="r"><code># calculate Gini for age groups
gini_young &lt;- 1-(92/(92+34))^2 - (34/(92+34))^2  # Gini: young
gini_old &lt;- 1-(41/(41+84))^2 - (84/(41+84))^2    # Gini: old
# calculate weighted average of Gini for Age
gini_age &lt;- 92/(92+34)* gini_young +  41/(41+84) * gini_old
gini_age</code></pre>
<pre><code>## [1] 0.4323148</code></pre>
<p>The gini for age is .432 and we continue by revisiting the status distribution and also perform a <span class="math inline">\(\chi\)</span><sup>2</sup>-test.</p>
<pre class="r"><code># re-inspect status distribution
tblikestatus &lt;- table(citdata$LikeUser, citdata$Status)
tblikestatus</code></pre>
<pre><code>##      
##       high low
##   no    33  85
##   yes   73  60</code></pre>
<pre class="r"><code>chisq.test(tblikestatus) # sig difference: data can be split!</code></pre>
<pre><code>## 
##  Pearson&#39;s Chi-squared test with Yates&#39; continuity correction
## 
## data:  tblikestatus
## X-squared = 17.488, df = 1, p-value = 0.00002892</code></pre>
<p>As the <span class="math inline">\(\chi\)</span><sup>2</sup>-test returns a significant result, we continue to calculate the gini value for status.</p>
<pre class="r"><code>gini_high &lt;- 1-(73/(33+73))^2 - (33/(33+73))^2   # Gini: high
gini_low &lt;- 1-(60/(60+85))^2 - (85/(60+85))^2    # Gini: low
# calculate weighted average of Gini for Status
gini_status &lt;- 73/(33+73)* gini_high +  60/(60+85) * gini_low
gini_status</code></pre>
<pre><code>## [1] 0.4960521</code></pre>
<p>The gini for status is .496 and we can now compare the gini values for age, gender, and status.</p>
<pre class="r"><code># compare age, gender, and status ginis
gini_age; gini_gender; gini_status</code></pre>
<pre><code>## [1] 0.4323148</code></pre>
<pre><code>## [1] 0.4611915</code></pre>
<pre><code>## [1] 0.4960521</code></pre>
<p>Since age has the lowest gini (impurity) value, our first split is by age and age, thus, represents our root node. Our manually calculated conditional inference tree right now looks as below.</p>
<p><img src="advancedstatztrees_files/figure-html/cit13-1.png" width="672" /></p>
<p>In a next step, we need to find out which of the remaining variables best separates the speakers who use discourse <em>like</em> from those that do not under the first node. In order to do so, we calculate the Gini values for Gender and SocialStatus for the “41-80” node.</p>
<p>In a first step, we tabulate the old speakers by gender, inspect the distribution, and use a <span class="math inline">\(\chi\)</span><sup>2</sup>-test to evaluate whether splitting makes sense.</p>
<pre class="r"><code># 2ND NODE
# split data according to first split (only old data for now)
old &lt;- citdata[citdata$Age == &quot;41-80&quot;,]
# inspect distribution
tboldgender &lt;- table(old$LikeUser, old$Gender)
tboldgender</code></pre>
<pre><code>##      
##       female male
##   no      26   58
##   yes     33    8</code></pre>
<pre class="r"><code>chisq.test(tboldgender) # sig difference: data can be split!</code></pre>
<pre><code>## 
##  Pearson&#39;s Chi-squared test with Yates&#39; continuity correction
## 
## data:  tboldgender
## X-squared = 25.176, df = 1, p-value = 0.0000005232</code></pre>
<p>As the <span class="math inline">\(\chi\)</span><sup>2</sup>-test returns a significant result, we continue to calculate the gini value for gender among old speakers.</p>
<pre class="r"><code># calculate Gini for Gender
# calculate Gini for men
gini_oldmen &lt;- 1-(tboldgender[2,2]/sum(tboldgender[,2]))^2 - (tboldgender[1,2]/sum(tboldgender[,2]))^2
# calculate Gini for women
gini_oldwomen &lt;- 1-(tboldgender[2,1]/sum(tboldgender[,1]))^2 - (tboldgender[1,1]/sum(tboldgender[,1]))^2
# # calculate weighted aAverage of Gini for Gender
gini_oldgender &lt;- sum(tboldgender[,2])/sum(tboldgender)* gini_oldmen +  sum(tboldgender[,1])/sum(tboldgender) * gini_oldwomen
gini_oldgender</code></pre>
<pre><code>## [1] 0.3451628</code></pre>
<p>The gini values for gender among old speakers is .345. We continue by inspecting the status distribution among old speakers.</p>
<pre class="r"><code># calculate Gini for Status
# inspect distribution
tboldstatus &lt;- table(old$LikeUser, old$Status)
tboldstatus</code></pre>
<pre><code>##      
##       high low
##   no    22  62
##   yes   16  25</code></pre>
<pre class="r"><code>chisq.test(tboldstatus) # sig difference: data can be split!</code></pre>
<pre><code>## 
##  Pearson&#39;s Chi-squared test with Yates&#39; continuity correction
## 
## data:  tboldstatus
## X-squared = 1.5811, df = 1, p-value = 0.2086</code></pre>
<p>Since the <span class="math inline">\(\chi\)</span><sup>2</sup>-test returns a non-significant result, we do not need to calculate the gini value for status and split the old speakers by gender without further calculation. In a next step, we determine if we need to further divide the 3<sup>rd</sup> node (old male speakers) by status. To do so, we inspect the status distribution for old male speakers and perform a <span class="math inline">\(\chi\)</span><sup>2</sup>-test.</p>
<pre class="r"><code># 3RD NODE
# split data according to first split (only old data for now)
oldmale &lt;- citdata %&gt;%
  dplyr::filter(Age == &quot;41-80&quot;) %&gt;%
  dplyr::filter(Gender == &quot;male&quot;)
# inspect distribution
tboldmalestatus &lt;- table(oldmale$LikeUser, oldmale$Status)
tboldmalestatus</code></pre>
<pre><code>##      
##       high low
##   no    17  41
##   yes    5   3</code></pre>
<pre class="r"><code>chisq.test(tboldmalestatus) # no sig difference: no more splits!</code></pre>
<pre><code>## 
##  Pearson&#39;s Chi-squared test with Yates&#39; continuity correction
## 
## data:  tboldmalestatus
## X-squared = 2.1514, df = 1, p-value = 0.1424</code></pre>
<p>Since the <span class="math inline">\(\chi\)</span><sup>2</sup>-test reports no significant differences, we have reached an end (“leave”) of this branch of our tree. We will now move on and check if we need to split the 4<sup>th</sup> node (old female speakers) by status.</p>
<pre class="r"><code># 4TH NODE
# split data according to first split (only old data for now)
oldfemale &lt;- citdata %&gt;%
  dplyr::filter(Age == &quot;41-80&quot;) %&gt;%
  dplyr::filter(Gender == &quot;female&quot;)
# inspect distribution
tboldfemalestatus &lt;- table(oldfemale$LikeUser, oldfemale$Status)
tboldfemalestatus</code></pre>
<pre><code>##      
##       high low
##   no     5  21
##   yes   11  22</code></pre>
<pre class="r"><code>chisq.test(tboldfemalestatus) # no sig difference: no more splits!</code></pre>
<pre><code>## 
##  Pearson&#39;s Chi-squared test with Yates&#39; continuity correction
## 
## data:  tboldfemalestatus
## X-squared = 0.83679, df = 1, p-value = 0.3603</code></pre>
<p>Here, too, we have reached the end of this branch (“leave”) because the <span class="math inline">\(\chi\)</span><sup>2</sup>-test reports no significant differences. We can thus move on to the young speakers (the 5<sup>th</sup> node) and test if and how to split this branch.</p>
<pre class="r"><code># 5TH NODE
# split data according to first split (only young data)
young &lt;- citdata[citdata$Age == &quot;15-40&quot;,]
# inspect distribution
tbyounggender &lt;- table(young$LikeUser, young$Gender)
tbyounggender</code></pre>
<pre><code>##      
##       female male
##   no      17   17
##   yes     58   34</code></pre>
<pre class="r"><code>chisq.test(tbyounggender) # no sig difference: do not split!</code></pre>
<pre><code>## 
##  Pearson&#39;s Chi-squared test with Yates&#39; continuity correction
## 
## data:  tbyounggender
## X-squared = 1.2535, df = 1, p-value = 0.2629</code></pre>
<p>As the <span class="math inline">\(\chi\)</span><sup>2</sup>-test does not report significant differences, we inspect the status distribution and check if low and high-status speakers differ significantly.</p>
<pre class="r"><code># calculate Gini for Status
# inspect distribution
tbyoungstatus &lt;- table(young$LikeUser, young$Status)
tbyoungstatus</code></pre>
<pre><code>##      
##       high low
##   no    11  23
##   yes   57  35</code></pre>
<pre class="r"><code>chisq.test(tbyoungstatus) # sig difference: split!</code></pre>
<pre><code>## 
##  Pearson&#39;s Chi-squared test with Yates&#39; continuity correction
## 
## data:  tbyoungstatus
## X-squared = 7.6066, df = 1, p-value = 0.005816</code></pre>
<p>Since the <span class="math inline">\(\chi\)</span><sup>2</sup>-test reports a significant result and gender was reported as being insignificant, we split by status without having to calculate the gini for status here. In a next step, we inspect the distribution of young low status speakers and test if we should split by gender.</p>
<pre class="r"><code># 6TH NODE
# split data according to first and second split (young and low status data)
younglow &lt;- citdata %&gt;%
  filter(Age == &quot;15-40&quot;) %&gt;%
  filter(Status == &quot;low&quot;)
# inspect gender distribution
tbyounglowgender &lt;- table(younglow$LikeUser, younglow$Gender)
tbyounglowgender</code></pre>
<pre><code>##      
##       female male
##   no      13   10
##   yes     21   14</code></pre>
<pre class="r"><code>chisq.test(tbyounglowgender) # no sig difference: no more splits!</code></pre>
<pre><code>## 
##  Pearson&#39;s Chi-squared test with Yates&#39; continuity correction
## 
## data:  tbyounglowgender
## X-squared = 0, df = 1, p-value = 1</code></pre>
<p>Here, too, we have reached the end of this branch (“leave”) because the <span class="math inline">\(\chi\)</span><sup>2</sup>-test reports no significant differences. We can thus move on to the young high-status speakers (the 7<sup>th</sup> node) and test if we need to split this branch by gender. Again, we inspect the distribution and perform a <span class="math inline">\(\chi\)</span><sup>2</sup>-test.</p>
<pre class="r"><code># 7TH node
# split data according to first and second split (young and high-status data)
younghigh &lt;- citdata %&gt;%
  filter(Age == &quot;15-40&quot;) %&gt;%
  filter(Status == &quot;high&quot;)
# inspect gender distribution
tbyounghighgender &lt;- table(younghigh$LikeUser, younghigh$Gender)
tbyounghighgender</code></pre>
<pre><code>##      
##       female male
##   no       4    7
##   yes     37   20</code></pre>
<pre class="r"><code>chisq.test(tbyounghighgender) # no sig difference: no more splits!</code></pre>
<pre><code>## 
##  Pearson&#39;s Chi-squared test with Yates&#39; continuity correction
## 
## data:  tbyounghighgender
## X-squared = 2.0598, df = 1, p-value = 0.1512</code></pre>
<p>The lack of a significant difference reported by the <span class="math inline">\(\chi\)</span><sup>2</sup>-test tells us that we have reached our final tree as all branches end in leaves now.</p>
<p>This analysis can be implemented much more smoothly in R, of course, which we will do below. Just a quick word on setting seeds. When we assign or set a seed, this means that any random number that is generated will be stored which makes analyses reproducible. Otherwise we would, of course, get different results if we relied on newly randomly generated numbers.</p>
<pre class="r"><code># set.seed (to store random numbers and thus make results reproducible)
set.seed(2019120202) 
# apply bonferroni correction (1 minus alpha multiplied by n of predictors)
control = ctree_control(mincriterion = 1-(.05*(ncol(citdata)-1)))
# create initial conditional inference tree model
citd.ctree &lt;- ctree(LikeUser ~ Age + Gender + Status, data = citdata)
plot(citd.ctree, gp = gpar(fontsize = 8)) # plot final ctree</code></pre>
<p><img src="advancedstatztrees_files/figure-html/cit23-1.png" width="672" /></p>
<p>In addition to plotting the conditional inference tree, we can also check its accuracy. To do so, we predict the use of like based on the conditional inference tree and compare them to the observed uses of like. Then we use the “confusionMatrix” function from the “caret” package to get an overview of the accuracy statistics.</p>
<pre class="r"><code>citprediction &lt;- predict(citd.ctree)
confusionMatrix(citprediction, citdata$LikeUser)</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction  no yes
##        no   58   8
##        yes  60 125
##                                           
##                Accuracy : 0.7291          
##                  95% CI : (0.6696, 0.7831)
##     No Information Rate : 0.5299          
##     P-Value [Acc &gt; NIR] : 0.00000000007873
##                                           
##                   Kappa : 0.4424          
##                                           
##  Mcnemar&#39;s Test P-Value : 0.00000000062237
##                                           
##             Sensitivity : 0.4915          
##             Specificity : 0.9398          
##          Pos Pred Value : 0.8788          
##          Neg Pred Value : 0.6757          
##              Prevalence : 0.4701          
##          Detection Rate : 0.2311          
##    Detection Prevalence : 0.2629          
##       Balanced Accuracy : 0.7157          
##                                           
##        &#39;Positive&#39; Class : no              
## </code></pre>
<p>The conditional inference tree has an accuracy of 72.91 percent which is significantly better than the base-line accuracy of 52.99 percent. To understand what the other statistics refer to and how they are calculated, run the command “?confusionMatrix”.</p>
<div id="splitting-numeric-ordinal-and-true-categorical-variables" class="section level2">
<h2><span class="header-section-number">3.1</span> Splitting numeric, ordinal, and true categorical variables</h2>
<p>While it is rather straight forward to calculate the Gini values for categorical variables, it may not seem quite as apparent how to calculate splits for numeric or ordinal variables. To illustrate how the algorithm works on such variables, consider the example data set shown below.</p>
<table>
<caption>Example data set to illustrate numeric splits in conditional inference trees.</caption>
<thead>
<tr class="header">
<th align="right">Age</th>
<th align="left">LikeUser</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">15</td>
<td align="left">yes</td>
</tr>
<tr class="even">
<td align="right">37</td>
<td align="left">no</td>
</tr>
<tr class="odd">
<td align="right">63</td>
<td align="left">no</td>
</tr>
<tr class="even">
<td align="right">42</td>
<td align="left">yes</td>
</tr>
<tr class="odd">
<td align="right">22</td>
<td align="left">yes</td>
</tr>
<tr class="even">
<td align="right">27</td>
<td align="left">yes</td>
</tr>
</tbody>
</table>
<p>In a first step, we order the numeric variable so that we arrive at the following table.</p>
<table>
<caption>Example data set to illustrate numeric splits in conditional inference trees.</caption>
<thead>
<tr class="header">
<th align="right">Age</th>
<th align="left">LikeUser</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">15</td>
<td align="left">yes</td>
</tr>
<tr class="even">
<td align="right">22</td>
<td align="left">yes</td>
</tr>
<tr class="odd">
<td align="right">27</td>
<td align="left">yes</td>
</tr>
<tr class="even">
<td align="right">37</td>
<td align="left">no</td>
</tr>
<tr class="odd">
<td align="right">42</td>
<td align="left">yes</td>
</tr>
<tr class="even">
<td align="right">63</td>
<td align="left">no</td>
</tr>
</tbody>
</table>
<p>Next, we calculate the means for each level of “Age”.</p>
<table>
<caption>Ordered data set with mean age levels to illustrate numeric splits in conditional inference trees.</caption>
<thead>
<tr class="header">
<th align="right">Age</th>
<th align="left">LikeUser</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">15.0</td>
<td align="left">yes</td>
</tr>
<tr class="even">
<td align="right">18.5</td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="right">22.0</td>
<td align="left">yes</td>
</tr>
<tr class="even">
<td align="right">24.5</td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="right">27.0</td>
<td align="left">yes</td>
</tr>
<tr class="even">
<td align="right">32.0</td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="right">37.0</td>
<td align="left">no</td>
</tr>
<tr class="even">
<td align="right">39.5</td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="right">42.0</td>
<td align="left">yes</td>
</tr>
<tr class="even">
<td align="right">52.5</td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="right">63.0</td>
<td align="left">no</td>
</tr>
</tbody>
</table>
<p>Now, we calculate the Gini values for each average level of age. How this is done is shown below for the first split.</p>
<p><span class="math display">\[\begin{equation}

G_{x} = 1 - ( p_{1} )^{2} - ( p_{0} )^{2}

\end{equation}\]</span></p>
<p>For an age smaller than 18.5 this would mean:</p>
<p><span class="math display">\[\begin{equation}

G_{youngerthan18.5} = 1-(\frac{1} {1+0})^{2} - (\frac{0} {1+0})^{2} = 0.0

\end{equation}\]</span></p>
<p>For an age greater than 18.5, we calculate G or Gini as follows:</p>
<p><span class="math display">\[\begin{equation}

G_{olerthan18.5} = 1-(\frac{2} {2+3})^{2} - (\frac{3} {2+3})^{2} = 0.48

\end{equation}\]</span></p>
<p>Now, we calculate the Gini for that split as we have done above.</p>
<p><span class="math display">\[\begin{equation}

G_{split18.5} = \frac{N_{youngerthan18.5}} {N_{Total}} \times G_{youngerthan18.5} +  \frac{N_{olderthan18.5}} {N_{Total}} \times G_{olderthan18.5}

G_{split18.5} = \frac{1} {6} \times 0.0 +  \frac{5} {6} \times 0.48 = 0.4

\end{equation}\]</span></p>
<p>We then have to calculate the gini values for all possible age splits which yields the following results:</p>
<pre class="r"><code># 18.5
1-(1/(1+0))^2 - (0/(1+0))^2
1-(2/(2+3))^2 - (3/(2+3))^2
1/6 * 0.0 +  5/6 * 0.48
# 24.4
1-(2/(2+0))^2 - (0/(2+0))^2
1-(3/(3+1))^2 - (2/(3+1))^2
2/6 * 0.0 +  4/6 * 0.1875
# 32
1-(3/(3+0))^2 - (0/(3+0))^2
1-(1/(1+2))^2 - (2/(1+2))^2
3/6 * 0.0 +  3/6 * 0.4444444
# 39.5
1-(3/(3+1))^2 - (1/(3+1))^2
1-(1/(1+1))^2 - (1/(1+1))^2
4/6 * 0.375 +  2/6 * 0.5
# 52.5
1-(4/(4+1))^2 - (1/(4+1))^2
1-(0/(0+1))^2 - (1/(0+1))^2
5/6 * 0.32 +  1/6 * 0.0</code></pre>
<table>
<caption>Possible age splits and their associated Gini values.</caption>
<thead>
<tr class="header">
<th align="right">AgeSplit</th>
<th align="right">Gini</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">18.5</td>
<td align="right">0.400</td>
</tr>
<tr class="even">
<td align="right">24.5</td>
<td align="right">0.500</td>
</tr>
<tr class="odd">
<td align="right">32.0</td>
<td align="right">0.444</td>
</tr>
<tr class="even">
<td align="right">39.5</td>
<td align="right">0.410</td>
</tr>
<tr class="odd">
<td align="right">52.5</td>
<td align="right">0.267</td>
</tr>
</tbody>
</table>
<p>The split at 52.5 years of age has the lowest Gini value. Accordingly, we would split the data between speakers who are younger than 52.5 and speakers who are older than 52.5 years of age. The lowest Gini value for any age split would also be the Gini value that would be compared to other variables.</p>
<p>The same procedure that we have used to determine potential splits for a numeric variable would apply to an ordinal variable with only two differences:</p>
<ul>
<li>The Gini values are calculated for the actual levels and not the means between variable levels.</li>
<li>The Gini value is nor calculated for the lowest and highest level as the calculation of the Gini values is impossible for extreme values. Extreme levels can, therefore, not serve as a potential split location.</li>
</ul>
<p>When dealing with categorical variables with more than two levels, the situation is slightly more complex as we would also have to calculate the Gini values for combinations of variable levels. While the calculations are, in principle, analogous to the ones performed for binary of nominal categorical variables, we would also have to check if combinations would lead to improved splits. For instance, imagine we have a variable with categories “A”, “B”, and “C”. In such cases we would not only have to calculate the Gini scores for “A”, “B”, and “C” but also for “A plus B”, “A plus C”, and “B plus C”. Note that we ignore the combination “A plus B plus C” as this combination would include all other potential combinations.</p>
</div>
<div id="problems-of-conditional-inference-trees" class="section level2">
<h2><span class="header-section-number">3.2</span> Problems of Conditional Inference Trees</h2>
<p>Conditional Inference Trees are very intuitive, multivariate, non-parametric, they do not require large data sets, and they are easy to implement. Despite these obvious advantages, they have at least two major short comings:</p>
<ul>
<li><p>they are prone to overfitting;</p></li>
<li><p>they do not perform well why they are applied to new data.</p></li>
</ul>
<p>An extension which remedies these problems is to grow many varied trees - this is called a Random Forest Analysis and will have a look at how Random Forests work and how to implement them in R in the next section.</p>
</div>
</div>
<div id="random-forests" class="section level1">
<h1><span class="header-section-number">4</span> Random Forests</h1>
<p>Random Forests (RFs) are an extension of Conditional Inference Trees <span class="citation">(Breiman <a href="#ref-breiman2001random" role="doc-biblioref">2001</a>)</span>. Like Conditional Inference Trees, Random Forests represent a multivariate, non-parametric partitioning method that is particularly useful when dealing with relatively small sample sizes and many predictors (including interactions) and they are insensitive to multicollinearity (if two predictors strongly correlate with the dependent variable AND are highly correlated or collinear, RFs will report both variables as highly important - the ordering in which they were included into the model is irrelevant). The latter point is a real advantage over regression models in particular. Also, RFs outperform CITs in that they are substantially less prone to overfitting and they perform much better when applied to new data. However, random forests have several issues:</p>
<ul>
<li>RFs only show variable importance but not if the variable is positively or negatively correlated with the dependent variable;</li>
<li>RFs do not report if a variable is important as a main effect or as part of an interactions</li>
<li>RFs do not indicate in which significant interactions a variable is involved.</li>
</ul>
<p>Therefore, Random Forest analyses are ideal for classification, imputing missing values, and - though to a lesser degree - as a variable selection procedure but they do not lend themselves for drawing inferences about the relationships of predictors and dependent variables.</p>
<p><strong>Bootstrapped Data</strong></p>
<p>Random Forests do not work on one-and-the-same data set (as CITs do) but in Random Forest analyses, many samples (with replacement) are drawn from the original data set. This generation of new data set based on an existing data set is called “bootstrapping”. Bootstrapping allows us to produce many trees based on variations of the original data set rather than dealing with only a single, fixed data set that would produce only a single tree. Therefore, because the data is different each time, the individual CITs are also different.</p>
<p>Imagine, we are dealing with a very small data set to which we want to apply a Random Forest Analysis. The original data set is displayed below.</p>
<table>
<caption>Example data set to illustrate Random Forests.</caption>
<thead>
<tr class="header">
<th align="right">ID</th>
<th align="left">Age</th>
<th align="left">Gender</th>
<th align="left">Status</th>
<th align="left">LikeUser</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">1</td>
<td align="left">15-40</td>
<td align="left">female</td>
<td align="left">high</td>
<td align="left">no</td>
</tr>
<tr class="even">
<td align="right">2</td>
<td align="left">15-40</td>
<td align="left">female</td>
<td align="left">high</td>
<td align="left">no</td>
</tr>
<tr class="odd">
<td align="right">3</td>
<td align="left">15-40</td>
<td align="left">male</td>
<td align="left">high</td>
<td align="left">no</td>
</tr>
<tr class="even">
<td align="right">4</td>
<td align="left">41-80</td>
<td align="left">female</td>
<td align="left">low</td>
<td align="left">yes</td>
</tr>
<tr class="odd">
<td align="right">5</td>
<td align="left">41-80</td>
<td align="left">male</td>
<td align="left">high</td>
<td align="left">no</td>
</tr>
<tr class="even">
<td align="right">6</td>
<td align="left">41-80</td>
<td align="left">male</td>
<td align="left">low</td>
<td align="left">no</td>
</tr>
</tbody>
</table>
<p>We now draw a sample from this data set and receive the following data set.</p>
<table>
<caption>Bootstrapped data set to illustrate Random Forests.</caption>
<thead>
<tr class="header">
<th align="right">ID</th>
<th align="left">Age</th>
<th align="left">Gender</th>
<th align="left">Status</th>
<th align="left">LikeUser</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">6</td>
<td align="left">41-80</td>
<td align="left">male</td>
<td align="left">low</td>
<td align="left">no</td>
</tr>
<tr class="even">
<td align="right">3</td>
<td align="left">15-40</td>
<td align="left">male</td>
<td align="left">high</td>
<td align="left">no</td>
</tr>
<tr class="odd">
<td align="right">4</td>
<td align="left">41-80</td>
<td align="left">female</td>
<td align="left">low</td>
<td align="left">yes</td>
</tr>
<tr class="even">
<td align="right">1</td>
<td align="left">15-40</td>
<td align="left">female</td>
<td align="left">high</td>
<td align="left">no</td>
</tr>
<tr class="odd">
<td align="right">2</td>
<td align="left">15-40</td>
<td align="left">female</td>
<td align="left">high</td>
<td align="left">no</td>
</tr>
<tr class="even">
<td align="right">2</td>
<td align="left">15-40</td>
<td align="left">female</td>
<td align="left">high</td>
<td align="left">no</td>
</tr>
</tbody>
</table>
<p>As you can see, the bootstrapped data contains the second row twice while the fifth row is missing.</p>
<p><strong>Out-Of-Bag data</strong></p>
<p>Because the data is reshuffled for every new tree, a part of the data (on average about 30%) remains unused for a given tree. The data that is not used is called Out-Of-Bag data or OOB. The OOB is important because the quality of the overall performance of the random forest can be assessed by applying the resulting tree-model to the data that it was not fit to. The quality of that tree is then measured in the OOB error, which is the error rate of the respective tree if applied to the OOB data.</p>
<p><strong>Random Variable Selection</strong></p>
<p>Random Forests also differ from simple CITs in that at each step, not all possible variables are considered for a node, but only a subset. For example, we have a data set with five predicting independent variables and one dependent variable. When generating a CIT, all possible variables (variables that do not represent a node further up in the tree) are considered as splitting candidates. In Random Forests, only a fixed number (typically the square-root of the number of independent variables) are considered as candidates for a node. So, at each potential split, a fixed number of randomly selected variables is considered potential node candidates.</p>
<div id="random-forests-in-r" class="section level2">
<h2><span class="header-section-number">4.1</span> Random Forests in R</h2>
<p>This section shows how a Random Forest Analysis can be implemented in R. Ina first step, we load and inspect the data.</p>
<pre class="r"><code># load random forest data
rfdata &lt;- read.delim(&quot;https://raw.githubusercontent.com/MartinSchweinberger/coedlss2019materials/master/datatables/rfdata.txt&quot;, header = T, sep = &quot;\t&quot;)
# inspect data
head(rfdata); str(rfdata)</code></pre>
<pre><code>##   Gender Age ConversationType Priming SUFLike
## 1    Men Old      MixedGender NoPrime      no
## 2    Men Old      MixedGender NoPrime      no
## 3    Men Old      MixedGender NoPrime     yes
## 4    Men Old      MixedGender NoPrime      no
## 5    Men Old      MixedGender NoPrime     yes
## 6    Men Old      MixedGender NoPrime      no</code></pre>
<pre><code>## &#39;data.frame&#39;:    10170 obs. of  5 variables:
##  $ Gender          : chr  &quot;Men&quot; &quot;Men&quot; &quot;Men&quot; &quot;Men&quot; ...
##  $ Age             : chr  &quot;Old&quot; &quot;Old&quot; &quot;Old&quot; &quot;Old&quot; ...
##  $ ConversationType: chr  &quot;MixedGender&quot; &quot;MixedGender&quot; &quot;MixedGender&quot; &quot;MixedGender&quot; ...
##  $ Priming         : chr  &quot;NoPrime&quot; &quot;NoPrime&quot; &quot;NoPrime&quot; &quot;NoPrime&quot; ...
##  $ SUFLike         : chr  &quot;no&quot; &quot;no&quot; &quot;yes&quot; &quot;no&quot; ...</code></pre>
<p>The data consists of four categorical variables (Gender, Age, ConversationType, and SUFLike). Our dependent variable is SUFLike which stands for speech-unit final like (a pragmatic marker that is common in Irish English and is used as in “A wee girl of her age, like”). While Age and Gender are pretty straight forward what they are called, ConversationType encodes whether a conversation has taken place between interlocutors of the same or of different genders.</p>
<p>Before going any further, we need to factorize the variables as tree-based models require factors instead of character variables (but they can, of course, handle numeric and ordinal variables). In addition, we will check if the data contains missing values (NAs).</p>
<pre class="r"><code># factorize variables (rf require factors instead of character vectors)
fcts &lt;- c(&quot;Gender&quot;, &quot;Age&quot;, &quot;ConversationType&quot;, &quot;Priming&quot;, &quot;SUFLike&quot;)
rfdata[fcts] &lt;- lapply(rfdata[fcts], factor)
# inspect data
str(rfdata)</code></pre>
<pre><code>## &#39;data.frame&#39;:    10170 obs. of  5 variables:
##  $ Gender          : Factor w/ 2 levels &quot;Men&quot;,&quot;Women&quot;: 1 1 1 1 1 1 1 1 1 1 ...
##  $ Age             : Factor w/ 2 levels &quot;Old&quot;,&quot;Young&quot;: 1 1 1 1 1 1 1 1 1 1 ...
##  $ ConversationType: Factor w/ 2 levels &quot;MixedGender&quot;,..: 1 1 1 1 1 1 1 1 1 1 ...
##  $ Priming         : Factor w/ 2 levels &quot;NoPrime&quot;,&quot;Prime&quot;: 1 1 1 1 1 1 1 1 1 1 ...
##  $ SUFLike         : Factor w/ 2 levels &quot;no&quot;,&quot;yes&quot;: 1 1 2 1 2 1 1 1 1 1 ...</code></pre>
<pre class="r"><code># check for NAs
natest &lt;- rfdata %&gt;%
  na.omit()
nrow(natest) # no NAs present in data (same number of rows with NAs omitted)</code></pre>
<pre><code>## [1] 10170</code></pre>
<p>In our case, the data does not contain missing values. Random Forests offer a very nice way to deal with missing data though. If NAs are present, they can either be deleted OR their values for any missing values can be imputed using proximities. In this way, such data points do not have to be removed which can be problematic especially when dealing with relatively small data sets. For imputing values, you could run the code below but as our data does not have NAs, we will skip this step and just show it here so you can have a look at how it is done.</p>
<pre class="r"><code># replacing NAs with estimates
data.imputed &lt;- rfImpute(SUFLike ~ ., data = rfdata, iter=6)</code></pre>
<p>The argument “iter” refers to the number of iterations to run. According to <span class="citation">(Breiman <a href="#ref-breiman2001random" role="doc-biblioref">2001</a>)</span>, 4 to 6 iterations is usually good enough. With this dataset (if it had NAs) and when we were to execute the code, the resulting OOB-error rates lie somewhere around 17 and 18 percent. When we were to set iter to 20, we get values a little better and a little worse, so doing more iterations doesn’t improve the situation.</p>
<p>Also, if you want to customize the “rfImpute” function, you can change the number of trees it uses (the default is 300) and the number of variables that it will consider at each step.</p>
<p>We will now create a first random forest object and inspect its model fit. As random forests rely on resampling, we set a seed so that we arrive at the same estimations.</p>
<pre class="r"><code># set.seed
set.seed(2019120204)
# create initial model
rfmodel1 &lt;- cforest(SUFLike ~ .,  data = rfdata, 
                    controls = cforest_unbiased(ntree = 50, mtry = 3))
# evaluate random forest (model diagnostics)
rfmodel1_pred &lt;- unlist(treeresponse(rfmodel1))[c(FALSE,TRUE)]
somers2(rfmodel1_pred, as.numeric(rfdata$SUFLike) - 1)</code></pre>
<pre><code>##             C           Dxy             n       Missing 
##     0.8299474     0.6598948 10170.0000000     0.0000000</code></pre>
<p>The model parameters are excellent: remember that if the C-value is 0.5, the predictions are random, while the predictions are perfect if the C-value is 1. C-values above 0.8 indicate real predictive capacity <span class="citation">(Baayen <a href="#ref-baayen2008analyzing" role="doc-biblioref">2008</a>, 204)</span>. Somers’ D<sub>xy</sub> is a value that represents a rank correlation between predicted probabilities and observed responses. Somers’ D<sub>xy</sub> values range between 0, which indicates complete randomness, and 1, which indicates perfect prediction <span class="citation">(Baayen <a href="#ref-baayen2008analyzing" role="doc-biblioref">2008</a>, 204)</span>.</p>
<p>In a next step, we extract the variable importance (conditional=T adjusts for correlations between predictors).</p>
<pre class="r"><code># extract variable importance based on mean decrease in accuracy
rfmodel1_varimp &lt;- varimp(rfmodel1, conditional = T) 
# show variable importance
rfmodel1_varimp</code></pre>
<pre><code>##           Gender              Age ConversationType          Priming 
##     0.0060288616     0.0603527525     0.0091394976     0.0002244789</code></pre>
<p>We can also calculate more robust variable importance using the “varimpAUC” function which calculates importance statistics that are corrected towards class imbalance, i.e. differences in the number of instances per category. The variable importance is easily visualized using the “plot” function.</p>
<pre class="r"><code># extract more robust variable importance 
rfmodel1_robustvarimp &lt;- varimpAUC(rfmodel1)  
# plot result
dotchart(sort(rfmodel1_robustvarimp), pch = 20, main = &quot;Conditional importance of variables&quot;)</code></pre>
<p><img src="advancedstatztrees_files/figure-html/rf8-1.png" width="672" /></p>
<p>The plot shows that Age is the most important predictor and that Priming is not really important as a predictor for speech-unit final like. Gender and ConversationType are equally important but both much less so than Age.</p>
<p>We will now use an alternative way to calculate RFs which allows us to use different diagnostics and pruning techniques by using the “randomForest” rather than the “cforest” function.</p>
<p>A few words on the parameters of the “randomForest” function: if the thing we’re trying to predict is a numeric variable, the “randomForest” function will set “mtry” (the number of variables considered at each step)to the total number of variables divided by 3 (rounded down), or to 1 if the division results in a value less than 1. If the thing we’re trying to predict is a “factor” (i.e. either “yes/no” or “ranked”), then randomForest() will set mtry to the square root of the number of variables (rounded down to the next integer value).Again, we start by setting a seed to store random numbers and thus make results reproducible.</p>
<pre class="r"><code># set.seed
set.seed(2019120205)
rfmodel2 &lt;- randomForest(SUFLike ~ ., data=rfdata, proximity=TRUE)
# inspect model
rfmodel2 </code></pre>
<pre><code>## 
## Call:
##  randomForest(formula = SUFLike ~ ., data = rfdata, proximity = TRUE) 
##                Type of random forest: classification
##                      Number of trees: 500
## No. of variables tried at each split: 2
## 
##         OOB estimate of  error rate: 17.5%
## Confusion matrix:
##      no  yes class.error
## no  821 1398  0.63001352
## yes 382 7569  0.04804427</code></pre>
<p>The output tells us that the OOB error rate is 17.5 percent and produces a confusion matrix. We can now check the development of the OOB error rate to see if it would suffice to use fewer trees (which would be quicker and save computing time).</p>
<pre class="r"><code># lower number of trees makes the computation much quicker
oob.error.data &lt;- data.frame(
  Trees=rep(1:nrow(rfmodel2$err.rate), times=3),
  Type=rep(c(&quot;OOB&quot;, &quot;no&quot;, &quot;yes&quot;), each=nrow(rfmodel2$err.rate)),
  Error=c(rfmodel2$err.rate[,&quot;OOB&quot;],
          rfmodel2$err.rate[,&quot;no&quot;],
          rfmodel2$err.rate[,&quot;yes&quot;]))
# plot error rates
ggplot(data=oob.error.data, aes(x=Trees, y=Error)) +
  geom_line(aes(color=Type)) </code></pre>
<p><img src="advancedstatztrees_files/figure-html/rf10-1.png" width="672" /></p>
<p>The output shows that the error rate is extremely stable and that we only need very few trees as the prediction accuracy does not improve if we grow more than 30 trees.</p>
<p>In addition to the number of trees, we can check if the model improves if we change the number of variables that are considered at each step (mtry).</p>
<pre class="r"><code># check what mtry is optimal
tuneRF(rfdata[, !colnames(rfdata)== &quot;SUFLike&quot;],
       rfdata[, colnames(rfdata)== &quot;SUFLike&quot;],
       stepFactor = 2, # 2 has the lowest value to be optimal
       plot = T, ntreeTry = 50, trace = T, improve = .05)</code></pre>
<pre><code>## mtry = 2  OOB error = 17.6% 
## Searching left ...
## mtry = 1     OOB error = 18.15% 
## -0.03128492 0.05 
## Searching right ...
## mtry = 4     OOB error = 17.5% 
## 0.005586592 0.05</code></pre>
<p><img src="advancedstatztrees_files/figure-html/rf11-1.png" width="672" /></p>
<pre><code>##       mtry  OOBError
## 1.OOB    1 0.1815143
## 2.OOB    2 0.1760079
## 4.OOB    4 0.1750246</code></pre>
<p>The error rate is very similar across different values of mtry but the lowest values is reported if all 4 variables are considered at once. We can now create a pruned or optimized RF based on the previous recommendadtions (ntree = 30, mtry = 4). Again, we begin by setting a seed.</p>
<pre class="r"><code># set.seed (to store random numbers and thus make results reporducible)
set.seed(2019120206)
# create a new model with fewer trees and that takes 2 variables at a time
rfmodel3 &lt;- randomForest(SUFLike ~ ., data=rfdata, ntree=30, mtry = 4, proximity=TRUE)
# inspect model
rfmodel3</code></pre>
<pre><code>## 
## Call:
##  randomForest(formula = SUFLike ~ ., data = rfdata, ntree = 30,      mtry = 4, proximity = TRUE) 
##                Type of random forest: classification
##                      Number of trees: 30
## No. of variables tried at each split: 4
## 
##         OOB estimate of  error rate: 17.5%
## Confusion matrix:
##      no  yes class.error
## no  821 1398  0.63001352
## yes 382 7569  0.04804427</code></pre>
<p>Despite optimization, the results have not changed but it may be very useful for other data. To evaluate the tree, we create a confusion matrix.</p>
<pre class="r"><code># save what the model predicted in a new variable
rfdata$Prediction &lt;- predict(rfmodel3, rfdata) 
# create confusion matrix to check accuracy
confusionMatrix(rfdata$Prediction, rfdata$SUFLike)  </code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction   no  yes
##        no   821  382
##        yes 1398 7569
##                                                
##                Accuracy : 0.825                
##                  95% CI : (0.8174, 0.8323)     
##     No Information Rate : 0.7818               
##     P-Value [Acc &gt; NIR] : &lt; 0.00000000000000022
##                                                
##                   Kappa : 0.3856               
##                                                
##  Mcnemar&#39;s Test P-Value : &lt; 0.00000000000000022
##                                                
##             Sensitivity : 0.36999              
##             Specificity : 0.95196              
##          Pos Pred Value : 0.68246              
##          Neg Pred Value : 0.84410              
##              Prevalence : 0.21819              
##          Detection Rate : 0.08073              
##    Detection Prevalence : 0.11829              
##       Balanced Accuracy : 0.66097              
##                                                
##        &#39;Positive&#39; Class : no                   
## </code></pre>
<p>The RF performs significantly better than a no-information base-line model but the base-line model already predicts 78.18 percent of cases correctly (compared to the RF with a prediction accuracy of 82.5 percent).</p>
<p>Unfortunately, we cannot easily compute robust variable importance for RF models nor C or Somers’ D<sub>xy</sub> which is why it is advisable to create analogous models using both the “cforest” and the “randomForest” functions. In a last step, we can now visualize the results of the optimized RF.</p>
<pre class="r"><code># plot variable importance
varImpPlot(rfmodel3, main = &quot;&quot;, pch = 20) </code></pre>
<p><img src="advancedstatztrees_files/figure-html/rf15-1.png" width="672" /></p>
<p>A second type of visualization that can provide insights in the partialPlot function which shows the effects of individual predictors - but remember that this still does not provide information about the way that the predictor interacts with other predictors.</p>
<pre class="r"><code># extract importance of individual variables
partialPlot(rfmodel3, rfdata, Age)</code></pre>
<p><img src="advancedstatztrees_files/figure-html/rf16-1.png" width="672" /></p>
<p>Another common way to evaluate the performance of RFs and prune them is to split the data into a test and a training set. the model is then fit to the training set and, after that, applied to the test set. This allows us to evaluate how well the RF performs on data that it was not trained on. This approach is particularly common in machine learning contexts.</p>
</div>
</div>
<div id="boruta" class="section level1">
<h1><span class="header-section-number">5</span> Boruta</h1>
<p>Boruta <span class="citation">(Kursa, Rudnicki, and others <a href="#ref-kursa2010feature" role="doc-biblioref">2010</a>)</span> is a variable selection procedure and it represents an extension of random forest analyses <span class="citation">(Breiman <a href="#ref-breiman2001random" role="doc-biblioref">2001</a>)</span>. The name “Boruta” is derived from a demon in Slavic mythology who dwelled in pine forests. Boruta is an alternative to regression modelling that is better equipped to handle small data sets because it uses a distributional approach during which hundreds of (random) forests are grown from permutated data sets. Boruta Boruta outperforms random forest analyses because:</p>
<ul>
<li>Boruta does not provide merely a single value for each predictor but a distribution of values leading to higher reliability.</li>
<li>Boruta provides definitive cut-off points for variables that have no meaningful relationship with the dependent variable. This is a crucial difference between RF and Boruta that make Boruta particularly interesting from a variable selection point of view.</li>
</ul>
<p>The Boruta procedure consists out of five steps.</p>
<ul>
<li><p>In a first step, the Boruta algorithm copies the data set and adds randomness to the data by (re-)shuffling data points and thereby creating randomized variables. These randomized variables are referred to as shadow features.</p></li>
<li><p>Secondly, a random forest classifier is trained on the extended data set.</p></li>
<li><p>In a third step, a feature importance measure (Mean Decrease Accuracy represented by z-scores) is calculated to determine the relative importance of all predictors (both original or real variables and the randomized shadow features).</p></li>
<li><p>In the next step, it is checked at each iteration of the process whether a real predictor has a higher importance compared with the best shadow feature. The algorithm keeps track of the performance of the original variables by storing whether they outperformed the best shadow feature or not in a vector.</p></li>
<li><p>In the fifth step, predictors that did not outperform the best shadow feature are removed and the process continues without them. After a set number of iterations, or if all the variables have been either confirmed as outperforming the best shadow feature, the algorithm stops.</p></li>
</ul>
<p>Despite its obvious advantages of Boruta over random forest analyses and regression modelling, it can neither handle multicollinearity not hierarchical data structures where data points are nested or grouped by a given predictor (as is the case in the present analysis as data points are grouped by adjective type). As Boruta is a variable selection procedure, it is also limited in the sense that it provides information on which predictors to include and how good these predictors are (compared to the shadow variables) while it is neither able to take hierarchical data structure into account, nor does it provide information about how one level of a factor compares to other factors. In other words, Boruta shows that a predictor is relevant and how strong it is but it does not provide information on how the likelihood of an outcome being used differs between variable levels, for instance between men and women.</p>
<div id="boruta-in-r" class="section level2">
<h2><span class="header-section-number">5.1</span> Boruta in R</h2>
<p>We begin by loading and inspecting the data.</p>
<pre class="r"><code># load data
borutadata &lt;- read.delim(&quot;https://raw.githubusercontent.com/MartinSchweinberger/coedlss2019materials/master/datatables/borutadata.txt&quot;, header = T, sep = &quot;\t&quot;)
# inspect Boruta data
str(borutadata)</code></pre>
<pre><code>## &#39;data.frame&#39;:    301 obs. of  12 variables:
##  $ Age             : chr  &quot;26-40&quot; &quot;26-40&quot; &quot;26-40&quot; &quot;17-25&quot; ...
##  $ Adjective       : chr  &quot;good&quot; &quot;good&quot; &quot;good&quot; &quot;nice&quot; ...
##  $ Function        : chr  &quot;Attributive&quot; &quot;Attributive&quot; &quot;Predicative&quot; &quot;Attributive&quot; ...
##  $ Priming         : chr  &quot;NoPrime&quot; &quot;NoPrime&quot; &quot;NoPrime&quot; &quot;NoPrime&quot; ...
##  $ Gender          : chr  &quot;Male&quot; &quot;Male&quot; &quot;Male&quot; &quot;Male&quot; ...
##  $ Occupation      : chr  &quot;AcademicManagerialProfessionals&quot; &quot;AcademicManagerialProfessionals&quot; &quot;AcademicManagerialProfessionals&quot; &quot;AcademicManagerialProfessionals&quot; ...
##  $ ConversationType: chr  &quot;SameSex&quot; &quot;SameSex&quot; &quot;SameSex&quot; &quot;SameSex&quot; ...
##  $ AudienceSize    : chr  &quot;3+&quot; &quot;3+&quot; &quot;3+&quot; &quot;2&quot; ...
##  $ really          : int  0 0 0 0 0 0 1 1 1 1 ...
##  $ Frequency       : num  27.848 27.848 27.848 7.293 0.617 ...
##  $ Gradabilty      : chr  &quot;NotGradable&quot; &quot;NotGradable&quot; &quot;NotGradable&quot; &quot;NotGradable&quot; ...
##  $ SemanticCategory: chr  &quot;Value&quot; &quot;Value&quot; &quot;Value&quot; &quot;HumanPropensity&quot; ...</code></pre>
<p>As the data contains non-factorized character variables, we convert those into factors.</p>
<pre class="r"><code># factorize variables (boruta - like rf - require factors instead of character vectors)
fcts &lt;- c(&quot;Age&quot;, &quot;Adjective&quot;, &quot;Function&quot;, &quot;Priming&quot;, &quot;Gender&quot;, &quot;Occupation&quot;, 
          &quot;ConversationType&quot;, &quot;AudienceSize&quot;, &quot;really&quot;, &quot;Gradabilty&quot;, &quot;SemanticCategory&quot;)
borutadata[fcts] &lt;- lapply(borutadata[fcts], factor)
# inspect data
str(borutadata)</code></pre>
<pre><code>## &#39;data.frame&#39;:    301 obs. of  12 variables:
##  $ Age             : Factor w/ 3 levels &quot;17-25&quot;,&quot;26-40&quot;,..: 2 2 2 1 3 3 1 1 1 1 ...
##  $ Adjective       : Factor w/ 6 levels &quot;bad&quot;,&quot;funny&quot;,..: 3 3 3 5 6 3 6 6 6 5 ...
##  $ Function        : Factor w/ 2 levels &quot;Attributive&quot;,..: 1 1 2 1 2 1 1 1 2 1 ...
##  $ Priming         : Factor w/ 2 levels &quot;NoPrime&quot;,&quot;Prime&quot;: 1 1 1 1 1 1 1 2 2 1 ...
##  $ Gender          : Factor w/ 2 levels &quot;Female&quot;,&quot;Male&quot;: 2 2 2 2 2 1 2 2 2 1 ...
##  $ Occupation      : Factor w/ 2 levels &quot;AcademicManagerialProfessionals&quot;,..: 1 1 1 1 1 1 1 1 1 1 ...
##  $ ConversationType: Factor w/ 2 levels &quot;MixedSex&quot;,&quot;SameSex&quot;: 2 2 2 2 2 1 1 1 1 2 ...
##  $ AudienceSize    : Factor w/ 2 levels &quot;2&quot;,&quot;3+&quot;: 2 2 2 1 1 2 2 2 2 1 ...
##  $ really          : Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 1 1 1 1 1 1 2 2 2 2 ...
##  $ Frequency       : num  27.848 27.848 27.848 7.293 0.617 ...
##  $ Gradabilty      : Factor w/ 3 levels &quot;GradabilityUndetermined&quot;,..: 3 3 3 3 3 3 1 3 3 3 ...
##  $ SemanticCategory: Factor w/ 5 levels &quot;Dimension&quot;,&quot;HumanPropensity&quot;,..: 5 5 5 2 5 5 1 4 4 2 ...</code></pre>
<p>We can now create our initial Boruta model and set a seed for reproducibility.</p>
<pre class="r"><code># set.seed 
set.seed(2019120207)
# initial run
boruta1 &lt;- Boruta(really~.,data=borutadata)
print(boruta1)</code></pre>
<pre><code>## Boruta performed 99 iterations in 5.382347 secs.
##  3 attributes confirmed important: Adjective, AudienceSize, Frequency;
##  3 attributes confirmed unimportant: Occupation, Priming,
## SemanticCategory;
##  5 tentative attributes left: Age, ConversationType, Function, Gender,
## Gradabilty;</code></pre>
<pre class="r"><code># extract decision
getConfirmedFormula(boruta1)</code></pre>
<pre><code>## really ~ Adjective + AudienceSize + Frequency
## &lt;environment: 0x000000003c4b0e60&gt;</code></pre>
<p>In a next step, we inspect the history to check if any of teh variables shows drastic fluctuations in their importance assessment.</p>
<pre class="r"><code>plotImpHistory(boruta1)</code></pre>
<p><img src="advancedstatztrees_files/figure-html/bo5-1.png" width="672" /></p>
<p>The fluctuations are do not show clear upward or downward trends (which what we want). If perdictors do perform worse than the shadow variables, then these variables should be excludded and the Boruta analysis should be re-run on the dataset that does no longer contain the superfluous variables. Tentative variables can remain but they are unlikely to have any substantial effect. We thus continue by removing variables that were confirmed as being unimportant, then setting a new seed, re-running the Boruta on the reduced data set, and again inspecting the decsions.</p>
<pre class="r"><code># remove irrelevant variables
rejected &lt;- names(boruta1$finalDecision)[which(boruta1$finalDecision == &quot;Rejected&quot;)]
# update data for boruta
borutadata &lt;- borutadata %&gt;%
  dplyr::select(-rejected)
# set.seed (to store random numbers and thus make results reporducible)
set.seed(2019120208)
# 2nd run
boruta2 &lt;- Boruta(really~.,data=borutadata)
print(boruta2)</code></pre>
<pre><code>## Boruta performed 99 iterations in 5.247692 secs.
##  3 attributes confirmed important: Adjective, AudienceSize, Frequency;
##  1 attributes confirmed unimportant: Age;
##  4 tentative attributes left: ConversationType, Function, Gender,
## Gradabilty;</code></pre>
<pre class="r"><code># extract decision
getConfirmedFormula(boruta2)</code></pre>
<pre><code>## really ~ Adjective + AudienceSize + Frequency
## &lt;environment: 0x0000000032b59da8&gt;</code></pre>
<p>Onlyadjective frequency and adjective type are confirmed as being important while all other variables are considered tentative. However, no more variables need to be removed as all remaining variables are not considered unimportnat. In a last step, we visualize the results of the Boruta analysis.</p>
<pre class="r"><code>par(mar = c(8, 8, 4, 2) + 0.1)
plot(boruta2, cex.axis=.75, las=2, xlab=&quot;&quot;, ylab = &quot;&quot;, cex = .75, 
     col = c(rep(&quot;grey50&quot;, 7),rep(&quot;grey90&quot;, 3)))
abline(v = 3.5, lty = &quot;dashed&quot;)
mtext(&quot;Predictors&quot;, 1, line = 7, at = 7, cex = 1)
mtext(&quot;Control&quot;, 1, line = 7, at = 2, cex = 1)
mtext(&quot;Importance&quot;, 2, line = 2.5, at = 2.5, cex = 1, las = 0)</code></pre>
<p><img src="advancedstatztrees_files/figure-html/bo7-1.png" width="672" /></p>
<pre class="r"><code>par(mar = c(5, 4, 4, 2) + 0.1)</code></pre>
<p>Of the remaining variables, adjective frequency and adjective type have the strongest effect and are confirmed as being important while syntactic function fails to perform better than the best shadow variable. All other variables have only a marginal effect on the use of really as an adjective amplifier.</p>
</div>
</div>
<div id="references" class="section level1 unnumbered">
<h1>References</h1>
<div id="refs" class="references">
<div id="ref-baayen2008analyzing">
<p>Baayen, R Harald. 2008. <em>Analyzing Linguistic Data. A Practical Introduction to Statistics Using R</em>. Cambridge: Cambridge University press.</p>
</div>
<div id="ref-breiman2001random">
<p>Breiman, Leo. 2001. “Random Forests.” <em>Machine Learning</em> 45 (1): 5–32.</p>
</div>
<div id="ref-kursa2010feature">
<p>Kursa, Miron B, Witold R Rudnicki, and others. 2010. “Feature Selection with the Boruta Package.” <em>Journal of Statistical Software</em> 36 (11): 1–13.</p>
</div>
</div>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_').toLowerCase();
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
