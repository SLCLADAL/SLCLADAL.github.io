<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />
<link rel="icon" 
      type="image/x-icon" 
      href="favicon.ico" />


<meta name="author" content="Martin Schweinberger" />

<meta name="date" content="2021-04-18" />

<title>Mixed-Effect Modeling in R</title>

<script src="site_libs/jquery-1.12.4/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cosmo.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<link href="site_libs/anchor-sections-1.0/anchor-sections.css" rel="stylesheet" />
<script src="site_libs/anchor-sections-1.0/anchor-sections.js"></script>
<script src="site_libs/kePrint-0.0.1/kePrint.js"></script>
<link href="site_libs/lightable-0.0.1/lightable.css" rel="stylesheet" />
<script src="site_libs/htmlwidgets-1.5.3/htmlwidgets.js"></script>
<link href="site_libs/datatables-css-0.0.0/datatables-crosstalk.css" rel="stylesheet" />
<script src="site_libs/datatables-binding-0.16/datatables.js"></script>
<link href="site_libs/dt-core-1.10.20/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="site_libs/dt-core-1.10.20/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="site_libs/dt-core-1.10.20/js/jquery.dataTables.min.js"></script>
<link href="site_libs/crosstalk-1.1.0.1/css/crosstalk.css" rel="stylesheet" />
<script src="site_libs/crosstalk-1.1.0.1/js/crosstalk.min.js"></script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>





<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #ffffff;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>


<!-- added by SKC for LADAL Style -->
<link rel="stylesheet" href="styles.css">
</head>

<body>


<div class="container-fluid main-container">





<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">



<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  
  <!-- Added by SKC - LADAL image and thicker top with   -->
  <div class="container-fluid navbar-top" >
    <a href="index.html"> <!-- Make entire top row and text clickable home link  -->
        <div class="row">
            <div class="navbar-brand col-md-12">
              <img src="ladal_icon_cas_tran_white_trimed.png" class="navbar-icon" alt="LADAL"/>
              <span class="navbar-title-note navbar-collapse collapse" >Language Technology and Data Analysis Laboratory</span>
            </div>
        </div>
    </a>
  </div>
  
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <!-- SKC removed  navbar brand -->
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">HOME</a>
</li>
<li>
  <a href="people.html">OUR PEOPLE</a>
</li>
<li>
  <a href="news.html">NEWS</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    DATA SCIENCE BASICS
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li class="dropdown-header">Introduction to Data Science</li>
    <li>
      <a href="comp.html">Working with Computers: Tips and Tricks</a>
    </li>
    <li>
      <a href="repro.html">Data Management, Version Control, and Reproducibility</a>
    </li>
    <li class="divider"></li>
    <li class="dropdown-header">Quantitative Research</li>
    <li>
      <a href="introquant.html">Introduction to Quantitative Reasoning</a>
    </li>
    <li>
      <a href="basicquant.html">Basic Concepts in Quantitative Research</a>
    </li>
    <li class="divider"></li>
    <li class="dropdown-header">Introduction to R</li>
    <li>
      <a href="intror.html">Getting started</a>
    </li>
    <li>
      <a href="string.html">String Processing</a>
    </li>
    <li>
      <a href="regex.html">Regular Expressions</a>
    </li>
    <li>
      <a href="table.html">Handling tables in R</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    TUTORIALS
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li class="dropdown-header">Data Visualization</li>
    <li>
      <a href="introviz.html">Introduction to Data Viz</a>
    </li>
    <li>
      <a href="dviz.html">Data Visualization with R</a>
    </li>
    <li>
      <a href="maps.html">Displaying Geo-Spatial Data</a>
    </li>
    <li>
      <a href="motion.html">Interactive Charts</a>
    </li>
    <li class="divider"></li>
    <li class="dropdown-header">Statistics</li>
    <li>
      <a href="dstats.html">Descriptive Statistics</a>
    </li>
    <li>
      <a href="basicstatz.html">Basic Inferential Statistics</a>
    </li>
    <li>
      <a href="regression.html">Regression Analysis</a>
    </li>
    <li>
      <a href="tree.html">Tree-Based Models</a>
    </li>
    <li>
      <a href="clust.html">Cluster and Correspondence Analysis</a>
    </li>
    <li>
      <a href="lexsim.html">Introduction to Lexical Similarity</a>
    </li>
    <li>
      <a href="svm.html">Semantic Vector Space Models</a>
    </li>
    <li>
      <a href="pwr.html">Power Analysis</a>
    </li>
    <li class="divider"></li>
    <li class="dropdown-header">Text Analytics</li>
    <li>
      <a href="textanalysis.html">Text Analysis and Distant Reading</a>
    </li>
    <li>
      <a href="kwics.html">Concordancing (keywords-in-context)</a>
    </li>
    <li>
      <a href="net.html">Network Analysis</a>
    </li>
    <li>
      <a href="coll.html">Co-occurrence and Collocation Analysis</a>
    </li>
    <li>
      <a href="topicmodels.html">Topic Modeling</a>
    </li>
    <li>
      <a href="sentiment.html">Sentiment Analysis</a>
    </li>
    <li>
      <a href="tagging.html">Tagging and Parsing</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    FOCUS STUDIES
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="lex.html">Lexicography with R: Generating Dictionaries</a>
    </li>
    <li>
      <a href="surveys.html">Questionnaires and Surveys: Analyses with R</a>
    </li>
    <li>
      <a href="corplingr.html">Corpus Linguistics with R: Swearing in Irish English</a>
    </li>
    <li>
      <a href="vc.html">Phonetics: Creating Vowel Charts with Praat and R</a>
    </li>
    <li>
      <a href="litsty.html">Computational Literary Stylistics with R</a>
    </li>
    <li class="divider"></li>
    <li class="dropdown-header">Useful How-To Tutorials</li>
    <li>
      <a href="convertpdf2txt.html">Converting PDFs to txt</a>
    </li>
    <li>
      <a href="webcrawling.html">Web Crawling using R</a>
    </li>
    <li>
      <a href="gutenberg.html">Downloading Texts from Project Gutenberg</a>
    </li>
  </ul>
</li>
<li>
  <a href="services.html">SERVICES | CONTACT</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Mixed-Effect Modeling in R</h1>
<h4 class="author">Martin Schweinberger</h4>
<h4 class="date">2021-04-18</h4>

</div>


<p><img src="https://slcladal.github.io/images/acqva.jpg" width="100%" /></p>
<div id="introduction" class="section level1 unnumbered">
<h1>Introduction</h1>
<p>This workshop introduces mixed-effects regression modeling using R. The R-markdown document for the tutorial can be downloaded <a href="https://slcladal.github.io/mmws.Rmd">here</a>. You will find more elaborate explanations and additional examples <a href="https://slcladal.github.io/regression.html">here</a>.</p>
<p>The workshop consists of 2 parts:</p>
<ol style="list-style-type: decimal">
<li><p>Theoretical background and basics This part deals with main concepts and the underlying logic of linear and logistic (mixed-effects) regression models</p></li>
<li><p>Practical examples and potential issues This part focuses on the practical implementation of linear and logistic mixed-models.</p></li>
</ol>
<div id="preparation-and-session-set-up" class="section level3 unnumbered">
<h3>Preparation and session set up</h3>
<pre class="r"><code># set options
options(stringsAsFactors = F)         # no automatic data transformation
options(&quot;scipen&quot; = 100, &quot;digits&quot; = 10) # suppress math annotation
# install packages
install.packages(c(&quot;boot&quot;, &quot;car&quot;, &quot;caret&quot;, &quot;tidyverse&quot;,  &quot;effects&quot;, &quot;foreign&quot;, 
                   &quot;Hmisc&quot;, &quot;DT&quot;, &quot;knitr&quot;, &quot;lme4&quot;, &quot;MASS&quot;, &quot;mlogit&quot;, &quot;msm&quot;, 
                   &quot;QuantPsyc&quot;, &quot;reshape2&quot;, &quot;rms&quot;, &quot;sandwich&quot;, &quot;sfsmisc&quot;, &quot;sjPlot&quot;, 
                   &quot;vcd&quot;, &quot;visreg&quot;, &quot;MuMIn&quot;))</code></pre>
<p>Once you have installed R and RStudio and initiated the session by executing the code shown above, you are good to go.</p>
</div>
</div>
<div id="theoretical-background" class="section level1">
<h1><span class="header-section-number">1</span> Theoretical Background</h1>
<p>Regression models are among the most widely used quantitative methods in the language sciences. Regressions are used because they are very flexible and can handle multiple predictors and responses. In general, regression models provide information about if and how predictors (variables or interactions between variables) correlate with a certain response.</p>
<p>The most widely use regression models are</p>
<ul>
<li><p>linear regression (dependent variable is numeric, no outliers)</p></li>
<li><p>logistic regression (dependent variable is binary)</p></li>
<li><p>ordinal regression (dependent variable represents an ordered factor, e.g. Likert items)</p></li>
<li><p>multinomial regression (dependent variable is categorical)</p></li>
</ul>
<p>If regression models contain a random effect structure which is used to model nestedness or dependence among data points, the regression models are called <em>mixed-effect models</em>. Regressions that do not have a random effect component to model nestedness or dependence are referred to as fixed-effect regressions (we will have a closer look at the difference between fixed and random effects below).</p>
<p>There exists a wealth of literature focusing on regression analysis and the concepts it is based on. For instance, there are <span class="citation">Achen (<a href="#ref-achen1982interpreting" role="doc-biblioref">1982</a>)</span>, <span class="citation">Bortz (<a href="#ref-bortz2006statistik" role="doc-biblioref">2006</a>)</span>, <span class="citation">Crawley (<a href="#ref-crawley2005statistics" role="doc-biblioref">2005</a>)</span>, <span class="citation">Faraway (<a href="#ref-faraway2002practical" role="doc-biblioref">2002</a>)</span>, <span class="citation">Field, Miles, and Field (<a href="#ref-field2012discovering" role="doc-biblioref">2012</a>)</span> (my personal favorite), <span class="citation">Gries (<a href="#ref-gries2021statistics" role="doc-biblioref">2021</a>)</span>, <span class="citation">Levshina (<a href="#ref-levshina2015linguistics" role="doc-biblioref">2015</a>)</span>, and <span class="citation">Wilcox (<a href="#ref-wilcox2009basic" role="doc-biblioref">2009</a>)</span> to name just a few. Introductions to regression modeling in R are <span class="citation">Baayen (<a href="#ref-baayen2008analyzing" role="doc-biblioref">2008</a>)</span>, <span class="citation">Crawley (<a href="#ref-crawley2012r" role="doc-biblioref">2012</a>)</span>, <span class="citation">Gries (<a href="#ref-gries2021statistics" role="doc-biblioref">2021</a>)</span>, or <span class="citation">Levshina (<a href="#ref-levshina2015linguistics" role="doc-biblioref">2015</a>)</span>.</p>
<p>The idea behind regression analysis is expressed formally in the equation below where<span class="math inline">\(f_{(x)}\)</span> is the <span class="math inline">\(y\)</span>-value we want to predict, <span class="math inline">\(\alpha\)</span> is the intercept (the point where the regression line crosses the <span class="math inline">\(y\)</span>-axis), <span class="math inline">\(\beta\)</span> is the coefficient (the slope of the regression line).</p>
<p><span class="math inline">\(f_{(x)} = \alpha + \beta_{i}x + \epsilon\)</span></p>
<p>In other words, to estimate how much some weights who is 180cm tall, we would multiply the coefficent (slope of the line) with 180 (<span class="math inline">\(x\)</span>) and add the value of the intercept (point where line crosses the <span class="math inline">\(y\)</span>-axis).</p>
<p>Residuals are the distance between the line and the points (the red lines) and it is also called <em>variance</em>. Regression lines are those lines where the sum of the red lines should be minimal. The slope of the regression line is called <em>coefficient</em> and the point where the regression line crosses the y-axis is called the <em>intercept</em>.</p>
<p>The basic principle</p>
<p><img src="mmws_files/figure-html/intro01-1.png" width="672" /></p>
<p>Extending the basic principle to logistic regression</p>
<p><img src="mmws_files/figure-html/intro01b-1.png" width="672" /></p>
<p>Extending the basic principle to mixed effects models..</p>
<p><img src="mmws_files/figure-html/intro02-1.png" width="672" /></p>
<div id="preposition-use-across-time" class="section level2 unnumbered">
<h2>Preposition use across time</h2>
<pre class="r"><code># load packages
library(car)
library(tidyverse)
# load functions
source(&quot;https://slcladal.github.io/rscripts/slrsummary.r&quot;)</code></pre>
<p>After preparing our session, we can now load and inspect the data to get a first impression of its properties.</p>
<pre class="r"><code># load data
slrdata &lt;- read.delim(&quot;https://slcladal.github.io/data/lmmdata.txt&quot;, header = TRUE)</code></pre>
<table class="table table-striped table-hover table-condensed table-responsive" style="width: auto !important; margin-left: auto; margin-right: auto;">
<caption>
First 20 rows of slrdata.
</caption>
<thead>
<tr>
<th style="text-align:right;">
Date
</th>
<th style="text-align:left;">
Genre
</th>
<th style="text-align:left;">
Text
</th>
<th style="text-align:right;">
Prepositions
</th>
<th style="text-align:left;">
Region
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
1736
</td>
<td style="text-align:left;">
Science
</td>
<td style="text-align:left;">
albin
</td>
<td style="text-align:right;">
166.01
</td>
<td style="text-align:left;">
North
</td>
</tr>
<tr>
<td style="text-align:right;">
1711
</td>
<td style="text-align:left;">
Education
</td>
<td style="text-align:left;">
anon
</td>
<td style="text-align:right;">
139.86
</td>
<td style="text-align:left;">
North
</td>
</tr>
<tr>
<td style="text-align:right;">
1808
</td>
<td style="text-align:left;">
PrivateLetter
</td>
<td style="text-align:left;">
austen
</td>
<td style="text-align:right;">
130.78
</td>
<td style="text-align:left;">
North
</td>
</tr>
<tr>
<td style="text-align:right;">
1878
</td>
<td style="text-align:left;">
Education
</td>
<td style="text-align:left;">
bain
</td>
<td style="text-align:right;">
151.29
</td>
<td style="text-align:left;">
North
</td>
</tr>
<tr>
<td style="text-align:right;">
1743
</td>
<td style="text-align:left;">
Education
</td>
<td style="text-align:left;">
barclay
</td>
<td style="text-align:right;">
145.72
</td>
<td style="text-align:left;">
North
</td>
</tr>
<tr>
<td style="text-align:right;">
1908
</td>
<td style="text-align:left;">
Education
</td>
<td style="text-align:left;">
benson
</td>
<td style="text-align:right;">
120.77
</td>
<td style="text-align:left;">
North
</td>
</tr>
<tr>
<td style="text-align:right;">
1906
</td>
<td style="text-align:left;">
Diary
</td>
<td style="text-align:left;">
benson
</td>
<td style="text-align:right;">
119.17
</td>
<td style="text-align:left;">
North
</td>
</tr>
<tr>
<td style="text-align:right;">
1897
</td>
<td style="text-align:left;">
Philosophy
</td>
<td style="text-align:left;">
boethja
</td>
<td style="text-align:right;">
132.96
</td>
<td style="text-align:left;">
North
</td>
</tr>
<tr>
<td style="text-align:right;">
1785
</td>
<td style="text-align:left;">
Philosophy
</td>
<td style="text-align:left;">
boethri
</td>
<td style="text-align:right;">
130.49
</td>
<td style="text-align:left;">
North
</td>
</tr>
<tr>
<td style="text-align:right;">
1776
</td>
<td style="text-align:left;">
Diary
</td>
<td style="text-align:left;">
boswell
</td>
<td style="text-align:right;">
135.94
</td>
<td style="text-align:left;">
North
</td>
</tr>
<tr>
<td style="text-align:right;">
1905
</td>
<td style="text-align:left;">
Travel
</td>
<td style="text-align:left;">
bradley
</td>
<td style="text-align:right;">
154.20
</td>
<td style="text-align:left;">
North
</td>
</tr>
<tr>
<td style="text-align:right;">
1711
</td>
<td style="text-align:left;">
Education
</td>
<td style="text-align:left;">
brightland
</td>
<td style="text-align:right;">
149.14
</td>
<td style="text-align:left;">
North
</td>
</tr>
<tr>
<td style="text-align:right;">
1762
</td>
<td style="text-align:left;">
Sermon
</td>
<td style="text-align:left;">
burton
</td>
<td style="text-align:right;">
159.71
</td>
<td style="text-align:left;">
North
</td>
</tr>
<tr>
<td style="text-align:right;">
1726
</td>
<td style="text-align:left;">
Sermon
</td>
<td style="text-align:left;">
butler
</td>
<td style="text-align:right;">
157.49
</td>
<td style="text-align:left;">
North
</td>
</tr>
<tr>
<td style="text-align:right;">
1835
</td>
<td style="text-align:left;">
PrivateLetter
</td>
<td style="text-align:left;">
carlyle
</td>
<td style="text-align:right;">
124.16
</td>
<td style="text-align:left;">
North
</td>
</tr>
<tr>
<td style="text-align:right;">
1837
</td>
<td style="text-align:left;">
History
</td>
<td style="text-align:left;">
carlyle
</td>
<td style="text-align:right;">
134.48
</td>
<td style="text-align:left;">
North
</td>
</tr>
<tr>
<td style="text-align:right;">
1774
</td>
<td style="text-align:left;">
Education
</td>
<td style="text-align:left;">
chapman
</td>
<td style="text-align:right;">
153.54
</td>
<td style="text-align:left;">
North
</td>
</tr>
<tr>
<td style="text-align:right;">
1776
</td>
<td style="text-align:left;">
Travel
</td>
<td style="text-align:left;">
cook
</td>
<td style="text-align:right;">
140.22
</td>
<td style="text-align:left;">
North
</td>
</tr>
<tr>
<td style="text-align:right;">
1712
</td>
<td style="text-align:left;">
Travel
</td>
<td style="text-align:left;">
cooke
</td>
<td style="text-align:right;">
150.00
</td>
<td style="text-align:left;">
North
</td>
</tr>
<tr>
<td style="text-align:right;">
1719
</td>
<td style="text-align:left;">
Fiction
</td>
<td style="text-align:left;">
defoe
</td>
<td style="text-align:right;">
131.48
</td>
<td style="text-align:left;">
North
</td>
</tr>
</tbody>
</table>
<p>We will now plot the data to get a better understanding of what the data looks like.</p>
<pre class="r"><code>ggplot(slrdata, aes(Date, Prepositions)) +
  geom_point() +
  theme_bw() +
  labs(x = &quot;Year&quot;) +
  labs(y = &quot;Prepositions per 1,000 words&quot;) +
  geom_smooth(method = &quot;lm&quot;) # with linear model smoothing!</code></pre>
<p><img src="mmws_files/figure-html/intro07-1.png" width="672" /></p>
<p>We now start the statistical analysis by generating a first regression model.</p>
<pre class="r"><code># create initial model
m1.lm &lt;- lm(Prepositions ~ Date, data = slrdata)
# inspect results
summary(m1.lm)</code></pre>
<pre><code>## 
## Call:
## lm(formula = Prepositions ~ Date, data = slrdata)
## 
## Residuals:
##         Min          1Q      Median          3Q         Max 
## -69.1012471 -13.8549421   0.5779091  13.3208913  62.8580401 
## 
## Coefficients:
##                    Estimate      Std. Error t value             Pr(&gt;|t|)    
## (Intercept) 104.02258335775  11.84756681931 8.78008 &lt; 0.0000000000000002 ***
## Date          0.01732180307   0.00726746646 2.38347             0.017498 *  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 19.4339648 on 535 degrees of freedom
## Multiple R-squared:  0.010507008,    Adjusted R-squared:  0.00865748837 
## F-statistic: 5.68093894 on 1 and 535 DF,  p-value: 0.017498081</code></pre>
<p>The Estimate for the intercept is the value of y at x = 0 (or, if the y-axis is located at x = 0, the value of y where the regression line crosses the y-axis). The estimate for Date represents the slope of the regression line and tells us that with each year, the predicted frequency of prepositions increase by .01732 prepositions. The t-value is the Estimate divided by the standard error (Std. Error). Based on the t-value, the p-value can be calculated manually as shown below.</p>
<pre class="r"><code># use pt function (which uses t-values and the degrees of freedom)
2*pt(-2.383, nrow(slrdata)-1)</code></pre>
<pre><code>## [1] 0.0175196401501</code></pre>
<p>The R<sup>2</sup>-values tell us how much variance is explained by our model. The baseline value represents a model that uses merely the mean. 0.0105 means that our model explains only 1.05 percent of the variance (0.010 x 100) - which is a tiny amount. The problem of the multiple R<sup>2</sup> is that it will increase even if we add variables that explain almost no variance. Hence, multiple R<sup>2</sup> encourages the inclusion of <em>junk</em> variables.</p>
<p><span class="math display">\[\begin{equation}

R^2_{multiple} = 1 - \frac{\sum (y_i - \hat{y_i})^2}{\sum (y_i - \bar y)^2}

\end{equation}\]</span></p>
<p>The adjusted R<sup>2</sup>-value takes the number of predictors into account and, thus, the adjusted R<sup>2</sup> will always be lower than the multiple R<sup>2</sup>. This is so because the adjusted R<sup>2</sup> penalizes models for having predictors. The equation for the adjusted R<sup>2</sup> below shows that the amount of variance that is explained by all the variables in the model (the top part of the fraction) must outweigh the inclusion of the number of variables (k) (lower part of the fraction). Thus, the adjusted R<sup>2</sup> will decrease when variables are added that explain little or even no variance while it will increase if variables are added that explain a lot of variance.</p>
<p><span class="math display">\[\begin{equation}

R^2_{adjusted} = 1 - (\frac{(1 - R^2)(n - 1)}{n - k - 1})

\end{equation}\]</span></p>
<p>If there is a big difference between the two R<sup>2</sup>-values, then the model contains (many) predictors that do not explain much variance which is not good. The F-statistic and the associated p-value tell us that the model, despite explaining almost no variance, is still significantly better than an intercept-only base-line model (or using the overall mean to predict the frequency of prepositions per text).</p>
<p>We can test this and also see where the F-values comes from by comparing our current model to the null-model (the model with only the intercept as a predictor).</p>
<pre class="r"><code># create intercept-only base-line model
m0.lm &lt;- lm(Prepositions ~ 1, data = slrdata)
# compare the base-line and the more saturated model
anova(m1.lm, m0.lm, test = &quot;F&quot;)</code></pre>
<pre><code>## Analysis of Variance Table
## 
## Model 1: Prepositions ~ Date
## Model 2: Prepositions ~ 1
##   Res.Df         RSS Df   Sum of Sq       F   Pr(&gt;F)  
## 1    535 202058.2576                                  
## 2    536 204203.8289 -1 -2145.57126 5.68094 0.017498 *
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>The F- and p-values are exactly those reported by the summary which shows where the F-values comes from and what they mean; namely they denote the difference between the base-line and the more saturated model.</p>
<p>The degrees of freedom associated with the residual standard error are the number of cases in the model minus the number of predictors (including the intercept). The residual standard error is the square root of the sum of the squared residuals of the model divided by the degrees of freedom. Have a look at he following to clear this up:</p>
<pre class="r"><code># DF = N - number of predictors (including intercept)
DegreesOfFreedom &lt;- nrow(slrdata)-length(coef(m1.lm))
# sum of the squared residuals
SumSquaredResiduals &lt;- sum(resid(m1.lm)^2)
# F-value
Fvalue &lt;- coef(summary(m1.lm))[6]^2
# Residual Standard Error
sqrt(SumSquaredResiduals/DegreesOfFreedom); DegreesOfFreedom; Fvalue; SumSquaredResiduals</code></pre>
<pre><code>## [1] 19.4339647585</code></pre>
<pre><code>## [1] 535</code></pre>
<pre><code>## [1] 5.68093894074</code></pre>
<pre><code>## [1] 202058.257635</code></pre>
</div>
<div id="model-fitting-and-assumptions" class="section level2 unnumbered">
<h2>Model fitting and assumptions</h2>
<p>We will now check if mathematical assumptions have been violated (homogeneity of variance) or whether the data contains outliers. We check this using diagnostic plots.</p>
<pre class="r"><code># generate data
df2 &lt;- data.frame(id = 1:length(resid(m1.lm)),
                 residuals = resid(m1.lm),
                 standard = rstandard(m1.lm),
                 studend = rstudent(m1.lm))
# generate plots
p1 &lt;- ggplot(df2, aes(x = id, y = residuals)) + 
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
  geom_point() +
  labs(y = &quot;Residuals&quot;, x = &quot;Index&quot;) +
  theme_bw()
p2 &lt;- ggplot(df2, aes(x = id, y = standard)) + 
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
  geom_point() +
  labs(y = &quot;Standardized Residuals&quot;, x = &quot;Index&quot;) +
  theme_bw()
p3 &lt;- ggplot(df2, aes(x = id, y = studend)) + 
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
  geom_point() +
  labs(y = &quot;Studentized Residuals&quot;, x = &quot;Index&quot;) +
  theme_bw()
# display plots
ggpubr::ggarrange(p1, p2, p3, ncol = 3, nrow = 1)</code></pre>
<p><img src="mmws_files/figure-html/slr12-1.png" width="672" /></p>
<p>The left graph shows the residuals of the model (i.e., the differences between the observed and the values predicted by the regression model). The problem with this plot is that the residuals are not standardized and so they cannot be compared to the residuals of other models. To remedy this deficiency, residuals are normalized by dividing the residuals by their standard deviation. Then, the normalized residuals can be plotted against the observed values (centre panel). In this way, not only are standardized residuals obtained, but the values of the residuals are transformed into z-values, and one can use the z-distribution to find problematic data points. There are three rules of thumb regarding finding problematic data points through standardized residuals <span class="citation">(Field, Miles, and Field <a href="#ref-field2012discovering" role="doc-biblioref">2012</a>, 268–69)</span>:</p>
<ul>
<li><p>Points with values higher than 3.29 should be removed from the data.</p></li>
<li><p>If more than 1% of the data points have values higher than 2.58, then the error rate of our model is too high.</p></li>
<li><p>If more than 5% of the data points have values greater than 1.96, then the error rate of our model is too high.</p></li>
</ul>
<p>The right panel shows the <em>studentized residuals</em> (adjusted predicted values: each data point is divided by the standard error of the residuals). In this way, it is possible to use Student’s t-distribution to diagnose our model.</p>
<p>Adjusted predicted values are residuals of a special kind: the model is calculated without a data point and then used to predict this data point. The difference between the observed data point and its predicted value is then called the adjusted predicted value. In summary, studentized residuals are very useful because they allow us to identify influential data points.</p>
<p>The plots show that there are two potentially problematic data points (the top-most and bottom-most point). These two points are clearly different from the other data points and may therefore be outliers. We will test later if these points need to be removed.</p>
<p>A few words on leverage as this is an importnat concept when determining if data points represent outiers. Leverage values range between 0 (no influence) and 1 (strong influence: suboptimal!). To test whether a specific data point has a high leverage value, we calculate a cut-off point that indicates whether the leverage is too strong or still acceptable. The following two formulas are used for this (N = number of cases in model, k = number of predictors in model (including the intercept)):</p>
<p>Leverage<sub>cutoff</sub> = <span class="math inline">\(\frac{3(k + 1)}{n} | \frac{2(k + 1)}{n} = \frac{3(2 + 1)}{537} = \frac{9}{537}\)</span> = 0.01676</p>
<p>We will now generate more diagnostic plots to check for potential probelms.</p>
<pre class="r"><code># load package
library(ggfortify)
# generate plots
autoplot(m1.lm) + 
  theme_bw() +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) </code></pre>
<p><img src="mmws_files/figure-html/slr13-1.png" width="672" /></p>
<p>The graph in the upper left panel is useful for finding outliers or for determining the correlation between residuals and predicted values: when a trend becomes visible in the line or points (e.g., a rising trend or a zigzag line), then this would indicate that the model would be problematic.</p>
<p>The graphic in the upper right panel indicates whether the residuals are normally distributed (the points lie on the line) or whether the residuals do not follow a normal distribution (points are not on the line at the top and bottom).</p>
<p>The graphic in the lower left panel provides information about <em>homoscedasticity</em>. Homoscedasticity means that the variance of the residuals remains constant and does not correlate with any independent variable. In unproblematic cases, the graphic shows a flat line. If there is a trend in the line or if the points for a funnel-like shape, then we are dealing with heteroscedasticity (correlation between independent variables and the residuals), which is very problematic for regressions.</p>
<p>The graph in the lower right panel shows problematic influential data points that disproportionately affect the regression (leverage). If such influential data points are present, they should be either weighted (one could generate a robust rather than a simple linear regression) or they must be removed. The graph displays Cook’s distance and data points that have a Cook’s distance value greater than 1 are problematic <span class="citation">(Field, Miles, and Field <a href="#ref-field2012discovering" role="doc-biblioref">2012</a>, 269)</span>.</p>
<p>We will end the current analysis by summarizing the results of the regression analysis in a table.</p>
<pre class="r"><code># create summary table
slrsummary(m1.lm)  </code></pre>
<table class="table table-striped table-hover table-condensed table-responsive" style="margin-left: auto; margin-right: auto;">
<caption>
Results of a simple linear regression analysis.
</caption>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:left;">
Estimate
</th>
<th style="text-align:left;">
Pearson’s r
</th>
<th style="text-align:left;">
Std. Error
</th>
<th style="text-align:left;">
t value
</th>
<th style="text-align:left;">
Pr(&gt;|t|)
</th>
<th style="text-align:left;">
P-value sig.
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
(Intercept)
</td>
<td style="text-align:left;">
104.02
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
11.85
</td>
<td style="text-align:left;">
8.78
</td>
<td style="text-align:left;">
0
</td>
<td style="text-align:left;">
p &lt; .001***
</td>
</tr>
<tr>
<td style="text-align:left;">
Date
</td>
<td style="text-align:left;">
0.02
</td>
<td style="text-align:left;">
0.1
</td>
<td style="text-align:left;">
0.01
</td>
<td style="text-align:left;">
2.38
</td>
<td style="text-align:left;">
0.0175
</td>
<td style="text-align:left;">
p &lt; .05*
</td>
</tr>
<tr>
<td style="text-align:left;">
Model statistics
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
Value
</td>
</tr>
<tr>
<td style="text-align:left;">
Number of cases in model
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
537
</td>
</tr>
<tr>
<td style="text-align:left;">
Residual standard error on 535 DF
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
19.43
</td>
</tr>
<tr>
<td style="text-align:left;">
Multiple R-squared
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
0.0105
</td>
</tr>
<tr>
<td style="text-align:left;">
Adjusted R-squared
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
0.0087
</td>
</tr>
<tr>
<td style="text-align:left;">
F-statistic (1, 535)
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
5.68
</td>
</tr>
<tr>
<td style="text-align:left;">
Model p-value
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
0.0175
</td>
</tr>
</tbody>
</table>
<p>An alternative but less informative summary table of the results of a regression analysis can be generated using the <code>tab_model</code> function from the <code>sjPlot</code> package (as is shown below).</p>
<pre class="r"><code>#load package
library(sjPlot)
# generate summary table
sjPlot::tab_model(m1.lm) </code></pre>
<table style="border-collapse:collapse; border:none;">
<tr>
<th style="border-top: double; text-align:center; font-style:normal; font-weight:bold; padding:0.2cm;  text-align:left; ">
 
</th>
<th colspan="3" style="border-top: double; text-align:center; font-style:normal; font-weight:bold; padding:0.2cm; ">
Prepositions
</th>
</tr>
<tr>
<td style=" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  text-align:left; ">
Predictors
</td>
<td style=" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  ">
Estimates
</td>
<td style=" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  ">
CI
</td>
<td style=" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  ">
p
</td>
</tr>
<tr>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; ">
(Intercept)
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  ">
104.02
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  ">
80.75 – 127.30
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  ">
<strong>&lt;0.001
</td>
</tr>
<tr>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; ">
Date
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  ">
0.02
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  ">
0.00 – 0.03
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  ">
<strong>0.017</strong>
</td>
</tr>
<tr>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; padding-top:0.1cm; padding-bottom:0.1cm; border-top:1px solid;">
Observations
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left; border-top:1px solid;" colspan="3">
537
</td>
</tr>
<tr>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; padding-top:0.1cm; padding-bottom:0.1cm;">
R<sup>2</sup> / R<sup>2</sup> adjusted
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left;" colspan="3">
0.011 / 0.009
</td>
</tr>
</table>
<hr />
<p>The results of simple linear regressions can be written up is provided below.</p>
<p>A simple linear regression has been fitted to the data. A visual assessment of the model diagnostic graphics did not indicate any problematic or disproportionately influential data points (outliers) and performed significantly better compared to an intercept-only base line model but only explained .87 percent of the variance (adjusted R<sup>2</sup>: .0087, F-statistic (1, 535): 5,68, p-value: 0.0175*). The final minimal adequate linear regression model is based on 537 data points and confirms a significant and positive correlation between the year in which the text was written and the relative frequency of prepositions (coefficient estimate (logged odds): .02, SE: 0.01, t-value: 2.38, p-value: .0175*).</p>
</div>
</div>
<div id="practical-examples" class="section level1">
<h1><span class="header-section-number">2</span> Practical Examples</h1>
<p>In contrast to fixed-effects regression models, mixed-effects models assume a hierarchical data structure in which data points are grouped or nested in higher order categories (e.g. students within classes). Mixed-effects models are rapidly increasing in use in data analysis because they allow us to incorporate hierarchical or nested data structures. Mixed-effects models are, of course, an extension of fixed-effects regression models and also multivariate and come in different types.</p>
<p>In the following, we will go over the most relevant and frequently used types of mixed-effect regression models, mixed-effects linear regression models and mixed-effects binomial logistic regression models.</p>
<p>The major difference between these types of models is that they take different types of dependent variables. While linear models take numeric dependent variables, logistic models take nominal variables.</p>
<div id="linear-mixed-effects-regression" class="section level2">
<h2><span class="header-section-number">2.1</span> Linear Mixed-Effects Regression</h2>
<p>The following focuses on an extension of ordinary multiple linear regressions: mixed-effects regression linear regression. Mixed-effects models have the following advantages over simpler statistical tests:</p>
<ul>
<li><p>Mixed-effects models are multivariate, i.e. they test the effect of several predictors simultaneously while controlling for the effect of all other predictors.</p></li>
<li><p>Mixed models allow to statistically incorporate within-speaker variability and are thus fit to model hierarchical or nested data structures. This applies if several observations are produced by an individual speaker, for instance.</p></li>
<li><p>Mixed-models provide a wealth of diagnostic statistics which enables us to control e.g. (multi-)collinearity, i.e. correlations between predictors, and to test whether conditions or requirements are violated (e.g. homogeneity of variance, etc.).</p></li>
</ul>
<p>Major disadvantages of mixed-effects regression modeling are that they are prone to producing high <span class="math inline">\(\beta\)</span>-errors <span class="citation">(see Johnson <a href="#ref-johnson2009getting" role="doc-biblioref">2009</a>)</span> and that they require rather large data sets.</p>
<div id="introduction-1" class="section level3 unnumbered">
<h3>Introduction</h3>
<p>So far, the regression models that we have used only had fixed-effects. Having only fixed-effects means that all data points are treated as if they are completely independent and thus on the same hierarchical level. However, it is very common that the data is nested in the sense that data points are not independent because they are, for instance produced by the same speaker or are grouped by some other characteristic. In such cases, the data is considered hierarchical and statistical models should incorporate such structural features of the data they work upon. With respect to regression modeling, hierarchical structures are incorporated by what is called <em>random effects</em>. When models only have a fixed-effects structure, then they make use of only a single intercept and/or slope (as in the left panel in the figure below), while mixed effects models have intercepts for each level of a random effect. If the random effect structure represents speakers then this would mean that a mixed-model would have a separate intercept and/or slope for each speaker (in addition to the overall intercept that is shown as an orange line in the figure below).</p>
<p><img src="mmws_files/figure-html/lmer02-1.png" width="672" /></p>
<p><img src="mmws_files/figure-html/lmer03-1.png" width="672" /></p>
<p><img src="mmws_files/figure-html/lmer04-1.png" width="672" /></p>
</div>
<div id="random-effects" class="section level3 unnumbered">
<h3>Random Effects</h3>
<p><em>Random Effects</em> can be visualized using two parameters: the intercept (the point where the regression line crosses the y-axis at x equals 0) and the slope (the acclivity of the regression line). In contrast to fixed-effects models, that have only 1 intercept and one slope (left panel in the figure above), mixed-effects models can therefore have various <em>random intercepts</em> (center panel) or various <em>random slopes</em>, or both, various <em>random intercepts</em> and various <em>random slopes</em> (right panel).</p>
<p>What features do distinguish random and fixed effects?</p>
<ol style="list-style-type: decimal">
<li><p>Random effects represent a higher level variable under which data points are grouped. This implies that random effects must be categorical (or nominal but <em>they a´cannot be continuous</em>!) <span class="citation">(see Winter <a href="#ref-winter2019statistics" role="doc-biblioref">2019</a>, 236)</span>.</p></li>
<li><p>Random effects represent a sample of an infinite number of possible levels. For instance, speakers, trials, items, subjects, or words represent a potentially infinite pool of elements from which many different samples can be drawn. Thus, random effects represent a random sample sample. Fixed effects, on the other hand, typically do not represent a random sample but a fixed set of variable levels (e.g. Age groups, or parts-of-speech).</p></li>
<li><p>Random effects typically represent many different levels while fixed effects typically have only a few. <span class="citation">Zuur, Hilbe, and Ieno (<a href="#ref-zuur2013beginner" role="doc-biblioref">2013</a>)</span> propose that a variable may be used as a fixed effect if it has less than 5 levels while it should be treated as a random effect if it has more than 10 levels. Variables with 5 to 10 levels can be used as both. However, this is a rule of thumb and ignores the theoretical reasons (random sample and nestedness) for considering something as a random effect and it also is at odds with the way that repeated measures are models (namely as mixed effects) although they typically only have very few levels.</p></li>
<li><p>Fixed effects represent an effect that if we draw many samples, the effect would be consistent across samples <span class="citation">(Winter <a href="#ref-winter2019statistics" role="doc-biblioref">2019</a>)</span> while random effects should vary for each new sample that is drawn.</p></li>
</ol>
<p>Models with only random intercepts are more common because including both random intercepts and random slopes requires larger data sets (but have a better fit because intercepts are not forced to be parallel and the lines therefore have a better fit). Always think about what random effects structure is appropriate for your model <span class="citation">(see Winter <a href="#ref-winter2019statistics" role="doc-biblioref">2019</a>, 241–44)</span>. Mixed-effects are called mixed-effects because they contain both random and fixed effects.</p>
<p>Random effects are added first.</p>
</div>
<div id="example-preposition-use-across-time-by-genre" class="section level3 unnumbered">
<h3>Example: Preposition Use across Time by Genre</h3>
<p>To explore how to implement a mixed-effects model in R we revisit the preposition data that contains relative frequencies of prepositions in English texts written between 1150 and 1913. As a first step, and to prepare our analysis, we load necessary R packages, specify options, and load as well as provide an overview of the data.</p>
<pre class="r"><code># suppress scientific notation
options(&quot;scipen&quot; = 100, &quot;digits&quot; = 4)      
# do not convert strings into factors
options(stringsAsFactors = F)              
# read in data
lmmdata &lt;- read.delim(&quot;https://slcladal.github.io/data/lmmdata.txt&quot;, header = TRUE) %&gt;%
# convert date into a numeric variable
    dplyr::mutate(Date = as.numeric(Date))</code></pre>
<table class="table table-striped table-hover table-condensed table-responsive" style="width: auto !important; margin-left: auto; margin-right: auto;">
<caption>
First 20 rows of lmmdata.
</caption>
<thead>
<tr>
<th style="text-align:right;">
Date
</th>
<th style="text-align:left;">
Genre
</th>
<th style="text-align:left;">
Text
</th>
<th style="text-align:right;">
Prepositions
</th>
<th style="text-align:left;">
Region
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
1736
</td>
<td style="text-align:left;">
Science
</td>
<td style="text-align:left;">
albin
</td>
<td style="text-align:right;">
166.0
</td>
<td style="text-align:left;">
North
</td>
</tr>
<tr>
<td style="text-align:right;">
1711
</td>
<td style="text-align:left;">
Education
</td>
<td style="text-align:left;">
anon
</td>
<td style="text-align:right;">
139.9
</td>
<td style="text-align:left;">
North
</td>
</tr>
<tr>
<td style="text-align:right;">
1808
</td>
<td style="text-align:left;">
PrivateLetter
</td>
<td style="text-align:left;">
austen
</td>
<td style="text-align:right;">
130.8
</td>
<td style="text-align:left;">
North
</td>
</tr>
<tr>
<td style="text-align:right;">
1878
</td>
<td style="text-align:left;">
Education
</td>
<td style="text-align:left;">
bain
</td>
<td style="text-align:right;">
151.3
</td>
<td style="text-align:left;">
North
</td>
</tr>
<tr>
<td style="text-align:right;">
1743
</td>
<td style="text-align:left;">
Education
</td>
<td style="text-align:left;">
barclay
</td>
<td style="text-align:right;">
145.7
</td>
<td style="text-align:left;">
North
</td>
</tr>
<tr>
<td style="text-align:right;">
1908
</td>
<td style="text-align:left;">
Education
</td>
<td style="text-align:left;">
benson
</td>
<td style="text-align:right;">
120.8
</td>
<td style="text-align:left;">
North
</td>
</tr>
<tr>
<td style="text-align:right;">
1906
</td>
<td style="text-align:left;">
Diary
</td>
<td style="text-align:left;">
benson
</td>
<td style="text-align:right;">
119.2
</td>
<td style="text-align:left;">
North
</td>
</tr>
<tr>
<td style="text-align:right;">
1897
</td>
<td style="text-align:left;">
Philosophy
</td>
<td style="text-align:left;">
boethja
</td>
<td style="text-align:right;">
133.0
</td>
<td style="text-align:left;">
North
</td>
</tr>
<tr>
<td style="text-align:right;">
1785
</td>
<td style="text-align:left;">
Philosophy
</td>
<td style="text-align:left;">
boethri
</td>
<td style="text-align:right;">
130.5
</td>
<td style="text-align:left;">
North
</td>
</tr>
<tr>
<td style="text-align:right;">
1776
</td>
<td style="text-align:left;">
Diary
</td>
<td style="text-align:left;">
boswell
</td>
<td style="text-align:right;">
135.9
</td>
<td style="text-align:left;">
North
</td>
</tr>
<tr>
<td style="text-align:right;">
1905
</td>
<td style="text-align:left;">
Travel
</td>
<td style="text-align:left;">
bradley
</td>
<td style="text-align:right;">
154.2
</td>
<td style="text-align:left;">
North
</td>
</tr>
<tr>
<td style="text-align:right;">
1711
</td>
<td style="text-align:left;">
Education
</td>
<td style="text-align:left;">
brightland
</td>
<td style="text-align:right;">
149.1
</td>
<td style="text-align:left;">
North
</td>
</tr>
<tr>
<td style="text-align:right;">
1762
</td>
<td style="text-align:left;">
Sermon
</td>
<td style="text-align:left;">
burton
</td>
<td style="text-align:right;">
159.7
</td>
<td style="text-align:left;">
North
</td>
</tr>
<tr>
<td style="text-align:right;">
1726
</td>
<td style="text-align:left;">
Sermon
</td>
<td style="text-align:left;">
butler
</td>
<td style="text-align:right;">
157.5
</td>
<td style="text-align:left;">
North
</td>
</tr>
<tr>
<td style="text-align:right;">
1835
</td>
<td style="text-align:left;">
PrivateLetter
</td>
<td style="text-align:left;">
carlyle
</td>
<td style="text-align:right;">
124.2
</td>
<td style="text-align:left;">
North
</td>
</tr>
<tr>
<td style="text-align:right;">
1837
</td>
<td style="text-align:left;">
History
</td>
<td style="text-align:left;">
carlyle
</td>
<td style="text-align:right;">
134.5
</td>
<td style="text-align:left;">
North
</td>
</tr>
<tr>
<td style="text-align:right;">
1774
</td>
<td style="text-align:left;">
Education
</td>
<td style="text-align:left;">
chapman
</td>
<td style="text-align:right;">
153.5
</td>
<td style="text-align:left;">
North
</td>
</tr>
<tr>
<td style="text-align:right;">
1776
</td>
<td style="text-align:left;">
Travel
</td>
<td style="text-align:left;">
cook
</td>
<td style="text-align:right;">
140.2
</td>
<td style="text-align:left;">
North
</td>
</tr>
<tr>
<td style="text-align:right;">
1712
</td>
<td style="text-align:left;">
Travel
</td>
<td style="text-align:left;">
cooke
</td>
<td style="text-align:right;">
150.0
</td>
<td style="text-align:left;">
North
</td>
</tr>
<tr>
<td style="text-align:right;">
1719
</td>
<td style="text-align:left;">
Fiction
</td>
<td style="text-align:left;">
defoe
</td>
<td style="text-align:right;">
131.5
</td>
<td style="text-align:left;">
North
</td>
</tr>
</tbody>
</table>
<p>The data set contains the date when the text was written (Date), the genre of the text (Genre), the name of the text (Text), the relative frequency of prepositions in the text (Prepositions), and the region in which the text was written (Region). We now plot the data to get a first impression of its structure.</p>
<pre class="r"><code>p1 &lt;- ggplot(lmmdata, aes(x = Date, y = Prepositions)) +
  geom_point() +
  geom_smooth(method = &quot;lm&quot;, se = F, color = &quot;red&quot;, linetype = &quot;dashed&quot;) +
  theme_bw() +
  labs(y = &quot;Frequency\n(Prepositions)&quot;)
p2 &lt;- ggplot(lmmdata, aes(x = reorder(Genre, -Prepositions), y = Prepositions)) +
  geom_boxplot() +
  theme_bw() + 
  theme(axis.text.x = element_text(angle=90)) +
  labs(x = &quot;Genre&quot;, y = &quot;Frequency\n(Prepositions)&quot;)
p3 &lt;- ggplot(lmmdata, aes(Prepositions)) +
  geom_histogram() +
  theme_bw() + 
  labs(y = &quot;Count&quot;, x = &quot;Frequency (Prepositions)&quot;)
grid.arrange(grobs = list(p1, p2, p3), widths = c(1, 1), layout_matrix = rbind(c(1, 1), c(2, 3)))</code></pre>
<p><img src="mmws_files/figure-html/lmm4-1.png" width="672" /></p>
<p>The scatter plot in the upper panel indicates that the use of prepositions has moderately increased over time while the boxplots in the lower left panel show that the genres differ quite substantially with respect to their median frequencies of prepositions per text. Finally, the histogram in the lower right panel show that preposition use is distributed normally with a mean of 132.2 prepositions per text.</p>
<p>Centering or even scaling numeric variables is useful for later interpretation of regression models: if the date variable were not centered, the regression would show the effects of variables at year 0(!). If numeric variables are centered, other variables are variables are considered relative not to 0 but to the mean of that variable (in this case the mean of years in our data). Centering simply means that the mean of the numeric variable is subtracted from each value.</p>
<pre class="r"><code>lmmdata$DateUnscaled &lt;- lmmdata$Date
lmmdata$Date &lt;- scale(lmmdata$Date, scale = F)</code></pre>
<table class="table table-striped table-hover table-condensed table-responsive" style="width: auto !important; margin-left: auto; margin-right: auto;">
<caption>
First 5 rows of lmmdata.
</caption>
<thead>
<tr>
<th style="text-align:right;">
Date
</th>
<th style="text-align:left;">
Genre
</th>
<th style="text-align:left;">
Text
</th>
<th style="text-align:right;">
Prepositions
</th>
<th style="text-align:left;">
Region
</th>
<th style="text-align:right;">
DateUnscaled
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
109.87
</td>
<td style="text-align:left;">
Science
</td>
<td style="text-align:left;">
albin
</td>
<td style="text-align:right;">
166.0
</td>
<td style="text-align:left;">
North
</td>
<td style="text-align:right;">
1736
</td>
</tr>
<tr>
<td style="text-align:right;">
84.87
</td>
<td style="text-align:left;">
Education
</td>
<td style="text-align:left;">
anon
</td>
<td style="text-align:right;">
139.9
</td>
<td style="text-align:left;">
North
</td>
<td style="text-align:right;">
1711
</td>
</tr>
<tr>
<td style="text-align:right;">
181.87
</td>
<td style="text-align:left;">
PrivateLetter
</td>
<td style="text-align:left;">
austen
</td>
<td style="text-align:right;">
130.8
</td>
<td style="text-align:left;">
North
</td>
<td style="text-align:right;">
1808
</td>
</tr>
<tr>
<td style="text-align:right;">
251.87
</td>
<td style="text-align:left;">
Education
</td>
<td style="text-align:left;">
bain
</td>
<td style="text-align:right;">
151.3
</td>
<td style="text-align:left;">
North
</td>
<td style="text-align:right;">
1878
</td>
</tr>
<tr>
<td style="text-align:right;">
116.87
</td>
<td style="text-align:left;">
Education
</td>
<td style="text-align:left;">
barclay
</td>
<td style="text-align:right;">
145.7
</td>
<td style="text-align:left;">
North
</td>
<td style="text-align:right;">
1743
</td>
</tr>
</tbody>
</table>
<p>We now set up a fixed-effects model with the <code>glm</code> function from the <code>stats</code>and a mixed-effects model with <em>Genre</em> as a random effect using the <code>lmer</code> function from the <code>lme4</code> package.</p>
<pre class="r"><code># Load package
library(lme4)
# generate models
m0.glm &lt;- glm(Prepositions ~ 1, family = gaussian, data = lmmdata)
m0.lmer = lmer(Prepositions ~ 1 + (1|Genre), data = lmmdata)</code></pre>
<p>Now that we have created the base-line models, we will test whether including a random effect structure is mathematically justified. It is important to note here that we are not going to test if including a random effect structure is theoretically motivated but simply if it causes a decrease in variance.</p>
</div>
<div id="testing-random-effects" class="section level3 unnumbered">
<h3>Testing Random Effects</h3>
<p>As a first step in the modeling process, we now need to determine whether or not including a random effect structure is justified. We do so by comparing the AIC of the base-line model without random intercepts to the AIC of the model with random intercepts.</p>
<pre class="r"><code>AIC(logLik(m0.glm))</code></pre>
<pre><code>## [1] 4718</code></pre>
<pre class="r"><code>AIC(logLik(m0.lmer))</code></pre>
<pre><code>## [1] 4498</code></pre>
<p>The inclusion of a random effect structure with random intercepts is justified as the AIC of the model with random intercepts is substantially lower than the AIC of the model without random intercepts.</p>
<p>While I do not how how to <em>test</em> if including a random effect is justified, there are often situations, which require to test exactly which random effect structure is best. When doing this, it is important to use <em>restricted maximum likelihood</em> (<code>REML = TRUE</code> or <code>method = REML</code>) rather than maximum likelihood <span class="citation">(see Pinheiro and Bates <a href="#ref-pinheiro2000mixedmodels" role="doc-biblioref">2000</a>; Winter <a href="#ref-winter2019statistics" role="doc-biblioref">2019</a>, 226)</span>.</p>
<pre class="r"><code># generate models with 2 different random effect structures
ma.lmer = lmer(Prepositions ~ Date + (1|Genre), REML = T, data = lmmdata)
mb.lmer = lmer(Prepositions ~ Date + (1 + Date | Genre), REML = T, data = lmmdata)
# compare models
anova(ma.lmer, mb.lmer, test = &quot;Chisq&quot;, refit = F)</code></pre>
<pre><code>## Data: lmmdata
## Models:
## ma.lmer: Prepositions ~ Date + (1 | Genre)
## mb.lmer: Prepositions ~ Date + (1 + Date | Genre)
##         npar  AIC  BIC logLik deviance Chisq Df Pr(&gt;Chisq)    
## ma.lmer    4 4499 4516  -2246     4491                        
## mb.lmer    6 4487 4512  -2237     4475  16.4  2    0.00027 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>The model comparison shows that the model with the more complex random effect structure has a significantly better fit to the data compared with the model with the simpler random effect structure. However, we will continue with the model with the simpler structure because this is just an example.</p>
<blockquote>
<p>In a real analysis, we would switch to a model with random intercepts and random slopes for Genre because it has a significantly better fit to the data.</p>
</blockquote>
</div>
<div id="model-fitting" class="section level3 unnumbered">
<h3>Model Fitting</h3>
<p>After having determined that including a random effect structure is justified, we can continue by fitting the model and including diagnostics as we go. Including diagnostics in the model fitting process can save time and prevent relying on models which only turn out to be unstable if we would perform the diagnostics after the fact.</p>
<p>We begin fitting our model by adding Date as a fixed effect and compare this model to our mixed-effects base-line model to see if Date improved the model fit by explaining variance and if Date significantly correlates with our dependent variable (this means that the difference between the models is the effect (size) of “Date”!)</p>
<pre class="r"><code>m1.lmer &lt;- lmer(Prepositions ~ (1|Genre) + Date, data = lmmdata)
anova(m1.lmer, m0.lmer, test = &quot;Chi&quot;)</code></pre>
<pre><code>## refitting model(s) with ML (instead of REML)</code></pre>
<pre><code>## Data: lmmdata
## Models:
## m0.lmer: Prepositions ~ 1 + (1 | Genre)
## m1.lmer: Prepositions ~ (1 | Genre) + Date
##         npar  AIC  BIC logLik deviance Chisq Df Pr(&gt;Chisq)   
## m0.lmer    3 4502 4515  -2248     4496                       
## m1.lmer    4 4495 4512  -2244     4487  8.93  1     0.0028 **
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>The model with Date is the better model (significant p-value and lower AIC). The significant p-value shows that <em>Date</em> correlates significantly with <em>Prepositions</em> (<span class="math inline">\(\chi\)</span><sup>2</sup>(1) = 8.93, p = .0028). The <span class="math inline">\(\chi\)</span><sup>2</sup> value here is labeled <em>Chisq</em> and the degrees of freedom are calculated by subtracting the smaller number of DFs from the larger number of DFs.</p>
<p>We now test whether Region should also be part of the final minimal adequate model. The easiest way to add predictors is by using the <code>update</code> function (it saves time and typing).</p>
<pre class="r"><code># generate model
m2.lmer &lt;- update(m1.lmer, .~.+ Region)
# test vifs
car::vif(m2.lmer)</code></pre>
<pre><code>##   Date Region 
##  1.203  1.203</code></pre>
<pre class="r"><code># compare models                
anova(m2.lmer, m1.lmer, test = &quot;Chi&quot;)</code></pre>
<pre><code>## refitting model(s) with ML (instead of REML)</code></pre>
<pre><code>## Data: lmmdata
## Models:
## m1.lmer: Prepositions ~ (1 | Genre) + Date
## m2.lmer: Prepositions ~ (1 | Genre) + Date + Region
##         npar  AIC  BIC logLik deviance Chisq Df Pr(&gt;Chisq)
## m1.lmer    4 4495 4512  -2244     4487                    
## m2.lmer    5 4495 4516  -2242     4485  2.39  1       0.12</code></pre>
<p>Three things tell us that Region should not be included: (i) the AIC does not decrease, (ii) the BIC increases(!), and the p-value is higher than .05. This means, that we will continue fitting the model without having Region included. Well… not quite - just as a note on including variables: while Region is not significant as a main effect, it must still be included in a model if it were part of a significant interaction. To test if this is indeed the case, we fit another model with the interaction between Date and Region as predictor.</p>
<pre class="r"><code># generate model
m3.lmer &lt;- update(m1.lmer, .~.+Region*Date)
# extract vifs
car::vif(m3.lmer)</code></pre>
<pre><code>##        Date      Region Date:Region 
##       1.969       1.203       1.780</code></pre>
<pre class="r"><code># compare models                
anova(m3.lmer, m1.lmer, test = &quot;Chi&quot;)</code></pre>
<pre><code>## refitting model(s) with ML (instead of REML)</code></pre>
<pre><code>## Data: lmmdata
## Models:
## m1.lmer: Prepositions ~ (1 | Genre) + Date
## m3.lmer: Prepositions ~ (1 | Genre) + Date + Region + Date:Region
##         npar  AIC  BIC logLik deviance Chisq Df Pr(&gt;Chisq)
## m1.lmer    4 4495 4512  -2244     4487                    
## m3.lmer    6 4496 4522  -2242     4484  2.89  2       0.24</code></pre>
<p>Again, the high p-value and the increase in AIC and BIC show that we have found our minimal adequate model with only contains Date as a main effect. In a next step, we can inspect the final minimal adequate model, i.e. the most parsimonious (the model that explains a maximum of variance with a minimum of predictors).</p>
<pre class="r"><code># inspect results
summary(m1.lmer)</code></pre>
<pre><code>## Linear mixed model fit by REML [&#39;lmerMod&#39;]
## Formula: Prepositions ~ (1 | Genre) + Date
##    Data: lmmdata
## 
## REML criterion at convergence: 4491
## 
## Scaled residuals: 
##    Min     1Q Median     3Q    Max 
## -3.735 -0.657  0.006  0.661  3.597 
## 
## Random effects:
##  Groups   Name        Variance Std.Dev.
##  Genre    (Intercept) 159      12.6    
##  Residual             229      15.1    
## Number of obs: 537, groups:  Genre, 16
## 
## Fixed effects:
##              Estimate Std. Error t value
## (Intercept) 133.88516    3.24749    41.2
## Date          0.01894    0.00632     3.0
## 
## Correlation of Fixed Effects:
##      (Intr)
## Date 0.005</code></pre>
<p>Before turning to the diagnostics, we will use the fitted (or predicted) and the observed values with a regression line for the predicted values. This will not only show how good the model fit the data but also the direction and magnitude of the effect.</p>
<pre class="r"><code># extract predicted values
lmmdata$Predicted &lt;- predict(m1.lmer, lmmdata)
# plot predicted values
ggplot(lmmdata, aes(DateUnscaled, Predicted)) +
  facet_wrap(~Genre) +
  geom_point(aes(x = DateUnscaled, y = Prepositions), color = &quot;gray80&quot;, size = .5) +
  geom_smooth(aes(y = Predicted), color = &quot;gray20&quot;, linetype = &quot;solid&quot;, 
              se = T, method = &quot;lm&quot;) +
  guides(color=guide_legend(override.aes=list(fill=NA))) +  
  theme_bw(base_size = 10) +
  theme(legend.position=&quot;top&quot;, legend.title = element_blank(),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank()) + 
  xlab(&quot;Date of composition&quot;) +
  theme_bw()</code></pre>
<p><img src="mmws_files/figure-html/lmm21b-1.png" width="672" /></p>
</div>
<div id="remarks-on-prediction" class="section level3 unnumbered">
<h3>Remarks on Prediction</h3>
<p>While the number of intercepts, the model reports, and the way how mixed- and fixed-effects arrive at predictions differ, their predictions are extremely similar and almost identical (at least when dealing with a simple random effect structure). Consider the following example where we create analogous fixed and mixed effect models and plot their predicted frequencies of prepositions per genre across the un-centered date of composition. The predictions of the mixed-effects model are plotted as a solid red line, while the predictions of the fixed-effects model are plotted as dashed blue lines.</p>
<pre class="r"><code># creat lm model
m5.lmeunweight &lt;- lm(Prepositions ~ DateUnscaled + Genre, data = lmmdata)
lmmdata$lmePredictions &lt;- fitted(m5.lmeunweight, lmmdata)
m5.lm &lt;- lm(Prepositions ~ DateUnscaled + Genre, data = lmmdata)
lmmdata$lmPredictions &lt;- fitted(m5.lm, lmmdata)
# plot predictions
ggplot(lmmdata, aes(x = DateUnscaled, y = lmePredictions, group = Genre)) +
  geom_line(aes(y = lmmdata$lmePredictions), linetype = &quot;solid&quot;, color = &quot;red&quot;) +
  geom_line(aes(y = lmmdata$lmPredictions), linetype = &quot;dashed&quot;, color = &quot;blue&quot;) +
  facet_wrap(~ Genre, nrow = 4) +
  theme_bw() +
  labs(x = &quot;Date of composition&quot;) +
  labs(y = &quot;Prepositions per 1,000 words&quot;) +
  coord_cartesian(ylim = c(0, 220))</code></pre>
<p><img src="mmws_files/figure-html/lmm29-1.png" width="672" /></p>
<p>The predictions overlap almost perfectly which means that the predictions of both are almost identical - irrespective of whether genre is part of the mixed or the fixed effects structure.</p>
</div>
<div id="model-diagnostics" class="section level3 unnumbered">
<h3>Model Diagnostics</h3>
<p>Below is a list of criteria that should be considered when diagnosing regression models:</p>
<ol style="list-style-type: decimal">
<li><p>Data points with standardized residuals &gt; 3.29 should be removed <span class="citation">(Field, Miles, and Field <a href="#ref-field2012discovering" role="doc-biblioref">2012</a>, 269)</span></p></li>
<li><p>If more than 1 percent of data points have standardized residuals exceeding values &gt; 2.58, then the error rate of the model is unacceptable <span class="citation">(Field, Miles, and Field <a href="#ref-field2012discovering" role="doc-biblioref">2012</a>, 269)</span>.</p></li>
<li><p>If more than 5 percent of data points have standardized residuals exceeding values &gt; 1.96, then the error rate of the model is unacceptable <span class="citation">(Field, Miles, and Field <a href="#ref-field2012discovering" role="doc-biblioref">2012</a>, 269)</span></p></li>
<li><p>In addition, data points with Cook’s D-values &gt; 1 should be removed <span class="citation">(Field, Miles, and Field <a href="#ref-field2012discovering" role="doc-biblioref">2012</a>, 269)</span></p></li>
<li><p>Also, data points with leverage values <span class="math inline">\(3(k + 1)/n\)</span> (k = Number of predictors, N = Number of cases in model) should be removed <span class="citation">(Field, Miles, and Field <a href="#ref-field2012discovering" role="doc-biblioref">2012</a>, 270)</span></p></li>
<li><p>There should not be (any) autocorrelation among predictors. This means that independent variables cannot be correlated with itself (for instance, because data points come from the same subject). If there is autocorrelation among predictors, then a Repeated Measures Design or a (hierarchical) mixed-effects model should be implemented instead.</p></li>
<li><p>Predictors cannot substantially correlate with each other (multicollinearity). If a model contains predictors that have variance inflation factors (VIF) &gt; 10 the model is unreliable <span class="citation">(Myers <a href="#ref-myers1990classical" role="doc-biblioref">1990</a>)</span> and predictors causing such VIFs should be removed. Indeed, even VIFs of 2.5 can be problematic <span class="citation">(Szmrecsanyi <a href="#ref-szmrecsanyi2006morphosyntactic" role="doc-biblioref">2006</a>, 215; Zuur, Ieno, and Elphick <a href="#ref-zuur2010protocol" role="doc-biblioref">2010</a>)</span> proposes that variables with VIFs exceeding 3 should be removed!</p></li>
</ol>
<blockquote>
<p>However, (multi-)collinearity is only an issue if one is interested in interpreting regression results! If the interpretation is irrelevant because what is relevant is prediction(!), then it does not matter if the model contains collinear predictors! See <span class="citation">Gries (<a href="#ref-gries2021statistics" role="doc-biblioref">2021</a>)</span> for a more elaborate explanation.</p>
</blockquote>
<ol start="8" style="list-style-type: decimal">
<li>The mean value of VIFs should be <span class="math inline">\(~\)</span> 1 <span class="citation">(Bowerman and O’Connell <a href="#ref-bowerman1990linear" role="doc-biblioref">1990</a>)</span>.</li>
</ol>
<p>We can now evaluate the goodness of fit of the model and check if mathematical requirements and assumptions have been violated. In a first step, we generate diagnostic plots that focus on the random effect structure.</p>
<pre class="r"><code>plot(m1.lmer, Genre ~ resid(.), abline = 0 ) # generate diagnostic plots</code></pre>
<p><img src="mmws_files/figure-html/lmm14-1.png" width="672" /></p>
<p>The plot shows that there are some outliers (points outside the boxes) and that the variability within letters is greater than in other genres we therefore examine the genres in isolation standardized residuals versus fitted values <span class="citation">(Pinheiro and Bates <a href="#ref-pinheiro2000mixedmodels" role="doc-biblioref">2000</a>, 175)</span>.</p>
<pre class="r"><code>plot(m1.lmer, resid(., type = &quot;pearson&quot;) ~ fitted(.) | Genre, id = 0.05, 
     adj = -0.3, pch = 20, col = &quot;gray40&quot;, cex = .5)</code></pre>
<p><img src="mmws_files/figure-html/lmm15-1.png" width="672" /></p>
<p>The plot shows the standardized residuals (or Pearson’s residuals) versus fitted values and suggests that there are outliers in the data (the names elements in the plots). To check if these outliers are a cause for concern, we will now use a Levene’s test to check if the variance is distributed homogeneously (homoscedasticity) or whether the assumption of variance homogeneity is violated (due to the outliers).</p>
<hr />
<blockquote>
<p>The use of Levene’s test to check if the model is heteroscedastic is generally not recommended as it is too lax when dealing with few observations (because in such cases it does not have the power to identify heteroscedasticity) while it is too harsh when dealing with many observations (when heteroscedasticity typically is not a severe problem).</p>
</blockquote>
<hr />
<p>We use Levene’s test here merely to check if it substantiates the impressions we got from the visual inspection.</p>
<pre class="r"><code># check homogeneity
leveneTest(lmmdata$Prepositions, lmmdata$Genre, center = mean)</code></pre>
<pre><code>## Levene&#39;s Test for Homogeneity of Variance (center = mean)
##        Df F value Pr(&gt;F)  
## group  15    1.74   0.04 *
##       521                 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>The Levene’s test shows that the variance is distributed unevenly across genres which means that we do not simply continue but should either remove problematic data points (outliers) or use a weighing method.</p>
<blockquote>
<p>In this case, we will ignore this because this is just an example. If this were a real analysis, we would create a new model which uses weights to compensate for variance heterogeneiety.</p>
</blockquote>
<p>We now create more diagnostic plots to check what other potential problems there are. What we wish to see in the diagnostic plots is a cloud of dots in the middle of the window without any structure. What we do not want to see is a funnel-shaped cloud because this indicates an increase of the errors/residuals with an increase of the predictor(s) (because this would indicate heteroscedasticity) <span class="citation">(Pinheiro and Bates <a href="#ref-pinheiro2000mixedmodels" role="doc-biblioref">2000</a>, 182)</span>.</p>
<pre class="r"><code># start plotting
par(mfrow = c(2, 2))           # display plots in 2 rows and 2 columns
plot(m1.lmer, pch = 20, col = &quot;black&quot;, lty = &quot;dotted&quot;); par(mfrow = c(1, 1))</code></pre>
<p><img src="mmws_files/figure-html/lmm22-1.png" width="672" /></p>
<p>The lack of structure tells us that the model is <em>healthy</em> and does not suffer from heteroscedasticity. We will now create more diagnostic plots to find potential problems <span class="citation">(Pinheiro and Bates <a href="#ref-pinheiro2000mixedmodels" role="doc-biblioref">2000</a>, 21)</span>.</p>
<p>We check the residuals of fitted values against observed values <span class="citation">(Pinheiro and Bates <a href="#ref-pinheiro2000mixedmodels" role="doc-biblioref">2000</a>, 179)</span>. What we would like to see is a straight, upwards going line.</p>
<pre class="r"><code>qqnorm(resid(m1.lmer))
qqline(resid(m1.lmer))</code></pre>
<p><img src="mmws_files/figure-html/lmm24-1.png" width="672" /></p>
<p>It is, unfortunately, rather common that the dots deviate from the straight line at the very bottom or the very top which means that the model is good at estimating values around the middle of the dependent variable but rather bad at estimating lower or higher values. Next, we check the residuals by <em>Genre</em> <span class="citation">(Pinheiro and Bates <a href="#ref-pinheiro2000mixedmodels" role="doc-biblioref">2000</a>, 179)</span>.</p>
<p>Now, we inspect the observed responses versus the within-group fitted values <span class="citation">(Pinheiro and Bates <a href="#ref-pinheiro2000mixedmodels" role="doc-biblioref">2000</a>, 178)</span>.</p>
<pre class="r"><code># observed responses versus the within-group fitted values
plot(m1.lmer, Prepositions ~ fitted(.), id = 0.05, adj = -0.3, 
     xlim = c(80, 220), cex = .8, pch = 20, col = &quot;blue&quot;)</code></pre>
<p><img src="mmws_files/figure-html/lmm26-1.png" width="672" /></p>
<p>Although some data points are named, the plot does not show any structure, like a funnel, which would have been problematic.</p>
</div>
<div id="model-summary" class="section level3 unnumbered">
<h3>Model Summary</h3>
<p>We will now summarize our results. We start by generating a summary table using the <code>tab_model</code> function from the <code>sjPlot</code> package.</p>
<pre class="r"><code>sjPlot::tab_model(m1.lmer)</code></pre>
<table style="border-collapse:collapse; border:none;">
<tr>
<th style="border-top: double; text-align:center; font-style:normal; font-weight:bold; padding:0.2cm;  text-align:left; ">
 
</th>
<th colspan="3" style="border-top: double; text-align:center; font-style:normal; font-weight:bold; padding:0.2cm; ">
Prepositions
</th>
</tr>
<tr>
<td style=" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  text-align:left; ">
Predictors
</td>
<td style=" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  ">
Estimates
</td>
<td style=" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  ">
CI
</td>
<td style=" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  ">
p
</td>
</tr>
<tr>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; ">
(Intercept)
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  ">
133.89
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  ">
127.52 – 140.25
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  ">
<strong>&lt;0.001
</td>
</tr>
<tr>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; ">
Date
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  ">
0.02
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  ">
0.01 – 0.03
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  ">
<strong>0.003</strong>
</td>
</tr>
<tr>
<td colspan="4" style="font-weight:bold; text-align:left; padding-top:.8em;">
Random Effects
</td>
</tr>
<tr>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; padding-top:0.1cm; padding-bottom:0.1cm;">
σ<sup>2</sup>
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left;" colspan="3">
228.76
</td>
<tr>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; padding-top:0.1cm; padding-bottom:0.1cm;">
τ<sub>00</sub> <sub>Genre</sub>
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left;" colspan="3">
159.02
</td>
<tr>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; padding-top:0.1cm; padding-bottom:0.1cm;">
ICC
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left;" colspan="3">
0.41
</td>
<tr>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; padding-top:0.1cm; padding-bottom:0.1cm;">
N <sub>Genre</sub>
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left;" colspan="3">
16
</td>
<tr>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; padding-top:0.1cm; padding-bottom:0.1cm; border-top:1px solid;">
Observations
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left; border-top:1px solid;" colspan="3">
537
</td>
</tr>
<tr>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; padding-top:0.1cm; padding-bottom:0.1cm;">
Marginal R<sup>2</sup> / Conditional R<sup>2</sup>
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left;" colspan="3">
0.012 / 0.417
</td>
</tr>
</table>
<hr />
<p>The <em>marginal R<sup>2</sup></em> (marginal coefficient of determination) represents the variance explained by the fixed effects while the <em>conditional R<sup>2</sup></em> is interpreted as a variance explained by the entire model, including both fixed and random effects <span class="citation">(Bartoń <a href="#ref-barton2020mumin" role="doc-biblioref">2020</a>)</span>.</p>
<p>The effects can be visualized using the <code>plot_model</code> function from the <code>sjPlot</code> package.</p>
<pre class="r"><code>sjPlot::plot_model(m1.lmer, type = &quot;pred&quot;, terms = c(&quot;Date&quot;)) +
  # show uncentered date rather than centered date
  scale_x_continuous(name = &quot;Date&quot;, 
                     breaks = seq(-500, 300, 100), 
                     labels = seq(1150, 1950, 100))</code></pre>
<p><img src="mmws_files/figure-html/lmm21d-1.png" width="672" /></p>
<p>While we have already shown that the effect of Date is significant, it is small which means that the number of prepositions per text does not correlate very strongly with time. This suggests that other factors that are not included in the model also impact the frequency of prepositions (and probably more meaningfully, too).</p>
<p>A mixed-effect linear regression model which contained the genre of texts as random effect was fit to the data in a step-wise-step up procedure. Due to the presence of outliers in the data, weights were included into the model which led to a significantly improved model fit compared to an un-weight model (<span class="math inline">\(\chi\)</span><sup>2</sup>(2): 39.17, p: 0.0006). The final minimal adequate model performed significantly better than an intercept-only base-line model (<span class="math inline">\(\chi\)</span><sup>2</sup>(1): 12.44, p =.0004) and showed that the frequency of prepositions increases significantly but only marginally with the date of composition (Estimate: 0.02, CI: 0.01-0.03, p &lt; .001, marginal R<sup>2</sup> = 0.0174, conditional R<sup>2</sup> = 0.4324). Neither the region where the text was composed nor a higher order interaction between genre and region significantly correlated with the use of prepositions in the data.</p>
</div>
</div>
<div id="mixed-effects-binomial-logistic-regression" class="section level2">
<h2><span class="header-section-number">2.2</span> Mixed-Effects Binomial Logistic Regression</h2>
<p>We now turn to an extension of binomial logistic regression: mixed-effects binomial logistic regression. As is the case with linear mixed-effects models logistic mixed effects models have the following advantages over simpler statistical tests:</p>
<ul>
<li><p>Mixed-effects models are multivariate, i.e. they test the effect of several predictors simultaneously while controlling for the effect of all other predictors.</p></li>
<li><p>Mixed models allow to statistically incorporate within-speaker variability and are thus fit to model hierarchical or nested data structures. This applies if several observations are produced by an individual speaker, for instance.</p></li>
<li><p>Mixed-models provide a wealth of diagnostic statistics which enables us to control e.g. multicollinearity, i.e. correlations between predictors, and to test whether conditions or requirements are violated (e.g. homogeneity of variance, etc.).</p></li>
</ul>
<p>Major disadvantages of regression modeling are that they are prone to producing high <span class="math inline">\(\beta\)</span>-errors <span class="citation">(see Johnson <a href="#ref-johnson2009getting" role="doc-biblioref">2009</a>)</span> and that they require rather large data sets.</p>
<div id="introduction-2" class="section level3 unnumbered">
<h3>Introduction</h3>
<p><img src="mmws_files/figure-html/blmm1-1.png" width="672" /></p>
</div>
<div id="example-discourse-like-in-irish-english" class="section level3 unnumbered">
<h3>Example: Discourse LIKE in Irish English</h3>
<p>In this example we will investigate which factors correlate with the use of <em>final discourse like</em> (e.g. “<em>The weather is shite, like!</em>”) in Irish English. The data set represents speech units in a corpus that were coded for the speaker who uttered a given speech unit, the gender (Gender: Men versus Women) and age of that speaker (Age: Old versus Young), whether the interlocutors were of the same or a different gender (ConversationType: SameGender versus MixedGender), and whether another <em>final discourse like</em> had been used up to three speech units before (Priming: NoPrime versus Prime), whether or not the speech unit contained an <em>final discourse like</em> (SUFLike: 1 = yes, 0 = no. To begin with, we load the data and inspect the structure of the data set,</p>
<pre class="r"><code># load data
mblrdata &lt;- read.table(&quot;https://slcladal.github.io/data/mblrdata.txt&quot;, 
                       comment.char = &quot;&quot;,# data does not contain comments
                       quote = &quot;&quot;,       # data does not contain quotes
                       sep = &quot;\t&quot;,       # data is tab separated
                       header = T)       # data has column names</code></pre>
<div id="htmlwidget-f37a3b681f1823e7caa5" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-f37a3b681f1823e7caa5">{"x":{"filter":"none","data":[["S1A-061$C","S1A-023$B","S1A-054$A","S1A-090$B","S1A-009$B","S1A-085$E","S1A-003$C","S1A-084$C","S1A-076$A","S1A-083$D","S1A-068$A","S1A-066$B","S1A-061$A","S1A-049$A","S1A-022$B","S1A-074$A","S1A-074$C","S1A-083$B","S1A-067$A","S1A-068$C","S1A-035$B","S1A-036$B","S1A-072$B","S1A-087$B","S1A-068$B","S1A-059$A","S1A-069$C","S1A-008$C","S1A-061$B","S1A-098$A","S1A-001$B","S1A-022$A","S1A-051$B","S1A-078$B","S1A-037$C","S1A-021$F","S1A-050$B","S1A-046$A","S1A-063$C","S1A-051$D","S1A-022$D","S1A-022$B","S1A-019$D","S1A-010$B","S1A-080$A","S1A-046$A","S1A-059$A","S1A-009$A","S1A-083$B","S1A-019$C","S1A-075$B","S1A-035$A","S1A-095$A","S1A-091$C","S1A-084$B","S1A-079$A","S1A-055$B","S1A-082$G","S1A-055$B","S1A-068$B","S1A-049$A","S1A-072$B","S1A-003$B","S1A-093$B","S1A-001$B","S1A-051$B","S1A-007$B","S1A-074$C","S1A-063$B","S1A-051$D","S1A-046$A","S1A-058$C","S1A-076$B","S1A-058$A","S1A-078$C","S1A-003$C","S1A-009$A","S1A-007$B","S1A-006$A","S1A-054$A","S1A-020$D","S1A-033$D","S1A-050$A","S1A-033$B","S1A-097$B","S1A-049$B","S1A-051$B","S1A-044$C","S1A-071$A","S1A-095$B","S1A-050$C","S1A-021$D","S1A-037$A","S1A-019$A","S1A-066$C","S1A-072$B","S1A-001$B","S1A-076$A","S1A-050$A","S1A-001$A","S1A-091$D","S1A-072$B","S1A-053$C","S1A-009$A","S1A-087$A","S1A-094$A","S1A-082$E","S1A-050$A","S1A-079$D","S1A-033$C","S1A-070$B","S1A-007$B","S1A-052$B","S1A-092$A","S1A-055$A","S1A-071$A","S1A-072$C","S1A-078$B","S1A-054$D","S1A-066$C","S1A-076$A","S1A-091$D","S1A-050$B","S1A-074$A","S1A-071$C","S1A-066$A","S1A-058$B","S1A-091$B","S1A-007$D","S1A-071$B","S1A-051$B","S1A-074$A","S1A-070$A","S1A-076$A","S1A-009$A","S1A-055$A","S1A-007$A","S1A-066$B","S1A-001$B","S1A-077$D","S1A-049$A","S1A-069$B","S1A-069$B","S1A-078$B","S1A-020$D","S1A-033$C","S1A-074$C","S1A-082$A","S1A-091$C","S1A-055$A","S1A-073$B","S1A-082$C","S1A-022$A","S1A-079$B","S1A-037$C","S1A-064$E","S1A-022$D","S1A-036$B","S1A-056$D","S1A-068$C","S1A-054$C","S1A-087$B","S1A-009$B","S1A-019$A","S1A-010$B","S1A-022$D","S1A-072$C","S1A-077$C","S1A-095$A","S1A-005$B","S1A-076$A","S1A-095$A","S1A-045$B","S1A-003$E","S1A-033$C","S1A-050$C","S1A-033$D","S1A-033$B","S1A-071$A","S1A-019$C","S1A-067$A","S1A-095$A","S1A-081$C","S1A-066$A","S1A-092$B","S1A-070$B","S1A-094$A","S1A-018$D","S1A-095$B","S1A-007$A","S1A-091$D","S1A-039$A","S1A-099$A","S1A-043$B","S1A-007$F","S1A-023$A","S1A-086$A","S1A-054$A","S1A-006$E","S1A-055$A","S1A-038$B","S1A-061$A","S1A-046$A","S1A-056$B","S1A-068$A","S1A-086$A","S1A-046$A","S1A-070$B","S1A-071$C","S1A-006$A","S1A-058$A","S1A-069$B","S1A-095$A","S1A-080$A","S1A-038$A","S1A-010$D","S1A-095$B","S1A-053$C","S1A-035$B","S1A-049$B","S1A-005$A","S1A-099$B","S1A-050$C","S1A-049$A","S1A-049$A","S1A-072$B","S1A-080$A","S1A-078$C","S1A-081$E","S1A-083$D","S1A-087$A","S1A-076$A","S1A-020$A","S1A-052$C","S1A-006$C","S1A-055$A","S1A-051$D","S1A-046$B","S1A-082$E","S1A-023$D","S1A-070$A","S1A-020$A","S1A-071$B","S1A-039$B","S1A-008$A","S1A-004$C","S1A-054$D","S1A-087$A","S1A-019$B","S1A-072$C","S1A-064$E","S1A-084$D","S1A-009$B","S1A-046$B","S1A-063$B","S1A-056$A","S1A-063$B","S1A-094$B","S1A-071$A","S1A-008$D","S1A-087$B","S1A-050$C","S1A-072$C","S1A-002$F","S1A-076$B","S1A-089$B","S1A-036$B","S1A-072$B","S1A-020$A","S1A-096$A","S1A-066$C","S1A-002$F","S1A-093$A","S1A-077$A","S1A-073$A","S1A-079$B","S1A-033$B","S1A-006$B","S1A-060$A","S1A-050$B","S1A-081$C","S1A-052$C","S1A-073$C","S1A-008$A","S1A-066$C","S1A-021$H","S1A-036$A","S1A-051$A","S1A-098$A","S1A-033$D","S1A-049$B","S1A-079$A","S1A-021$A","S1A-076$A","S1A-082$E","S1A-086$A","S1A-009$B","S1A-094$A","S1A-074$B","S1A-094$B","S1A-004$A","S1A-039$B","S1A-082$E","S1A-018$D","S1A-054$C","S1A-070$B","S1A-086$B","S1A-097$B","S1A-054$A","S1A-077$D","S1A-082$E","S1A-092$A","S1A-010$C","S1A-005$A","S1A-037$B","S1A-084$C","S1A-005$A","S1A-002$A","S1A-053$C","S1A-060$A","S1A-081$E","S1A-060$A","S1A-003$D","S1A-054$D","S1A-092$B","S1A-068$A","S1A-035$A","S1A-059$A","S1A-069$C","S1A-063$B","S1A-002$A","S1A-070$A","S1A-093$B","S1A-081$H","S1A-020$D","S1A-053$B","S1A-071$B","S1A-058$C","S1A-069$B","S1A-020$A","S1A-087$C","S1A-061$A","S1A-084$B","S1A-084$D","S1A-074$C","S1A-073$A","S1A-009$A","S1A-006$A","S1A-005$B","S1A-002$A","S1A-082$A","S1A-019$A","S1A-020$D","S1A-049$A","S1A-082$A","S1A-082$A","S1A-038$B","S1A-066$B","S1A-079$B","S1A-050$C","S1A-019$B","S1A-063$A","S1A-066$C","S1A-099$A","S1A-077$A","S1A-082$G","S1A-008$C","S1A-072$B","S1A-067$A","S1A-097$B","S1A-051$A","S1A-092$B","S1A-072$C","S1A-009$B","S1A-079$A","S1A-005$B","S1A-002$F","S1A-061$B","S1A-002$A","S1A-006$A","S1A-038$A","S1A-004$A","S1A-098$A","S1A-073$A","S1A-087$A","S1A-084$B","S1A-064$C","S1A-074$B","S1A-009$A","S1A-086$A","S1A-035$A","S1A-021$A","S1A-052$C","S1A-099$B","S1A-053$B","S1A-094$A","S1A-069$A","S1A-070$A","S1A-075$B","S1A-001$A","S1A-067$A","S1A-092$B","S1A-063$B","S1A-023$D","S1A-022$B","S1A-066$C","S1A-078$E","S1A-070$A","S1A-011$A","S1A-038$B","S1A-069$A","S1A-066$C","S1A-004$A","S1A-039$B","S1A-010$B","S1A-090$B","S1A-083$B","S1A-009$B","S1A-050$B","S1A-050$B","S1A-050$A","S1A-001$B","S1A-070$B","S1A-007$A","S1A-001$A","S1A-071$B","S1A-038$B","S1A-067$A","S1A-093$B","S1A-007$F","S1A-079$B","S1A-094$B","S1A-088$A","S1A-070$A","S1A-086$B","S1A-022$A","S1A-069$C","S1A-059$A","S1A-098$A","S1A-006$A","S1A-002$C","S1A-010$C","S1A-021$B","S1A-010$D","S1A-078$B","S1A-020$A","S1A-004$B","S1A-010$D","S1A-049$A","S1A-091$B","S1A-072$A","S1A-036$B","S1A-086$A","S1A-052$A","S1A-023$A","S1A-020$A","S1A-076$B","S1A-047$B","S1A-033$B","S1A-067$A","S1A-086$B","S1A-020$D","S1A-033$B","S1A-011$A","S1A-069$C","S1A-036$A","S1A-046$B","S1A-046$A","S1A-063$B","S1A-053$C","S1A-066$B","S1A-082$B","S1A-087$A","S1A-087$B","S1A-074$C","S1A-077$C","S1A-060$A","S1A-087$B","S1A-056$D","S1A-053$A","S1A-003$D","S1A-081$E","S1A-084$C","S1A-037$D","S1A-002$A","S1A-035$A","S1A-082$G","S1A-036$B","S1A-046$B","S1A-002$D","S1A-036$A","S1A-019$B","S1A-074$C","S1A-004$A","S1A-051$D","S1A-002$F","S1A-076$A","S1A-093$A","S1A-004$A","S1A-083$E","S1A-049$A","S1A-005$A","S1A-055$A","S1A-038$A","S1A-051$A","S1A-023$B","S1A-055$B","S1A-049$A","S1A-002$A","S1A-006$B","S1A-047$B","S1A-002$F","S1A-096$A","S1A-072$B","S1A-053$B","S1A-068$A","S1A-075$B","S1A-064$D","S1A-084$D","S1A-051$B","S1A-010$D","S1A-054$D","S1A-060$A","S1A-080$B","S1A-063$B","S1A-077$D","S1A-091$D","S1A-007$C","S1A-006$B","S1A-053$B","S1A-054$C","S1A-084$B","S1A-039$C","S1A-091$D","S1A-086$B","S1A-079$B","S1A-078$B","S1A-052$C","S1A-006$A","S1A-066$C","S1A-092$B","S1A-033$C","S1A-090$B","S1A-019$C","S1A-001$B","S1A-005$B","S1A-096$A","S1A-068$C","S1A-086$A","S1A-050$C","S1A-078$B","S1A-022$B","S1A-004$A","S1A-072$B","S1A-079$A","S1A-090$B","S1A-092$A","S1A-093$A","S1A-037$C","S1A-083$B","S1A-072$B","S1A-004$B","S1A-023$D","S1A-054$D","S1A-053$D","S1A-084$B","S1A-055$B","S1A-068$C","S1A-063$A","S1A-098$A","S1A-086$A","S1A-052$A","S1A-033$B","S1A-059$A","S1A-073$A","S1A-009$B","S1A-033$D","S1A-003$E","S1A-082$C","S1A-086$B","S1A-007$D","S1A-073$B","S1A-005$A","S1A-040$D","S1A-080$A","S1A-033$B","S1A-043$B","S1A-080$B","S1A-037$B","S1A-084$C","S1A-053$D","S1A-078$D","S1A-059$A","S1A-094$B","S1A-007$A","S1A-055$B","S1A-037$B","S1A-082$C","S1A-079$B","S1A-054$D","S1A-051$B","S1A-005$A","S1A-054$D","S1A-096$A","S1A-096$A","S1A-086$A","S1A-073$C","S1A-051$D","S1A-069$B","S1A-003$D","S1A-074$C","S1A-082$A","S1A-095$B","S1A-038$A","S1A-083$B","S1A-020$D","S1A-011$A","S1A-092$B","S1A-067$A","S1A-058$A","S1A-052$A","S1A-083$F","S1A-036$B","S1A-050$C","S1A-004$A","S1A-079$A","S1A-007$B","S1A-061$A","S1A-087$B","S1A-036$B","S1A-070$B","S1A-070$A","S1A-094$B","S1A-061$C","S1A-022$A","S1A-005$B","S1A-096$A","S1A-055$B","S1A-077$D","S1A-096$A","S1A-022$B","S1A-080$A","S1A-073$A","S1A-095$A","S1A-009$A","S1A-071$C","S1A-082$G","S1A-021$B","S1A-046$A","S1A-072$B","S1A-087$A","S1A-079$B","S1A-084$A","S1A-079$A","S1A-097$B","S1A-084$B","S1A-080$B","S1A-056$B","S1A-078$D","S1A-095$A","S1A-055$B","S1A-095$A","S1A-006$C","S1A-072$B","S1A-073$A","S1A-084$C","S1A-050$C","S1A-099$A","S1A-079$A","S1A-087$A","S1A-076$A","S1A-007$B","S1A-070$A","S1A-072$C","S1A-050$C","S1A-099$A","S1A-020$B","S1A-008$C","S1A-068$C","S1A-049$B","S1A-056$D","S1A-055$D","S1A-055$B","S1A-066$C","S1A-055$A","S1A-038$A","S1A-047$B","S1A-066$C","S1A-075$B","S1A-076$A","S1A-086$B","S1A-033$B","S1A-094$B","S1A-092$B","S1A-055$A","S1A-035$A","S1A-006$A","S1A-006$E","S1A-080$B","S1A-069$A","S1A-033$D","S1A-005$B","S1A-061$B","S1A-084$B","S1A-051$B","S1A-095$A","S1A-095$A","S1A-083$B","S1A-096$A","S1A-079$B","S1A-076$A","S1A-007$E","S1A-074$C","S1A-002$F","S1A-070$A","S1A-021$F","S1A-059$A","S1A-094$A","S1A-086$A","S1A-051$A","S1A-066$C","S1A-079$B","S1A-067$A","S1A-088$A","S1A-006$B","S1A-022$A","S1A-007$B","S1A-068$A","S1A-023$B","S1A-099$A","S1A-086$A","S1A-077$D","S1A-020$C","S1A-053$A","S1A-086$A","S1A-019$B","S1A-077$D","S1A-097$B","S1A-033$D","S1A-051$A","S1A-009$B","S1A-079$A","S1A-066$B","S1A-098$A","S1A-033$B","S1A-009$B","S1A-006$A","S1A-051$A","S1A-010$D","S1A-072$A","S1A-061$B","S1A-039$C","S1A-056$C","S1A-055$B","S1A-058$C","S1A-084$D","S1A-058$D","S1A-074$C","S1A-092$B","S1A-082$G","S1A-022$B","S1A-076$A","S1A-084$C","S1A-099$A","S1A-067$A","S1A-073$C","S1A-068$C","S1A-023$D","S1A-086$A","S1A-001$B","S1A-082$G","S1A-020$B","S1A-069$A","S1A-004$A","S1A-006$A","S1A-037$D","S1A-054$D","S1A-047$B","S1A-049$A","S1A-018$D","S1A-074$B","S1A-096$A","S1A-077$C","S1A-095$A","S1A-019$C","S1A-095$B","S1A-058$A","S1A-009$A","S1A-050$A","S1A-099$A","S1A-095$B","S1A-008$A","S1A-087$A","S1A-050$C","S1A-076$A","S1A-095$B","S1A-070$B","S1A-083$A","S1A-095$A","S1A-011$A","S1A-074$B","S1A-063$A","S1A-023$B","S1A-078$C","S1A-037$C","S1A-050$C","S1A-038$B","S1A-071$C","S1A-019$A","S1A-055$A","S1A-058$B","S1A-084$B","S1A-033$B","S1A-086$A","S1A-087$B","S1A-009$B","S1A-086$A","S1A-067$A","S1A-072$B","S1A-094$A","S1A-058$A","S1A-021$F","S1A-099$A","S1A-056$D","S1A-055$E","S1A-068$A","S1A-071$C","S1A-058$C","S1A-011$A","S1A-066$C","S1A-022$B","S1A-051$A","S1A-072$B","S1A-080$B","S1A-037$D","S1A-082$C","S1A-007$E","S1A-023$D","S1A-046$A","S1A-093$B","S1A-071$A","S1A-001$B","S1A-092$A","S1A-069$A","S1A-070$A","S1A-074$A","S1A-002$D","S1A-072$C","S1A-008$D","S1A-082$A","S1A-061$A","S1A-046$B","S1A-061$A","S1A-064$D","S1A-051$D","S1A-033$B","S1A-007$E","S1A-053$D","S1A-071$B","S1A-086$A","S1A-076$A","S1A-002$D","S1A-053$C","S1A-083$B","S1A-067$A","S1A-040$D","S1A-098$A","S1A-037$B","S1A-055$E","S1A-086$B","S1A-055$B","S1A-001$B","S1A-051$A","S1A-035$B","S1A-096$A","S1A-093$B","S1A-096$A","S1A-061$A","S1A-009$B","S1A-068$A","S1A-058$B","S1A-069$B","S1A-035$B","S1A-074$A","S1A-075$C","S1A-082$E","S1A-007$B","S1A-079$B","S1A-092$A","S1A-081$C","S1A-095$A","S1A-019$A","S1A-078$E","S1A-008$A","S1A-087$A","S1A-056$C","S1A-095$B","S1A-078$B","S1A-068$A","S1A-005$A","S1A-081$E","S1A-069$B","S1A-058$C","S1A-049$A","S1A-079$B","S1A-072$B","S1A-009$B","S1A-098$A","S1A-077$D","S1A-099$B","S1A-020$D","S1A-093$A","S1A-091$A","S1A-051$B","S1A-075$B","S1A-093$B","S1A-078$C","S1A-092$A","S1A-087$B","S1A-097$B","S1A-063$B","S1A-055$A","S1A-072$C","S1A-020$D","S1A-007$F","S1A-047$B","S1A-071$A","S1A-052$B","S1A-003$C","S1A-023$B","S1A-046$B","S1A-001$B","S1A-007$B","S1A-051$A","S1A-070$B","S1A-006$C","S1A-039$B","S1A-072$A","S1A-081$A","S1A-077$A","S1A-004$A","S1A-097$B","S1A-086$A","S1A-091$D","S1A-081$E","S1A-072$B","S1A-084$D","S1A-007$A","S1A-087$B","S1A-086$B","S1A-007$A","S1A-078$B","S1A-095$B","S1A-061$C","S1A-003$E","S1A-072$B","S1A-091$D","S1A-010$C","S1A-077$C","S1A-054$C","S1A-079$A","S1A-059$A","S1A-010$D","S1A-046$A","S1A-039$B","S1A-035$A","S1A-072$B","S1A-023$A","S1A-077$A","S1A-061$A","S1A-061$A","S1A-020$D","S1A-055$B","S1A-091$C","S1A-038$B","S1A-033$C","S1A-073$B","S1A-021$D","S1A-073$B","S1A-098$A","S1A-049$A","S1A-097$B","S1A-070$A","S1A-050$C","S1A-084$D","S1A-019$A","S1A-022$D","S1A-075$C","S1A-046$B","S1A-099$B","S1A-070$A","S1A-039$B","S1A-091$D","S1A-078$B","S1A-084$D","S1A-074$A","S1A-053$A","S1A-072$B","S1A-023$B","S1A-006$A","S1A-094$A","S1A-092$A","S1A-006$B","S1A-009$A","S1A-035$A","S1A-019$C","S1A-039$C","S1A-001$B","S1A-010$B","S1A-083$D","S1A-002$C","S1A-056$D","S1A-099$A","S1A-061$A","S1A-093$B","S1A-067$A","S1A-006$D","S1A-087$A","S1A-084$A","S1A-054$D","S1A-060$A","S1A-074$A","S1A-071$A","S1A-077$D","S1A-074$A","S1A-036$B","S1A-099$B","S1A-001$B","S1A-074$A","S1A-070$A","S1A-022$B","S1A-095$B","S1A-006$A","S1A-053$A","S1A-046$A","S1A-050$C","S1A-083$B","S1A-052$B","S1A-049$A","S1A-051$D","S1A-046$B","S1A-037$B","S1A-091$D","S1A-051$B","S1A-093$A","S1A-047$B","S1A-063$A","S1A-094$B","S1A-006$C","S1A-049$A","S1A-073$B","S1A-087$B","S1A-022$A","S1A-050$C","S1A-072$B","S1A-075$B","S1A-060$A","S1A-009$A","S1A-091$B","S1A-050$C","S1A-060$A","S1A-047$B","S1A-049$B","S1A-002$A","S1A-001$A","S1A-035$B","S1A-093$B","S1A-074$A","S1A-033$A","S1A-086$B","S1A-049$A","S1A-003$D","S1A-088$A","S1A-006$B","S1A-002$D","S1A-055$B","S1A-074$B","S1A-072$B","S1A-081$E","S1A-072$A","S1A-073$C","S1A-006$A","S1A-076$A","S1A-007$A","S1A-003$B","S1A-058$D","S1A-010$C","S1A-083$E","S1A-098$A","S1A-086$B","S1A-074$A","S1A-033$B","S1A-066$C","S1A-058$B","S1A-093$B","S1A-007$D","S1A-082$A","S1A-055$B","S1A-036$B","S1A-060$A","S1A-087$B","S1A-055$B","S1A-069$B","S1A-045$B","S1A-084$B","S1A-073$A","S1A-096$A","S1A-078$C","S1A-038$B","S1A-058$C","S1A-086$A","S1A-019$B","S1A-076$A","S1A-053$C","S1A-070$B","S1A-035$A","S1A-084$B","S1A-098$A","S1A-091$C","S1A-053$A","S1A-087$B","S1A-039$C","S1A-023$A","S1A-095$A","S1A-075$B","S1A-022$A","S1A-092$A","S1A-072$C","S1A-068$C","S1A-050$C","S1A-094$A","S1A-076$B","S1A-020$A","S1A-037$B","S1A-036$B","S1A-051$D","S1A-035$A","S1A-021$D","S1A-002$C","S1A-023$B","S1A-051$A","S1A-005$A","S1A-053$C","S1A-076$A","S1A-009$A","S1A-068$C","S1A-009$B","S1A-036$A","S1A-068$C","S1A-059$A","S1A-056$A","S1A-080$B","S1A-074$B","S1A-068$A","S1A-036$A","S1A-006$A","S1A-056$B","S1A-049$A","S1A-074$A","S1A-073$B","S1A-074$A","S1A-076$A","S1A-070$A","S1A-046$A","S1A-074$B","S1A-033$B","S1A-053$D","S1A-003$E","S1A-096$A","S1A-054$C","S1A-072$C","S1A-099$A","S1A-072$C","S1A-076$B","S1A-095$B","S1A-079$D","S1A-063$B","S1A-052$A","S1A-021$A","S1A-068$A","S1A-046$B","S1A-046$B","S1A-080$B","S1A-074$A","S1A-053$A","S1A-079$A","S1A-007$E","S1A-033$B","S1A-078$B","S1A-004$A","S1A-069$B","S1A-007$C","S1A-067$A","S1A-079$A","S1A-078$C","S1A-087$B","S1A-049$A","S1A-049$A","S1A-074$C","S1A-071$C","S1A-060$A","S1A-078$B","S1A-008$D","S1A-055$A","S1A-090$B","S1A-064$E","S1A-072$B","S1A-083$D","S1A-010$B","S1A-074$A","S1A-079$A","S1A-036$B","S1A-076$A","S1A-073$B","S1A-084$B","S1A-094$A","S1A-035$B","S1A-082$A","S1A-050$B","S1A-086$A","S1A-063$A","S1A-005$B","S1A-063$A","S1A-008$C","S1A-007$F","S1A-019$B","S1A-097$B","S1A-095$B","S1A-087$B","S1A-004$A","S1A-052$A","S1A-007$B","S1A-055$A","S1A-071$A","S1A-010$D","S1A-046$B","S1A-097$B","S1A-067$A","S1A-002$D","S1A-051$D","S1A-050$C","S1A-002$F","S1A-021$A","S1A-075$C","S1A-073$B","S1A-085$E","S1A-068$A","S1A-052$C","S1A-003$E","S1A-046$A","S1A-023$D","S1A-019$C","S1A-037$B","S1A-094$A","S1A-078$B","S1A-073$B","S1A-059$A","S1A-069$C","S1A-037$C","S1A-080$A","S1A-068$A","S1A-085$E","S1A-023$B","S1A-077$D","S1A-095$A","S1A-019$A","S1A-059$A","S1A-082$C","S1A-094$A","S1A-084$B","S1A-072$B","S1A-035$A","S1A-037$B","S1A-036$B","S1A-002$F","S1A-096$A","S1A-036$A","S1A-033$B","S1A-092$A","S1A-010$B","S1A-063$A","S1A-038$B","S1A-036$A","S1A-037$B","S1A-087$B","S1A-068$B","S1A-077$D","S1A-071$B","S1A-068$A","S1A-036$A","S1A-005$B","S1A-051$D","S1A-072$C","S1A-076$B","S1A-035$A","S1A-082$E","S1A-056$B","S1A-085$E","S1A-004$A","S1A-071$A","S1A-036$B","S1A-002$A","S1A-007$A","S1A-054$D","S1A-074$B","S1A-084$C","S1A-004$C","S1A-061$A","S1A-067$A","S1A-093$B","S1A-074$A","S1A-038$B","S1A-083$B","S1A-044$B","S1A-063$C","S1A-059$A","S1A-051$B","S1A-010$C","S1A-002$A","S1A-051$B","S1A-004$A","S1A-004$A","S1A-051$D","S1A-078$B","S1A-053$B","S1A-050$B","S1A-002$D","S1A-073$A","S1A-049$A","S1A-083$D","S1A-023$B","S1A-068$A","S1A-094$A","S1A-020$B","S1A-056$A","S1A-083$E","S1A-094$B","S1A-049$A","S1A-090$B","S1A-076$A","S1A-023$A","S1A-067$A","S1A-050$C","S1A-091$B","S1A-036$B","S1A-072$A","S1A-080$B","S1A-058$D","S1A-091$B","S1A-005$B","S1A-051$A","S1A-083$D","S1A-089$B","S1A-087$B","S1A-093$A","S1A-070$B","S1A-036$B","S1A-004$C","S1A-067$A","S1A-094$B","S1A-001$B","S1A-070$B","S1A-078$C","S1A-077$D","S1A-053$C","S1A-045$B","S1A-072$B","S1A-083$A","S1A-078$B","S1A-058$D","S1A-007$B","S1A-093$B","S1A-079$A","S1A-003$C","S1A-084$B","S1A-058$D","S1A-019$C","S1A-077$A","S1A-094$A","S1A-055$B","S1A-082$A","S1A-007$F","S1A-003$B","S1A-046$B","S1A-005$B","S1A-061$C","S1A-045$B","S1A-005$B","S1A-053$D","S1A-074$C","S1A-053$D","S1A-066$B","S1A-052$B","S1A-056$D","S1A-052$C","S1A-090$B","S1A-093$B","S1A-066$C","S1A-073$B","S1A-003$B","S1A-010$B","S1A-046$A","S1A-005$A","S1A-074$A","S1A-066$B","S1A-082$A","S1A-058$A","S1A-070$B","S1A-061$A","S1A-006$B","S1A-002$D","S1A-003$C","S1A-074$C","S1A-010$C","S1A-054$B","S1A-069$B","S1A-080$A","S1A-087$B","S1A-069$B","S1A-066$C","S1A-082$G","S1A-009$B","S1A-094$A","S1A-093$B","S1A-039$B","S1A-076$A","S1A-083$E","S1A-023$A","S1A-019$A","S1A-050$B","S1A-056$A","S1A-099$A","S1A-067$A","S1A-061$A","S1A-038$A","S1A-072$A","S1A-086$A","S1A-010$D","S1A-002$A","S1A-045$B","S1A-072$C","S1A-076$A","S1A-054$A","S1A-054$D","S1A-050$C","S1A-054$C","S1A-070$B","S1A-066$C","S1A-021$D","S1A-084$B","S1A-023$B","S1A-043$B","S1A-005$B","S1A-086$B","S1A-071$A","S1A-064$C","S1A-059$A","S1A-078$B","S1A-058$B","S1A-054$D","S1A-053$C","S1A-049$A","S1A-023$D","S1A-046$B","S1A-037$C","S1A-010$B","S1A-079$A","S1A-049$B","S1A-068$A","S1A-093$A","S1A-061$A","S1A-061$A","S1A-069$A","S1A-084$C","S1A-094$B","S1A-071$A","S1A-004$B","S1A-036$B","S1A-083$E","S1A-035$B","S1A-063$B","S1A-055$B","S1A-079$A","S1A-060$A","S1A-095$A","S1A-011$A","S1A-050$B","S1A-092$A","S1A-055$A","S1A-095$B","S1A-056$A","S1A-059$A","S1A-073$C","S1A-023$D","S1A-077$C","S1A-093$A","S1A-064$D","S1A-084$C","S1A-036$A","S1A-076$A","S1A-094$A","S1A-080$B","S1A-021$A","S1A-066$C","S1A-092$B","S1A-079$A","S1A-035$A","S1A-003$E","S1A-039$A","S1A-086$A","S1A-052$B","S1A-004$A","S1A-069$C","S1A-077$D","S1A-082$A","S1A-054$D","S1A-061$B","S1A-055$A","S1A-010$B","S1A-066$C","S1A-059$A","S1A-087$B","S1A-046$B","S1A-054$C","S1A-040$D","S1A-039$B","S1A-079$A","S1A-051$A","S1A-080$B","S1A-004$A","S1A-063$A","S1A-004$B","S1A-019$C","S1A-066$C","S1A-020$D","S1A-033$A","S1A-005$B","S1A-019$B","S1A-099$B","S1A-005$A","S1A-063$A","S1A-036$B","S1A-099$B","S1A-072$A","S1A-070$A","S1A-093$A","S1A-033$B","S1A-068$C","S1A-056$D","S1A-036$B","S1A-073$B","S1A-069$B","S1A-023$B","S1A-087$A","S1A-010$B","S1A-038$B","S1A-079$B","S1A-009$B","S1A-054$C","S1A-069$A","S1A-097$B","S1A-003$D","S1A-083$E","S1A-004$B","S1A-050$A","S1A-007$B","S1A-008$D","S1A-066$C","S1A-099$B","S1A-050$C","S1A-036$A","S1A-087$A","S1A-051$B","S1A-069$C","S1A-006$D","S1A-063$C","S1A-053$A","S1A-005$A","S1A-051$B","S1A-061$A","S1A-067$A","S1A-046$B","S1A-046$B","S1A-084$C","S1A-072$B","S1A-069$C","S1A-091$B","S1A-086$B","S1A-064$E","S1A-049$A","S1A-051$E","S1A-038$A","S1A-039$B","S1A-003$E","S1A-001$B","S1A-049$A","S1A-072$B","S1A-099$B","S1A-085$E","S1A-010$C","S1A-056$C","S1A-050$B","S1A-094$A","S1A-007$B","S1A-073$A","S1A-063$B","S1A-019$A","S1A-046$B","S1A-072$B","S1A-019$B","S1A-067$A","S1A-087$B","S1A-049$B","S1A-007$C","S1A-086$A","S1A-002$D","S1A-093$A","S1A-073$A","S1A-043$B","S1A-084$B","S1A-037$C","S1A-054$A","S1A-003$E","S1A-037$C","S1A-051$A","S1A-094$A","S1A-090$B","S1A-020$D","S1A-053$A","S1A-072$B","S1A-068$C","S1A-049$A","S1A-054$D","S1A-039$A","S1A-039$A","S1A-002$D","S1A-008$D","S1A-010$C","S1A-001$B","S1A-002$D","S1A-080$A","S1A-022$A","S1A-090$B","S1A-071$B","S1A-038$B","S1A-035$B","S1A-039$B","S1A-007$B","S1A-055$B","S1A-075$B","S1A-084$C","S1A-095$A","S1A-081$C","S1A-050$C","S1A-072$B","S1A-021$A","S1A-087$A","S1A-047$B","S1A-003$E","S1A-058$D","S1A-072$A","S1A-005$B","S1A-069$C","S1A-023$D","S1A-049$A","S1A-076$B","S1A-005$A","S1A-083$E","S1A-052$C","S1A-046$A","S1A-084$A","S1A-046$A","S1A-038$B","S1A-022$A","S1A-083$A","S1A-086$A","S1A-033$C","S1A-060$A","S1A-021$A","S1A-071$C","S1A-022$B","S1A-036$B","S1A-038$A","S1A-035$B","S1A-008$D","S1A-093$B","S1A-061$B","S1A-073$A","S1A-071$B","S1A-068$A","S1A-003$C","S1A-002$C","S1A-001$A","S1A-046$B","S1A-082$E","S1A-050$C","S1A-021$H","S1A-079$B","S1A-099$B","S1A-036$B","S1A-051$E","S1A-058$A","S1A-020$B","S1A-092$B","S1A-020$D","S1A-037$C","S1A-004$A","S1A-094$B","S1A-098$A","S1A-067$A","S1A-099$B","S1A-011$A","S1A-019$A","S1A-055$B","S1A-087$C","S1A-003$C","S1A-086$B","S1A-005$A","S1A-067$A","S1A-070$B","S1A-055$B","S1A-050$C","S1A-076$A","S1A-077$C","S1A-082$E","S1A-071$C","S1A-095$A","S1A-054$C","S1A-074$B","S1A-069$C","S1A-093$B","S1A-046$B","S1A-071$B","S1A-052$A","S1A-086$A","S1A-075$B","S1A-072$A","S1A-055$B","S1A-082$B","S1A-073$C","S1A-091$C","S1A-010$D","S1A-020$D","S1A-079$C","S1A-074$A","S1A-076$B","S1A-087$A","S1A-051$E","S1A-010$C","S1A-050$C","S1A-003$E","S1A-001$A","S1A-076$A","S1A-022$A","S1A-094$B","S1A-019$B","S1A-007$A","S1A-070$B","S1A-009$A","S1A-054$D","S1A-035$B","S1A-010$C","S1A-007$B","S1A-086$A","S1A-010$C","S1A-087$B","S1A-087$A","S1A-004$A","S1A-074$A","S1A-078$B","S1A-072$A","S1A-008$C","S1A-083$E","S1A-082$E","S1A-005$A","S1A-098$A","S1A-066$A","S1A-055$B","S1A-040$D","S1A-051$A","S1A-076$A","S1A-003$E","S1A-094$A","S1A-076$A","S1A-006$A","S1A-076$A","S1A-071$C","S1A-078$B","S1A-059$A","S1A-063$B","S1A-035$A","S1A-002$D","S1A-075$C","S1A-022$A","S1A-019$C","S1A-004$A","S1A-079$B","S1A-095$B","S1A-021$F","S1A-004$A","S1A-099$A","S1A-079$B","S1A-086$A","S1A-006$C","S1A-001$A","S1A-037$D","S1A-092$B","S1A-070$B","S1A-068$A","S1A-095$B","S1A-006$A","S1A-078$B","S1A-073$B","S1A-080$B","S1A-035$A","S1A-077$D","S1A-036$A","S1A-049$A","S1A-005$B","S1A-051$A","S1A-087$C","S1A-046$B","S1A-020$D","S1A-071$B","S1A-054$A","S1A-094$A","S1A-005$B","S1A-046$B","S1A-049$A","S1A-093$A","S1A-076$A","S1A-072$B","S1A-053$A","S1A-093$B","S1A-087$B","S1A-080$B","S1A-046$A","S1A-056$D","S1A-074$B","S1A-066$B","S1A-001$A","S1A-083$B","S1A-052$B","S1A-096$A","S1A-081$H","S1A-070$B","S1A-063$A","S1A-019$C","S1A-022$B","S1A-077$A","S1A-053$D","S1A-053$A","S1A-037$C","S1A-096$A","S1A-082$C","S1A-079$A","S1A-082$G","S1A-039$B","S1A-053$D","S1A-072$A","S1A-043$B","S1A-058$A","S1A-078$B","S1A-072$A","S1A-021$F","S1A-053$A","S1A-038$A","S1A-063$A","S1A-039$A","S1A-084$B","S1A-061$A","S1A-060$A","S1A-010$D","S1A-055$A","S1A-051$D","S1A-092$A","S1A-079$A","S1A-066$B","S1A-052$C","S1A-091$D","S1A-084$C","S1A-002$C","S1A-054$C","S1A-071$B","S1A-064$D","S1A-069$A","S1A-079$B","S1A-002$D","S1A-007$E","S1A-005$B","S1A-053$D","S1A-023$B","S1A-069$B","S1A-082$B","S1A-009$B","S1A-008$D","S1A-018$D","S1A-061$A","S1A-096$A","S1A-040$D","S1A-071$C","S1A-051$E","S1A-094$B","S1A-004$A","S1A-074$C","S1A-003$B","S1A-054$A","S1A-090$B","S1A-071$A","S1A-019$A","S1A-073$B","S1A-079$B","S1A-082$E","S1A-077$D","S1A-056$B","S1A-023$B","S1A-001$B","S1A-056$B","S1A-070$A","S1A-066$B","S1A-053$A","S1A-006$B","S1A-079$B","S1A-036$B","S1A-054$C","S1A-007$D","S1A-083$A","S1A-056$A","S1A-022$A","S1A-094$A","S1A-038$A","S1A-040$D","S1A-086$B","S1A-098$A","S1A-052$B","S1A-098$A","S1A-092$A","S1A-020$A","S1A-050$A","S1A-018$D","S1A-002$D","S1A-004$C","S1A-066$A","S1A-055$D","S1A-092$A","S1A-066$C","S1A-002$D","S1A-045$B","S1A-045$B","S1A-019$B","S1A-037$D","S1A-049$A","S1A-051$D","S1A-054$A","S1A-077$C","S1A-070$A","S1A-055$A","S1A-066$C","S1A-004$A","S1A-033$B","S1A-096$A","S1A-053$D","S1A-093$B","S1A-086$B","S1A-059$A","S1A-055$E","S1A-093$A","S1A-051$E","S1A-054$D","S1A-038$A","S1A-023$B","S1A-033$B","S1A-091$C","S1A-059$A","S1A-076$A","S1A-099$A","S1A-035$B","S1A-090$B","S1A-008$A","S1A-005$B","S1A-087$B","S1A-035$A","S1A-080$A","S1A-005$B","S1A-087$B","S1A-045$B","S1A-099$B","S1A-038$B","S1A-086$A","S1A-086$A","S1A-055$B","S1A-001$B","S1A-066$C","S1A-035$B","S1A-072$B","S1A-095$A","S1A-052$B","S1A-022$A","S1A-069$C","S1A-091$A"],["Women","Women","Women","Women","Women","Men","Women","Women","Women","Men","Women","Women","Men","Women","Women","Women","Women","Women","Women","Women","Women","Women","Men","Men","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Men","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Men","Women","Women","Women","Women","Women","Women","Women","Women","Women","Men","Women","Men","Women","Women","Women","Women","Women","Women","Men","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Men","Women","Women","Women","Men","Women","Women","Women","Men","Women","Men","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Men","Women","Men","Women","Women","Women","Women","Women","Women","Women","Women","Women","Men","Women","Women","Women","Women","Men","Women","Women","Women","Women","Women","Men","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Men","Men","Women","Women","Women","Women","Women","Men","Women","Women","Women","Women","Men","Women","Women","Women","Men","Women","Women","Women","Men","Women","Women","Women","Women","Women","Women","Women","Women","Men","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Men","Women","Men","Women","Women","Women","Women","Women","Men","Men","Men","Women","Women","Women","Women","Women","Women","Men","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Men","Women","Women","Women","Women","Men","Women","Women","Women","Men","Women","Women","Women","Men","Women","Men","Women","Men","Women","Men","Women","Women","Men","Men","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Men","Women","Women","Women","Women","Women","Women","Men","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Men","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Men","Women","Women","Men","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Men","Women","Women","Men","Women","Women","Women","Women","Women","Women","Women","Women","Women","Men","Women","Women","Women","Women","Women","Men","Men","Women","Women","Men","Women","Women","Men","Women","Men","Women","Women","Women","Women","Women","Women","Men","Women","Women","Women","Women","Women","Women","Women","Men","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Men","Women","Women","Women","Women","Women","Women","Women","Men","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Men","Women","Women","Men","Women","Men","Women","Women","Women","Men","Women","Women","Men","Women","Women","Women","Men","Women","Women","Women","Women","Women","Women","Men","Women","Women","Men","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Men","Men","Women","Women","Men","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Men","Women","Men","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Men","Women","Men","Women","Women","Women","Men","Women","Women","Women","Women","Women","Women","Women","Men","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Men","Men","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Men","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Men","Women","Women","Women","Women","Women","Women","Women","Men","Women","Women","Women","Women","Women","Women","Women","Men","Women","Women","Women","Women","Women","Women","Men","Women","Men","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Men","Men","Women","Women","Women","Men","Women","Women","Women","Women","Men","Women","Women","Women","Women","Women","Women","Women","Women","Women","Men","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Men","Men","Women","Women","Women","Women","Women","Women","Men","Women","Women","Women","Women","Women","Women","Women","Women","Women","Men","Women","Men","Women","Men","Women","Women","Women","Women","Women","Women","Women","Women","Men","Women","Women","Women","Women","Men","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Men","Women","Women","Men","Women","Women","Women","Women","Women","Women","Women","Women","Women","Men","Men","Women","Women","Women","Men","Women","Men","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Men","Women","Women","Women","Women","Men","Women","Women","Women","Women","Women","Women","Women","Men","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Men","Women","Women","Men","Women","Women","Women","Women","Women","Women","Men","Women","Men","Women","Women","Women","Women","Men","Women","Women","Women","Men","Women","Women","Women","Women","Women","Women","Women","Men","Women","Women","Women","Women","Women","Men","Women","Women","Women","Women","Men","Women","Men","Women","Women","Women","Men","Women","Women","Women","Women","Women","Women","Men","Women","Men","Women","Women","Women","Women","Women","Men","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Men","Women","Men","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Men","Women","Women","Women","Women","Women","Men","Women","Women","Women","Women","Women","Women","Women","Women","Women","Men","Women","Women","Men","Women","Women","Men","Women","Women","Women","Women","Men","Men","Women","Women","Women","Women","Women","Women","Women","Women","Men","Women","Men","Women","Women","Women","Women","Women","Women","Men","Women","Women","Men","Women","Women","Women","Women","Women","Women","Men","Women","Women","Women","Women","Women","Women","Women","Women","Men","Men","Men","Women","Women","Men","Men","Women","Women","Women","Women","Women","Men","Men","Men","Women","Women","Women","Women","Women","Women","Women","Men","Women","Women","Women","Women","Men","Women","Women","Women","Women","Women","Men","Women","Women","Women","Women","Women","Women","Men","Women","Women","Women","Women","Men","Men","Women","Women","Men","Men","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Men","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Men","Men","Women","Women","Men","Women","Women","Women","Women","Women","Women","Women","Women","Women","Men","Women","Men","Women","Women","Women","Women","Women","Women","Women","Women","Women","Men","Men","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Men","Women","Women","Women","Women","Women","Men","Women","Women","Women","Women","Women","Women","Women","Men","Women","Women","Women","Women","Women","Women","Men","Women","Women","Women","Women","Men","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Men","Men","Men","Women","Women","Men","Women","Women","Women","Women","Women","Men","Women","Women","Women","Women","Men","Women","Women","Women","Women","Women","Women","Men","Women","Women","Women","Women","Men","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Men","Women","Women","Women","Men","Women","Women","Women","Men","Women","Women","Men","Men","Women","Women","Women","Women","Women","Men","Women","Women","Women","Women","Women","Women","Women","Men","Women","Women","Women","Women","Women","Women","Men","Women","Women","Women","Men","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Men","Women","Women","Men","Women","Women","Men","Women","Women","Women","Women","Men","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Men","Men","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Men","Women","Women","Men","Women","Women","Men","Women","Women","Women","Men","Women","Women","Women","Women","Women","Women","Women","Women","Women","Men","Women","Women","Men","Men","Men","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Men","Women","Women","Women","Women","Women","Women","Women","Women","Men","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Men","Women","Men","Women","Men","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Men","Men","Women","Women","Women","Men","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Men","Women","Women","Men","Women","Women","Women","Women","Men","Women","Men","Women","Men","Women","Men","Women","Women","Women","Men","Women","Women","Women","Women","Women","Men","Women","Women","Women","Women","Women","Women","Women","Women","Men","Women","Women","Women","Women","Women","Men","Men","Women","Women","Women","Women","Women","Women","Women","Women","Men","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Men","Women","Women","Men","Men","Women","Women","Women","Women","Women","Women","Women","Women","Women","Men","Men","Women","Women","Women","Women","Women","Women","Men","Men","Men","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Men","Women","Women","Women","Women","Men","Women","Men","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Men","Women","Men","Women","Men","Women","Women","Women","Women","Women","Women","Men","Women","Women","Women","Men","Women","Women","Women","Men","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Men","Women","Women","Men","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Men","Women","Women","Women","Women","Women","Women","Women","Women","Men","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Men","Women","Men","Women","Men","Women","Women","Men","Women","Women","Women","Women","Women","Women","Women","Men","Women","Women","Women","Men","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Men","Women","Women","Men","Women","Women","Women","Women","Women","Women","Men","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Men","Women","Women","Women","Men","Men","Women","Women","Women","Women","Women","Women","Women","Women","Men","Women","Women","Men","Women","Women","Women","Women","Women","Women","Women","Men","Women","Women","Women","Women","Women","Women","Men","Women","Men","Women","Women","Men","Women","Men","Women","Women","Women","Women","Women","Men","Women","Women","Women","Women","Men","Women","Men","Women","Women","Women","Men","Women","Women","Men","Women","Women","Men","Men","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Men","Women","Women","Women","Women","Women","Women","Women","Men","Women","Women","Men","Women","Men","Women","Men","Women","Women","Women","Women","Women","Men","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Men","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Men","Women","Women","Women","Women","Women","Women","Women","Women","Men","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Men","Women","Women","Women","Men","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Men","Women","Women","Women","Women","Women","Women","Women","Men","Women","Men","Women","Women","Women","Men","Women","Women","Women","Women","Men","Women","Women","Men","Women","Women","Men","Women","Men","Women","Men","Men","Women","Women","Women","Men","Women","Men","Women","Men","Women","Men","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Men","Women","Women","Men","Women","Women","Men","Women","Women","Women","Women","Men","Women","Men","Women","Women","Men","Women","Women","Women","Women","Men","Women","Women","Men","Women","Men","Women","Men","Women","Women","Women","Men","Women","Women","Women","Men","Women","Women","Women","Men","Women","Women","Men","Women","Women","Women","Women","Women","Women","Women","Women","Women","Men","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Men","Women","Women","Women","Women","Women","Women","Women","Men","Women","Women","Women","Women","Women","Women","Women","Women","Men","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Men","Women","Women","Women","Women","Women","Men","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Men","Men","Men","Women","Men","Men","Women","Women","Women","Women","Women","Women","Women","Women","Women","Men","Women","Men","Women","Women","Women"],["Young","Young","Young","Young","Old","Young","Young","Young","Young","Old","Young","Young","Old","Young","Young","Old","Young","Old","Young","Young","Young","Young","Young","Young","Young","Young","Young","Old","Young","Young","Old","Young","Young","Young","Old","Young","Young","Young","Young","Young","Old","Young","Young","Young","Young","Young","Young","Old","Old","Old","Young","Young","Young","Young","Young","Young","Old","Young","Old","Young","Young","Young","Young","Old","Old","Young","Old","Young","Young","Young","Young","Young","Young","Young","Young","Young","Old","Old","Young","Young","Young","Old","Young","Old","Old","Young","Young","Young","Young","Young","Young","Old","Old","Young","Young","Young","Old","Young","Young","Old","Young","Young","Young","Old","Young","Young","Young","Young","Young","Old","Young","Old","Young","Young","Old","Young","Young","Young","Young","Young","Young","Young","Young","Old","Young","Young","Young","Young","Young","Young","Young","Old","Young","Young","Old","Old","Old","Young","Old","Young","Young","Young","Young","Young","Young","Old","Young","Young","Young","Old","Old","Young","Young","Young","Old","Young","Old","Young","Young","Young","Young","Young","Old","Young","Young","Old","Young","Old","Young","Young","Young","Young","Young","Young","Old","Young","Old","Old","Young","Old","Young","Young","Young","Young","Young","Young","Young","Young","Young","Old","Young","Young","Young","Young","Young","Young","Young","Young","Young","Old","Young","Old","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Old","Young","Young","Young","Young","Young","Young","Young","Young","Old","Young","Young","Young","Young","Old","Old","Young","Young","Young","Old","Young","Young","Young","Young","Old","Old","Young","Young","Young","Young","Young","Young","Old","Young","Young","Young","Young","Young","Young","Old","Young","Young","Young","Young","Young","Young","Young","Young","Young","Old","Young","Young","Young","Old","Old","Young","Old","Old","Young","Young","Young","Young","Young","Old","Young","Young","Young","Young","Young","Old","Young","Young","Young","Young","Young","Young","Old","Young","Old","Young","Old","Young","Young","Young","Young","Young","Young","Old","Young","Young","Young","Young","Young","Old","Old","Young","Old","Young","Young","Young","Young","Young","Old","Young","Young","Young","Young","Young","Young","Young","Young","Young","Old","Young","Young","Young","Young","Young","Young","Young","Young","Old","Young","Young","Young","Old","Old","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Old","Young","Old","Young","Young","Old","Young","Young","Young","Old","Young","Young","Young","Young","Young","Young","Young","Old","Young","Old","Young","Young","Young","Old","Old","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Old","Young","Young","Young","Old","Young","Young","Young","Young","Young","Young","Young","Young","Old","Young","Young","Young","Old","Old","Young","Young","Young","Old","Young","Old","Old","Young","Young","Young","Old","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Old","Young","Young","Young","Young","Young","Old","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Old","Old","Young","Young","Young","Old","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Old","Young","Young","Young","Young","Old","Young","Young","Old","Young","Young","Young","Young","Young","Old","Young","Young","Young","Old","Young","Young","Young","Young","Old","Old","Young","Old","Old","Young","Young","Young","Old","Young","Young","Old","Old","Young","Old","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Old","Old","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Old","Young","Old","Old","Young","Old","Young","Young","Young","Young","Young","Old","Young","Young","Young","Young","Young","Old","Old","Young","Old","Old","Young","Young","Young","Old","Young","Young","Young","Young","Young","Old","Young","Old","Old","Old","Young","Young","Young","Young","Old","Old","Young","Young","Old","Young","Young","Old","Young","Young","Old","Young","Young","Old","Old","Old","Young","Young","Young","Young","Old","Young","Old","Old","Young","Young","Young","Young","Old","Young","Young","Young","Young","Old","Young","Young","Young","Young","Young","Young","Young","Young","Young","Old","Young","Old","Old","Young","Young","Young","Young","Young","Young","Young","Young","Old","Old","Young","Old","Young","Young","Old","Young","Old","Young","Young","Young","Young","Young","Young","Young","Young","Young","Old","Young","Young","Young","Old","Young","Old","Young","Old","Young","Old","Young","Young","Young","Young","Young","Young","Old","Young","Young","Young","Young","Young","Old","Young","Young","Young","Old","Old","Young","Old","Young","Old","Young","Young","Young","Young","Old","Young","Young","Old","Young","Young","Young","Young","Young","Old","Young","Young","Young","Young","Young","Young","Old","Old","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Old","Young","Old","Young","Young","Young","Young","Young","Old","Young","Young","Young","Young","Old","Old","Young","Old","Young","Young","Young","Old","Old","Young","Young","Young","Young","Young","Young","Young","Old","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Old","Young","Old","Young","Young","Young","Old","Young","Old","Young","Old","Young","Young","Old","Old","Old","Young","Old","Young","Young","Old","Young","Young","Young","Old","Young","Young","Young","Young","Young","Old","Young","Young","Old","Young","Young","Young","Old","Young","Young","Young","Young","Old","Young","Young","Old","Young","Young","Old","Young","Young","Young","Young","Young","Young","Young","Young","Old","Young","Young","Young","Young","Young","Young","Young","Young","Young","Old","Young","Young","Old","Young","Old","Young","Old","Young","Young","Young","Old","Old","Young","Old","Young","Old","Young","Old","Young","Young","Old","Young","Young","Young","Young","Young","Old","Young","Old","Young","Young","Young","Old","Old","Young","Old","Old","Young","Young","Old","Old","Old","Old","Old","Young","Young","Young","Young","Old","Young","Young","Old","Young","Young","Young","Young","Young","Young","Old","Young","Young","Young","Young","Young","Old","Young","Young","Young","Young","Young","Young","Old","Young","Young","Young","Young","Young","Young","Young","Young","Old","Young","Young","Young","Old","Young","Old","Young","Young","Young","Old","Young","Young","Young","Young","Young","Old","Old","Young","Young","Old","Young","Young","Old","Old","Old","Old","Young","Young","Young","Young","Young","Old","Young","Young","Old","Young","Young","Young","Young","Young","Young","Young","Old","Young","Young","Young","Young","Young","Young","Young","Young","Young","Old","Old","Old","Young","Old","Young","Young","Old","Old","Old","Old","Young","Young","Old","Young","Young","Young","Young","Old","Young","Young","Young","Young","Young","Young","Young","Young","Old","Young","Young","Young","Young","Young","Young","Old","Old","Young","Old","Young","Old","Young","Old","Old","Young","Young","Old","Old","Young","Young","Young","Young","Young","Young","Old","Young","Young","Old","Young","Young","Old","Old","Young","Young","Young","Young","Young","Young","Young","Old","Young","Young","Young","Young","Old","Young","Young","Young","Old","Young","Young","Old","Young","Old","Young","Young","Young","Young","Young","Young","Old","Young","Young","Young","Old","Young","Young","Old","Young","Old","Old","Old","Young","Young","Old","Young","Old","Old","Old","Old","Young","Young","Young","Young","Young","Young","Old","Young","Young","Young","Old","Young","Young","Old","Old","Young","Young","Old","Young","Young","Old","Young","Young","Young","Old","Young","Young","Young","Old","Old","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Old","Young","Young","Young","Old","Old","Young","Young","Old","Young","Young","Old","Young","Old","Young","Young","Young","Young","Young","Old","Young","Young","Young","Young","Young","Old","Old","Old","Young","Young","Young","Old","Old","Young","Young","Old","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Old","Young","Young","Young","Old","Young","Old","Young","Old","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Old","Old","Young","Young","Young","Old","Young","Old","Young","Young","Young","Old","Young","Young","Young","Young","Young","Young","Young","Young","Young","Old","Young","Young","Old","Young","Young","Old","Young","Old","Old","Young","Young","Young","Old","Young","Old","Young","Young","Young","Young","Young","Old","Young","Young","Young","Young","Young","Old","Old","Old","Young","Young","Old","Young","Young","Old","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Old","Young","Young","Old","Young","Old","Young","Young","Young","Young","Young","Old","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Old","Young","Young","Young","Old","Young","Old","Young","Old","Old","Young","Old","Old","Young","Old","Young","Young","Young","Young","Young","Young","Young","Old","Old","Young","Young","Young","Young","Old","Old","Young","Old","Young","Young","Young","Young","Young","Old","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Old","Young","Young","Young","Young","Young","Old","Young","Young","Old","Young","Young","Young","Young","Young","Young","Old","Young","Young","Old","Old","Young","Young","Young","Young","Old","Old","Young","Old","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Old","Young","Old","Young","Young","Young","Old","Old","Young","Young","Young","Young","Old","Old","Old","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Old","Young","Old","Young","Young","Old","Young","Young","Young","Young","Young","Young","Old","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Old","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Old","Young","Old","Young","Young","Young","Young","Young","Old","Old","Young","Young","Young","Young","Old","Young","Old","Young","Young","Old","Young","Young","Young","Young","Young","Young","Old","Young","Young","Young","Young","Old","Old","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Old","Young","Young","Young","Young","Young","Old","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Old","Young","Old","Old","Young","Young","Old","Young","Young","Young","Old","Young","Young","Young","Young","Young","Young","Old","Young","Young","Young","Old","Young","Young","Young","Young","Young","Young","Old","Young","Young","Old","Old","Old","Old","Young","Old","Old","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Old","Young","Old","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Old","Young","Young","Young","Young","Young","Young","Young","Young","Old","Old","Young","Young","Young","Young","Young","Young","Young","Young","Old","Young","Old","Young","Old","Young","Young","Old","Young","Young","Old","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Old","Old","Young","Old","Old","Young","Young","Young","Young","Young","Young","Young","Old","Old","Young","Young","Young","Young","Young","Young","Young","Young","Old","Young","Young","Young","Young","Young","Old","Young","Young","Old","Old","Young","Young","Young","Young","Young","Young","Old","Young","Old","Young","Young","Young","Young","Young","Young","Young","Old","Old","Young","Old","Young","Young","Young","Old","Old","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Old","Old","Young","Young","Young","Young","Young","Young","Old","Young","Young","Young","Old","Young","Young","Old","Young","Young","Old","Young","Young","Young","Young","Old","Young","Old","Young","Young","Young","Young","Young","Young","Old","Young","Young","Young","Young","Young","Young","Old","Young","Young","Young","Young","Young","Young","Old","Young","Young","Young","Young","Old","Young","Old","Young","Young","Young","Old","Young","Young","Young","Young","Old","Old","Young","Young","Old","Old","Young","Old","Young","Young","Old","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Old","Young","Young","Old","Old","Young","Young","Young","Old","Young","Young","Young","Old","Old","Old","Young","Young","Young","Young","Young","Young","Old","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Old","Young","Young","Young","Young","Old","Young","Old","Old","Young","Old","Young","Young","Young","Old","Young","Old","Young","Young","Old","Old","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Old","Young","Young","Old","Young","Young","Young","Young","Young","Young","Young","Old","Young","Young","Young","Young","Young","Old","Young","Young","Young","Young","Young","Young","Old","Old","Young","Old","Old","Young","Young","Young","Young","Old","Young","Young","Young","Young","Young","Young","Old","Young","Young","Young","Young","Young","Old","Young","Young","Young","Young","Old","Young","Young","Young","Young","Old","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Old","Old","Young","Old","Young","Young","Old","Young","Young","Young","Old","Young","Young","Young","Old","Young","Old","Young","Old","Old","Old","Young","Old","Young","Young","Old","Young","Young","Young","Young","Young","Old","Young","Young","Young","Young","Young","Young","Old","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Old","Old","Young","Young","Young","Young","Young","Young","Young","Young"],["MixedGender","MixedGender","SameGender","MixedGender","SameGender","MixedGender","MixedGender","SameGender","SameGender","MixedGender","SameGender","SameGender","MixedGender","SameGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","SameGender","MixedGender","SameGender","MixedGender","MixedGender","SameGender","SameGender","MixedGender","SameGender","MixedGender","SameGender","MixedGender","MixedGender","SameGender","MixedGender","SameGender","MixedGender","SameGender","SameGender","SameGender","SameGender","MixedGender","MixedGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","MixedGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","MixedGender","MixedGender","MixedGender","SameGender","SameGender","MixedGender","MixedGender","MixedGender","MixedGender","SameGender","SameGender","MixedGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","MixedGender","MixedGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","MixedGender","SameGender","SameGender","MixedGender","SameGender","SameGender","SameGender","MixedGender","MixedGender","SameGender","SameGender","SameGender","SameGender","MixedGender","SameGender","SameGender","MixedGender","SameGender","MixedGender","SameGender","SameGender","SameGender","SameGender","SameGender","MixedGender","SameGender","MixedGender","MixedGender","MixedGender","MixedGender","SameGender","SameGender","SameGender","SameGender","SameGender","MixedGender","MixedGender","SameGender","SameGender","SameGender","SameGender","MixedGender","SameGender","MixedGender","SameGender","SameGender","SameGender","MixedGender","SameGender","SameGender","MixedGender","SameGender","SameGender","MixedGender","MixedGender","MixedGender","SameGender","SameGender","MixedGender","MixedGender","SameGender","MixedGender","MixedGender","MixedGender","MixedGender","SameGender","SameGender","MixedGender","MixedGender","SameGender","SameGender","SameGender","SameGender","MixedGender","SameGender","SameGender","SameGender","MixedGender","MixedGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","MixedGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","MixedGender","SameGender","MixedGender","SameGender","SameGender","SameGender","SameGender","MixedGender","SameGender","SameGender","SameGender","MixedGender","SameGender","MixedGender","SameGender","MixedGender","SameGender","SameGender","SameGender","MixedGender","SameGender","MixedGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","MixedGender","SameGender","SameGender","MixedGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","MixedGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","MixedGender","SameGender","MixedGender","MixedGender","MixedGender","MixedGender","SameGender","SameGender","MixedGender","SameGender","MixedGender","SameGender","SameGender","MixedGender","MixedGender","SameGender","SameGender","MixedGender","MixedGender","SameGender","SameGender","SameGender","MixedGender","SameGender","MixedGender","MixedGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","MixedGender","SameGender","MixedGender","SameGender","MixedGender","MixedGender","SameGender","MixedGender","SameGender","MixedGender","SameGender","SameGender","SameGender","MixedGender","MixedGender","SameGender","MixedGender","SameGender","SameGender","SameGender","SameGender","SameGender","MixedGender","MixedGender","MixedGender","SameGender","SameGender","MixedGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","MixedGender","SameGender","MixedGender","SameGender","SameGender","SameGender","MixedGender","SameGender","SameGender","MixedGender","MixedGender","MixedGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","MixedGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","MixedGender","SameGender","SameGender","MixedGender","SameGender","MixedGender","SameGender","SameGender","SameGender","MixedGender","SameGender","MixedGender","SameGender","MixedGender","SameGender","MixedGender","MixedGender","SameGender","SameGender","MixedGender","SameGender","MixedGender","SameGender","MixedGender","MixedGender","SameGender","SameGender","MixedGender","MixedGender","SameGender","SameGender","SameGender","MixedGender","MixedGender","SameGender","SameGender","SameGender","MixedGender","MixedGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","MixedGender","SameGender","MixedGender","MixedGender","SameGender","SameGender","SameGender","MixedGender","SameGender","SameGender","SameGender","MixedGender","MixedGender","MixedGender","SameGender","SameGender","SameGender","SameGender","MixedGender","MixedGender","SameGender","MixedGender","MixedGender","SameGender","SameGender","MixedGender","MixedGender","MixedGender","SameGender","SameGender","SameGender","MixedGender","SameGender","SameGender","SameGender","MixedGender","SameGender","SameGender","MixedGender","MixedGender","SameGender","MixedGender","SameGender","SameGender","SameGender","MixedGender","SameGender","SameGender","MixedGender","SameGender","MixedGender","MixedGender","SameGender","SameGender","SameGender","SameGender","MixedGender","SameGender","SameGender","SameGender","MixedGender","SameGender","MixedGender","MixedGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","MixedGender","MixedGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","MixedGender","SameGender","SameGender","SameGender","SameGender","SameGender","MixedGender","SameGender","SameGender","MixedGender","MixedGender","SameGender","SameGender","SameGender","SameGender","MixedGender","SameGender","SameGender","SameGender","SameGender","MixedGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","MixedGender","MixedGender","MixedGender","MixedGender","SameGender","SameGender","MixedGender","SameGender","SameGender","MixedGender","MixedGender","SameGender","SameGender","MixedGender","MixedGender","MixedGender","SameGender","SameGender","MixedGender","SameGender","SameGender","MixedGender","SameGender","SameGender","MixedGender","SameGender","MixedGender","SameGender","MixedGender","SameGender","SameGender","MixedGender","SameGender","SameGender","MixedGender","MixedGender","SameGender","MixedGender","SameGender","SameGender","MixedGender","SameGender","MixedGender","SameGender","SameGender","SameGender","MixedGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","MixedGender","SameGender","SameGender","SameGender","MixedGender","MixedGender","SameGender","SameGender","SameGender","SameGender","MixedGender","SameGender","MixedGender","SameGender","SameGender","SameGender","SameGender","SameGender","MixedGender","MixedGender","SameGender","MixedGender","SameGender","MixedGender","SameGender","MixedGender","SameGender","MixedGender","MixedGender","SameGender","MixedGender","SameGender","SameGender","SameGender","MixedGender","SameGender","SameGender","SameGender","SameGender","MixedGender","SameGender","SameGender","MixedGender","SameGender","SameGender","MixedGender","MixedGender","SameGender","SameGender","MixedGender","SameGender","MixedGender","SameGender","SameGender","MixedGender","SameGender","SameGender","SameGender","SameGender","MixedGender","SameGender","SameGender","SameGender","MixedGender","SameGender","MixedGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","MixedGender","SameGender","MixedGender","MixedGender","MixedGender","MixedGender","SameGender","SameGender","MixedGender","SameGender","SameGender","SameGender","MixedGender","SameGender","MixedGender","MixedGender","SameGender","SameGender","SameGender","SameGender","SameGender","MixedGender","MixedGender","SameGender","SameGender","SameGender","SameGender","MixedGender","MixedGender","SameGender","SameGender","MixedGender","SameGender","SameGender","MixedGender","SameGender","MixedGender","SameGender","SameGender","MixedGender","MixedGender","SameGender","SameGender","MixedGender","MixedGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","MixedGender","SameGender","MixedGender","SameGender","SameGender","MixedGender","MixedGender","SameGender","SameGender","SameGender","SameGender","MixedGender","SameGender","SameGender","SameGender","MixedGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","MixedGender","MixedGender","SameGender","MixedGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","MixedGender","MixedGender","SameGender","SameGender","SameGender","MixedGender","SameGender","SameGender","MixedGender","SameGender","SameGender","SameGender","SameGender","MixedGender","SameGender","SameGender","SameGender","SameGender","MixedGender","MixedGender","SameGender","MixedGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","MixedGender","SameGender","SameGender","MixedGender","SameGender","SameGender","MixedGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","MixedGender","MixedGender","MixedGender","SameGender","MixedGender","SameGender","SameGender","SameGender","MixedGender","SameGender","MixedGender","MixedGender","SameGender","SameGender","SameGender","MixedGender","MixedGender","SameGender","MixedGender","SameGender","MixedGender","MixedGender","SameGender","MixedGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","MixedGender","MixedGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","MixedGender","SameGender","SameGender","SameGender","SameGender","MixedGender","SameGender","SameGender","MixedGender","SameGender","MixedGender","MixedGender","SameGender","SameGender","SameGender","MixedGender","SameGender","MixedGender","SameGender","SameGender","SameGender","SameGender","MixedGender","SameGender","SameGender","MixedGender","MixedGender","SameGender","SameGender","MixedGender","SameGender","SameGender","MixedGender","SameGender","MixedGender","SameGender","SameGender","SameGender","MixedGender","SameGender","MixedGender","SameGender","SameGender","MixedGender","SameGender","MixedGender","SameGender","MixedGender","MixedGender","MixedGender","SameGender","MixedGender","SameGender","MixedGender","MixedGender","MixedGender","SameGender","MixedGender","MixedGender","SameGender","MixedGender","MixedGender","SameGender","SameGender","SameGender","SameGender","MixedGender","SameGender","SameGender","MixedGender","SameGender","MixedGender","MixedGender","MixedGender","SameGender","SameGender","MixedGender","SameGender","MixedGender","MixedGender","SameGender","MixedGender","SameGender","MixedGender","SameGender","MixedGender","SameGender","SameGender","SameGender","MixedGender","MixedGender","MixedGender","SameGender","MixedGender","SameGender","SameGender","SameGender","MixedGender","SameGender","SameGender","MixedGender","SameGender","MixedGender","SameGender","SameGender","MixedGender","SameGender","SameGender","MixedGender","MixedGender","SameGender","SameGender","SameGender","MixedGender","SameGender","SameGender","SameGender","SameGender","SameGender","MixedGender","SameGender","SameGender","SameGender","MixedGender","MixedGender","SameGender","MixedGender","SameGender","SameGender","MixedGender","MixedGender","SameGender","SameGender","SameGender","SameGender","MixedGender","MixedGender","MixedGender","SameGender","MixedGender","SameGender","SameGender","SameGender","SameGender","MixedGender","MixedGender","MixedGender","SameGender","SameGender","SameGender","SameGender","SameGender","MixedGender","MixedGender","SameGender","SameGender","MixedGender","SameGender","SameGender","MixedGender","SameGender","MixedGender","MixedGender","MixedGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","MixedGender","MixedGender","MixedGender","MixedGender","SameGender","MixedGender","MixedGender","SameGender","MixedGender","SameGender","SameGender","SameGender","MixedGender","MixedGender","MixedGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","MixedGender","SameGender","SameGender","SameGender","SameGender","MixedGender","SameGender","MixedGender","SameGender","MixedGender","SameGender","MixedGender","MixedGender","SameGender","SameGender","SameGender","SameGender","SameGender","MixedGender","SameGender","MixedGender","MixedGender","SameGender","MixedGender","SameGender","SameGender","SameGender","MixedGender","MixedGender","MixedGender","SameGender","MixedGender","SameGender","SameGender","SameGender","MixedGender","MixedGender","SameGender","MixedGender","SameGender","SameGender","MixedGender","MixedGender","SameGender","MixedGender","SameGender","SameGender","SameGender","SameGender","SameGender","MixedGender","MixedGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","MixedGender","SameGender","SameGender","SameGender","SameGender","SameGender","MixedGender","MixedGender","MixedGender","SameGender","MixedGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","MixedGender","SameGender","MixedGender","MixedGender","MixedGender","SameGender","SameGender","SameGender","MixedGender","SameGender","SameGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","SameGender","SameGender","SameGender","MixedGender","SameGender","SameGender","MixedGender","SameGender","SameGender","MixedGender","SameGender","SameGender","SameGender","MixedGender","SameGender","MixedGender","MixedGender","SameGender","SameGender","MixedGender","MixedGender","MixedGender","SameGender","SameGender","MixedGender","SameGender","MixedGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","MixedGender","SameGender","SameGender","SameGender","SameGender","MixedGender","MixedGender","MixedGender","SameGender","SameGender","MixedGender","SameGender","MixedGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","MixedGender","MixedGender","SameGender","MixedGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","MixedGender","SameGender","SameGender","SameGender","SameGender","SameGender","MixedGender","MixedGender","MixedGender","SameGender","SameGender","SameGender","MixedGender","SameGender","SameGender","MixedGender","SameGender","SameGender","MixedGender","SameGender","MixedGender","SameGender","SameGender","SameGender","SameGender","MixedGender","MixedGender","SameGender","SameGender","SameGender","SameGender","MixedGender","SameGender","SameGender","SameGender","SameGender","MixedGender","SameGender","MixedGender","SameGender","MixedGender","SameGender","MixedGender","MixedGender","SameGender","SameGender","MixedGender","MixedGender","SameGender","MixedGender","SameGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","SameGender","MixedGender","SameGender","SameGender","SameGender","MixedGender","SameGender","SameGender","MixedGender","MixedGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","MixedGender","SameGender","MixedGender","SameGender","MixedGender","MixedGender","SameGender","SameGender","SameGender","MixedGender","MixedGender","SameGender","SameGender","MixedGender","MixedGender","SameGender","MixedGender","MixedGender","SameGender","MixedGender","MixedGender","SameGender","MixedGender","SameGender","SameGender","SameGender","MixedGender","MixedGender","SameGender","MixedGender","SameGender","SameGender","SameGender","MixedGender","MixedGender","SameGender","SameGender","SameGender","SameGender","MixedGender","SameGender","SameGender","MixedGender","MixedGender","SameGender","SameGender","MixedGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","MixedGender","SameGender","SameGender","MixedGender","SameGender","SameGender","SameGender","SameGender","MixedGender","SameGender","MixedGender","MixedGender","SameGender","MixedGender","SameGender","MixedGender","SameGender","MixedGender","SameGender","SameGender","MixedGender","SameGender","SameGender","MixedGender","SameGender","MixedGender","MixedGender","SameGender","MixedGender","SameGender","SameGender","SameGender","SameGender","SameGender","MixedGender","SameGender","SameGender","SameGender","SameGender","MixedGender","SameGender","SameGender","MixedGender","MixedGender","SameGender","MixedGender","MixedGender","SameGender","SameGender","SameGender","SameGender","MixedGender","SameGender","SameGender","MixedGender","SameGender","MixedGender","MixedGender","SameGender","SameGender","SameGender","MixedGender","SameGender","SameGender","SameGender","SameGender","SameGender","MixedGender","MixedGender","MixedGender","MixedGender","SameGender","SameGender","SameGender","MixedGender","SameGender","MixedGender","SameGender","MixedGender","SameGender","SameGender","SameGender","MixedGender","MixedGender","MixedGender","SameGender","SameGender","MixedGender","SameGender","MixedGender","SameGender","SameGender","SameGender","SameGender","SameGender","MixedGender","MixedGender","SameGender","MixedGender","SameGender","SameGender","MixedGender","SameGender","SameGender","SameGender","MixedGender","SameGender","SameGender","MixedGender","SameGender","MixedGender","MixedGender","MixedGender","SameGender","MixedGender","MixedGender","SameGender","SameGender","SameGender","MixedGender","SameGender","MixedGender","SameGender","SameGender","MixedGender","SameGender","MixedGender","MixedGender","MixedGender","SameGender","SameGender","MixedGender","SameGender","MixedGender","MixedGender","SameGender","MixedGender","SameGender","SameGender","MixedGender","MixedGender","SameGender","MixedGender","MixedGender","SameGender","SameGender","SameGender","SameGender","MixedGender","MixedGender","SameGender","MixedGender","SameGender","SameGender","MixedGender","SameGender","MixedGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","MixedGender","SameGender","MixedGender","MixedGender","SameGender","SameGender","MixedGender","MixedGender","SameGender","MixedGender","SameGender","SameGender","SameGender","SameGender","MixedGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","MixedGender","MixedGender","MixedGender","MixedGender","SameGender","SameGender","MixedGender","SameGender","SameGender","MixedGender","MixedGender","SameGender","MixedGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","MixedGender","SameGender","SameGender","SameGender","MixedGender","MixedGender","SameGender","MixedGender","MixedGender","SameGender","SameGender","SameGender","SameGender","SameGender","MixedGender","SameGender","SameGender","SameGender","MixedGender","MixedGender","MixedGender","SameGender","MixedGender","SameGender","MixedGender","SameGender","MixedGender","SameGender","MixedGender","MixedGender","SameGender","SameGender","SameGender","MixedGender","SameGender","SameGender","MixedGender","MixedGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","MixedGender","SameGender","MixedGender","SameGender","SameGender","SameGender","SameGender","MixedGender","MixedGender","MixedGender","MixedGender","SameGender","SameGender","SameGender","SameGender","SameGender","MixedGender","SameGender","MixedGender","MixedGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","MixedGender","SameGender","MixedGender","SameGender","SameGender","SameGender","SameGender","SameGender","MixedGender","MixedGender","SameGender","SameGender","SameGender","MixedGender","MixedGender","SameGender","SameGender","MixedGender","SameGender","SameGender","SameGender","MixedGender","MixedGender","MixedGender","SameGender","MixedGender","SameGender","MixedGender","SameGender","SameGender","SameGender","SameGender","SameGender","MixedGender","SameGender","SameGender","SameGender","MixedGender","SameGender","MixedGender","MixedGender","SameGender","SameGender","SameGender","MixedGender","MixedGender","MixedGender","MixedGender","SameGender","SameGender","SameGender","MixedGender","SameGender","SameGender","SameGender","MixedGender","SameGender","SameGender","MixedGender","SameGender","SameGender","SameGender","MixedGender","MixedGender","MixedGender","SameGender","SameGender","MixedGender","MixedGender","SameGender","MixedGender","MixedGender","MixedGender","SameGender","MixedGender","MixedGender","SameGender","MixedGender","SameGender","SameGender","SameGender","MixedGender","SameGender","MixedGender","MixedGender","MixedGender","SameGender","MixedGender","SameGender","MixedGender","SameGender","MixedGender","MixedGender","SameGender","SameGender","SameGender","MixedGender","MixedGender","SameGender","SameGender","SameGender","SameGender","MixedGender","MixedGender","SameGender","SameGender","SameGender","MixedGender","MixedGender","MixedGender","SameGender","SameGender","MixedGender","SameGender","MixedGender","MixedGender","MixedGender","MixedGender","SameGender","MixedGender","SameGender","SameGender","SameGender","MixedGender","SameGender","MixedGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","MixedGender","SameGender","SameGender","SameGender","MixedGender","MixedGender","MixedGender","SameGender","SameGender","MixedGender","SameGender","MixedGender","SameGender","SameGender","SameGender","MixedGender","MixedGender","SameGender","SameGender","MixedGender","MixedGender","MixedGender","SameGender","MixedGender","MixedGender","SameGender","SameGender","MixedGender","MixedGender","MixedGender","MixedGender","SameGender","SameGender","SameGender","SameGender","MixedGender","SameGender","MixedGender","SameGender","SameGender","SameGender","MixedGender","SameGender","SameGender","MixedGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","MixedGender","SameGender","SameGender","SameGender","SameGender","MixedGender","MixedGender","SameGender","MixedGender","MixedGender","MixedGender","SameGender","MixedGender","MixedGender","SameGender","SameGender","SameGender","MixedGender","MixedGender","SameGender","SameGender","MixedGender","SameGender","SameGender","SameGender","SameGender","MixedGender","MixedGender","SameGender","SameGender","MixedGender","MixedGender","SameGender","MixedGender","SameGender","SameGender","SameGender","SameGender","MixedGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","MixedGender","MixedGender","SameGender","MixedGender","SameGender","SameGender","SameGender","SameGender","SameGender","MixedGender","SameGender","SameGender","MixedGender","SameGender","SameGender","SameGender","SameGender","SameGender","MixedGender","SameGender","MixedGender","SameGender","MixedGender","MixedGender","SameGender","SameGender","SameGender","MixedGender","SameGender","SameGender","MixedGender","MixedGender","SameGender","MixedGender","SameGender","SameGender","SameGender","MixedGender","SameGender","SameGender","SameGender","SameGender","SameGender","MixedGender","SameGender","MixedGender","MixedGender","SameGender","MixedGender","MixedGender","SameGender","MixedGender","SameGender","MixedGender","SameGender","SameGender","SameGender","MixedGender","SameGender","MixedGender","SameGender","SameGender","MixedGender","SameGender","SameGender","SameGender","SameGender","MixedGender","SameGender","SameGender","SameGender","SameGender","MixedGender","MixedGender","MixedGender","SameGender","MixedGender","SameGender","SameGender","SameGender","MixedGender","MixedGender","MixedGender","SameGender","SameGender","MixedGender","MixedGender","SameGender","MixedGender","MixedGender","SameGender","SameGender","SameGender","MixedGender","MixedGender","SameGender","MixedGender","MixedGender","SameGender","MixedGender","SameGender","MixedGender","SameGender","SameGender","MixedGender","MixedGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","MixedGender","SameGender","MixedGender","SameGender","SameGender","MixedGender","SameGender","SameGender","MixedGender","SameGender","SameGender","SameGender","SameGender","MixedGender","MixedGender","SameGender","SameGender","MixedGender","SameGender","SameGender","MixedGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","MixedGender","SameGender","SameGender","SameGender","SameGender","SameGender","MixedGender","SameGender","SameGender","MixedGender","MixedGender","SameGender","SameGender","SameGender","MixedGender","SameGender","SameGender","SameGender","SameGender","SameGender","MixedGender","MixedGender","SameGender","SameGender","MixedGender","MixedGender","SameGender","SameGender","MixedGender","SameGender","SameGender","SameGender","SameGender","SameGender","MixedGender","MixedGender","SameGender","MixedGender","MixedGender","SameGender","MixedGender","MixedGender","MixedGender","SameGender"],["NoPrime","NoPrime","NoPrime","NoPrime","Prime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","Prime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","Prime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","Prime","Prime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","Prime","NoPrime","NoPrime","Prime","Prime","Prime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","Prime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","Prime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","Prime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","Prime","NoPrime","Prime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","Prime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","Prime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","Prime","NoPrime","NoPrime","Prime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime"],[0,0,0,0,0,1,1,0,0,1,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,1,0,0,1,1,1,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,1,0,0,1,1,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,1,0,0,1,0,0,1,0,1,0,0,1,0,1,0,0,1,1,0,0,0,0,0,1,1,0,0,1,0,1,0,0,0,1,0,0,1,0,0,1,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,1,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,1,1,0,0,0,0,1,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,1,1,0,0,0,0,0,0,1,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,1,0,1,0,0,0,1,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,1,0,0,0,1,0,1,0,0,0,0,0,0,1,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,1,1,0,0,1,0,0,0,0,0,0,1,1,0,0,0,0,0,0,1,0,0,0,0,1,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,1,0,1,0,0,0,0,0,0,0,1,1,1,0,0,1,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,1,1,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,1,1,0,1,1,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,1,0,0,1,1,1,0,0,0,0,0,1,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,1,0,0,1,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,1,0,1,0,1,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,1,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,1,0,0,1,0,0,0,0,1,0,0,1,1,0,0,0,1,0,0,0,0,0,0,0,0,1,1,1,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,1,0,0,0,0,0,1,0,1,1,0,1,1,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,1,1,0,1,1,0,0,0,0,1,1,0,0,0,0,0,0,1,0,0,1,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,1,0,1,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th>ID<\/th>\n      <th>Gender<\/th>\n      <th>Age<\/th>\n      <th>ConversationType<\/th>\n      <th>Priming<\/th>\n      <th>SUFlike<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"pageLength":5,"scrollX":true,"columnDefs":[{"className":"dt-right","targets":5}],"order":[],"autoWidth":false,"orderClasses":false,"lengthMenu":[5,10,25,50,100]}},"evals":[],"jsHooks":[]}</script>
<p>As all variables except for the dependent variable (<code>SUFlike</code>) are character strings, we factorize the independent variables.</p>
<pre class="r"><code># def. variables to be factorized
vrs &lt;- c(&quot;ID&quot;, &quot;Age&quot;, &quot;Gender&quot;, &quot;ConversationType&quot;, &quot;Priming&quot;)
# def. vector with variables
fctr &lt;- which(colnames(mblrdata) %in% vrs)     
# factorize variables
mblrdata[,fctr] &lt;- lapply(mblrdata[,fctr], factor)
# relevel Age (Young = Reference)
mblrdata$Age &lt;- relevel(mblrdata$Age, &quot;Young&quot;)
# order data by ID
mblrdata &lt;- mblrdata %&gt;%
  dplyr::arrange(ID)</code></pre>
<div id="htmlwidget-8884ced2907e90a43969" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-8884ced2907e90a43969">{"x":{"filter":"none","data":[["S1A-001$A","S1A-001$A","S1A-001$A","S1A-001$A","S1A-001$A","S1A-001$A","S1A-001$A","S1A-001$A","S1A-001$B","S1A-001$B","S1A-001$B","S1A-001$B","S1A-001$B","S1A-001$B","S1A-001$B","S1A-001$B","S1A-001$B","S1A-001$B","S1A-001$B","S1A-001$B","S1A-001$B","S1A-001$B","S1A-001$B","S1A-001$B","S1A-001$B","S1A-002$A","S1A-002$A","S1A-002$A","S1A-002$A","S1A-002$A","S1A-002$A","S1A-002$A","S1A-002$A","S1A-002$A","S1A-002$A","S1A-002$C","S1A-002$C","S1A-002$C","S1A-002$C","S1A-002$C","S1A-002$D","S1A-002$D","S1A-002$D","S1A-002$D","S1A-002$D","S1A-002$D","S1A-002$D","S1A-002$D","S1A-002$D","S1A-002$D","S1A-002$D","S1A-002$D","S1A-002$D","S1A-002$D","S1A-002$F","S1A-002$F","S1A-002$F","S1A-002$F","S1A-002$F","S1A-002$F","S1A-002$F","S1A-002$F","S1A-003$B","S1A-003$B","S1A-003$B","S1A-003$B","S1A-003$B","S1A-003$C","S1A-003$C","S1A-003$C","S1A-003$C","S1A-003$C","S1A-003$C","S1A-003$C","S1A-003$D","S1A-003$D","S1A-003$D","S1A-003$D","S1A-003$D","S1A-003$E","S1A-003$E","S1A-003$E","S1A-003$E","S1A-003$E","S1A-003$E","S1A-003$E","S1A-003$E","S1A-003$E","S1A-003$E","S1A-003$E","S1A-004$A","S1A-004$A","S1A-004$A","S1A-004$A","S1A-004$A","S1A-004$A","S1A-004$A","S1A-004$A","S1A-004$A","S1A-004$A","S1A-004$A","S1A-004$A","S1A-004$A","S1A-004$A","S1A-004$A","S1A-004$A","S1A-004$A","S1A-004$A","S1A-004$A","S1A-004$A","S1A-004$A","S1A-004$A","S1A-004$B","S1A-004$B","S1A-004$B","S1A-004$B","S1A-004$B","S1A-004$C","S1A-004$C","S1A-004$C","S1A-004$C","S1A-005$A","S1A-005$A","S1A-005$A","S1A-005$A","S1A-005$A","S1A-005$A","S1A-005$A","S1A-005$A","S1A-005$A","S1A-005$A","S1A-005$A","S1A-005$A","S1A-005$A","S1A-005$A","S1A-005$B","S1A-005$B","S1A-005$B","S1A-005$B","S1A-005$B","S1A-005$B","S1A-005$B","S1A-005$B","S1A-005$B","S1A-005$B","S1A-005$B","S1A-005$B","S1A-005$B","S1A-005$B","S1A-005$B","S1A-005$B","S1A-005$B","S1A-005$B","S1A-005$B","S1A-006$A","S1A-006$A","S1A-006$A","S1A-006$A","S1A-006$A","S1A-006$A","S1A-006$A","S1A-006$A","S1A-006$A","S1A-006$A","S1A-006$A","S1A-006$A","S1A-006$A","S1A-006$A","S1A-006$A","S1A-006$B","S1A-006$B","S1A-006$B","S1A-006$B","S1A-006$B","S1A-006$B","S1A-006$B","S1A-006$B","S1A-006$C","S1A-006$C","S1A-006$C","S1A-006$C","S1A-006$C","S1A-006$D","S1A-006$D","S1A-006$E","S1A-006$E","S1A-007$A","S1A-007$A","S1A-007$A","S1A-007$A","S1A-007$A","S1A-007$A","S1A-007$A","S1A-007$A","S1A-007$A","S1A-007$B","S1A-007$B","S1A-007$B","S1A-007$B","S1A-007$B","S1A-007$B","S1A-007$B","S1A-007$B","S1A-007$B","S1A-007$B","S1A-007$B","S1A-007$B","S1A-007$B","S1A-007$B","S1A-007$C","S1A-007$C","S1A-007$C","S1A-007$D","S1A-007$D","S1A-007$D","S1A-007$D","S1A-007$E","S1A-007$E","S1A-007$E","S1A-007$E","S1A-007$E","S1A-007$F","S1A-007$F","S1A-007$F","S1A-007$F","S1A-007$F","S1A-008$A","S1A-008$A","S1A-008$A","S1A-008$A","S1A-008$A","S1A-008$C","S1A-008$C","S1A-008$C","S1A-008$C","S1A-008$C","S1A-008$D","S1A-008$D","S1A-008$D","S1A-008$D","S1A-008$D","S1A-008$D","S1A-008$D","S1A-009$A","S1A-009$A","S1A-009$A","S1A-009$A","S1A-009$A","S1A-009$A","S1A-009$A","S1A-009$A","S1A-009$A","S1A-009$A","S1A-009$A","S1A-009$A","S1A-009$B","S1A-009$B","S1A-009$B","S1A-009$B","S1A-009$B","S1A-009$B","S1A-009$B","S1A-009$B","S1A-009$B","S1A-009$B","S1A-009$B","S1A-009$B","S1A-009$B","S1A-009$B","S1A-009$B","S1A-009$B","S1A-010$B","S1A-010$B","S1A-010$B","S1A-010$B","S1A-010$B","S1A-010$B","S1A-010$B","S1A-010$B","S1A-010$B","S1A-010$B","S1A-010$C","S1A-010$C","S1A-010$C","S1A-010$C","S1A-010$C","S1A-010$C","S1A-010$C","S1A-010$C","S1A-010$C","S1A-010$C","S1A-010$C","S1A-010$D","S1A-010$D","S1A-010$D","S1A-010$D","S1A-010$D","S1A-010$D","S1A-010$D","S1A-010$D","S1A-010$D","S1A-010$D","S1A-011$A","S1A-011$A","S1A-011$A","S1A-011$A","S1A-011$A","S1A-011$A","S1A-011$A","S1A-018$D","S1A-018$D","S1A-018$D","S1A-018$D","S1A-018$D","S1A-019$A","S1A-019$A","S1A-019$A","S1A-019$A","S1A-019$A","S1A-019$A","S1A-019$A","S1A-019$A","S1A-019$A","S1A-019$A","S1A-019$A","S1A-019$B","S1A-019$B","S1A-019$B","S1A-019$B","S1A-019$B","S1A-019$B","S1A-019$B","S1A-019$B","S1A-019$B","S1A-019$B","S1A-019$C","S1A-019$C","S1A-019$C","S1A-019$C","S1A-019$C","S1A-019$C","S1A-019$C","S1A-019$C","S1A-019$C","S1A-019$C","S1A-019$D","S1A-020$A","S1A-020$A","S1A-020$A","S1A-020$A","S1A-020$A","S1A-020$A","S1A-020$A","S1A-020$A","S1A-020$B","S1A-020$B","S1A-020$B","S1A-020$B","S1A-020$C","S1A-020$D","S1A-020$D","S1A-020$D","S1A-020$D","S1A-020$D","S1A-020$D","S1A-020$D","S1A-020$D","S1A-020$D","S1A-020$D","S1A-020$D","S1A-020$D","S1A-020$D","S1A-020$D","S1A-021$A","S1A-021$A","S1A-021$A","S1A-021$A","S1A-021$A","S1A-021$A","S1A-021$A","S1A-021$B","S1A-021$B","S1A-021$D","S1A-021$D","S1A-021$D","S1A-021$D","S1A-021$F","S1A-021$F","S1A-021$F","S1A-021$F","S1A-021$F","S1A-021$H","S1A-021$H","S1A-022$A","S1A-022$A","S1A-022$A","S1A-022$A","S1A-022$A","S1A-022$A","S1A-022$A","S1A-022$A","S1A-022$A","S1A-022$A","S1A-022$A","S1A-022$A","S1A-022$A","S1A-022$B","S1A-022$B","S1A-022$B","S1A-022$B","S1A-022$B","S1A-022$B","S1A-022$B","S1A-022$B","S1A-022$B","S1A-022$B","S1A-022$D","S1A-022$D","S1A-022$D","S1A-022$D","S1A-023$A","S1A-023$A","S1A-023$A","S1A-023$A","S1A-023$A","S1A-023$A","S1A-023$B","S1A-023$B","S1A-023$B","S1A-023$B","S1A-023$B","S1A-023$B","S1A-023$B","S1A-023$B","S1A-023$B","S1A-023$B","S1A-023$B","S1A-023$B","S1A-023$B","S1A-023$B","S1A-023$D","S1A-023$D","S1A-023$D","S1A-023$D","S1A-023$D","S1A-023$D","S1A-023$D","S1A-023$D","S1A-023$D","S1A-033$A","S1A-033$A","S1A-033$B","S1A-033$B","S1A-033$B","S1A-033$B","S1A-033$B","S1A-033$B","S1A-033$B","S1A-033$B","S1A-033$B","S1A-033$B","S1A-033$B","S1A-033$B","S1A-033$B","S1A-033$B","S1A-033$B","S1A-033$B","S1A-033$B","S1A-033$B","S1A-033$C","S1A-033$C","S1A-033$C","S1A-033$C","S1A-033$C","S1A-033$C","S1A-033$D","S1A-033$D","S1A-033$D","S1A-033$D","S1A-033$D","S1A-033$D","S1A-035$A","S1A-035$A","S1A-035$A","S1A-035$A","S1A-035$A","S1A-035$A","S1A-035$A","S1A-035$A","S1A-035$A","S1A-035$A","S1A-035$A","S1A-035$A","S1A-035$A","S1A-035$A","S1A-035$A","S1A-035$B","S1A-035$B","S1A-035$B","S1A-035$B","S1A-035$B","S1A-035$B","S1A-035$B","S1A-035$B","S1A-035$B","S1A-035$B","S1A-035$B","S1A-035$B","S1A-036$A","S1A-036$A","S1A-036$A","S1A-036$A","S1A-036$A","S1A-036$A","S1A-036$A","S1A-036$A","S1A-036$A","S1A-036$A","S1A-036$A","S1A-036$B","S1A-036$B","S1A-036$B","S1A-036$B","S1A-036$B","S1A-036$B","S1A-036$B","S1A-036$B","S1A-036$B","S1A-036$B","S1A-036$B","S1A-036$B","S1A-036$B","S1A-036$B","S1A-036$B","S1A-036$B","S1A-036$B","S1A-036$B","S1A-036$B","S1A-036$B","S1A-036$B","S1A-037$A","S1A-037$B","S1A-037$B","S1A-037$B","S1A-037$B","S1A-037$B","S1A-037$B","S1A-037$B","S1A-037$B","S1A-037$B","S1A-037$C","S1A-037$C","S1A-037$C","S1A-037$C","S1A-037$C","S1A-037$C","S1A-037$C","S1A-037$C","S1A-037$C","S1A-037$C","S1A-037$D","S1A-037$D","S1A-037$D","S1A-037$D","S1A-037$D","S1A-038$A","S1A-038$A","S1A-038$A","S1A-038$A","S1A-038$A","S1A-038$A","S1A-038$A","S1A-038$A","S1A-038$A","S1A-038$A","S1A-038$A","S1A-038$B","S1A-038$B","S1A-038$B","S1A-038$B","S1A-038$B","S1A-038$B","S1A-038$B","S1A-038$B","S1A-038$B","S1A-038$B","S1A-038$B","S1A-038$B","S1A-038$B","S1A-039$A","S1A-039$A","S1A-039$A","S1A-039$A","S1A-039$A","S1A-039$B","S1A-039$B","S1A-039$B","S1A-039$B","S1A-039$B","S1A-039$B","S1A-039$B","S1A-039$B","S1A-039$B","S1A-039$B","S1A-039$B","S1A-039$C","S1A-039$C","S1A-039$C","S1A-039$C","S1A-040$D","S1A-040$D","S1A-040$D","S1A-040$D","S1A-040$D","S1A-040$D","S1A-043$B","S1A-043$B","S1A-043$B","S1A-043$B","S1A-043$B","S1A-044$B","S1A-044$C","S1A-045$B","S1A-045$B","S1A-045$B","S1A-045$B","S1A-045$B","S1A-045$B","S1A-045$B","S1A-045$B","S1A-046$A","S1A-046$A","S1A-046$A","S1A-046$A","S1A-046$A","S1A-046$A","S1A-046$A","S1A-046$A","S1A-046$A","S1A-046$A","S1A-046$A","S1A-046$A","S1A-046$A","S1A-046$A","S1A-046$A","S1A-046$A","S1A-046$B","S1A-046$B","S1A-046$B","S1A-046$B","S1A-046$B","S1A-046$B","S1A-046$B","S1A-046$B","S1A-046$B","S1A-046$B","S1A-046$B","S1A-046$B","S1A-046$B","S1A-046$B","S1A-046$B","S1A-046$B","S1A-046$B","S1A-046$B","S1A-046$B","S1A-046$B","S1A-046$B","S1A-047$B","S1A-047$B","S1A-047$B","S1A-047$B","S1A-047$B","S1A-047$B","S1A-047$B","S1A-047$B","S1A-049$A","S1A-049$A","S1A-049$A","S1A-049$A","S1A-049$A","S1A-049$A","S1A-049$A","S1A-049$A","S1A-049$A","S1A-049$A","S1A-049$A","S1A-049$A","S1A-049$A","S1A-049$A","S1A-049$A","S1A-049$A","S1A-049$A","S1A-049$A","S1A-049$A","S1A-049$A","S1A-049$A","S1A-049$A","S1A-049$A","S1A-049$A","S1A-049$A","S1A-049$A","S1A-049$A","S1A-049$A","S1A-049$B","S1A-049$B","S1A-049$B","S1A-049$B","S1A-049$B","S1A-049$B","S1A-049$B","S1A-050$A","S1A-050$A","S1A-050$A","S1A-050$A","S1A-050$A","S1A-050$A","S1A-050$A","S1A-050$B","S1A-050$B","S1A-050$B","S1A-050$B","S1A-050$B","S1A-050$B","S1A-050$B","S1A-050$B","S1A-050$B","S1A-050$B","S1A-050$C","S1A-050$C","S1A-050$C","S1A-050$C","S1A-050$C","S1A-050$C","S1A-050$C","S1A-050$C","S1A-050$C","S1A-050$C","S1A-050$C","S1A-050$C","S1A-050$C","S1A-050$C","S1A-050$C","S1A-050$C","S1A-050$C","S1A-050$C","S1A-050$C","S1A-050$C","S1A-050$C","S1A-050$C","S1A-050$C","S1A-050$C","S1A-051$A","S1A-051$A","S1A-051$A","S1A-051$A","S1A-051$A","S1A-051$A","S1A-051$A","S1A-051$A","S1A-051$A","S1A-051$A","S1A-051$A","S1A-051$A","S1A-051$A","S1A-051$A","S1A-051$A","S1A-051$B","S1A-051$B","S1A-051$B","S1A-051$B","S1A-051$B","S1A-051$B","S1A-051$B","S1A-051$B","S1A-051$B","S1A-051$B","S1A-051$B","S1A-051$B","S1A-051$B","S1A-051$D","S1A-051$D","S1A-051$D","S1A-051$D","S1A-051$D","S1A-051$D","S1A-051$D","S1A-051$D","S1A-051$D","S1A-051$D","S1A-051$D","S1A-051$D","S1A-051$D","S1A-051$E","S1A-051$E","S1A-051$E","S1A-051$E","S1A-051$E","S1A-052$A","S1A-052$A","S1A-052$A","S1A-052$A","S1A-052$A","S1A-052$A","S1A-052$B","S1A-052$B","S1A-052$B","S1A-052$B","S1A-052$B","S1A-052$B","S1A-052$B","S1A-052$B","S1A-052$C","S1A-052$C","S1A-052$C","S1A-052$C","S1A-052$C","S1A-052$C","S1A-052$C","S1A-052$C","S1A-053$A","S1A-053$A","S1A-053$A","S1A-053$A","S1A-053$A","S1A-053$A","S1A-053$A","S1A-053$A","S1A-053$A","S1A-053$A","S1A-053$A","S1A-053$A","S1A-053$B","S1A-053$B","S1A-053$B","S1A-053$B","S1A-053$B","S1A-053$C","S1A-053$C","S1A-053$C","S1A-053$C","S1A-053$C","S1A-053$C","S1A-053$C","S1A-053$C","S1A-053$C","S1A-053$D","S1A-053$D","S1A-053$D","S1A-053$D","S1A-053$D","S1A-053$D","S1A-053$D","S1A-053$D","S1A-053$D","S1A-053$D","S1A-054$A","S1A-054$A","S1A-054$A","S1A-054$A","S1A-054$A","S1A-054$A","S1A-054$A","S1A-054$A","S1A-054$A","S1A-054$B","S1A-054$C","S1A-054$C","S1A-054$C","S1A-054$C","S1A-054$C","S1A-054$C","S1A-054$C","S1A-054$C","S1A-054$C","S1A-054$C","S1A-054$C","S1A-054$D","S1A-054$D","S1A-054$D","S1A-054$D","S1A-054$D","S1A-054$D","S1A-054$D","S1A-054$D","S1A-054$D","S1A-054$D","S1A-054$D","S1A-054$D","S1A-054$D","S1A-054$D","S1A-054$D","S1A-054$D","S1A-055$A","S1A-055$A","S1A-055$A","S1A-055$A","S1A-055$A","S1A-055$A","S1A-055$A","S1A-055$A","S1A-055$A","S1A-055$A","S1A-055$A","S1A-055$A","S1A-055$A","S1A-055$A","S1A-055$A","S1A-055$A","S1A-055$B","S1A-055$B","S1A-055$B","S1A-055$B","S1A-055$B","S1A-055$B","S1A-055$B","S1A-055$B","S1A-055$B","S1A-055$B","S1A-055$B","S1A-055$B","S1A-055$B","S1A-055$B","S1A-055$B","S1A-055$B","S1A-055$B","S1A-055$B","S1A-055$B","S1A-055$B","S1A-055$B","S1A-055$B","S1A-055$D","S1A-055$D","S1A-055$E","S1A-055$E","S1A-055$E","S1A-056$A","S1A-056$A","S1A-056$A","S1A-056$A","S1A-056$A","S1A-056$A","S1A-056$B","S1A-056$B","S1A-056$B","S1A-056$B","S1A-056$B","S1A-056$B","S1A-056$C","S1A-056$C","S1A-056$C","S1A-056$D","S1A-056$D","S1A-056$D","S1A-056$D","S1A-056$D","S1A-056$D","S1A-056$D","S1A-056$D","S1A-058$A","S1A-058$A","S1A-058$A","S1A-058$A","S1A-058$A","S1A-058$A","S1A-058$A","S1A-058$A","S1A-058$B","S1A-058$B","S1A-058$B","S1A-058$B","S1A-058$B","S1A-058$C","S1A-058$C","S1A-058$C","S1A-058$C","S1A-058$C","S1A-058$C","S1A-058$D","S1A-058$D","S1A-058$D","S1A-058$D","S1A-058$D","S1A-058$D","S1A-059$A","S1A-059$A","S1A-059$A","S1A-059$A","S1A-059$A","S1A-059$A","S1A-059$A","S1A-059$A","S1A-059$A","S1A-059$A","S1A-059$A","S1A-059$A","S1A-059$A","S1A-059$A","S1A-059$A","S1A-059$A","S1A-059$A","S1A-059$A","S1A-060$A","S1A-060$A","S1A-060$A","S1A-060$A","S1A-060$A","S1A-060$A","S1A-060$A","S1A-060$A","S1A-060$A","S1A-060$A","S1A-060$A","S1A-060$A","S1A-060$A","S1A-061$A","S1A-061$A","S1A-061$A","S1A-061$A","S1A-061$A","S1A-061$A","S1A-061$A","S1A-061$A","S1A-061$A","S1A-061$A","S1A-061$A","S1A-061$A","S1A-061$A","S1A-061$A","S1A-061$A","S1A-061$A","S1A-061$A","S1A-061$A","S1A-061$B","S1A-061$B","S1A-061$B","S1A-061$B","S1A-061$B","S1A-061$B","S1A-061$C","S1A-061$C","S1A-061$C","S1A-061$C","S1A-063$A","S1A-063$A","S1A-063$A","S1A-063$A","S1A-063$A","S1A-063$A","S1A-063$A","S1A-063$A","S1A-063$A","S1A-063$A","S1A-063$A","S1A-063$B","S1A-063$B","S1A-063$B","S1A-063$B","S1A-063$B","S1A-063$B","S1A-063$B","S1A-063$B","S1A-063$B","S1A-063$B","S1A-063$B","S1A-063$B","S1A-063$C","S1A-063$C","S1A-063$C","S1A-064$C","S1A-064$C","S1A-064$D","S1A-064$D","S1A-064$D","S1A-064$D","S1A-064$E","S1A-064$E","S1A-064$E","S1A-064$E","S1A-066$A","S1A-066$A","S1A-066$A","S1A-066$A","S1A-066$B","S1A-066$B","S1A-066$B","S1A-066$B","S1A-066$B","S1A-066$B","S1A-066$B","S1A-066$B","S1A-066$B","S1A-066$B","S1A-066$C","S1A-066$C","S1A-066$C","S1A-066$C","S1A-066$C","S1A-066$C","S1A-066$C","S1A-066$C","S1A-066$C","S1A-066$C","S1A-066$C","S1A-066$C","S1A-066$C","S1A-066$C","S1A-066$C","S1A-066$C","S1A-066$C","S1A-066$C","S1A-066$C","S1A-066$C","S1A-066$C","S1A-066$C","S1A-066$C","S1A-067$A","S1A-067$A","S1A-067$A","S1A-067$A","S1A-067$A","S1A-067$A","S1A-067$A","S1A-067$A","S1A-067$A","S1A-067$A","S1A-067$A","S1A-067$A","S1A-067$A","S1A-067$A","S1A-067$A","S1A-067$A","S1A-067$A","S1A-067$A","S1A-067$A","S1A-067$A","S1A-067$A","S1A-067$A","S1A-068$A","S1A-068$A","S1A-068$A","S1A-068$A","S1A-068$A","S1A-068$A","S1A-068$A","S1A-068$A","S1A-068$A","S1A-068$A","S1A-068$A","S1A-068$A","S1A-068$A","S1A-068$A","S1A-068$A","S1A-068$A","S1A-068$A","S1A-068$B","S1A-068$B","S1A-068$B","S1A-068$C","S1A-068$C","S1A-068$C","S1A-068$C","S1A-068$C","S1A-068$C","S1A-068$C","S1A-068$C","S1A-068$C","S1A-068$C","S1A-068$C","S1A-069$A","S1A-069$A","S1A-069$A","S1A-069$A","S1A-069$A","S1A-069$A","S1A-069$A","S1A-069$A","S1A-069$B","S1A-069$B","S1A-069$B","S1A-069$B","S1A-069$B","S1A-069$B","S1A-069$B","S1A-069$B","S1A-069$B","S1A-069$B","S1A-069$B","S1A-069$B","S1A-069$B","S1A-069$C","S1A-069$C","S1A-069$C","S1A-069$C","S1A-069$C","S1A-069$C","S1A-069$C","S1A-069$C","S1A-069$C","S1A-069$C","S1A-069$C","S1A-070$A","S1A-070$A","S1A-070$A","S1A-070$A","S1A-070$A","S1A-070$A","S1A-070$A","S1A-070$A","S1A-070$A","S1A-070$A","S1A-070$A","S1A-070$A","S1A-070$A","S1A-070$A","S1A-070$A","S1A-070$A","S1A-070$A","S1A-070$B","S1A-070$B","S1A-070$B","S1A-070$B","S1A-070$B","S1A-070$B","S1A-070$B","S1A-070$B","S1A-070$B","S1A-070$B","S1A-070$B","S1A-070$B","S1A-070$B","S1A-070$B","S1A-070$B","S1A-070$B","S1A-070$B","S1A-071$A","S1A-071$A","S1A-071$A","S1A-071$A","S1A-071$A","S1A-071$A","S1A-071$A","S1A-071$A","S1A-071$A","S1A-071$A","S1A-071$A","S1A-071$A","S1A-071$B","S1A-071$B","S1A-071$B","S1A-071$B","S1A-071$B","S1A-071$B","S1A-071$B","S1A-071$B","S1A-071$B","S1A-071$B","S1A-071$B","S1A-071$C","S1A-071$C","S1A-071$C","S1A-071$C","S1A-071$C","S1A-071$C","S1A-071$C","S1A-071$C","S1A-071$C","S1A-071$C","S1A-072$A","S1A-072$A","S1A-072$A","S1A-072$A","S1A-072$A","S1A-072$A","S1A-072$A","S1A-072$A","S1A-072$A","S1A-072$A","S1A-072$A","S1A-072$A","S1A-072$B","S1A-072$B","S1A-072$B","S1A-072$B","S1A-072$B","S1A-072$B","S1A-072$B","S1A-072$B","S1A-072$B","S1A-072$B","S1A-072$B","S1A-072$B","S1A-072$B","S1A-072$B","S1A-072$B","S1A-072$B","S1A-072$B","S1A-072$B","S1A-072$B","S1A-072$B","S1A-072$B","S1A-072$B","S1A-072$B","S1A-072$B","S1A-072$B","S1A-072$B","S1A-072$B","S1A-072$B","S1A-072$B","S1A-072$B","S1A-072$B","S1A-072$C","S1A-072$C","S1A-072$C","S1A-072$C","S1A-072$C","S1A-072$C","S1A-072$C","S1A-072$C","S1A-072$C","S1A-072$C","S1A-072$C","S1A-072$C","S1A-072$C","S1A-073$A","S1A-073$A","S1A-073$A","S1A-073$A","S1A-073$A","S1A-073$A","S1A-073$A","S1A-073$A","S1A-073$A","S1A-073$A","S1A-073$A","S1A-073$B","S1A-073$B","S1A-073$B","S1A-073$B","S1A-073$B","S1A-073$B","S1A-073$B","S1A-073$B","S1A-073$B","S1A-073$B","S1A-073$B","S1A-073$B","S1A-073$B","S1A-073$C","S1A-073$C","S1A-073$C","S1A-073$C","S1A-073$C","S1A-073$C","S1A-074$A","S1A-074$A","S1A-074$A","S1A-074$A","S1A-074$A","S1A-074$A","S1A-074$A","S1A-074$A","S1A-074$A","S1A-074$A","S1A-074$A","S1A-074$A","S1A-074$A","S1A-074$A","S1A-074$A","S1A-074$A","S1A-074$A","S1A-074$A","S1A-074$A","S1A-074$B","S1A-074$B","S1A-074$B","S1A-074$B","S1A-074$B","S1A-074$B","S1A-074$B","S1A-074$B","S1A-074$B","S1A-074$B","S1A-074$C","S1A-074$C","S1A-074$C","S1A-074$C","S1A-074$C","S1A-074$C","S1A-074$C","S1A-074$C","S1A-074$C","S1A-074$C","S1A-074$C","S1A-074$C","S1A-074$C","S1A-075$B","S1A-075$B","S1A-075$B","S1A-075$B","S1A-075$B","S1A-075$B","S1A-075$B","S1A-075$B","S1A-075$B","S1A-075$C","S1A-075$C","S1A-075$C","S1A-075$C","S1A-076$A","S1A-076$A","S1A-076$A","S1A-076$A","S1A-076$A","S1A-076$A","S1A-076$A","S1A-076$A","S1A-076$A","S1A-076$A","S1A-076$A","S1A-076$A","S1A-076$A","S1A-076$A","S1A-076$A","S1A-076$A","S1A-076$A","S1A-076$A","S1A-076$A","S1A-076$A","S1A-076$A","S1A-076$A","S1A-076$A","S1A-076$A","S1A-076$A","S1A-076$A","S1A-076$A","S1A-076$A","S1A-076$A","S1A-076$A","S1A-076$B","S1A-076$B","S1A-076$B","S1A-076$B","S1A-076$B","S1A-076$B","S1A-076$B","S1A-076$B","S1A-077$A","S1A-077$A","S1A-077$A","S1A-077$A","S1A-077$A","S1A-077$A","S1A-077$C","S1A-077$C","S1A-077$C","S1A-077$C","S1A-077$C","S1A-077$C","S1A-077$C","S1A-077$D","S1A-077$D","S1A-077$D","S1A-077$D","S1A-077$D","S1A-077$D","S1A-077$D","S1A-077$D","S1A-077$D","S1A-077$D","S1A-077$D","S1A-077$D","S1A-077$D","S1A-077$D","S1A-078$B","S1A-078$B","S1A-078$B","S1A-078$B","S1A-078$B","S1A-078$B","S1A-078$B","S1A-078$B","S1A-078$B","S1A-078$B","S1A-078$B","S1A-078$B","S1A-078$B","S1A-078$B","S1A-078$B","S1A-078$B","S1A-078$B","S1A-078$B","S1A-078$B","S1A-078$C","S1A-078$C","S1A-078$C","S1A-078$C","S1A-078$C","S1A-078$C","S1A-078$C","S1A-078$D","S1A-078$D","S1A-078$E","S1A-078$E","S1A-079$A","S1A-079$A","S1A-079$A","S1A-079$A","S1A-079$A","S1A-079$A","S1A-079$A","S1A-079$A","S1A-079$A","S1A-079$A","S1A-079$A","S1A-079$A","S1A-079$A","S1A-079$A","S1A-079$A","S1A-079$A","S1A-079$A","S1A-079$A","S1A-079$A","S1A-079$B","S1A-079$B","S1A-079$B","S1A-079$B","S1A-079$B","S1A-079$B","S1A-079$B","S1A-079$B","S1A-079$B","S1A-079$B","S1A-079$B","S1A-079$B","S1A-079$B","S1A-079$B","S1A-079$B","S1A-079$B","S1A-079$B","S1A-079$B","S1A-079$C","S1A-079$D","S1A-079$D","S1A-080$A","S1A-080$A","S1A-080$A","S1A-080$A","S1A-080$A","S1A-080$A","S1A-080$A","S1A-080$A","S1A-080$A","S1A-080$B","S1A-080$B","S1A-080$B","S1A-080$B","S1A-080$B","S1A-080$B","S1A-080$B","S1A-080$B","S1A-080$B","S1A-080$B","S1A-080$B","S1A-080$B","S1A-081$A","S1A-081$C","S1A-081$C","S1A-081$C","S1A-081$C","S1A-081$E","S1A-081$E","S1A-081$E","S1A-081$E","S1A-081$E","S1A-081$E","S1A-081$H","S1A-081$H","S1A-082$A","S1A-082$A","S1A-082$A","S1A-082$A","S1A-082$A","S1A-082$A","S1A-082$A","S1A-082$A","S1A-082$A","S1A-082$A","S1A-082$A","S1A-082$B","S1A-082$B","S1A-082$B","S1A-082$C","S1A-082$C","S1A-082$C","S1A-082$C","S1A-082$C","S1A-082$C","S1A-082$E","S1A-082$E","S1A-082$E","S1A-082$E","S1A-082$E","S1A-082$E","S1A-082$E","S1A-082$E","S1A-082$E","S1A-082$E","S1A-082$E","S1A-082$G","S1A-082$G","S1A-082$G","S1A-082$G","S1A-082$G","S1A-082$G","S1A-082$G","S1A-082$G","S1A-083$A","S1A-083$A","S1A-083$A","S1A-083$A","S1A-083$B","S1A-083$B","S1A-083$B","S1A-083$B","S1A-083$B","S1A-083$B","S1A-083$B","S1A-083$B","S1A-083$B","S1A-083$B","S1A-083$D","S1A-083$D","S1A-083$D","S1A-083$D","S1A-083$D","S1A-083$D","S1A-083$E","S1A-083$E","S1A-083$E","S1A-083$E","S1A-083$E","S1A-083$E","S1A-083$E","S1A-083$E","S1A-083$F","S1A-084$A","S1A-084$A","S1A-084$A","S1A-084$B","S1A-084$B","S1A-084$B","S1A-084$B","S1A-084$B","S1A-084$B","S1A-084$B","S1A-084$B","S1A-084$B","S1A-084$B","S1A-084$B","S1A-084$B","S1A-084$B","S1A-084$B","S1A-084$B","S1A-084$B","S1A-084$C","S1A-084$C","S1A-084$C","S1A-084$C","S1A-084$C","S1A-084$C","S1A-084$C","S1A-084$C","S1A-084$C","S1A-084$C","S1A-084$C","S1A-084$C","S1A-084$D","S1A-084$D","S1A-084$D","S1A-084$D","S1A-084$D","S1A-084$D","S1A-084$D","S1A-085$E","S1A-085$E","S1A-085$E","S1A-085$E","S1A-085$E","S1A-086$A","S1A-086$A","S1A-086$A","S1A-086$A","S1A-086$A","S1A-086$A","S1A-086$A","S1A-086$A","S1A-086$A","S1A-086$A","S1A-086$A","S1A-086$A","S1A-086$A","S1A-086$A","S1A-086$A","S1A-086$A","S1A-086$A","S1A-086$A","S1A-086$A","S1A-086$A","S1A-086$A","S1A-086$A","S1A-086$A","S1A-086$A","S1A-086$A","S1A-086$A","S1A-086$A","S1A-086$B","S1A-086$B","S1A-086$B","S1A-086$B","S1A-086$B","S1A-086$B","S1A-086$B","S1A-086$B","S1A-086$B","S1A-086$B","S1A-086$B","S1A-086$B","S1A-086$B","S1A-086$B","S1A-086$B","S1A-087$A","S1A-087$A","S1A-087$A","S1A-087$A","S1A-087$A","S1A-087$A","S1A-087$A","S1A-087$A","S1A-087$A","S1A-087$A","S1A-087$A","S1A-087$A","S1A-087$A","S1A-087$A","S1A-087$A","S1A-087$B","S1A-087$B","S1A-087$B","S1A-087$B","S1A-087$B","S1A-087$B","S1A-087$B","S1A-087$B","S1A-087$B","S1A-087$B","S1A-087$B","S1A-087$B","S1A-087$B","S1A-087$B","S1A-087$B","S1A-087$B","S1A-087$B","S1A-087$B","S1A-087$B","S1A-087$B","S1A-087$B","S1A-087$B","S1A-087$B","S1A-087$C","S1A-087$C","S1A-087$C","S1A-088$A","S1A-088$A","S1A-088$A","S1A-089$B","S1A-089$B","S1A-090$B","S1A-090$B","S1A-090$B","S1A-090$B","S1A-090$B","S1A-090$B","S1A-090$B","S1A-090$B","S1A-090$B","S1A-090$B","S1A-090$B","S1A-091$A","S1A-091$A","S1A-091$B","S1A-091$B","S1A-091$B","S1A-091$B","S1A-091$B","S1A-091$B","S1A-091$C","S1A-091$C","S1A-091$C","S1A-091$C","S1A-091$C","S1A-091$C","S1A-091$D","S1A-091$D","S1A-091$D","S1A-091$D","S1A-091$D","S1A-091$D","S1A-091$D","S1A-091$D","S1A-091$D","S1A-091$D","S1A-092$A","S1A-092$A","S1A-092$A","S1A-092$A","S1A-092$A","S1A-092$A","S1A-092$A","S1A-092$A","S1A-092$A","S1A-092$A","S1A-092$A","S1A-092$A","S1A-092$A","S1A-092$B","S1A-092$B","S1A-092$B","S1A-092$B","S1A-092$B","S1A-092$B","S1A-092$B","S1A-092$B","S1A-092$B","S1A-092$B","S1A-092$B","S1A-093$A","S1A-093$A","S1A-093$A","S1A-093$A","S1A-093$A","S1A-093$A","S1A-093$A","S1A-093$A","S1A-093$A","S1A-093$A","S1A-093$A","S1A-093$A","S1A-093$B","S1A-093$B","S1A-093$B","S1A-093$B","S1A-093$B","S1A-093$B","S1A-093$B","S1A-093$B","S1A-093$B","S1A-093$B","S1A-093$B","S1A-093$B","S1A-093$B","S1A-093$B","S1A-093$B","S1A-093$B","S1A-093$B","S1A-094$A","S1A-094$A","S1A-094$A","S1A-094$A","S1A-094$A","S1A-094$A","S1A-094$A","S1A-094$A","S1A-094$A","S1A-094$A","S1A-094$A","S1A-094$A","S1A-094$A","S1A-094$A","S1A-094$A","S1A-094$A","S1A-094$A","S1A-094$A","S1A-094$A","S1A-094$A","S1A-094$B","S1A-094$B","S1A-094$B","S1A-094$B","S1A-094$B","S1A-094$B","S1A-094$B","S1A-094$B","S1A-094$B","S1A-094$B","S1A-094$B","S1A-094$B","S1A-094$B","S1A-095$A","S1A-095$A","S1A-095$A","S1A-095$A","S1A-095$A","S1A-095$A","S1A-095$A","S1A-095$A","S1A-095$A","S1A-095$A","S1A-095$A","S1A-095$A","S1A-095$A","S1A-095$A","S1A-095$A","S1A-095$A","S1A-095$A","S1A-095$A","S1A-095$A","S1A-095$B","S1A-095$B","S1A-095$B","S1A-095$B","S1A-095$B","S1A-095$B","S1A-095$B","S1A-095$B","S1A-095$B","S1A-095$B","S1A-095$B","S1A-095$B","S1A-095$B","S1A-095$B","S1A-095$B","S1A-096$A","S1A-096$A","S1A-096$A","S1A-096$A","S1A-096$A","S1A-096$A","S1A-096$A","S1A-096$A","S1A-096$A","S1A-096$A","S1A-096$A","S1A-096$A","S1A-096$A","S1A-096$A","S1A-096$A","S1A-096$A","S1A-096$A","S1A-096$A","S1A-097$B","S1A-097$B","S1A-097$B","S1A-097$B","S1A-097$B","S1A-097$B","S1A-097$B","S1A-097$B","S1A-097$B","S1A-097$B","S1A-097$B","S1A-098$A","S1A-098$A","S1A-098$A","S1A-098$A","S1A-098$A","S1A-098$A","S1A-098$A","S1A-098$A","S1A-098$A","S1A-098$A","S1A-098$A","S1A-098$A","S1A-098$A","S1A-098$A","S1A-098$A","S1A-099$A","S1A-099$A","S1A-099$A","S1A-099$A","S1A-099$A","S1A-099$A","S1A-099$A","S1A-099$A","S1A-099$A","S1A-099$A","S1A-099$A","S1A-099$A","S1A-099$A","S1A-099$B","S1A-099$B","S1A-099$B","S1A-099$B","S1A-099$B","S1A-099$B","S1A-099$B","S1A-099$B","S1A-099$B","S1A-099$B","S1A-099$B","S1A-099$B"],["Men","Men","Men","Men","Men","Men","Men","Men","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Men","Men","Men","Men","Men","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Men","Men","Men","Men","Men","Men","Men","Men","Men","Men","Men","Men","Men","Men","Men","Men","Men","Men","Men","Men","Men","Men","Men","Men","Men","Men","Men","Men","Men","Men","Men","Men","Men","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Men","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Men","Men","Men","Men","Men","Men","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Men","Men","Men","Men","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Men","Men","Men","Men","Men","Men","Men","Men","Men","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Men","Men","Men","Men","Men","Men","Men","Men","Men","Men","Men","Men","Men","Men","Men","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Men","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Men","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Men","Men","Women","Women","Women","Women","Men","Women","Women","Women","Women","Women","Women","Men","Men","Men","Men","Men","Men","Men","Men","Men","Men","Men","Men","Men","Men","Men","Men","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Men","Men","Men","Men","Men","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Men","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Men","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Men","Women","Men","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Men","Men","Men","Men","Men","Men","Men","Men","Men","Men","Men","Men","Men","Men","Men","Men","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Men","Men","Men","Men","Men","Men","Men","Men","Men","Men","Men","Men","Men","Men","Men","Men","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Men","Men","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Men","Men","Men","Men","Men","Men","Men","Men","Men","Men","Men","Men","Men","Men","Men","Men","Men","Men","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Men","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Men","Men","Men","Men","Men","Men","Men","Men","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Men","Women","Women","Men","Women","Women","Women","Women","Women","Women","Men","Men","Men","Men","Men","Men","Men","Men","Men","Men","Men","Men","Men","Men","Men","Men","Men","Men","Men","Men","Men","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Men","Men","Men","Men","Men","Men","Men","Men","Men","Men","Men","Men","Men","Men","Men","Men","Men","Men","Men","Men","Men","Men","Men","Men","Men","Men","Men","Men","Men","Men","Men","Men","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Men","Men","Men","Men","Men","Men","Men","Men","Men","Men","Men","Men","Men","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Men","Men","Men","Men","Men","Men","Men","Men","Men","Men","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Men","Men","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Men","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Men","Men","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Men","Men","Men","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Men","Men","Men","Men","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Men","Men","Men","Men","Men","Men","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Men","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Men","Men","Men","Men","Men","Men","Men","Men","Men","Men","Men","Men","Men","Men","Men","Men","Men","Men","Men","Men","Men","Men","Men","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Men","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Men","Women","Women","Men","Women","Men","Men","Men","Men","Men","Men","Men","Men","Men","Men","Men","Men","Men","Men","Men","Men","Men","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women","Women"],["Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Old","Old","Old","Old","Old","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Young","Young","Young","Young","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Old","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Old","Old","Old","Old","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Old","Old","Old","Old","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Old","Old","Old","Old","Old","Old","Old","Old","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Young","Young","Young","Young","Young","Young","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Old","Old","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Old","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young","Young"],["SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","SameGender","SameGender","SameGender","SameGender","SameGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","SameGender","SameGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","SameGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","MixedGender","MixedGender","SameGender","MixedGender","MixedGender","SameGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","SameGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","SameGender","SameGender","SameGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","MixedGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender","SameGender"],["NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","Prime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","Prime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","Prime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","Prime","Prime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","Prime","NoPrime","Prime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","Prime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","Prime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","Prime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","Prime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","Prime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","Prime","Prime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","Prime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","Prime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","Prime","Prime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","Prime","Prime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","Prime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","Prime","Prime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","NoPrime","Prime","NoPrime","Prime","Prime","NoPrime","Prime","NoPrime","Prime"],[0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,1,0,0,0,1,0,1,0,1,0,0,0,0,0,0,0,0,1,0,0,1,0,0,1,0,0,0,1,1,0,0,0,0,1,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,1,1,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,1,0,1,0,0,0,0,0,1,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,1,0,1,0,0,1,1,0,1,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,1,1,0,1,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,1,0,0,0,0,1,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,1,0,0,1,0,0,0,0,1,1,1,0,0,1,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,1,1,0,1,1,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,1,1,0,0,1,1,1,1,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,1,1,0,1,0,0,0,0,0,0,1,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,1,0,0,0,0,1,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,1,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,1,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,1,1,1,0,1,0,0,0,0,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,1,1,0,0,1,0,1,0,0,0,0,0,1,0,0,0,0,1,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,1,0,1,0,1,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,1,0,0,0,0,1,1,0,0,0,1,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,1,0,1,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,1,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,1,0,0,0,0,1,1,1,0,0,0,0,0,0,0,0,1,1,0,0,1,0,0,1,1,0,1,0,0,1,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,1,0,1,0,0,0,1,1,0,1,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,1,0,0,0,0,1,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,1,0,1,0,0,1,1,0,1,0,1,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,1,0,0,0,1,1,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,1,1,0,0,1,0,1,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,1,0,1,0,1,0,1,0,1,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,1,0,1,0,1,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,1,0,1]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th>ID<\/th>\n      <th>Gender<\/th>\n      <th>Age<\/th>\n      <th>ConversationType<\/th>\n      <th>Priming<\/th>\n      <th>SUFlike<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"pageLength":5,"scrollX":true,"columnDefs":[{"className":"dt-right","targets":5}],"order":[],"autoWidth":false,"orderClasses":false,"lengthMenu":[5,10,25,50,100]}},"evals":[],"jsHooks":[]}</script>
<p>Before continuing, a few words about the minimum number of random effect levels and the minimum number of observations per random effect level are in order.</p>
<p>While many data points per random variable level increases statistical power and thus to more robust estimates of the random effects <span class="citation">(Austin and Leckie <a href="#ref-austin2018multilevel" role="doc-biblioref">2018</a>)</span>, it has been shown that small numbers of observations per random effect variable level do not cause serious bias and it does not negatively affect the estimates of the fixed-effects coefficients <span class="citation">(Bell, Ferron, and Kromrey <a href="#ref-bell2008multilevel" role="doc-biblioref">2008</a>; Clarke <a href="#ref-clarke2008can" role="doc-biblioref">2008</a>; Clarke and Wheaton <a href="#ref-clarke2007addressing" role="doc-biblioref">2007</a>; Maas and Hox <a href="#ref-maas2005sufficient" role="doc-biblioref">2005</a>)</span>. The minimum number of observations per random effect variable level is therefore 1.</p>
<p>In simulation study, <span class="citation">(Bell, Ferron, and Kromrey <a href="#ref-bell2008multilevel" role="doc-biblioref">2008</a>)</span> tested the impact of random variable levels with only a single observation ranging from 0 to 70 percent. As long as there was a relatively high number of random effect variable levels (500 or more), small numbers of observations had almost no impact on bias and Type 1 error control.</p>
<p>We now plot the data to inspect the relationships within the data set.</p>
<pre class="r"><code>ggplot(mblrdata, aes(Gender, SUFlike, color = Priming)) +
  facet_wrap(Age~ConversationType) +
  stat_summary(fun = mean, geom = &quot;point&quot;) +
  stat_summary(fun.data = mean_cl_boot, geom = &quot;errorbar&quot;, width = 0.2) +
  theme_set(theme_bw(base_size = 10)) +
  theme(legend.position = &quot;top&quot;) +
  labs(x = &quot;&quot;, y = &quot;Observed Probabilty of discourse like&quot;) +
  scale_color_manual(values = c(&quot;gray20&quot;, &quot;gray70&quot;))</code></pre>
<p><img src="mmws_files/figure-html/blmm8-1.png" width="672" /></p>
</div>
<div id="model-building" class="section level3 unnumbered">
<h3>Model Building</h3>
<p>In a first step, we set the options.</p>
<pre class="r"><code>library(rms)
# set options
options(contrasts  =c(&quot;contr.treatment&quot;, &quot;contr.poly&quot;))
mblrdata.dist &lt;- datadist(mblrdata)
options(datadist = &quot;mblrdata.dist&quot;)</code></pre>
<p>In a next step, we generate fixed-effects minimal base-line models and a base-line mixed-model using the “glmer” function with a random intercept for ID (a lmer object of the final minimal adequate model will be created later).</p>
<pre class="r"><code># baseline model glm
m0.glm = glm(SUFlike ~ 1, family = binomial, data = mblrdata) 
# base-line mixed-model
m0.glmer = glmer(SUFlike ~ (1|ID), data = mblrdata, family = binomial) </code></pre>
</div>
<div id="testing-the-random-effect" class="section level3 unnumbered">
<h3>Testing the Random Effect</h3>
<p>Now, we check if including the random effect is permitted by comparing the AICs from the glm to AIC from the glmer model. If the AIC of the glmer object is smaller than the AIC of the glm object, then this indicates that including random intercepts is justified.</p>
<pre class="r"><code>aic.glmer &lt;- AIC(logLik(m0.glmer))
aic.glm &lt;- AIC(logLik(m0.glm))
aic.glmer; aic.glm</code></pre>
<pre><code>## [1] 1828</code></pre>
<pre><code>## [1] 1838</code></pre>
<p>The AIC of the glmer object is smaller which shows that including the random intercepts is justified.</p>
</div>
<div id="model-fitting-1" class="section level3 unnumbered">
<h3>Model Fitting</h3>
<p>The next step is to fit the model which means that we aim to find the “best” model, i.e. the minimal adequate model. In this case, we will use a manual step-wise step-up, forward elimination procedure.</p>
<hr />
</div>
<div id="excursion-automatic-model-fitting-and-why-you-should-not-use-it" class="section level3 unnumbered">
<h3>Excursion: Automatic Model Fitting and Why You Should Not Use It</h3>
<details>
<p><summary>In this section, we will use a step-wise step-down procedure that uses decreases in AIC (<em>Akaike Information Criterion</em>) as the criterion to minimize the model in a step-wise manner. </summary> This procedure aims at finding the model with the lowest AIC values by evaluating - step-by-step - whether the removal of a predictor (term) leads to a lower AIC value.</p>
<p>We use this method here just so that you know it exists and how to implement it but you should rather avoid using automated model fitting. The reason for avoiding automated model fitting is that the algorithsm only checks if the AIC has decreased but not if the model is stable or reliable. Thus, automated model fitting has the problem that you can never be sure that the way that lead you to the final model is reliable and that all models were indeed stable. Imagine you want to climb down from a roof top and you have a ladder. The problem is that you do not know if and how many steps are broken. This is similar to using automated model fitting. In other sections, we will explore better methods to fit models (manual step-wise step-up and step-down procedures, for example).</p>
<p>The AIC is calculated using the equation below. The lower the AIC value, the better the balance between explained variance and the number of predictors. AIC values can and should only be compared for models that are fit on the same dataset with the same (number of) cases (<span class="math inline">\(LL\)</span> stands for LogLikelihood and <span class="math inline">\(k\)</span> represents the number of predictors in the model).</p>
<p>Akaike Information Criterion (AIC) = <span class="math inline">\(-2LL + 2k\)</span></p>
<p>An alternative to the AIC is the BIC (Bayesian Information Criterion). Both AIC and BIC penalize models for including variables in a model. The penalty of the BIC is bigger than the penalty of the AIC and it includes the number of cases in the model (<em>LL</em> stands for <em>logged likelihood</em> or <em>LogLikelihood</em>, <em>k</em> represents the number of predictors in the model (including the intercept), and <em>N</em> represents the number of cases in the model).</p>
<p>Bayesian Information Criterion (BIC) = <span class="math inline">\(-2LL + 2k * log(N)\)</span></p>
<p>Interactions are evaluated first and only if all insignificant interactions have been removed would the procedure start removing insignificant main effects (that are not part of significant interactions). Other model fitting procedures (forced entry, step-wise step up, hierarchical) are discussed during the implementation of other regression models. We cannot discuss all procedures here as model fitting is rather complex and a discussion of even the most common procedures would to lengthy and time consuming at this point. It is important to note though that there is not perfect model fitting procedure and automated approaches should be handled with care as they are likely to ignore violations of model parameters that can be detected during manual - but time consuming - model fitting procedures. As a general rule of thumb, it is advisable to fit models as carefully and deliberately as possible. We will now begin to fit the model.</p>
</details>
<hr />
<p>Before we begin with the model fitting process we need to add ´control = glmerControl(optimizer = “bobyqa”)´ to avoid unnecessary failures to converge.</p>
<pre class="r"><code>m0.glmer &lt;- glmer(SUFlike ~ 1+ (1|ID), family = binomial, data = mblrdata, control=glmerControl(optimizer=&quot;bobyqa&quot;))</code></pre>
<p>During each step of the fitting procedure, we test whether certain assumptions on which the model relies are violated. To avoid <em>incomplete information</em> (a combination of variables does not occur in the data), we tabulate the variables we intend to include and make sure that all possible combinations are present in the data. Including variables although not all combinations are present in the data would lead to unreliable models that report (vastly) inaccurate results. A special case of incomplete information is <em>complete separation</em> which occurs if one predictor perfectly explains an outcome (in that case the incomplete information would be caused by a level of the dependent variable). In addition, we make sure that the VIFs do not exceed a maximum of 3 for main effects <span class="citation">(Zuur, Ieno, and Elphick <a href="#ref-zuur2010protocol" role="doc-biblioref">2010</a>)</span> - <span class="citation">Booth GD (<a href="#ref-booth1994regression" role="doc-biblioref">1994</a>)</span> suggest that VIFs should ideally be lower than 3 for as higher values would indicate multicollinearity and thus that the model is unstable. The value of 3 should be taken with a pinch of salt because there is no clear consensus about what the maximum VIF for interactions should be or if it should be considered at all. The reason is that we would, of course, expect the VIFs to increase when we are dealing with interactions as the main effects that are part of the interaction are very likely to correlate with the interaction itself. However, if the VIFs are too high, then this will still cause the issues with the attribution of variance. The value of 3 was chosen based on recommendations in the standard literature on multicollinearity <span class="citation">(Zuur et al. <a href="#ref-zuur2009mixedmodels" role="doc-biblioref">2009</a>; Neter, Wasserman, and Kutner <a href="#ref-neter1990vif" role="doc-biblioref">1990</a>)</span>. Only once we have confirmed that the incomplete information, complete separation, and <em>multicollinearity</em> are not a major concern, we generate the more saturated model and test whether the inclusion of a predictor leads to a significant reduction in residual deviance. If the predictor explains a significant amount of variance, it is retained in the model while being disregarded in case it does not explain a sufficient quantity of variance.</p>
<pre class="r"><code># add Priming
ifelse(min(ftable(mblrdata$Priming, mblrdata$SUFlike)) == 0, &quot;incomplete information&quot;, &quot;okay&quot;)</code></pre>
<pre><code>## [1] &quot;okay&quot;</code></pre>
<pre class="r"><code>m1.glmer &lt;- update(m0.glmer, .~.+Priming)
anova(m1.glmer, m0.glmer, test = &quot;Chi&quot;) </code></pre>
<pre><code>## Data: mblrdata
## Models:
## m0.glmer: SUFlike ~ 1 + (1 | ID)
## m1.glmer: SUFlike ~ (1 | ID) + Priming
##          npar  AIC  BIC logLik deviance Chisq Df          Pr(&gt;Chisq)    
## m0.glmer    2 1828 1840   -912     1824                                 
## m1.glmer    3 1703 1720   -848     1697   128  1 &lt;0.0000000000000002 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>Since the tests do not show problems relating to incomplete information, because including <em>Priming</em> significantly improves the model fit (decrease in AIC and BIC values), and since it correlates significantly with our dependent variable, we include <em>Priming</em> into our model.</p>
<pre class="r"><code># add Age
ifelse(min(ftable(mblrdata$Age, mblrdata$SUFlike)) == 0, &quot;incomplete information&quot;, &quot;okay&quot;)</code></pre>
<pre><code>## [1] &quot;okay&quot;</code></pre>
<pre class="r"><code>m2.glmer &lt;- update(m1.glmer, .~.+ Age)
ifelse(max(car::vif(m2.glmer)) &lt;= 3,  &quot;VIFs okay&quot;, &quot;VIFs unacceptable&quot;) </code></pre>
<pre><code>## [1] &quot;VIFs okay&quot;</code></pre>
<pre class="r"><code>anova(m2.glmer, m1.glmer, test = &quot;Chi&quot;)   </code></pre>
<pre><code>## Data: mblrdata
## Models:
## m1.glmer: SUFlike ~ (1 | ID) + Priming
## m2.glmer: SUFlike ~ (1 | ID) + Priming + Age
##          npar  AIC  BIC logLik deviance Chisq Df Pr(&gt;Chisq)
## m1.glmer    3 1703 1720   -848     1697                    
## m2.glmer    4 1704 1727   -848     1696  0.56  1       0.45</code></pre>
<pre class="r"><code>Anova(m2.glmer, test = &quot;Chi&quot;)</code></pre>
<pre><code>## Analysis of Deviance Table (Type II Wald chisquare tests)
## 
## Response: SUFlike
##          Chisq Df          Pr(&gt;Chisq)    
## Priming 129.51  1 &lt;0.0000000000000002 ***
## Age       0.57  1                0.45    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>The ANOVAs show that <em>Age</em> is not significant and the first ANOVA also shows that the BIC has increased which indicates that <em>Age</em> does not decrease variance. In such cases, the variable should not be included.</p>
<p>However, if the second ANOVA would report <em>Age</em> as being marginally significant, a case could be made for including it but it would be better to change the ordering in which predictors are added to the model. This is, however, just a theoretical issue here as <em>Age</em> is clearly not significant.</p>
<pre class="r"><code># add Gender
ifelse(min(ftable(mblrdata$Gender, mblrdata$SUFlike)) == 0, &quot;incomplete information&quot;, &quot;okay&quot;)</code></pre>
<pre><code>## [1] &quot;okay&quot;</code></pre>
<pre class="r"><code>m3.glmer &lt;- update(m1.glmer, .~.+Gender)
ifelse(max(car::vif(m3.glmer)) &lt;= 3,  &quot;VIFs okay&quot;, &quot;VIFs unacceptable&quot;) </code></pre>
<pre><code>## [1] &quot;VIFs okay&quot;</code></pre>
<pre class="r"><code>anova(m3.glmer, m1.glmer, test = &quot;Chi&quot;)</code></pre>
<pre><code>## Data: mblrdata
## Models:
## m1.glmer: SUFlike ~ (1 | ID) + Priming
## m3.glmer: SUFlike ~ (1 | ID) + Priming + Gender
##          npar  AIC  BIC logLik deviance Chisq Df Pr(&gt;Chisq)    
## m1.glmer    3 1703 1720   -848     1697                        
## m3.glmer    4 1679 1702   -836     1671  25.4  1 0.00000047 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<pre class="r"><code>Anova(m3.glmer, test = &quot;Chi&quot;)</code></pre>
<pre><code>## Analysis of Deviance Table (Type II Wald chisquare tests)
## 
## Response: SUFlike
##         Chisq Df           Pr(&gt;Chisq)    
## Priming 124.4  1 &lt; 0.0000000000000002 ***
## Gender   28.6  1           0.00000009 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p><em>Gender</em> is significant and will therefore be included as a predictor (you can also observe that including Gender has substantially decreased both AIC and BIC).</p>
<pre class="r"><code># add ConversationType
ifelse(min(ftable(mblrdata$ConversationType, mblrdata$SUFlike)) == 0, &quot;incomplete information&quot;, &quot;okay&quot;)</code></pre>
<pre><code>## [1] &quot;okay&quot;</code></pre>
<pre class="r"><code>m4.glmer &lt;- update(m3.glmer, .~.+ConversationType)
ifelse(max(car::vif(m4.glmer)) &lt;= 3,  &quot;VIFs okay&quot;, &quot;VIFs unacceptable&quot;) </code></pre>
<pre><code>## [1] &quot;VIFs okay&quot;</code></pre>
<pre class="r"><code>anova(m4.glmer, m3.glmer, test = &quot;Chi&quot;) </code></pre>
<pre><code>## Data: mblrdata
## Models:
## m3.glmer: SUFlike ~ (1 | ID) + Priming + Gender
## m4.glmer: SUFlike ~ (1 | ID) + Priming + Gender + ConversationType
##          npar  AIC  BIC logLik deviance Chisq Df Pr(&gt;Chisq)    
## m3.glmer    4 1679 1702   -836     1671                        
## m4.glmer    5 1669 1697   -829     1659  12.8  1    0.00034 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<pre class="r"><code>Anova(m4.glmer, test = &quot;Chi&quot;)</code></pre>
<pre><code>## Analysis of Deviance Table (Type II Wald chisquare tests)
## 
## Response: SUFlike
##                  Chisq Df           Pr(&gt;Chisq)    
## Priming          130.7  1 &lt; 0.0000000000000002 ***
## Gender            13.4  1              0.00025 ***
## ConversationType  13.0  1              0.00031 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p><em>ConversationType</em> improves model fit (AIC and BIC decrease and it is reported as being significant) and will, therefore, be included in the model.</p>
<pre class="r"><code># add Priming*Age
ifelse(min(ftable(mblrdata$Priming, mblrdata$Age, mblrdata$SUFlike)) == 0, &quot;incomplete information&quot;, &quot;okay&quot;)</code></pre>
<pre><code>## [1] &quot;okay&quot;</code></pre>
<pre class="r"><code>m5.glmer &lt;- update(m4.glmer, .~.+Priming*Age)
ifelse(max(car::vif(m5.glmer)) &lt;= 3,  &quot;VIFs okay&quot;, &quot;WARNING: high VIFs!&quot;) </code></pre>
<pre><code>## [1] &quot;VIFs okay&quot;</code></pre>
<pre class="r"><code>anova(m5.glmer, m4.glmer, test = &quot;Chi&quot;) </code></pre>
<pre><code>## Data: mblrdata
## Models:
## m4.glmer: SUFlike ~ (1 | ID) + Priming + Gender + ConversationType
## m5.glmer: SUFlike ~ (1 | ID) + Priming + Gender + ConversationType + Age + 
## m5.glmer:     Priming:Age
##          npar  AIC  BIC logLik deviance Chisq Df Pr(&gt;Chisq)
## m4.glmer    5 1669 1697   -829     1659                    
## m5.glmer    7 1672 1711   -829     1658  0.98  2       0.61</code></pre>
<p>The interaction between <em>Priming</em> and <em>Age</em> is not significant and we thus not be included.</p>
<pre class="r"><code># add Priming*Gender
ifelse(min(ftable(mblrdata$Priming, mblrdata$Gender, mblrdata$SUFlike)) == 0, &quot;incomplete information&quot;, &quot;okay&quot;)</code></pre>
<pre><code>## [1] &quot;okay&quot;</code></pre>
<pre class="r"><code>m6.glmer &lt;- update(m4.glmer, .~.+Priming*Gender)
ifelse(max(car::vif(m6.glmer)) &lt;= 3,  &quot;VIFs okay&quot;, &quot;WARNING: high VIFs!&quot;) </code></pre>
<pre><code>## [1] &quot;WARNING: high VIFs!&quot;</code></pre>
<p>We get the warning that the VIFs are high (&gt;= 3) which means that the model suffers from (multi-)collinearity. We thus check the VIFs to determine how to proceed. If the VIFs are &gt; 10, then we definitely cannot use the model as the multicollinearity is excessive.</p>
<pre class="r"><code>car::vif(m6.glmer)</code></pre>
<pre><code>##          Priming           Gender ConversationType   Priming:Gender 
##            4.357            1.433            1.203            4.583</code></pre>
<p>The VIFs are below 5 which is not good (VIFs of 5 mean “that column in the model matrix is explainable from the others with an R<sup>2</sup> of 0.8” <span class="citation">(Gries <a href="#ref-gries2021statistics" role="doc-biblioref">2021</a>)</span>) but it is still arguably acceptable and we will thus check if including the interaction between <em>Priming</em> and <em>Gender</em> significantly improved model fit.</p>
<pre class="r"><code>anova(m6.glmer, m4.glmer, test = &quot;Chi&quot;) </code></pre>
<pre><code>## Data: mblrdata
## Models:
## m4.glmer: SUFlike ~ (1 | ID) + Priming + Gender + ConversationType
## m6.glmer: SUFlike ~ (1 | ID) + Priming + Gender + ConversationType + Priming:Gender
##          npar  AIC  BIC logLik deviance Chisq Df Pr(&gt;Chisq)   
## m4.glmer    5 1669 1697   -829     1659                       
## m6.glmer    6 1663 1697   -826     1651  7.42  1     0.0065 **
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<pre class="r"><code>Anova(m6.glmer, test = &quot;Chi&quot;)</code></pre>
<pre><code>## Analysis of Deviance Table (Type II Wald chisquare tests)
## 
## Response: SUFlike
##                   Chisq Df           Pr(&gt;Chisq)    
## Priming          131.96  1 &lt; 0.0000000000000002 ***
## Gender            13.58  1              0.00023 ***
## ConversationType  10.71  1              0.00107 ** 
## Priming:Gender     7.45  1              0.00633 ** 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>The interaction between <em>Priming</em> and <em>Gender</em> improved model fit (AIC and BIC reduction) and significantly correlates with the use of speech-unit final <em>like</em>. It will therefore be included in the model.</p>
<pre class="r"><code># add Priming*ConversationType
ifelse(min(ftable(mblrdata$Priming, mblrdata$ConversationType, mblrdata$SUFlike)) == 0, &quot;incomplete information&quot;, &quot;okay&quot;)</code></pre>
<pre><code>## [1] &quot;okay&quot;</code></pre>
<pre class="r"><code>m7.glmer &lt;- update(m6.glmer, .~.+Priming*ConversationType)
ifelse(max(car::vif(m7.glmer)) &lt;= 3,  &quot;VIFs okay&quot;, &quot;WARNING: high VIFs!&quot;) </code></pre>
<pre><code>## [1] &quot;WARNING: high VIFs!&quot;</code></pre>
<p>When including the interaction between <em>Priming</em> and <em>ConversationType</em> we get a warning that there are high VIFs (multicollinearity) so we inspect the VIFs in more detail.</p>
<pre class="r"><code># check VIFs
car::vif(m7.glmer) </code></pre>
<pre><code>##                  Priming                   Gender         ConversationType 
##                    5.106                    1.493                    1.492 
##           Priming:Gender Priming:ConversationType 
##                    4.929                    3.410</code></pre>
<p>The VIF of <em>Priming</em> is above 5 so we would normally continue without checking if including the interaction between <em>Priming</em> and <em>ConversationType</em> leads to a significant improvement in model fit. However, given that this is just a practical example, we check if including this interaction significantly improves model fit.</p>
<pre class="r"><code>anova(m7.glmer, m6.glmer, test = &quot;Chi&quot;)</code></pre>
<pre><code>## Data: mblrdata
## Models:
## m6.glmer: SUFlike ~ (1 | ID) + Priming + Gender + ConversationType + Priming:Gender
## m7.glmer: SUFlike ~ (1 | ID) + Priming + Gender + ConversationType + Priming:Gender + 
## m7.glmer:     Priming:ConversationType
##          npar  AIC  BIC logLik deviance Chisq Df Pr(&gt;Chisq)
## m6.glmer    6 1663 1697   -826     1651                    
## m7.glmer    7 1663 1702   -825     1649  2.03  1       0.15</code></pre>
<p>The interaction between <em>Priming</em> and <em>ConversationType</em> does not significantly correlate with the use of speech-unit final <em>like</em> and it does not explain much variance (AIC and BIC increase). It will be not be included in the model.</p>
<pre class="r"><code># add Age*Gender
ifelse(min(ftable(mblrdata$Age, mblrdata$Gender, mblrdata$SUFlike)) == 0, &quot;incomplete information&quot;, &quot;okay&quot;)</code></pre>
<pre><code>## [1] &quot;okay&quot;</code></pre>
<pre class="r"><code>m8.glmer &lt;- update(m6.glmer, .~.+Age*Gender)
ifelse(max(car::vif(m8.glmer)) &lt;= 3,  &quot;VIFs okay&quot;, &quot;WARNING: high VIFs!&quot;) </code></pre>
<pre><code>## [1] &quot;WARNING: high VIFs!&quot;</code></pre>
<p>When including the interaction between <em>Age</em> and <em>Gender</em> we get a warning that there are high VIFs (multicollinearity) so we inspect the VIFs in more detail.</p>
<pre class="r"><code># check VIFs
car::vif(m8.glmer)</code></pre>
<pre><code>##          Priming           Gender ConversationType              Age 
##            4.462            2.142            1.206            3.252 
##   Priming:Gender       Gender:Age 
##            4.668            3.213</code></pre>
<p>The VIFs are all below 5 so we test if including the interaction between <em>Gender</em> and <em>Age</em> significantly improves model fit.</p>
<pre class="r"><code>anova(m8.glmer, m6.glmer, test = &quot;Chi&quot;) </code></pre>
<pre><code>## Data: mblrdata
## Models:
## m6.glmer: SUFlike ~ (1 | ID) + Priming + Gender + ConversationType + Priming:Gender
## m8.glmer: SUFlike ~ (1 | ID) + Priming + Gender + ConversationType + Age + 
## m8.glmer:     Priming:Gender + Gender:Age
##          npar  AIC  BIC logLik deviance Chisq Df Pr(&gt;Chisq)
## m6.glmer    6 1663 1697   -826     1651                    
## m8.glmer    8 1666 1710   -825     1650  1.53  2       0.47</code></pre>
<p>The interaction between <em>Age</em> and <em>Gender</em> is not significant and will thus continue without it.</p>
<pre class="r"><code># add Age*ConversationType
ifelse(min(ftable(mblrdata$Age, mblrdata$ConversationType, mblrdata$SUFlike)) == 0, &quot;incomplete information&quot;, &quot;okay&quot;)</code></pre>
<pre><code>## [1] &quot;okay&quot;</code></pre>
<pre class="r"><code>m9.glmer &lt;- update(m6.glmer, .~.+Age*ConversationType)
ifelse(max(car::vif(m9.glmer)) &lt;= 3,  &quot;VIFs okay&quot;, &quot;WARNING: high VIFs!&quot;) </code></pre>
<pre><code>## [1] &quot;WARNING: high VIFs!&quot;</code></pre>
<p>When including the interaction between <em>Age</em> and <em>ConversationType</em> we get a warning that there are high VIFs (multicollinearity) so we inspect the VIFs in more detail.</p>
<pre class="r"><code># check VIFs
car::vif(m9.glmer)</code></pre>
<pre><code>##              Priming               Gender     ConversationType 
##                4.378                1.507                1.515 
##                  Age       Priming:Gender ConversationType:Age 
##                1.969                4.614                2.059</code></pre>
<p>The VIFs are all below 5 so we test if including the interaction between <em>ConversationType</em> and <em>Age</em> significantly improves model fit.</p>
<pre class="r"><code>anova(m9.glmer, m6.glmer, test = &quot;Chi&quot;) </code></pre>
<pre><code>## Data: mblrdata
## Models:
## m6.glmer: SUFlike ~ (1 | ID) + Priming + Gender + ConversationType + Priming:Gender
## m9.glmer: SUFlike ~ (1 | ID) + Priming + Gender + ConversationType + Age + 
## m9.glmer:     Priming:Gender + ConversationType:Age
##          npar  AIC  BIC logLik deviance Chisq Df Pr(&gt;Chisq)
## m6.glmer    6 1663 1697   -826     1651                    
## m9.glmer    8 1666 1711   -825     1650   0.9  2       0.64</code></pre>
<p>The interaction between <em>Age</em> and <em>ConversationType</em> is insignificant and does not improve model fit (AIC and BIC reduction). It will therefore not be included in the model.</p>
<pre class="r"><code># add Gender*ConversationType
ifelse(min(ftable(mblrdata$Gender, mblrdata$ConversationType, mblrdata$SUFlike)) == 0, &quot;incomplete information&quot;, &quot;okay&quot;)</code></pre>
<pre><code>## [1] &quot;okay&quot;</code></pre>
<pre class="r"><code>m10.glmer &lt;- update(m6.glmer, .~.+Gender*ConversationType)
ifelse(max(car::vif(m10.glmer)) &lt;= 3,  &quot;VIFs okay&quot;, &quot;WARNING: high VIFs!&quot;)</code></pre>
<pre><code>## [1] &quot;WARNING: high VIFs!&quot;</code></pre>
<p>When including the interaction between <em>Gender</em> and <em>ConversationType</em> we get a warning that there are high VIFs (multicollinearity) so we inspect the VIFs in more detail.</p>
<pre class="r"><code># check VIFs
car::vif(m10.glmer) </code></pre>
<pre><code>##                 Priming                  Gender        ConversationType 
##                   4.962                   1.733                   7.766 
##          Priming:Gender Gender:ConversationType 
##                   5.119                   9.251</code></pre>
<p>The highest VIF is almost 10 (9.251) which is why the interaction between <em>Gender</em> and <em>ConversationType</em> will not be included in the model.</p>
<pre class="r"><code># add Priming*Age*Gender
ifelse(min(ftable(mblrdata$Priming,mblrdata$Age, mblrdata$Gender, mblrdata$SUFlike)) == 0, &quot;incomplete information&quot;, &quot;okay&quot;)</code></pre>
<pre><code>## [1] &quot;okay&quot;</code></pre>
<pre class="r"><code>m11.glmer &lt;- update(m6.glmer, .~.+Priming*Age*Gender)
ifelse(max(car::vif(m11.glmer)) &lt;= 3,  &quot;VIFs okay&quot;, &quot;WARNING: high VIFs!&quot;) </code></pre>
<pre><code>## [1] &quot;WARNING: high VIFs!&quot;</code></pre>
<p>When including the interaction between <em>Priming</em>, <em>Age</em>, and <em>Gender</em> we get a warning that there are high VIFs (multicollinearity) so we inspect the VIFs in more detail.</p>
<pre class="r"><code># check VIFs
car::vif(m11.glmer)</code></pre>
<pre><code>##            Priming             Gender   ConversationType                Age 
##              6.573              2.284              1.223              3.660 
##     Priming:Gender        Priming:Age         Gender:Age Priming:Gender:Age 
##              6.659              5.852              3.758              5.979</code></pre>
<p>There are several VIFs with values greater than 5 and we will thus continue without including the interaction between <em>Priming</em>, <em>Age</em>, and <em>Gender</em> into the model.</p>
<pre class="r"><code># add Priming*Age*ConversationType
ifelse(min(ftable(mblrdata$Priming,mblrdata$Age, mblrdata$ConversationType, mblrdata$SUFlike)) == 0, &quot;incomplete information&quot;, &quot;okay&quot;)</code></pre>
<pre><code>## [1] &quot;okay&quot;</code></pre>
<pre class="r"><code>m12.glmer &lt;- update(m6.glmer, .~.+Priming*Age*ConversationType)
ifelse(max(car::vif(m12.glmer)) &lt;= 3,  &quot;VIFs okay&quot;, &quot;WARNING: high VIFs!&quot;) </code></pre>
<pre><code>## [1] &quot;WARNING: high VIFs!&quot;</code></pre>
<p>When including the interaction between <em>Priming</em>, <em>Age</em>, and <em>Gender</em> we get a warning that there are high VIFs (multicollinearity) so we inspect the VIFs in more detail.</p>
<pre class="r"><code># check VIFs
car::vif(m12.glmer)</code></pre>
<pre><code>##                      Priming                       Gender 
##                        7.147                        1.611 
##             ConversationType                          Age 
##                        1.832                        2.202 
##               Priming:Gender                  Priming:Age 
##                        4.993                        3.332 
##     Priming:ConversationType         ConversationType:Age 
##                        4.874                        2.383 
## Priming:ConversationType:Age 
##                        2.999</code></pre>
<p>The VIF of Priming is very high (7.1469) which is why we will thus continue without including the interaction between <em>Priming</em>, <em>Age</em>, and <em>Gender</em> in the model.</p>
<pre class="r"><code># add Priming*Gender*ConversationType
ifelse(min(ftable(mblrdata$Priming,mblrdata$Gender, mblrdata$ConversationType, mblrdata$SUFlike)) == 0, &quot;incomplete information&quot;, &quot;okay&quot;)</code></pre>
<pre><code>## [1] &quot;okay&quot;</code></pre>
<pre class="r"><code>m13.glmer &lt;- update(m6.glmer, .~.+Priming*Gender*ConversationType)
ifelse(max(car::vif(m13.glmer)) &lt;= 3,  &quot;VIFs okay&quot;, &quot;WARNING: high VIFs!&quot;) </code></pre>
<pre><code>## [1] &quot;WARNING: high VIFs!&quot;</code></pre>
<p>The VIFs are excessive with a maximum value is 23.8693 which shows an unacceptable degree of multicollinearity so that we abort and move on the next model.</p>
<pre class="r"><code>car::vif(m13.glmer)</code></pre>
<pre><code>##                         Priming                          Gender 
##                           7.696                           1.812 
##                ConversationType                  Priming:Gender 
##                          21.932                          10.014 
##        Priming:ConversationType         Gender:ConversationType 
##                          21.622                          23.869 
## Priming:Gender:ConversationType 
##                          22.669</code></pre>
<p>The VIFs are excessive with a maximum value is 23.8693 which shows an unacceptable degree of multicollinearity so that we abort and move on the next model.</p>
<pre class="r"><code># add Age*Gender*ConversationType
ifelse(min(ftable(mblrdata$Age,mblrdata$Gender, mblrdata$ConversationType, mblrdata$SUFlike)) == 0, &quot;incomplete information&quot;, &quot;okay&quot;)</code></pre>
<pre><code>## [1] &quot;okay&quot;</code></pre>
<pre class="r"><code>m14.glmer &lt;- update(m6.glmer, .~.+Age*Gender*ConversationType)
ifelse(max(car::vif(m14.glmer)) &lt;= 3,  &quot;VIFs okay&quot;, &quot;WARNING: high VIFs!&quot;) </code></pre>
<pre><code>## [1] &quot;WARNING: high VIFs!&quot;</code></pre>
<p>When including the interaction between <em>Age</em>, <em>Gender</em>, <em>ConversationType</em>, we get a warning that there are high VIFs (multicollinearity) so we inspect the VIFs in more detail.</p>
<pre class="r"><code>car::vif(m14.glmer)</code></pre>
<pre><code>##                     Priming                      Gender 
##                       5.333                       2.601 
##            ConversationType                         Age 
##                      11.947                       3.801 
##              Priming:Gender                  Gender:Age 
##                       5.455                       5.662 
##        ConversationType:Age     Gender:ConversationType 
##                      16.993                      13.812 
## Gender:ConversationType:Age 
##                      19.084</code></pre>
<p>Again, the VIFs are excessive with a maximum value of 19.0842 which shows an unacceptable degree of multicollinearity so that we abort and move on the next model.</p>
<pre class="r"><code># add Priming*Age*Gender*ConversationType
ifelse(min(ftable(mblrdata$Priming,mblrdata$Age,mblrdata$Gender, mblrdata$ConversationType, mblrdata$SUFlike)) == 0, &quot;incomplete information&quot;, &quot;okay&quot;)</code></pre>
<pre><code>## [1] &quot;incomplete information&quot;</code></pre>
<p>The model suffers from incomplete information! As this was the last possible model, we have found our final minimal adequate model in m6.glmer.</p>
<p>In a next step, we create an overview of model comparisons which serves as a summary for the model fitting process and provides AIC, BIC, and <span class="math inline">\(\chi\)</span><sup>2</sup> values.</p>
<pre class="r"><code>source(&quot;https://slcladal.github.io/rscripts/ModelFittingSummarySWSU.r&quot;) 
# comparisons of glmer objects
m1.m0 &lt;- anova(m1.glmer, m0.glmer, test = &quot;Chi&quot;) 
m2.m1 &lt;- anova(m2.glmer, m1.glmer, test = &quot;Chi&quot;)   
m3.m1 &lt;- anova(m3.glmer, m1.glmer, test = &quot;Chi&quot;)
m4.m3 &lt;- anova(m4.glmer, m3.glmer, test = &quot;Chi&quot;) 
m5.m4 &lt;- anova(m5.glmer, m4.glmer, test = &quot;Chi&quot;) 
m6.m4 &lt;- anova(m6.glmer, m4.glmer, test = &quot;Chi&quot;) 
m7.m6 &lt;- anova(m7.glmer, m6.glmer, test = &quot;Chi&quot;)
m8.m6 &lt;- anova(m8.glmer, m6.glmer, test = &quot;Chi&quot;) 
m9.m6 &lt;- anova(m9.glmer, m6.glmer, test = &quot;Chi&quot;) 
# create a list of the model comparisons
mdlcmp &lt;- list(m1.m0, m2.m1, m3.m1, m4.m3, m5.m4, m6.m4, m7.m6, m8.m6, m9.m6)
# summary table for model fitting
mdlft &lt;- mdl.fttng.swsu(mdlcmp)
mdlft &lt;- mdlft[,-2]</code></pre>
<table class="table table-striped table-hover table-condensed table-responsive" style="width: auto !important; margin-left: auto; margin-right: auto;">
<caption>
</caption>
<thead>
<tr>
<th style="text-align:left;">
Model
</th>
<th style="text-align:left;">
Term Added
</th>
<th style="text-align:left;">
Compared to…
</th>
<th style="text-align:left;">
DF
</th>
<th style="text-align:left;">
AIC
</th>
<th style="text-align:left;">
BIC
</th>
<th style="text-align:left;">
LogLikelihood
</th>
<th style="text-align:left;">
Residual Deviance
</th>
<th style="text-align:left;">
X2
</th>
<th style="text-align:left;">
X2DF
</th>
<th style="text-align:left;">
p-value
</th>
<th style="text-align:left;">
Significance
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
m1.glmer
</td>
<td style="text-align:left;">
1+Priming
</td>
<td style="text-align:left;">
m0.glmer
</td>
<td style="text-align:left;">
3
</td>
<td style="text-align:left;">
1702.77
</td>
<td style="text-align:left;">
1719.58
</td>
<td style="text-align:left;">
-848.39
</td>
<td style="text-align:left;">
1696.77
</td>
<td style="text-align:left;">
127.72
</td>
<td style="text-align:left;">
1
</td>
<td style="text-align:left;">
0
</td>
<td style="text-align:left;">
p &lt; .001***
</td>
</tr>
<tr>
<td style="text-align:left;">
m2.glmer
</td>
<td style="text-align:left;">
Age
</td>
<td style="text-align:left;">
m1.glmer
</td>
<td style="text-align:left;">
4
</td>
<td style="text-align:left;">
1704.21
</td>
<td style="text-align:left;">
1726.61
</td>
<td style="text-align:left;">
-848.11
</td>
<td style="text-align:left;">
1696.21
</td>
<td style="text-align:left;">
0.56
</td>
<td style="text-align:left;">
1
</td>
<td style="text-align:left;">
0.45323
</td>
<td style="text-align:left;">
n.s.
</td>
</tr>
<tr>
<td style="text-align:left;">
m3.glmer
</td>
<td style="text-align:left;">
Gender
</td>
<td style="text-align:left;">
m1.glmer
</td>
<td style="text-align:left;">
4
</td>
<td style="text-align:left;">
1679.4
</td>
<td style="text-align:left;">
1701.8
</td>
<td style="text-align:left;">
-835.7
</td>
<td style="text-align:left;">
1671.4
</td>
<td style="text-align:left;">
25.38
</td>
<td style="text-align:left;">
1
</td>
<td style="text-align:left;">
0
</td>
<td style="text-align:left;">
p &lt; .001***
</td>
</tr>
<tr>
<td style="text-align:left;">
m4.glmer
</td>
<td style="text-align:left;">
ConversationType
</td>
<td style="text-align:left;">
m3.glmer
</td>
<td style="text-align:left;">
5
</td>
<td style="text-align:left;">
1668.58
</td>
<td style="text-align:left;">
1696.59
</td>
<td style="text-align:left;">
-829.29
</td>
<td style="text-align:left;">
1658.58
</td>
<td style="text-align:left;">
12.81
</td>
<td style="text-align:left;">
1
</td>
<td style="text-align:left;">
0.00034
</td>
<td style="text-align:left;">
p &lt; .001***
</td>
</tr>
<tr>
<td style="text-align:left;">
m5.glmer
</td>
<td style="text-align:left;">
Age+Priming:Age
</td>
<td style="text-align:left;">
m4.glmer
</td>
<td style="text-align:left;">
7
</td>
<td style="text-align:left;">
1671.6
</td>
<td style="text-align:left;">
1710.81
</td>
<td style="text-align:left;">
-828.8
</td>
<td style="text-align:left;">
1657.6
</td>
<td style="text-align:left;">
0.98
</td>
<td style="text-align:left;">
2
</td>
<td style="text-align:left;">
0.61134
</td>
<td style="text-align:left;">
n.s.
</td>
</tr>
<tr>
<td style="text-align:left;">
m6.glmer
</td>
<td style="text-align:left;">
Priming:Gender
</td>
<td style="text-align:left;">
m4.glmer
</td>
<td style="text-align:left;">
6
</td>
<td style="text-align:left;">
1663.16
</td>
<td style="text-align:left;">
1696.77
</td>
<td style="text-align:left;">
-825.58
</td>
<td style="text-align:left;">
1651.16
</td>
<td style="text-align:left;">
7.42
</td>
<td style="text-align:left;">
1
</td>
<td style="text-align:left;">
0.00645
</td>
<td style="text-align:left;">
p &lt; .01 **
</td>
</tr>
<tr>
<td style="text-align:left;">
m7.glmer
</td>
<td style="text-align:left;">
Priming:ConversationType
</td>
<td style="text-align:left;">
m6.glmer
</td>
<td style="text-align:left;">
7
</td>
<td style="text-align:left;">
1663.13
</td>
<td style="text-align:left;">
1702.34
</td>
<td style="text-align:left;">
-824.57
</td>
<td style="text-align:left;">
1649.13
</td>
<td style="text-align:left;">
2.03
</td>
<td style="text-align:left;">
1
</td>
<td style="text-align:left;">
0.15427
</td>
<td style="text-align:left;">
n.s.
</td>
</tr>
<tr>
<td style="text-align:left;">
m8.glmer
</td>
<td style="text-align:left;">
Age+Gender:Age
</td>
<td style="text-align:left;">
m6.glmer
</td>
<td style="text-align:left;">
8
</td>
<td style="text-align:left;">
1665.63
</td>
<td style="text-align:left;">
1710.44
</td>
<td style="text-align:left;">
-824.82
</td>
<td style="text-align:left;">
1649.63
</td>
<td style="text-align:left;">
1.53
</td>
<td style="text-align:left;">
2
</td>
<td style="text-align:left;">
0.46535
</td>
<td style="text-align:left;">
n.s.
</td>
</tr>
<tr>
<td style="text-align:left;">
m9.glmer
</td>
<td style="text-align:left;">
Age+ConversationType:Age
</td>
<td style="text-align:left;">
m6.glmer
</td>
<td style="text-align:left;">
8
</td>
<td style="text-align:left;">
1666.26
</td>
<td style="text-align:left;">
1711.07
</td>
<td style="text-align:left;">
-825.13
</td>
<td style="text-align:left;">
1650.26
</td>
<td style="text-align:left;">
0.9
</td>
<td style="text-align:left;">
2
</td>
<td style="text-align:left;">
0.63689
</td>
<td style="text-align:left;">
n.s.
</td>
</tr>
</tbody>
</table>
<p>We now rename our final minimal adequate model, test whether it performs significantly better than the minimal base-line model, and print the regression summary.</p>
<pre class="r"><code># rename final minimal adequate model
mlr.glmer &lt;- m6.glmer 
# final model better than base-line model
sigfit &lt;- anova(mlr.glmer, m0.glmer, test = &quot;Chi&quot;) 
# inspect
sigfit</code></pre>
<pre><code>## Data: mblrdata
## Models:
## m0.glmer: SUFlike ~ 1 + (1 | ID)
## mlr.glmer: SUFlike ~ (1 | ID) + Priming + Gender + ConversationType + Priming:Gender
##           npar  AIC  BIC logLik deviance Chisq Df          Pr(&gt;Chisq)    
## m0.glmer     2 1828 1840   -912     1824                                 
## mlr.glmer    6 1663 1697   -826     1651   173  4 &lt;0.0000000000000002 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<pre class="r"><code># inspect final minimal adequate model
print(mlr.glmer, corr = F)</code></pre>
<pre><code>## Generalized linear mixed model fit by maximum likelihood (Laplace
##   Approximation) [glmerMod]
##  Family: binomial  ( logit )
## Formula: 
## SUFlike ~ (1 | ID) + Priming + Gender + ConversationType + Priming:Gender
##    Data: mblrdata
##      AIC      BIC   logLik deviance df.resid 
##   1663.2   1696.8   -825.6   1651.2     1994 
## Random effects:
##  Groups Name        Std.Dev.
##  ID     (Intercept) 0.292   
## Number of obs: 2000, groups:  ID, 208
## Fixed Effects:
##                (Intercept)                PrimingPrime  
##                     -0.921                       1.060  
##                GenderWomen  ConversationTypeSameGender  
##                     -0.868                      -0.492  
##   PrimingPrime:GenderWomen  
##                      1.036</code></pre>
<p>To extract the effect sizes of the significant fixed effects, we compare the model with that effect to a model without that effect. This can be problematic when checking the effect of main effects that are involved in significant interactions though <span class="citation">(Field, Miles, and Field <a href="#ref-field2012discovering" role="doc-biblioref">2012</a>, 622)</span>.</p>
<pre class="r"><code># effect of ConversationType
ef_conv &lt;- anova(m4.glmer, m3.glmer, test = &quot;Chi&quot;) 
# inspect
ef_conv</code></pre>
<pre><code>## Data: mblrdata
## Models:
## m3.glmer: SUFlike ~ (1 | ID) + Priming + Gender
## m4.glmer: SUFlike ~ (1 | ID) + Priming + Gender + ConversationType
##          npar  AIC  BIC logLik deviance Chisq Df Pr(&gt;Chisq)    
## m3.glmer    4 1679 1702   -836     1671                        
## m4.glmer    5 1669 1697   -829     1659  12.8  1    0.00034 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<pre class="r"><code># effect of Priming:Gender
ef_prigen &lt;- anova(m6.glmer, m4.glmer, test = &quot;Chi&quot;)
# inspect
ef_prigen</code></pre>
<pre><code>## Data: mblrdata
## Models:
## m4.glmer: SUFlike ~ (1 | ID) + Priming + Gender + ConversationType
## m6.glmer: SUFlike ~ (1 | ID) + Priming + Gender + ConversationType + Priming:Gender
##          npar  AIC  BIC logLik deviance Chisq Df Pr(&gt;Chisq)   
## m4.glmer    5 1669 1697   -829     1659                       
## m6.glmer    6 1663 1697   -826     1651  7.42  1     0.0065 **
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
</div>
<div id="visualizing-effects" class="section level3 unnumbered">
<h3>Visualizing Effects</h3>
<p>As we will see the effects in the final summary, we visualize the effects here by showing the probability of discourse <em>like</em> based on the predicted values.</p>
<pre class="r"><code># extract predicted values
mblrdata$Predicted &lt;- predict(m6.glmer, mblrdata, type = &quot;response&quot;)
# plot
ggplot(mblrdata, aes(ConversationType, Predicted)) +
  stat_summary(fun = mean, geom = &quot;point&quot;) +
  stat_summary(fun.data = mean_cl_boot, geom = &quot;errorbar&quot;, width = 0.2) +
  theme_set(theme_bw(base_size = 10)) +
  theme(legend.position = &quot;top&quot;) +
    ylim(0, .75) +
  labs(x = &quot;&quot;, y = &quot;Predicted Probabilty of discourse like&quot;) +
  scale_color_manual(values = c(&quot;gray20&quot;, &quot;gray70&quot;))</code></pre>
<p><img src="mmws_files/figure-html/blmm33-1.png" width="672" /></p>
<p>A proper visualization of the marginal effects can be extracted using the <code>sjPlot</code> package.</p>
<pre class="r"><code>plot_model(m6.glmer, type = &quot;pred&quot;, terms = c(&quot;Priming&quot;, &quot;Gender&quot;))</code></pre>
<p><img src="mmws_files/figure-html/blmm33b-1.png" width="672" /></p>
<p>We can see that discourse like is more likely to surface in primed contexts but that in contrast to women and men in same-gender conversations as well as women in mixed-gender conversations, priming appears to affect the use of discourse like by men in mixed-gender conversations only very little.</p>
</div>
<div id="extracting-model-fit-parameters" class="section level3 unnumbered">
<h3>Extracting Model Fit Parameters</h3>
<p>We now extract model fit parameters <span class="citation">(Baayen <a href="#ref-baayen2008analyzing" role="doc-biblioref">2008</a>, 281)</span>.</p>
<pre class="r"><code>probs = 1/(1+exp(-fitted(mlr.glmer)))
probs = binomial()$linkinv(fitted(mlr.glmer))
somers2(probs, as.numeric(mblrdata$SUFlike))</code></pre>
<pre><code>##         C       Dxy         n   Missing 
##    0.7583    0.5167 2000.0000    0.0000</code></pre>
<p>The two lines that start with <code>probs</code> are simply two different ways to do the same thing (you only need one of these).</p>
<p>The model fit parameters indicate a suboptimal fit. Both the C-value and Somers’s D<sub>xy</sub> show poor fit between predicted and observed occurrences of discourse <em>like</em>. If the C-value is 0.5, the predictions are random, while the predictions are perfect if the C-value is 1. C-values above 0.8 indicate real predictive capacity <span class="citation">(Baayen <a href="#ref-baayen2008analyzing" role="doc-biblioref">2008</a>, 204)</span>. Somers’ D<sub>xy</sub> is a value that represents a rank correlation between predicted probabilities and observed responses. Somers’ D<sub>xy</sub> values range between 0, which indicates complete randomness, and 1, which indicates perfect prediction <span class="citation">(Baayen <a href="#ref-baayen2008analyzing" role="doc-biblioref">2008</a>, 204)</span>. The C.value of 0.7583 suggests that the model has some predictive and explanatory power, but not at an optimal level. We will now perform the model diagnostics.</p>
</div>
<div id="model-diagnostics-1" class="section level3 unnumbered">
<h3>Model Diagnostics</h3>
<p>We begin the model diagnostics by generating a diagnostic that plots the fitted or predicted values against the residuals.</p>
<pre class="r"><code>plot(mlr.glmer, pch = 20, col = &quot;black&quot;, lty = &quot;dotted&quot;)</code></pre>
<p><img src="mmws_files/figure-html/blmm38-1.png" width="672" /></p>
<p>As a final step, we summarize our findings in tabulated form.</p>
<pre class="r"><code># summarize final model
sjPlot::tab_model(mlr.glmer)</code></pre>
<table style="border-collapse:collapse; border:none;">
<tr>
<th style="border-top: double; text-align:center; font-style:normal; font-weight:bold; padding:0.2cm;  text-align:left; ">
 
</th>
<th colspan="3" style="border-top: double; text-align:center; font-style:normal; font-weight:bold; padding:0.2cm; ">
SUFlike
</th>
</tr>
<tr>
<td style=" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  text-align:left; ">
Predictors
</td>
<td style=" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  ">
Odds Ratios
</td>
<td style=" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  ">
CI
</td>
<td style=" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  ">
p
</td>
</tr>
<tr>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; ">
(Intercept)
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  ">
0.40
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  ">
0.29 – 0.54
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  ">
<strong>&lt;0.001
</td>
</tr>
<tr>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; ">
Priming [Prime]
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  ">
2.89
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  ">
1.50 – 5.56
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  ">
<strong>0.002</strong>
</td>
</tr>
<tr>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; ">
Gender [Women]
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  ">
0.42
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  ">
0.29 – 0.61
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  ">
<strong>&lt;0.001
</td>
</tr>
<tr>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; ">
ConversationType<br>[SameGender]
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  ">
0.61
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  ">
0.46 – 0.82
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  ">
<strong>0.001</strong>
</td>
</tr>
<tr>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; ">
Priming [Prime] * Gender<br>[Women]
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  ">
2.82
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  ">
1.34 – 5.93
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  ">
<strong>0.006</strong>
</td>
</tr>
<tr>
<td colspan="4" style="font-weight:bold; text-align:left; padding-top:.8em;">
Random Effects
</td>
</tr>
<tr>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; padding-top:0.1cm; padding-bottom:0.1cm;">
σ<sup>2</sup>
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left;" colspan="3">
3.29
</td>
<tr>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; padding-top:0.1cm; padding-bottom:0.1cm;">
τ<sub>00</sub> <sub>ID</sub>
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left;" colspan="3">
0.09
</td>
<tr>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; padding-top:0.1cm; padding-bottom:0.1cm;">
ICC
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left;" colspan="3">
0.03
</td>
<tr>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; padding-top:0.1cm; padding-bottom:0.1cm;">
N <sub>ID</sub>
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left;" colspan="3">
208
</td>
<tr>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; padding-top:0.1cm; padding-bottom:0.1cm; border-top:1px solid;">
Observations
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left; border-top:1px solid;" colspan="3">
2000
</td>
</tr>
<tr>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; padding-top:0.1cm; padding-bottom:0.1cm;">
Marginal R<sup>2</sup> / Conditional R<sup>2</sup>
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left;" colspan="3">
0.138 / 0.160
</td>
</tr>
</table>
<hr />
<p>A mixed-effect binomial logistic regression model with random intercepts for speakers was fit to the data in a step-wise-step up procedure. The final minimal adequate model performed significantly better than an intercept-only base line model (<span class="math inline">\(\chi\)</span><sup>2</sup>(4): 173.3278, p = 0) and a good but not optimal fit (C: 0.7583, Somers’ D<sub>xy</sub>: 0.5167). The final minimal adequate model reported that speakers use more discourse <em>like</em> in mixed-gender conversations compared to same-gender conversations (<span class="math inline">\(\chi\)</span><sup>2</sup>(1): `r 12.8138, p = 0.0003) and that there is an interaction between priming and gender with men using more discourse <em>like</em> in un-primed contexts while this gender difference is not present in primed contexts where speakers more more likely to use discourse <em>like</em> regardless of gender (<span class="math inline">\(\chi\)</span><sup>2</sup>(1): 7.4187, p = 0.0064).</p>
</div>
</div>
</div>
<div id="citation-session-info" class="section level1 unnumbered">
<h1>Citation &amp; Session Info</h1>
<p>Schweinberger, Martin. 2021. <em>Mixed-Effects Regression Models in R. Workshop at UIT AcqVA Aurora</em>. Brisbane: The University of Queensland. url: <a href="https://slcladal.github.io/mmws.html" class="uri">https://slcladal.github.io/mmws.html</a> (Version 2021.04.18).</p>
<pre><code>@manual{schweinberger2021mmws,
  author = {Schweinberger, Martin},
  title = {Fixed- and Mixed-Effects Regression Models in R},
  note = {https://slcladal.github.io/regression.html},
  year = {2021},
  organization = &quot;Arctic University of Norway, AcqVA Aurora Center},
  address = {Tromsø},
  edition = {2021.04.18}
}</code></pre>
<pre class="r"><code>sessionInfo()</code></pre>
<pre><code>## R version 4.0.3 (2020-10-10)
## Platform: x86_64-w64-mingw32/x64 (64-bit)
## Running under: Windows 10 x64 (build 19041)
## 
## Matrix products: default
## 
## locale:
## [1] LC_COLLATE=German_Germany.1252  LC_CTYPE=German_Germany.1252   
## [3] LC_MONETARY=German_Germany.1252 LC_NUMERIC=C                   
## [5] LC_TIME=German_Germany.1252    
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] rms_6.0-1        SparseM_1.78     Hmisc_4.4-1      Formula_1.2-4   
##  [5] survival_3.2-7   lattice_0.20-41  sjPlot_2.8.6     ggfortify_0.4.11
##  [9] DT_0.16          kableExtra_1.3.1 knitr_1.30       car_3.0-10      
## [13] carData_3.0-4    lme4_1.1-25      Matrix_1.2-18    vip_0.3.2       
## [17] ggpubr_0.4.0     forcats_0.5.0    stringr_1.4.0    dplyr_1.0.2     
## [21] purrr_0.3.4      readr_1.4.0      tidyr_1.1.2      tibble_3.0.4    
## [25] ggplot2_3.3.3    tidyverse_1.3.0 
## 
## loaded via a namespace (and not attached):
##   [1] TH.data_1.0-10      minqa_1.2.4         colorspace_1.4-1   
##   [4] ggsignif_0.6.1      ellipsis_0.3.1      rio_0.5.16         
##   [7] sjlabelled_1.1.7    htmlTable_2.1.0     estimability_1.3   
##  [10] base64enc_0.1-3     parameters_0.12.0   fs_1.5.0           
##  [13] rstudioapi_0.11     farver_2.0.3        MatrixModels_0.4-1 
##  [16] fansi_0.4.1         mvtnorm_1.1-1       lubridate_1.7.9    
##  [19] xml2_1.3.2          codetools_0.2-16    splines_4.0.3      
##  [22] sjmisc_2.8.5        jsonlite_1.7.1      nloptr_1.2.2.2     
##  [25] ggeffects_0.16.0    broom_0.7.2         cluster_2.1.0      
##  [28] dbplyr_2.0.0        png_0.1-7           effectsize_0.4.4   
##  [31] compiler_4.0.3      httr_1.4.2          sjstats_0.18.0     
##  [34] emmeans_1.5.2-1     backports_1.1.10    assertthat_0.2.1   
##  [37] cli_2.1.0           quantreg_5.75       htmltools_0.5.0    
##  [40] tools_4.0.3         coda_0.19-4         gtable_0.3.0       
##  [43] glue_1.4.2          Rcpp_1.0.5          cellranger_1.1.0   
##  [46] vctrs_0.3.4         nlme_3.1-149        conquer_1.0.2      
##  [49] crosstalk_1.1.0.1   insight_0.13.1      xfun_0.19          
##  [52] openxlsx_4.2.3      rvest_0.3.6         lifecycle_0.2.0    
##  [55] statmod_1.4.35      rstatix_0.6.0       polspline_1.1.19   
##  [58] MASS_7.3-53         zoo_1.8-8           scales_1.1.1       
##  [61] hms_0.5.3           sandwich_3.0-0      RColorBrewer_1.1-2 
##  [64] yaml_2.2.1          curl_4.3            gridExtra_2.3      
##  [67] rpart_4.1-15        latticeExtra_0.6-29 stringi_1.5.3      
##  [70] highr_0.8           bayestestR_0.8.2    checkmate_2.0.0    
##  [73] boot_1.3-25         zip_2.1.1           matrixStats_0.57.0 
##  [76] rlang_0.4.8         pkgconfig_2.0.3     evaluate_0.14      
##  [79] htmlwidgets_1.5.3   labeling_0.4.2      cowplot_1.1.0      
##  [82] tidyselect_1.1.0    magrittr_1.5        R6_2.5.0           
##  [85] generics_0.1.0      multcomp_1.4-14     DBI_1.1.0          
##  [88] pillar_1.4.6        haven_2.3.1         foreign_0.8-80     
##  [91] withr_2.3.0         mgcv_1.8-33         nnet_7.3-14        
##  [94] abind_1.4-5         performance_0.5.1   modelr_0.1.8       
##  [97] crayon_1.3.4        rmarkdown_2.5       jpeg_0.1-8.1       
## [100] grid_4.0.3          readxl_1.3.1        data.table_1.13.2  
## [103] reprex_0.3.0        digest_0.6.27       webshot_0.5.2      
## [106] xtable_1.8-4        munsell_0.5.0       viridisLite_0.3.0</code></pre>
<hr />
<p><a href="#introduction">Back to top</a></p>
<hr />
</div>
<div id="references" class="section level1 unnumbered">
<h1>References</h1>
<div id="refs" class="references">
<div id="ref-achen1982interpreting">
<p>Achen, Christopher H. 1982. <em>Interpreting and Using Regression</em>. Vol. 29. Sage.</p>
</div>
<div id="ref-austin2018multilevel">
<p>Austin, Peter C., and George Leckie. 2018. “The Effect of Number of Clusters and Cluster Size on Statistical Power and Type I Error Rates When Testing Random Effects Variance Components in Multilevel Linear and Logistic Regression Models.” <em>Journal of Statistical Computation and Simulation</em> 88 (16): 3151–63.</p>
</div>
<div id="ref-baayen2008analyzing">
<p>Baayen, R Harald. 2008. <em>Analyzing Linguistic Data. A Practical Introduction to Statistics Using R</em>. Cambridge: Cambridge University press.</p>
</div>
<div id="ref-barton2020mumin">
<p>Bartoń, Kamil. 2020. “Package Mumin - Multi-Model Inference.” The Comprehensive R Archive Network. <a href="https://cran.r-project.org/web/packages/MuMIn/MuMIn.pdf">https://cran.r-project.org/web/packages/MuMIn/MuMIn.pdf</a>.</p>
</div>
<div id="ref-bell2008multilevel">
<p>Bell, Bethany A., John M. Ferron, and Jeffrey D. Kromrey. 2008. “Cluster Size in Multilevel Models: The Impact of Sparse Data Structures on Point and Interval Estimates in Two-Level Models.” In <em>JSM Proceedings. Section on Survey Research Methods</em>, 1122–9. American Statistical Association Alexandria, VA.</p>
</div>
<div id="ref-booth1994regression">
<p>Booth GD, Schuster EG, Niccolucci MJ. 1994. “Identifying Proxy Sets in Multiple Linear Regression: An Aid to Better Coefficient Interpretation. Research Paper Int-470.”</p>
</div>
<div id="ref-bortz2006statistik">
<p>Bortz, Jürgen. 2006. <em>Statistik: Für Human-Und Sozialwissenschaftler</em>. Springer-Verlag.</p>
</div>
<div id="ref-bowerman1990linear">
<p>Bowerman, Bruce L, and Richard T O’Connell. 1990. <em>Linear Statistical Models: An Applied Approach</em>. Boston: PWS-Kent.</p>
</div>
<div id="ref-clarke2008can">
<p>Clarke, Philippa. 2008. “When Can Group Level Clustering Be Ignored? Multilevel Models Versus Single-Level Models with Sparse Data.” <em>Journal of Epidemiology &amp; Community Health</em> 62 (8): 752–58.</p>
</div>
<div id="ref-clarke2007addressing">
<p>Clarke, Philippa, and Blair Wheaton. 2007. “Addressing Data Sparseness in Contextual Population Research: Using Cluster Analysis to Create Synthetic Neighborhoods.” <em>Sociological Methods &amp; Research</em> 35 (3): 311–51.</p>
</div>
<div id="ref-crawley2005statistics">
<p>Crawley, Michael J. 2005. <em>Statistics: An Introduction Using R. 2005</em>. Chichester, West Sussex: John Wiley &amp; Sons.</p>
</div>
<div id="ref-crawley2012r">
<p>———. 2012. <em>The R Book</em>. John Wiley &amp; Sons.</p>
</div>
<div id="ref-faraway2002practical">
<p>Faraway, Julian J. 2002. <em>Practical Regression and Anova Using R.</em> University of Bath.</p>
</div>
<div id="ref-field2012discovering">
<p>Field, Andy, Jeremy Miles, and Zoe Field. 2012. <em>Discovering Statistics Using R</em>. Sage.</p>
</div>
<div id="ref-gries2021statistics">
<p>Gries, Stefan Th. 2021. <em>Statistics for Linguistics Using R: A Practical Introduction</em>. Berlin &amp; New York: Mouton de Gruyter.</p>
</div>
<div id="ref-johnson2009getting">
<p>Johnson, Daniel Ezra. 2009. “Getting Off the Goldvarb Standard: Introducing Rbrul for Mixed-Effects Variable Rule Analysis.” <em>Language and Linguistics Compass</em> 3 (1): 359–83.</p>
</div>
<div id="ref-levshina2015linguistics">
<p>Levshina, Natalia. 2015. <em>How to Do Linguistics with R: Data Exploration and Statistical Analysis</em>. Amsterdam: John Benjamins Publishing Company.</p>
</div>
<div id="ref-maas2005sufficient">
<p>Maas, Cora JM, and Joop J Hox. 2005. “Sufficient Sample Sizes for Multilevel Modeling.” <em>Methodology</em> 1 (3): 86–92.</p>
</div>
<div id="ref-myers1990classical">
<p>Myers, Raymond H. 1990. <em>Classical and Modern Regression with Applications</em>. Vol. 2. Duxbury Press Belmont, CA.</p>
</div>
<div id="ref-neter1990vif">
<p>Neter, J., W. Wasserman, and MH Kutner. 1990. <em>Applied Linear Statistical Models. Regression, Analysis of Variance, and Experimental Design</em>. Irwin, Homewood, USA.</p>
</div>
<div id="ref-pinheiro2000mixedmodels">
<p>Pinheiro, J. C., and D. M. Bates. 2000. <em>Mixed-Effects Models in S and S-Plus. Statistics and Computing</em>. New York: Springer.</p>
</div>
<div id="ref-szmrecsanyi2006morphosyntactic">
<p>Szmrecsanyi, Benedikt. 2006. <em>Morphosyntactic Persistence in Spoken English: A Corpus Study at the Intersection of Variationist Sociolinguistics, Psycholinguistics, and Discourse Analysis</em>. Berlin &amp; New York: Walter de Gruyter.</p>
</div>
<div id="ref-wilcox2009basic">
<p>Wilcox, Rand R. 2009. <em>Basic Statistics: Understanding Conventional Methods and Modern Insights</em>. Oxford University Press.</p>
</div>
<div id="ref-winter2019statistics">
<p>Winter, Bodo. 2019. <em>Statistics for Linguists: An Introduction Using R</em>. Routledge.</p>
</div>
<div id="ref-zuur2013beginner">
<p>Zuur, Alain F., Joseph M. Hilbe, and Elena N. Ieno. 2013. <em>A Beginner’s Guide to Glm and Glmm with. R. Beginner’s Guide Series</em>. Newburgh, United Kingdom: Highland Statistics Ltd.</p>
</div>
<div id="ref-zuur2010protocol">
<p>Zuur, Alain F., Elena N. Ieno, and Chris S. Elphick. 2010. “A Protocol for Data Exploration to Avoid Common Statistical Problems.” <em>Methods in Ecology and Evolution</em> 1 (1): 3–14.</p>
</div>
<div id="ref-zuur2009mixedmodels">
<p>Zuur, Alain F., Elena N. Ieno, Neil J. Walker, Anatoly A. Saveliev, and Graham M Smith. 2009. <em>Mixed Effects Models and Extensions in Ecology with R</em>. Springer.</p>
</div>
</div>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>


</body>
</html>
