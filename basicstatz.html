<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />


<meta name="author" content="Martin Schweinberger" />

<meta name="date" content="2020-09-29" />

<title>Basic Inferential Statistics using R</title>

<script src="site_libs/jquery-1.12.4/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cosmo.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<script src="site_libs/htmlwidgets-1.5.1/htmlwidgets.js"></script>
<link href="site_libs/datatables-css-0.0.0/datatables-crosstalk.css" rel="stylesheet" />
<script src="site_libs/datatables-binding-0.15/datatables.js"></script>
<link href="site_libs/dt-core-1.10.20/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="site_libs/dt-core-1.10.20/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="site_libs/dt-core-1.10.20/js/jquery.dataTables.min.js"></script>
<link href="site_libs/crosstalk-1.1.0.1/css/crosstalk.css" rel="stylesheet" />
<script src="site_libs/crosstalk-1.1.0.1/js/crosstalk.min.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>




<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #ffffff;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">LADAL</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="people.html">OUR PEOPLE</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    NEWS
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="news.html">News &amp; Announcements</a>
    </li>
    <li>
      <a href="conferences.html">Events &amp; Presentations</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    DATA SCIENCE BASICS
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li class="dropdown-header">Introduction to Data Science</li>
    <li>
      <a href="introcomputer.html">Working with Computers: Tips and Tricks</a>
    </li>
    <li>
      <a href="reproducibility.html">Data Management, Version Control, and Reproducibility</a>
    </li>
    <li class="divider"></li>
    <li class="dropdown-header">Quantitative Research</li>
    <li>
      <a href="introquant.html">Introduction to Quantitative Reasoning</a>
    </li>
    <li>
      <a href="basicquant.html">Basic Concepts in Quantitative Research</a>
    </li>
    <li class="divider"></li>
    <li class="dropdown-header">Introduction to R</li>
    <li>
      <a href="IntroR_workshop.html">Getting started</a>
    </li>
    <li>
      <a href="stringprocessing.html">String Processing</a>
    </li>
    <li>
      <a href="regularexpressions.html">Regular Expressions</a>
    </li>
    <li>
      <a href="introtables.html">Working with Tables</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    TUTORIALS
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li class="dropdown-header">Data Visualization</li>
    <li>
      <a href="introviz.html">Introduction to Data Viz</a>
    </li>
    <li>
      <a href="basicgraphs.html">Common Visualization Types</a>
    </li>
    <li>
      <a href="basicgraphs.html">Advanced Visualization Methods</a>
    </li>
    <li>
      <a href="maps.html">Displaying Geo-Spatial Data</a>
    </li>
    <li>
      <a href="motion.html">Interactive Charts</a>
    </li>
    <li class="divider"></li>
    <li class="dropdown-header">Statistics</li>
    <li>
      <a href="descriptivestatz.html">Descriptive Statistics</a>
    </li>
    <li>
      <a href="basicstatz.html">Basic Inferential Statistics</a>
    </li>
    <li>
      <a href="regression.html">Regression Analysis</a>
    </li>
    <li>
      <a href="advancedstatztrees.html">Tree-Based Models</a>
    </li>
    <li>
      <a href="groupingstatz.html">Cluster and Correspondence Analysis</a>
    </li>
    <li>
      <a href="svm.html">Semantic Vector Space Models</a>
    </li>
    <li class="divider"></li>
    <li class="dropdown-header">Text Analytics</li>
    <li>
      <a href="textanalysis.html">Text Analysis and Distant Reading</a>
    </li>
    <li>
      <a href="kwics.html">Concordancing (keywords-in-context)</a>
    </li>
    <li>
      <a href="basicnetwork.html">Network Analysis</a>
    </li>
    <li>
      <a href="collocations.html">Co-occurrence and Collocation Analysis</a>
    </li>
    <li>
      <a href="topicmodels.html">Topic Modeling</a>
    </li>
    <li>
      <a href="sentiment.html">Sentiment Analysis</a>
    </li>
    <li>
      <a href="tagging.html">Tagging and Parsing</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    FOCUS &amp; CASE STUDIES
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="lex.html">Lexicography with R: Generating Dictionaries</a>
    </li>
    <li>
      <a href="surveys.html">Questionnaires and Surveys: Analyses with R</a>
    </li>
    <li>
      <a href="corplingr.html">Corpus Linguistics with R: Swearing in Irish English</a>
    </li>
    <li>
      <a href="convertpdf2txt.html">Converting PDFs to txt</a>
    </li>
    <li>
      <a href="webcrawling.html">Web Crawling using R</a>
    </li>
    <li>
      <a href="vc.html">Creating Vowel Charts with Praat and R</a>
    </li>
  </ul>
</li>
<li>
  <a href="services.html">SERVICES &amp; CONTACT</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Basic Inferential Statistics using R</h1>
<h4 class="author">Martin Schweinberger</h4>
<h4 class="date">2020-09-29</h4>

</div>


<p><img src="images/uq1.jpg" width="100%" /></p>
<div id="introduction" class="section level1 unnumbered">
<h1>Introduction</h1>
<p>This tutorial introduces the basic statistical techniques for inferential statistics for hypothesis testing with R. The R-markdown document of this tutorial can be downloaded <a href="https://slcladal.github.io/rscripts/basicstatz.Rmd">here</a>. The first part of this tutorial focuses on basic non-parametric tests such as Fisher’s Exact test, the second part focuses on the <span class="math inline">\(\chi\)</span><sup>2</sup> family of tests, and the third part focuses on simple linear regression.</p>
<div id="preparation-and-session-set-up" class="section level2 unnumbered">
<h2>Preparation and session set up</h2>
<p>This tutorial is based on R. If you have not installed R or are new to it, you will find an introduction to and more information how to use R <a href="https://slcladal.github.io/IntroR_workshop.html">here</a>. For this tutorials, we need to install certain <em>packages</em> from an R <em>library</em> so that the scripts shown below are executed without errors. Before turning to the code below, please install the packages by running the code below this paragraph. If you have already installed the packages mentioned below, then you can skip ahead ignore this section. To install the necessary packages, simply run the following code - it may take some time (between 1 and 5 minutes to install all of the libraries so you do not need to worry if it takes some time).</p>
<pre class="r"><code># install libraries
install.packages(c(&quot;stringr&quot;, &quot;VGAM&quot;, &quot;fGarch&quot;, &quot;stringr&quot;, &quot;cfa&quot;, &quot;dplyr&quot;, 
                   &quot;gridExtra&quot;, &quot;calibrate&quot;, &quot;car&quot;, &quot;ggplot2&quot;, &quot;QuantPsyc&quot;, 
                   &quot;DT&quot;))</code></pre>
<p>Once you have installed R, R-Studio, and have also initiated the session by executing the code shown above, you are good to go.</p>
</div>
</div>
<div id="fishers-exact-test" class="section level1">
<h1><span class="header-section-number">1</span> Fisher’s Exact Test</h1>
<p>Non-Parametric Tests do not require the data (or the errors of the dependent variable, to be more precise) to be distributed normally. Tests that do not require normal data are referred to as <em>non-parametric tests</em> (tests that require the data to be distributed normally are analogously called <em>parametric tests</em>). We focus on non-parametric tests first, as this family of test in frequently used in linguistics. In the later part of this section, we will focus on regression modeling where assumptions of about the data become more important.</p>
<p>Fisher’s Exact test is very useful because it does not rely on distributional assumptions relying on normality. Instead, Fisher’s Exact Test calculates the probabilities of all possible outcomes and uses these to determine significance. To understand how a Fisher’s Exact test, we will use a very simple example.</p>
<p>Imagine you are interested in adjective modification and you want to find out if <em>very</em> and <em>truly</em> differ in their collocational preferences. So you extract all instances of <em>cool</em>, all instances of <em>very</em>, and all instances of <em>truly</em> from a corpus. Now that you have gathered this data, you want to test if <em>truly</em> and <em>very</em> differ with respect to their preference to co-occur with <em>cool</em>. Accordingly, you tabulate the results and get the following table.</p>
<div id="htmlwidget-7ed690d2b49fcec4ca82" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-7ed690d2b49fcec4ca82">{"x":{"filter":"none","data":[["truly","very"],["5","17"],["40","41"]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th>Adverb<\/th>\n      <th>with cool<\/th>\n      <th>with other adjectives<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"pageLength":5,"scrollX":true,"order":[],"autoWidth":false,"orderClasses":false,"lengthMenu":[5,10,25,50,100]}},"evals":[],"jsHooks":[]}</script>
<p>To perform a Fisher’s Exact test, we first need to create a table with these results in R.</p>
<pre class="r"><code># create table
coolmx &lt;- matrix(
  c(5, 17, 40, 41),
  nrow = 2, # number of rows of the table
  # def. dimension names
  dimnames = list(
    Adverbs = c(&quot;truly&quot;, &quot;very&quot;),
    Adjectives = c(&quot;cool&quot;, &quot;other adjective&quot;))
)
# inspect data
datatable(coolmx, rownames = FALSE, filter=&quot;none&quot;, options = list(pageLength = 5, scrollX=T))</code></pre>
<div id="htmlwidget-93aca6f24425ad4a233d" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-93aca6f24425ad4a233d">{"x":{"filter":"none","data":[[5,17],[40,41]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th>cool<\/th>\n      <th>other adjective<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"pageLength":5,"scrollX":true,"columnDefs":[{"className":"dt-right","targets":[0,1]}],"order":[],"autoWidth":false,"orderClasses":false,"lengthMenu":[5,10,25,50,100]}},"evals":[],"jsHooks":[]}</script>
<p>Once we have the created a matrix with these numbers, we can perform the Fisher’s Exact test to see if <em>very</em> and <em>truly</em> differ in their preference to co-occur with <em>cool</em>. The null hypothesis is that there is no difference between the adverbs.</p>
<pre class="r"><code># perform test
fisher.test(coolmx)</code></pre>
<pre><code>## 
##  Fisher&#39;s Exact Test for Count Data
## 
## data:  coolmx
## p-value = 0.03024
## alternative hypothesis: true odds ratio is not equal to 1
## 95 percent confidence interval:
##  0.08015294 0.96759831
## sample estimates:
## odds ratio 
##  0.3048159</code></pre>
<p>The results of the Fisher’s Exact test show that the p-value is lower than .05, which means we reject the null hypothesis, and we are therefore justified in assuming that <em>very</em> and <em>truly</em> differ in their collocational preferences to co-occur with <em>cool</em>.</p>
</div>
<div id="ranking-and-sign-based-non-parametric-tests" class="section level1">
<h1><span class="header-section-number">2</span> Ranking and Sign-Based Non-Parametric Tests</h1>
<p>If the depended variable is neither nominal nor categorical, but ordinal (that is if the dependent variable represents an order factor such as ranks), using a chi-square test is not warranted. In such cases it is advisable to apply tests that are designed to handle ordinal data. In the following, we will therefore briefly touch on bi-variate tests that can handle ordinal dependent variables.</p>
<div id="mann-whitney-u-test-wilcoxon-rank-sum-test" class="section level2 unnumbered">
<h2>Mann-Whitney U-Test / Wilcoxon Rank Sum-Test</h2>
<p>It is a rather frequent case that numeric depend variables are transformed or converted into ordinal variables because the distribution of residuals does not allow the application of a linear regression. Because we are dealing with ordinal data, the application of a chi-square test is unwarranted and we need to use another test. In such cases, the Mann-Whitney U-test (also called Wilcoxon rank sum-test) can be used. The Mann-Whitney U-test can also be used if the groups under investigation represent identical participants that are tested under two conditions.</p>
<p>Imagine we wanted to determine if two language families differed with respect to the size of their phoneme inventories. You have already ranked the inventory sizes and would now like to now if language family correlates with inventory size. To answer this question, you create the table shown below.</p>
<pre class="r"><code># create table
rank &lt;- c(1,3,5,6,8,9,10,11,17,19, 2,4,7,12,13,14,15,16,18,20)
groups &lt;- c(rep(&quot;Lang.Fam.1&quot;, 10), rep(&quot;Lang.Fam.2&quot;, 10))
language_family_tb &lt;- data.frame(groups, rank)
# inspect data
datatable(language_family_tb, rownames = FALSE, filter=&quot;none&quot;, options = list(pageLength = 5, scrollX=T))</code></pre>
<div id="htmlwidget-9d25093f24be9499d89c" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-9d25093f24be9499d89c">{"x":{"filter":"none","data":[["Lang.Fam.1","Lang.Fam.1","Lang.Fam.1","Lang.Fam.1","Lang.Fam.1","Lang.Fam.1","Lang.Fam.1","Lang.Fam.1","Lang.Fam.1","Lang.Fam.1","Lang.Fam.2","Lang.Fam.2","Lang.Fam.2","Lang.Fam.2","Lang.Fam.2","Lang.Fam.2","Lang.Fam.2","Lang.Fam.2","Lang.Fam.2","Lang.Fam.2"],[1,3,5,6,8,9,10,11,17,19,2,4,7,12,13,14,15,16,18,20]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th>groups<\/th>\n      <th>rank<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"pageLength":5,"scrollX":true,"columnDefs":[{"className":"dt-right","targets":1}],"order":[],"autoWidth":false,"orderClasses":false,"lengthMenu":[5,10,25,50,100]}},"evals":[],"jsHooks":[]}</script>
<p>We will also briefly inspect the data visually using a box plot.</p>
<pre class="r"><code>boxplot(rank ~ groups, col = c(&quot;orange&quot;, &quot;darkgrey&quot;), main = &quot;&quot;, data = language_family_tb)</code></pre>
<p><img src="basicstatz_files/figure-html/mwu2-1.png" width="672" /></p>
<p>To use Mann-Whitney U test, the dependent variable (Rank) must be ordinal and independent variable (Group) must be a binary factor. We briefly check this by inspecting the structure of the data.</p>
<pre class="r"><code># perform test
str(language_family_tb)</code></pre>
<pre><code>## &#39;data.frame&#39;:    20 obs. of  2 variables:
##  $ groups: chr  &quot;Lang.Fam.1&quot; &quot;Lang.Fam.1&quot; &quot;Lang.Fam.1&quot; &quot;Lang.Fam.1&quot; ...
##  $ rank  : num  1 3 5 6 8 9 10 11 17 19 ...</code></pre>
<p>As the variables are what we need them to be, we can now perform the Mann-Whitney U test on the table. The null hypothesis is that there is no difference between the 2 groups.</p>
<pre class="r"><code># perform test
wilcox.test(language_family_tb$rank ~ language_family_tb$groups) </code></pre>
<pre><code>## 
##  Wilcoxon rank sum exact test
## 
## data:  language_family_tb$rank by language_family_tb$groups
## W = 34, p-value = 0.2475
## alternative hypothesis: true location shift is not equal to 0</code></pre>
<p>Since the p-value is greater than 0.05, we fail to reject the null hypothesis. The results of the Wilcoxon rank sum test tell us that the two language families do not differ significantly with respect to their phoneme inventory size.</p>
<p>The Wilcoxon rank sum test can also be used with continuity correction. A continuity correction is necessary when both variables represent numeric values that are non-normal. In the following example, we want to test if the reaction time for identifying a word as real is correlated with its token frequency.</p>
<p>For this example, we generate data is deliberately non-normal.</p>
<pre class="r"><code># activate packages
library(fGarch)
library(gridExtra)
library(ggplot2)
# generate non-normal skewed numeric data
r &lt;- .1 
frequency &lt;- rsnorm(100,0,2,4)
normal_reaction &lt;- rsnorm(100,0,2,4)
reaction_times &lt;- frequency*r+normal_reaction*sqrt(1-r^2)
# combine into data frame
wxdata &lt;- data.frame(frequency, normal_reaction, reaction_times)
# plot data
p1 &lt;- ggplot(wxdata, aes(frequency)) + # define data
  geom_density(fill = &quot;orange&quot;, alpha = .2) + # define plot type (density)
  theme_bw() +                                # black + white background
  labs(y=&quot;Density&quot;, x = &quot;Frequency&quot;) +        # axes titles
  coord_cartesian(ylim = c(0, .5),            # define y-axis coordinates
                  xlim = c(-5, 10))           # define x-axis coordinates
p2 &lt;- ggplot(wxdata, aes(reaction_times)) +
  geom_density(fill = &quot;lightgray&quot;, alpha = .2) +
  theme_bw() +
  labs(y=&quot;Density&quot;, x = &quot;Reaction Time&quot;) +
  coord_cartesian(ylim = c(0, .5), xlim = c(-5, 10))
grid.arrange(p1, p2, nrow = 1)             # 2 plots in one window</code></pre>
<p><img src="basicstatz_files/figure-html/mwu5-1.png" width="672" /></p>
<p>Both variables are skewed (non-normally distributed) but we can use the Wilcoxon rank sum test with continuity correction which takes the skewness into account. The null hypothesis is that there is no difference between the 2 groups.</p>
<pre class="r"><code># perform test
wilcox.test(reaction_times, frequency) </code></pre>
<pre><code>## 
##  Wilcoxon rank sum test with continuity correction
## 
## data:  reaction_times and frequency
## W = 5056, p-value = 0.8921
## alternative hypothesis: true location shift is not equal to 0</code></pre>
<p>The p-value is greater than 0.05, therefore we cannot reject the null hypothesis. There is no statistically significant evidence to assume the groups are different.</p>
<p>When performing the Wilcoxon rank sum test with data that represent the same individuals that were tested under two condition, i.e. if the samples are dependent, then the argument “paired” has to be “TRUE”.</p>
<p>In this example, the same individuals had to read tongue twisters when they were sober and when they were intoxicated. A Wilcoxon signed rank test with continuity correction is used to test if the number of errors that occur when reading tongue twisters correlates with being sober/intoxicated. Again, we create fictitious data.</p>
<pre class="r"><code># create data
sober &lt;- sample(0:9, 15, replace = T)
intoxicated &lt;-  sample(3:12, 15, replace = T) 
# tabulate data
intoxtb &lt;- data.frame(sober, intoxicated) 
# inspect data
datatable(intoxtb, rownames = FALSE, filter=&quot;none&quot;, options = list(pageLength = 15, scrollX=T))</code></pre>
<div id="htmlwidget-64b3ec8083a4460e9b83" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-64b3ec8083a4460e9b83">{"x":{"filter":"none","data":[[2,9,3,9,3,2,4,8,9,2,5,3,8,8,6],[3,5,3,6,4,11,5,4,3,7,6,7,8,3,9]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th>sober<\/th>\n      <th>intoxicated<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"pageLength":15,"scrollX":true,"columnDefs":[{"className":"dt-right","targets":[0,1]}],"order":[],"autoWidth":false,"orderClasses":false,"lengthMenu":[10,15,25,50,100]}},"evals":[],"jsHooks":[]}</script>
<p>Now, we briefly plot the data.</p>
<pre class="r"><code>intoxtb2 &lt;- data.frame(c(rep(&quot;sober&quot;, nrow(intoxtb)), rep(&quot;intoxicated&quot;, nrow(intoxtb))), 
                       c(intoxtb$sober, intoxtb$intoxicated))
colnames(intoxtb2) &lt;- c(&quot;State&quot;, &quot;Errors&quot;)
ggplot(intoxtb2, aes(State, Errors)) +
  geom_boxplot(fill = c(&quot;orange&quot;, &quot;darkgrey&quot;), width=0.5) +
  labs(y = &quot;Number of errors&quot;, x = &quot;State&quot;) +
  theme_bw()</code></pre>
<p><img src="basicstatz_files/figure-html/mwu8-1.png" width="672" /></p>
<p>The boxes indicate a significant difference. Finally, we perform the Wilcoxon signed rank test with continuity correction. The null hypothesis is that the two groups are the same.</p>
<pre class="r"><code># perform test
wilcox.test(intoxtb$sober, intoxtb$intoxicated, paired=T) </code></pre>
<pre><code>## 
##  Wilcoxon signed rank test with continuity correction
## 
## data:  intoxtb$sober and intoxtb$intoxicated
## V = 44, p-value = 0.944
## alternative hypothesis: true location shift is not equal to 0</code></pre>
<p>The p-value is lower than 0.05 (rejecting the null hypothesis) which means that the number of errors when reading tongue twisters is affected by one’s state (sober/intoxicated) - at least in this fictitious example.</p>
</div>
<div id="kruskal-wallis-rank-sum-test" class="section level2 unnumbered">
<h2>Kruskal-Wallis Rank Sum Test</h2>
<p>The Kruskal-Wallis rank sum test is a type of ANOVA (Analysis of Variance). For this reason, the Kruskal Wallis Test is also referred to as a “one-way Anova by ranks” which can handle numeric and ordinal data.</p>
<p>In the example below, <em>uhm</em> represents the number of filled pauses in a short 5 minute interview while speaker represents whether the speaker was a native speaker or a learner of English. As before, the data is generated and thus artificial.</p>
<pre class="r"><code># create data
uhms &lt;- c(15, 13, 10, 8, 37, 23, 31, 52, 11, 17)
speaker &lt;- c(rep(&quot;Learner&quot;, 5), rep(&quot;NativeSpeaker&quot;, 5))
# create table
uhmtb &lt;- data.frame(speaker, uhms)
# inspect data
datatable(uhmtb, rownames = FALSE, filter=&quot;none&quot;, options = list(pageLength = 10, scrollX=T))</code></pre>
<div id="htmlwidget-50b88abe725831c8d65d" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-50b88abe725831c8d65d">{"x":{"filter":"none","data":[["Learner","Learner","Learner","Learner","Learner","NativeSpeaker","NativeSpeaker","NativeSpeaker","NativeSpeaker","NativeSpeaker"],[15,13,10,8,37,23,31,52,11,17]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th>speaker<\/th>\n      <th>uhms<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"pageLength":10,"scrollX":true,"columnDefs":[{"className":"dt-right","targets":1}],"order":[],"autoWidth":false,"orderClasses":false}},"evals":[],"jsHooks":[]}</script>
<p>Now, we briefly plot the data.</p>
<pre class="r"><code>ggplot(uhmtb, aes(speaker, uhms)) +
  geom_boxplot(fill = c(&quot;orange&quot;, &quot;darkgrey&quot;)) +
  theme_bw() +
  labs(x = &quot;Speaker type&quot;, y = &quot;Errors&quot;)</code></pre>
<p><img src="basicstatz_files/figure-html/kwt2-1.png" width="672" /></p>
<p>Now, we test for statistical significance. The null hypothesis is that there is no difference between the groups.</p>
<pre class="r"><code>kruskal.test(uhmtb$speaker~uhmtb$uhms) </code></pre>
<pre><code>## 
##  Kruskal-Wallis rank sum test
## 
## data:  uhmtb$speaker by uhmtb$uhms
## Kruskal-Wallis chi-squared = 9, df = 9, p-value = 0.4373</code></pre>
<p>The p-value is greater than 0.05, therefore we fail to reject the null hypothesis. The Kruskal-Wallis test does not report a significant difference for the number of <em>uhms</em> produced by native speakers and learners of English in the fictitious data.</p>
</div>
<div id="the-friedman-rank-sum-test" class="section level2 unnumbered">
<h2>The Friedman Rank Sum Test</h2>
<p>The Friedman rank sum test is also called a randomized block design and it is used when the correlation between a numeric dependent variable, a grouping factor and a blocking factor is tested. The Friedman rank sum test assumes that each combination of the grouping factor (Gender) and the blocking factor (Age) occur only once. Thus, imagine that the values of <em>uhms</em> represent the means of the respective groups.</p>
<pre class="r"><code># create data
uhms &lt;- c(7.2, 9.1, 14.6, 13.8)
gender &lt;- c(&quot;Female&quot;, &quot;Male&quot;, &quot;Female&quot;, &quot;Male&quot;)
age &lt;- c(&quot;Young&quot;, &quot;Young&quot;, &quot;Old&quot;, &quot;Old&quot;)
# create table
uhmtb2 &lt;- data.frame(gender, age, uhms)
# inspect data
datatable(uhmtb2, rownames = FALSE, filter=&quot;none&quot;, options = list(pageLength = 10, scrollX=T))</code></pre>
<div id="htmlwidget-f4fb0de29b81d1f8f7de" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-f4fb0de29b81d1f8f7de">{"x":{"filter":"none","data":[["Female","Male","Female","Male"],["Young","Young","Old","Old"],[7.2,9.1,14.6,13.8]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th>gender<\/th>\n      <th>age<\/th>\n      <th>uhms<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"pageLength":10,"scrollX":true,"columnDefs":[{"className":"dt-right","targets":2}],"order":[],"autoWidth":false,"orderClasses":false}},"evals":[],"jsHooks":[]}</script>
<p>We now perform the Friedman rank sum test.</p>
<pre class="r"><code>friedman.test(uhms ~ age | gender, data = uhmtb2)</code></pre>
<pre><code>## 
##  Friedman rank sum test
## 
## data:  uhms and age and gender
## Friedman chi-squared = 2, df = 1, p-value = 0.1573</code></pre>
<p>In our example, age does not affect the use of filled pauses even if we control for gender as the p-value is higher than .05.</p>
</div>
</div>
<div id="pearsonss-chi-square-test" class="section level1">
<h1><span class="header-section-number">3</span> (Pearsons’s) Chi-Square Test</h1>
<p>One of the most frequently used statistical test in linguistics is the <span class="math inline">\(\chi\)</span><sup>2</sup> test (or Pearsons’s chi-square test, chi-squared test, or chi-square test). We will use a simple, practical example to explore how this test works. In this example, we will test whether speakers of American English (AmE) and speakers of British English (BrE) differ in their use of the near-synonyms <em>sort of</em> and <em>kind of</em> as in “<em>He’s sort of stupid</em>” and “<em>He’s kind of stupid</em>”. As a first step, we formulate the hypothesis that we want to test (H<sub>1</sub>) and its Nullhypothesis (H<sub>0</sub>). The Alternative- or Test Hypothesis reads:</p>
<p>H<sub>1</sub>: Speakers of AmE and BrE differ with respect to their preference for <em>sort of</em> und <em>kind of</em>.</p>
<p>while the Null Hypothesis (H<sub>0</sub>) states</p>
<p>H<sub>0</sub>: Speakers of AmE and BrE do not differ with respect to their preference for <em>sort of</em> und <em>kind of</em>.</p>
<p>The H<sub>0</sub> claims the non-existence of something (which is the more conservative position) and in our example the non-existence of a correlation between variety of English and the use of <em>sort of</em> und <em>kind of</em>. The question now arises what has to be the case in order to reject the H<sub>0</sub> in favour of the H<sub>1</sub>.</p>
<p>To answer this question, we require information about the probability of error, i.e. the probability that the H<sub>0</sub> does indeed hold for the entire population. Before performing the chi-square test, we follow the convention that the required significance level is 5 percent. In other words, we will reject the H<sub>0</sub> if the likelihood for the H<span class="math inline">\(_{0}\)</span> being true is less than 5 percent given the distribution of the data. In that case, i.e. in case that the likelihood for the H<sub>0</sub> being true is less than 5 percent, we consider the result of the chi-square test as statistically significant. This means that the observed distribution makes it very unlikely that there is no correlation between the variety of English and the use of <em>sort of</em> and <em>kind of</em>.</p>
<p>Let us now assume that we have performed a search for <em>sort of</em> and <em>kind of</em> in two corpora representing American and British English and that we have obtained the following frequencies:</p>
<div id="htmlwidget-3511bdbef0828877a472" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-3511bdbef0828877a472">{"x":{"filter":"none","caption":"<caption>Observed frequencies of *sort of* and *kind of* in American and British English<\/caption>","data":[[181,177],[655,67]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th>BrE<\/th>\n      <th>AmE<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"pageLength":10,"scrollX":true,"columnDefs":[{"className":"dt-right","targets":[0,1]}],"order":[],"autoWidth":false,"orderClasses":false}},"evals":[],"jsHooks":[]}</script>
<p>In a first step, we now have to calculate the row and column sums of our table.</p>
<div id="htmlwidget-a53072127b6820c77e1a" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-a53072127b6820c77e1a">{"x":{"filter":"none","caption":"<caption>Observed frequencies of *sort of* and *kind of* in American and British English with row and column totals<\/caption>","data":[[181,177,358],[655,67,722],[836,244,1080]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th>BrE<\/th>\n      <th>AmE<\/th>\n      <th>Total<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"pageLength":10,"scrollX":true,"columnDefs":[{"className":"dt-right","targets":[0,1,2]}],"order":[],"autoWidth":false,"orderClasses":false}},"evals":[],"jsHooks":[]}</script>
<p>Next, we calculate, the values that would have expected if there was no correlation between variety of English and the use of <em>sort of</em> and <em>kind of</em>. In order to get these “expected” frequencies, we apply the equation below to all cells in our table.</p>
<p><span class="math inline">\(\frac{Column total*Row total}{Overall total}\)</span></p>
<p>In our example this means that for the cell with [+]BrE [+]kindof we get:</p>
<p><span class="math inline">\(\frac{836*358}{1080} = \frac{299288}{1080} = 277.1185\)</span></p>
<p>For the entire table this means we get the following expected values:</p>
<div id="htmlwidget-9dab64d9c14547df08ee" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-9dab64d9c14547df08ee">{"x":{"filter":"none","caption":"<caption>Expected frequencies of *sort of* and *kind of* in American and British English with row and column totals<\/caption>","data":[[277.1185,80.88148,358],[558.8815,163.11852,722],[836,244,1080]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th>BrE<\/th>\n      <th>AmE<\/th>\n      <th>Total<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"pageLength":10,"scrollX":true,"columnDefs":[{"className":"dt-right","targets":[0,1,2]}],"order":[],"autoWidth":false,"orderClasses":false}},"evals":[],"jsHooks":[]}</script>
<p>In a next step, we calculate the contribution of each cell to the overall <span class="math inline">\(\chi\)</span><sup>2</sup> value (<span class="math inline">\(\chi\)</span><sup>2</sup> contribution). To get <span class="math inline">\(\chi\)</span><sup>2</sup> contribution for each cell, we apply the equation below to each cell.</p>
<p><span class="math inline">\(\frac{(observed – expected)^{2}}{expected}\)</span></p>
<p>In our example this means that for the cell with [+]BrE [+]kindof we get:</p>
<p><span class="math inline">\(\frac{(181 – 277.1185)^{2}}{277.1185} = \frac{-96.1185^{2}}{277.1185} = \frac{9238.766}{277.1185} = 33.33868\)</span></p>
<p>For the entire table this means we get the following <span class="math inline">\(\chi^{2}\)</span> values:</p>
<div id="htmlwidget-8054b858294e93325daa" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-8054b858294e93325daa">{"x":{"filter":"none","caption":"<caption>Chi values of *sort of* and *kind of* in American and British English with row and column totals<\/caption>","data":[[33.33869,114.22602,147.5647],[16.53082,56.63839,73.16921],[49.86951,170.8644,220.7339]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th>BrE<\/th>\n      <th>AmE<\/th>\n      <th>Total<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"pageLength":10,"scrollX":true,"columnDefs":[{"className":"dt-right","targets":[0,1,2]}],"order":[],"autoWidth":false,"orderClasses":false}},"evals":[],"jsHooks":[]}</script>
<p>The sum of <span class="math inline">\(\chi\)</span><sup>2</sup> contributions in our example is 220.7339. To see if this value is statistically significant, we need to calculate the degrees of freedom because the <span class="math inline">\(\chi\)</span> distribution differs across degrees of freedom. Degrees of freedom are calculated according to the equation below.</p>
<p><span class="math inline">\(DF = (rows -1) * (columns – 1) = (2-1) * (2-1) = 1 * 1 = 1\)</span></p>
<p>In a last step, we check whether the <span class="math inline">\(\chi\)</span><sup>2</sup> value that we have calculated is higher than a critical value (in which case the correlation in our table is significant). Degrees of freedom are relevant here because the critical values are dependent upon the degrees of freedom: the more degrees of freedom, the higher the critical value, i.e. the harder it is to breach the level of significance.</p>
<p>Since there is only 1 degree of freedom in our case, we need to consider only the first column in the table of critical values below.</p>
<div id="htmlwidget-27fefe9dda1b870bbdc6" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-27fefe9dda1b870bbdc6">{"x":{"filter":"none","caption":"<caption>Critical chi values for 1 to 5 degrees of freedom<\/caption>","data":[[1,2,3,4,5],[3.84,5.99,7.82,9.49,11.07],[6.64,9.21,11.35,13.28,15.09],[10.83,13.82,16.27,18.47,20.52]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th>DF<\/th>\n      <th>p&lt;.05<\/th>\n      <th>p&lt;.01<\/th>\n      <th>p&lt;.001<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"pageLength":10,"scrollX":true,"columnDefs":[{"className":"dt-right","targets":[0,1,2,3]}],"order":[],"autoWidth":false,"orderClasses":false}},"evals":[],"jsHooks":[]}</script>
<p>Since the <span class="math inline">\(\chi\)</span><sup>2</sup> value that we have calculated is much higher than the critical value provided for p&lt;.05, we can reject the H<sub>0</sub> and may now claim that speakers of AmE and BrE differ with respect to their preference for <em>sort of</em> und <em>kind of</em>.</p>
<p>Before we summarize the results, we will calculate the effect size which is a measure for how strong the correlations are.</p>
<div id="effect-sizes-in-chi-square" class="section level2 unnumbered">
<h2>Effect Sizes in Chi-Square</h2>
<p>Effect sizes are important because they correlations may be highly significant but the effect between variables can be extremely weak. The effect size is therefore a measure how strong the correlation or the explanatory and predictive power between variables is.</p>
<p>The effect size measure for <span class="math inline">\(\chi\)</span><sup>2</sup> tests can be either the <span class="math inline">\(\phi\)</span>-coefficient (phi-coefficient) or Cramer’s <span class="math inline">\(\phi\)</span> (Cramer’s phi). The <span class="math inline">\(\phi\)</span>-coefficient is used when dealing with 2x2 tables while Cramer’s <span class="math inline">\(\phi\)</span> is used when dealing with tables with more than 4 cells. The <span class="math inline">\(\phi\)</span> coefficient can be calculated by using the equation below (N = overall sample size).</p>
<p><span class="math inline">\(\phi = \sqrt{\frac{\chi^{2}}{N}}\)</span></p>
<p>In our case, this means:</p>
<p><span class="math inline">\(\phi = \sqrt{\frac{220.7339}{1080}} = \sqrt{0.2043832} = 0.4520876\)</span></p>
<p>The <span class="math inline">\(\phi\)</span> coefficient varies between 0 (no effect) and 1 (perfect correlation). For the division into weak, moderate and strong effects one can follow the division for <span class="math inline">\(\omega\)</span> (small omega), so that with values beginning with .1 represent weak, values between 0.3 and .5 represent moderate and values above .5 represent strong effects <span class="citation">(Bühner and Ziegler <a href="#ref-buehner2009statistik" role="doc-biblioref">2009</a>, 266)</span>. So, in this example we are dealing with a medium-sized effect/correlation.</p>
</div>
<div id="chi-square-in-r" class="section level2 unnumbered">
<h2>Chi-Square in R</h2>
<p>Before we summarize the results, we will see how to perform a chi-square test in R. In addition to what we have done above, we will also visualize the data. To begin with, we will have a look at the data set (which is the same data we have used above).</p>
<pre class="r"><code># inspect data
datatable(chidata, rownames = FALSE, filter=&quot;none&quot;, options = list(pageLength = 10, scrollX=T), caption = &quot;Critical chi values for 1 to 5 degrees of freedom&quot;)</code></pre>
<div id="htmlwidget-babfa27c0224b5667211" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-babfa27c0224b5667211">{"x":{"filter":"none","caption":"<caption>Critical chi values for 1 to 5 degrees of freedom<\/caption>","data":[[181,177],[655,67]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th>BrE<\/th>\n      <th>AmE<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"pageLength":10,"scrollX":true,"columnDefs":[{"className":"dt-right","targets":[0,1]}],"order":[],"autoWidth":false,"orderClasses":false}},"evals":[],"jsHooks":[]}</script>
<p>We will now visualize the data with an association. Bars above the dashed line indicate that a feature combination occurs more frequently than expected by chance. The width of the bars indicates the frequency of the feature combination.</p>
<p><img src="basicstatz_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<p>The fact that the bars are distributed complimentarily (top left red and below bar; top right black above bar; bottom left black above bar; bottom right red below bar) indicates that the use of “sort of” and “kind of” differs across AmE and BrE. We will check whether the mosaic plot confirms this impression.</p>
<pre class="r"><code>mosaicplot(chidata, shade = TRUE, type = &quot;pearson&quot;, main = &quot;&quot;)  # mosaic plot</code></pre>
<p><img src="basicstatz_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<p>The colour contrasts in the mosaic plot substantiate the impression that the two varieties of English differ significantly. To ascertain whether the differences are statistically significant, we can now apply the chi-square test.</p>
<pre class="r"><code>chisq.test(chidata, corr = F)  # perform chi square test</code></pre>
<pre><code>## 
##  Pearson&#39;s Chi-squared test
## 
## data:  chidata
## X-squared = 220.73, df = 1, p-value &lt; 2.2e-16</code></pre>
<p>The results reported by <code>R</code> are identical to the results we derived by hand and confirm that BrE and AmE differ significantly in their use of “sort of” and “kind of”. In a next step, we calculate the effect size.</p>
<pre class="r"><code># calculate effect size
sqrt(chisq.test(chidata, corr = F)$statistic / sum(chidata) * (min(dim(chidata))-1))</code></pre>
<pre><code>## X-squared 
## 0.4520877</code></pre>
<p>The phi coefficient of .45 shows that variety of English correlates moderately with the use of “sort of” and “kind of”. We will now summarize the results.</p>
</div>
<div id="summarizing-chi-square-results" class="section level2 unnumbered">
<h2>Summarizing Chi-Square Results</h2>
<p>The results of our analysis can be summarised as follows: A <span class="math inline">\(\chi\)</span><sup>2</sup>-test confirms a highly significant correlation of moderate size between the variety of English and the use of the near-synonymous hedges <em>sort of</em> and <em>kind of</em> (<span class="math inline">\(\chi\)</span><sup>2</sup> = 220.73, df = 1, p &lt; .001***, <span class="math inline">\(\phi\)</span> = .452).</p>
</div>
<div id="requirements-of-chi-square" class="section level2 unnumbered">
<h2>Requirements of Chi-Square</h2>
<p>Chi-square tests depend on certain requirements that, if violated, negatively affect the reliability of the results of the test. To provide reliable results, 80 percent of cells in a table to which the chi-square test is applied have to have expected values of 5 or higher and at most 20 percent of expected values can be smaller than 5 <span class="citation">(see <span class="citeproc-not-found" data-reference-id="bortz1990verteilungsfreie"><strong>???</strong></span>)</span>. In addition, none of the expected values can be smaller than 1 <span class="citation">(see <span class="citeproc-not-found" data-reference-id="bortz1990verteilungsfreie"><strong>???</strong></span>)</span> because then, the estimation, which relies on the <span class="math inline">\(\chi\)</span><sup>2</sup>-distribution, becomes too imprecise to allow meaningful inferences <span class="citation">(Cochran <a href="#ref-cochran1954somemethods" role="doc-biblioref">1954</a>)</span>.</p>
<p>If these requirements are violated, then the <em>Fisher’s Exact Test</em> is more reliable and offers the additional advantage that these tests can also be applied to data that represent very small sample sizes. When applying the Fisher’s Exact Test, the probabilities for all possible outcomes are calculated and the summed probability for the observed or more extreme results are determined. If this sum of probabilities exceeds five percent, then the result is deemed statistically significant.</p>
</div>
<div id="chi-square-exercises" class="section level2 unnumbered">
<h2>Chi-Square Exercises</h2>
<ol style="list-style-type: decimal">
<li>Imagine you are interested in whether older or younger speakers tend to refer to themselves linguistically. The underlying hypothesis is that - contrary to common belief - older people are more narcissistic compared with younger people. Given this research question, perform a chi-square test and summarize the results on the data below.</li>
</ol>
<div id="htmlwidget-8842f6c2d2d085fa826d" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-8842f6c2d2d085fa826d">{"x":{"filter":"none","caption":"<caption>Table adapted from Gries (2014: 9)<\/caption>","data":[["Young","Old","Total"],["61","42","103"],["43","36","79"],["104","78","182"]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th>V1<\/th>\n      <th>1SGPN<\/th>\n      <th>PN without 1SG<\/th>\n      <th>Total<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"pageLength":10,"scrollX":true,"order":[],"autoWidth":false,"orderClasses":false}},"evals":[],"jsHooks":[]}</script>
<ol start="2" style="list-style-type: decimal">
<li>Imagine you are interested in whether young men or young women exhibit a preference for the word <em>whatever</em> because you have made the unsystematic, anecdotal observation that young men use this word more frequently than young women. Given this research question, perform a chi-square test and summarize the results on the data below.</li>
</ol>
<div id="htmlwidget-9674d1ff269d2ff33874" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-9674d1ff269d2ff33874">{"x":{"filter":"none","caption":"<caption>Observed frequency with row- and column totals for the use of *whatever* by male and female speakers.<\/caption>","data":[["whatever","other words","Total"],["17","345128","345145"],["55","916552","916607"],["71","1261680","1261752"]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th>V1<\/th>\n      <th>YoungMales<\/th>\n      <th>YoungFemales<\/th>\n      <th>Total<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"pageLength":10,"scrollX":true,"order":[],"autoWidth":false,"orderClasses":false}},"evals":[],"jsHooks":[]}</script>
<ol start="3" style="list-style-type: decimal">
<li>Find a partner and discuss the relationship between significance and effect size. Then, go and find another partner and discuss problems that may arise when testing the frequency of certain words compared with the overall frequency of words in a corpus.</li>
</ol>
</div>
</div>
<div id="extensions-of-chi-square" class="section level1">
<h1><span class="header-section-number">4</span> Extensions of Chi-Square</h1>
<p>In the following, we will have a look at tests and methods that can be used if the requirements for ordinary (Pearson’s) chi-square tests are violated and their use would be inappropriate</p>
<div id="the-yates-correction" class="section level2 unnumbered">
<h2>The Yates-Correction</h2>
<p>If all requirements for ordinary chi-square tests are acceptable and only the sample size is the issue, then applying a so-called <em>Yates-correction</em> may be appropriate. This type of correction is applied in cases where the overall sample size lies in-between 60 and 15 cases (<span class="citation">(<span class="citeproc-not-found" data-reference-id="bortz1990verteilungsfreie"><strong>???</strong></span>)</span> 91). The difference between the ordinary chi-square and a Yates-corrected chi-square lies in the fact that the Yates-corrected chi-square is calculated according to the equation below.</p>
<p><span class="math inline">\(\frac{(|observed – expected|-0.5)^{2}}{expected}\)</span></p>
<p>According to this formula, we would get the values shown below rather than the values tabulated above. It is important to note here that this is only a demonstration because a Yates-Correction would actually be inappropriate as our sample size exceeds 60 cases.</p>
<div id="htmlwidget-d4e4bb1cb9c8373cfba3" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-d4e4bb1cb9c8373cfba3">{"x":{"filter":"none","caption":"<caption>Corrected chi-square values for sort of and kind of in BrE and AmE<\/caption>","data":[["kind of","sort of","Total"],["32.9927","16.3593","49.352"],["113.0407","56.0507","169.0914"],["146.0335","72.41","218.4434"]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th>Variant<\/th>\n      <th>BrE<\/th>\n      <th>AmE<\/th>\n      <th>Total<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"pageLength":10,"scrollX":true,"order":[],"autoWidth":false,"orderClasses":false}},"evals":[],"jsHooks":[]}</script>
<p>If the Yates-correction were applied, then this results in a slightly lower <span class="math inline">\(\chi\)</span><sup>2</sup>-value and thus in more conservative results compared with the traditional test according to Pearson.</p>
</div>
<div id="chi-square-within-2k-tables-" class="section level2">
<h2><span class="header-section-number">4.1</span> Chi-Square within 2*k Tables{-}</h2>
<p>Although the <span class="math inline">\(\chi\)</span><sup>2</sup>-test is widely used, it is often used inappropriately. This is especially the case when chi-square tests are applied to data representing tables with more than two rows and more than two columns. It is important to note that applying the common Pearson’s’ chi-square test to sub-tables of a larger table is inappropriate because, in such cases, a modified variant of Pearson’s’ chi-square test is warranted. We will go through two examples that represent different scenarios where we are dealing with subsamples of larger tables and a modified version of the <span class="math inline">\(\chi\)</span><sup>2</sup>-test should be used rather than Pearson’s’ chi-square.</p>
<p>In this first example, we are dealing with a table consisting of two columns and multiple rows, a so-called 2*k table (two-by-k table). In order to test if a feature combination, that is represented by a row in the 2*k table, is significantly more common compared with other feature combinations, we need to implement the <span class="math inline">\(\chi\)</span><sup>2</sup>-equation from <span class="citation">(<span class="citeproc-not-found" data-reference-id="bortz1990verteilungsfreie"><strong>???</strong></span>)</span>.</p>
<p>In this example, we want to find out whether soft and hard X-rays differ in their effect on grasshopper larva. The question is whether the larva reach or do not reach a certain life cycle depending on whether they are exposed to soft X-rays, hard X-rays, light, or beta rays. The data for this example is provided below.</p>
<div id="htmlwidget-93af088e729b5e179cc0" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-93af088e729b5e179cc0">{"x":{"filter":"none","caption":"<caption>Data adapted from Bortz (1990: 126)<\/caption>","data":[["X-ray soft","X-ray hard","Beta-rays","Light","Total"],["21","18","24","13","76"],["14","13","12","30","69"],["35","31","36","43","145"]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th>V1<\/th>\n      <th>Mitosis not reached<\/th>\n      <th>Mitosis reached<\/th>\n      <th>Total<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"pageLength":10,"scrollX":true,"order":[],"autoWidth":false,"orderClasses":false}},"evals":[],"jsHooks":[]}</script>
<p>If we would apply an ordinary chi-square test, we would ignore that all data were collected together and using only a subsample would ignore the data set of which the subsample is part of. In other words, the subsample is not independent from the other data (as it represents a subsection of the whole data set). However, for exemplary reasons, we will apply an ordinary chi-square test first and then compare its results to results provided by the correct version of the chi-square test. In a first step, we create a table with all the data.</p>
<pre class="r"><code># create tdata
wholetable &lt;- matrix(c(21, 14, 18, 13, 24, 12, 13, 30), byrow = T, nrow = 4)
colnames(wholetable) &lt;- c(&quot;reached&quot;, &quot;notreached&quot;)           # add column names
rownames(wholetable) &lt;- c(&quot;rsoft&quot;, &quot;rhard&quot;, &quot;beta&quot;, &quot;light&quot;) # add row names
# inspect data
datatable(wholetable, rownames = FALSE, filter=&quot;none&quot;, options = list(pageLength = 10, scrollX=T))</code></pre>
<div id="htmlwidget-c88eb5c3821fde913392" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-c88eb5c3821fde913392">{"x":{"filter":"none","data":[[21,18,24,13],[14,13,12,30]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th>reached<\/th>\n      <th>notreached<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"pageLength":10,"scrollX":true,"columnDefs":[{"className":"dt-right","targets":[0,1]}],"order":[],"autoWidth":false,"orderClasses":false}},"evals":[],"jsHooks":[]}</script>
<p>Now, we extract the subsample from the data.</p>
<pre class="r"><code>subtable &lt;- wholetable[1:2,] # extract subtable
# inspect data
datatable(subtable, rownames = FALSE, filter=&quot;none&quot;, options = list(pageLength = 10, scrollX=T))</code></pre>
<div id="htmlwidget-25053e9c6342619af897" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-25053e9c6342619af897">{"x":{"filter":"none","data":[[21,18],[14,13]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th>reached<\/th>\n      <th>notreached<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"pageLength":10,"scrollX":true,"columnDefs":[{"className":"dt-right","targets":[0,1]}],"order":[],"autoWidth":false,"orderClasses":false}},"evals":[],"jsHooks":[]}</script>
<p>Next, we apply the ordinary chi-square test to the subsample.</p>
<pre class="r"><code># simple x2-test
chisq.test(subtable, corr = F)</code></pre>
<pre><code>## 
##  Pearson&#39;s Chi-squared test
## 
## data:  subtable
## X-squared = 0.025476, df = 1, p-value = 0.8732</code></pre>
<p>Finally, we perform the correct chi-square test.</p>
<pre class="r"><code># load function for correct chi-square
source(&quot;https://slcladal.github.io/rscripts/x2.2k.r&quot;) 
x2.2k(wholetable, 1, 2)</code></pre>
<pre><code>## $Description
## [1] &quot;rsoft  against  rhard  by  reached  vs  notreached&quot;
## 
## $`Chi-Squared`
## [1] 0.025
## 
## $df
## [1] 1
## 
## $`p-value`
## [1] 0.8744
## 
## $Phi
## [1] 0.013
## 
## $Report
## [1] &quot;Conclusion: the null hypothesis cannot be rejected! Results are not significant!&quot;</code></pre>
<p>Below is a table comparing the results of the two chi-square tests.</p>
<div id="htmlwidget-a5e3c4d0e2526c91db05" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-a5e3c4d0e2526c91db05">{"x":{"filter":"none","caption":"<caption>Table adapted from Bortz (1990: 126)<\/caption>","data":[["chi-squared","p-value"],["0.0255","0.8732"],["0.025","0.8744"]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th>V1<\/th>\n      <th>chi-square<\/th>\n      <th>chi-square in 2*k-tables<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"pageLength":10,"scrollX":true,"order":[],"autoWidth":false,"orderClasses":false}},"evals":[],"jsHooks":[]}</script>
<p>The comparison shows that, in this example, the results of the two tests are very similar but this may not always be the case.</p>
</div>
<div id="chi-square-within-zk-tables-" class="section level2">
<h2><span class="header-section-number">4.2</span> Chi-Square within z*k Tables{-}</h2>
<p>Another application in which the <span class="math inline">\(\chi\)</span><sup>2</sup> test is often applied incorrectly is when ordinary Parsons’s <span class="math inline">\(\chi\)</span><sup>2</sup> tests are used to test portions of tables with more than two rows and more than two columns, that is z*k tables (z: row, k: column). An example is discussed by <span class="citation">Gries (<a href="#ref-gries2014frequency" role="doc-biblioref">2014</a>)</span> who also wrote the <code>R</code> Script for the correct version of the <span class="math inline">\(\chi\)</span><sup>2</sup> test.</p>
<p>Let’s first load the data discussed in the example of <span class="citation">Gries (<a href="#ref-gries2014frequency" role="doc-biblioref">2014</a>)</span> 9. The example deals with metaphors across registers. Based on a larger table, a <span class="math inline">\(\chi\)</span><sup>2</sup> confirmed that registers differ with respect to the frequency of EMOTION metaphors. The more refined question is whether the use of the metaphors EMOTION IS LIGHT and EMOTION IS A FORCE OF NATURE differs between spoken conversation and fiction.</p>
<pre class="r"><code># create table
wholetable &lt;- matrix(c(8, 31, 44, 36, 5, 14, 25, 38, 4, 22, 17, 12, 8, 11, 16, 24), ncol=4)
attr(wholetable, &quot;dimnames&quot;)&lt;-list(Register=c(&quot;acad&quot;, &quot;spoken&quot;, &quot;fiction&quot;, &quot;new&quot;),
Metaphor = c(&quot;Heated fluid&quot;, &quot;Light&quot;, &quot;NatForce&quot;, &quot;Other&quot;))</code></pre>
<p>Based on the table above, we can extract the following subtable.</p>
<div id="htmlwidget-90dc8b2295220d6aa4b4" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-90dc8b2295220d6aa4b4">{"x":{"filter":"none","caption":"<caption>Table adapted from Gries (2014: 9)<\/caption>","data":[["acad","spoken","fiction","new"],["8","31","44","36"],["5","14","25","38"],["4","22","17","12"],["8","11","16","24"]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th>Register<\/th>\n      <th>Heated fluid<\/th>\n      <th>Light<\/th>\n      <th>NatForce<\/th>\n      <th>Other<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"pageLength":10,"scrollX":true,"order":[],"autoWidth":false,"orderClasses":false}},"evals":[],"jsHooks":[]}</script>
<p>If we used an ordinary Pearson’s <span class="math inline">\(\chi\)</span><sup>2</sup> test (the use of which would be inappropriate here), it would reveal that spoken conversations do not differ significantly from fiction in their use of EMOTION IS LIGHT and EMOTION IS A FORCE OF NATURE (<span class="math inline">\(\chi\)</span><sup>2</sup>=3.3016, df=1, p=.069, <span class="math inline">\(\phi\)</span>=.2057).</p>
<pre class="r"><code># create table
subtable &lt;- matrix(c(14, 25, 22, 17), ncol=2)
chisq.results &lt;- chisq.test(subtable, correct=FALSE) # WRONG!
phi.coefficient = sqrt(chisq.results$statistic / sum(subtable) * (min(dim(subtable))-1))
chisq.results</code></pre>
<pre><code>## 
##  Pearson&#39;s Chi-squared test
## 
## data:  subtable
## X-squared = 3.3016, df = 1, p-value = 0.06921</code></pre>
<pre class="r"><code>phi.coefficient</code></pre>
<pre><code>## X-squared 
## 0.2057378</code></pre>
<p>The correct analysis takes into account that it is a subtable that is not independent of the overall table. This means that the correct analysis should take into account the total number of cases, as well as the row and column totals [vgl.@bortz1990verteilungsfreie 144-148].</p>
<p>In order to perform the correct analysis, we must either implement the equation proposed in <span class="citation">(<span class="citeproc-not-found" data-reference-id="bortz1990verteilungsfreie"><strong>???</strong></span>)</span> 144-148 or read in the function written by <span class="citation">Gries (<a href="#ref-gries2014frequency" role="doc-biblioref">2014</a>)</span> and apply it to the subtable.</p>
<pre class="r"><code># load function for chi square test for subtables
source(&quot;https://slcladal.github.io/rscripts/sub.table.r&quot;) 
# apply test
results &lt;- sub.table(wholetable, 2:3, 2:3, out=&quot;short&quot;)
# inspect results
results</code></pre>
<pre><code>## $`Whole table`
##          Metaphor
## Register  Heated fluid Light NatForce Other Sum
##   acad               8     5        4     8  25
##   spoken            31    14       22    11  78
##   fiction           44    25       17    16 102
##   new               36    38       12    24 110
##   Sum              119    82       55    59 315
## 
## $`Sub-table`
##          Metaphor
## Register  Light NatForce Sum
##   spoken     14       22  36
##   fiction    25       17  42
##   Sum        39       39  78
## 
## $`Chi-square tests`
##                                   Chi-square Df    p-value
## Cells of sub-table to whole table  7.2682190  3 0.06382273
## Rows (within sub-table)            0.2526975  1 0.61518204
## Columns (within sub-table)         3.1519956  1 0.07583417
## Contingency (within sub-table)     3.8635259  1 0.04934652</code></pre>
<p>The results show that the difference is, in fact, statistically significant (<span class="math inline">\(\chi^{2}\)</span>=3.864, df=1, p=.0</p>
</div>
<div id="configural-frequency-analysis-cfa" class="section level2 unnumbered">
<h2>Configural Frequency Analysis (CFA)</h2>
<pre class="r"><code># load library
library(cfa)
# load data
cfadata &lt;- read.delim(&quot;https://slcladal.github.io/data/cfadata.txt&quot;, 
                      header = T, sep = &quot;\t&quot;)
# inspect data
datatable(cfadata, rownames = FALSE, filter=&quot;none&quot;, options = list(pageLength = 10, scrollX=T))</code></pre>
<div id="htmlwidget-ffd2dd847f235d724368" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-ffd2dd847f235d724368">{"x":{"filter":"none","data":[["American","British","British","American","American","British","American","British","British","American","British","British","American","British","British","British","British","British","British","British","British","American","British","British","British","British","American","British","British","British","American","British","British","British","British","British","British","British","British","British","American","American","British","British","British","British","American","British","British","British","British","American","British","British","British","British","British","British","British","British","British","British","British","American","British","British","British","British","American","British","British","British","British","British","British","American","American","British","American","American","British","British","American","American","British","American","American","British","British","American","British","British","British","American","British","British","British","British","British","British","British","British","British","British","British","American","American","American","British","British","American","American","American","American","British","American","American","British","American","British","British","British","American","American","American","British","British","British","British","British","American","British","British","British","British","British","British","British","American","American","American","British","British","British","British","British","British","British","British","American","American","British","American","British","British","British","British","British","British","British","British","British","British","American","British","American","British","British","British","British","British","British","British","American","British","British","British","British","American","American","British","British","American","British","American","British","British","American","American","American","British","British","British","British","British","American","British","British","British","British","British","British","British","British","American","British","British","British","British","British","British","American","American","British","British","British","British","American","British","British","British","British","American","British","British","British","American","American","American","American","British","British","British","American","British","British","British","British","British","British","British","British","American","British","British","American","British","British","British","American"],["Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Young","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Young","Old","Old","Old","Old","Old","Old","Old","Old","Young","Old","Old","Old","Old","Young","Old","Young","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Young","Old","Young","Old","Young","Old","Old","Young","Old","Young","Old","Old","Old","Young","Old","Old","Old","Old","Old","Old","Old","Old","Young","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Young","Old","Old","Old","Old","Young","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Young","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Young","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Old","Young","Old","Old","Old","Old","Old","Old","Old","Young","Old","Old","Old","Old","Old","Young","Old"],["Man","Woman","Man","Woman","Man","Man","Man","Man","Man","Woman","Woman","Man","Woman","Woman","Man","Woman","Man","Man","Man","Man","Man","Woman","Man","Woman","Woman","Man","Man","Man","Man","Man","Woman","Woman","Man","Woman","Woman","Woman","Man","Man","Man","Woman","Man","Man","Man","Woman","Woman","Man","Man","Man","Man","Woman","Man","Man","Woman","Woman","Man","Man","Man","Man","Man","Man","Man","Woman","Man","Woman","Man","Man","Man","Man","Woman","Man","Woman","Man","Woman","Man","Man","Man","Man","Woman","Man","Man","Man","Man","Man","Man","Man","Man","Woman","Man","Woman","Man","Woman","Woman","Man","Woman","Woman","Man","Man","Man","Man","Man","Man","Woman","Man","Man","Man","Woman","Woman","Man","Man","Woman","Woman","Man","Man","Woman","Woman","Man","Man","Man","Man","Man","Woman","Man","Man","Man","Woman","Woman","Man","Man","Woman","Man","Man","Man","Woman","Man","Man","Man","Man","Man","Man","Man","Man","Man","Woman","Man","Man","Man","Man","Man","Man","Man","Woman","Woman","Man","Woman","Woman","Man","Man","Man","Man","Man","Woman","Man","Man","Woman","Man","Man","Man","Man","Man","Man","Man","Woman","Man","Woman","Woman","Man","Woman","Man","Woman","Man","Man","Man","Man","Woman","Woman","Man","Man","Woman","Man","Man","Woman","Man","Woman","Man","Man","Woman","Man","Man","Woman","Man","Woman","Man","Woman","Woman","Man","Man","Man","Man","Man","Man","Woman","Woman","Man","Man","Man","Woman","Woman","Man","Woman","Woman","Woman","Woman","Man","Man","Woman","Woman","Man","Woman","Man","Man","Man","Man","Man","Man","Man","Man","Man","Man","Woman","Man","Man","Woman","Man","Man","Man","Man","Man","Man","Man","Man"],["Middle","Middle","Middle","Working","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Working","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Working","Working","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Working","Middle","Middle","Working","Middle","Middle","Middle","Working","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Working","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Working","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Working","Middle","Working","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Working","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Working","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Working","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Working","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Working","Working","Middle","Middle","Middle","Middle","Working","Middle","Working","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Middle","Working","Middle","Middle","Middle"],[7,9,6,2,5,8,8,6,3,8,4,0,6,4,1,7,7,3,7,0,4,0,1,6,6,8,7,0,3,8,6,5,9,7,6,1,0,3,7,8,8,9,4,1,9,9,1,1,3,2,9,1,4,6,8,7,3,0,3,5,0,0,7,6,6,5,2,7,6,6,3,5,8,0,3,2,4,6,5,6,2,8,3,3,9,6,1,2,4,8,8,6,4,4,6,7,6,9,3,7,0,2,6,9,5,2,5,0,5,4,1,3,2,2,2,1,0,9,0,4,7,7,6,3,6,1,9,9,1,2,5,2,5,2,0,4,2,8,9,1,6,4,7,5,4,1,0,7,5,6,9,1,7,4,9,4,7,2,7,1,6,9,5,3,0,6,1,8,3,6,7,4,6,7,6,5,1,9,3,0,8,0,6,0,1,9,2,5,0,2,5,4,4,8,6,2,3,2,2,7,9,4,5,4,2,3,4,3,3,1,7,9,4,6,2,2,4,6,3,7,2,9,9,3,1,4,0,5,4,0,4,4,3,4,9,3,4,8,9,3,3,0,8,9,9,2,6,2,5,0]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th>Variety<\/th>\n      <th>Age<\/th>\n      <th>Gender<\/th>\n      <th>Class<\/th>\n      <th>Frequency<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"pageLength":10,"scrollX":true,"columnDefs":[{"className":"dt-right","targets":4}],"order":[],"autoWidth":false,"orderClasses":false}},"evals":[],"jsHooks":[]}</script>
<p>In a next step, we define the configurations and separate them from the counts.</p>
<pre class="r"><code># load library
library(dplyr)
# define configurations
configs &lt;- cfadata %&gt;%
  select(Variety, Age, Gender, Class)
# define counts
counts &lt;- cfadata$Frequency</code></pre>
<p>Now that configurations and counts are separated, we can perform the configural frequency analysis.</p>
<pre class="r"><code># perform cfa
cfa(configs,counts) </code></pre>
<pre><code>## 
## *** Analysis of configuration frequencies (CFA) ***
## 
##                          label   n   expected            Q       chisq
## 1     American Old Man Working   9  17.269530 0.0074991397 3.959871781
## 2    American Young Man Middle  20  13.322419 0.0060338993 3.346996519
## 3    British Old Woman Working  33  24.277715 0.0079603059 3.133665860
## 4   British Young Woman Middle  12  18.728819 0.0061100471 2.417504403
## 5  American Young Woman Middle  10   6.362422 0.0032663933 2.079707490
## 6      British Old Man Working  59  50.835658 0.0076361897 1.311214959
## 7     British Young Man Middle  44  39.216698 0.0044257736 0.583424432
## 8    American Old Woman Middle  81  76.497023 0.0043152503 0.265066491
## 9     British Old Woman Middle 218 225.181379 0.0080255135 0.229025170
## 10     American Old Man Middle 156 160.178850 0.0043537801 0.109020569
## 11  American Old Woman Working   8   8.247454 0.0002225797 0.007424506
## 12      British Old Man Middle 470 471.512390 0.0023321805 0.004851037
##       p.chisq sig.chisq          z        p.z sig.z
## 1  0.04659725     FALSE -2.1267203 0.98327834 FALSE
## 2  0.06732776     FALSE  1.7026500 0.04431680 FALSE
## 3  0.07669111     FALSE  1.6871254 0.04578962 FALSE
## 4  0.11998594     FALSE -1.6845116 0.95395858 FALSE
## 5  0.14926878     FALSE  1.2474422 0.10611771 FALSE
## 6  0.25217480     FALSE  1.1002146 0.13561931 FALSE
## 7  0.44497317     FALSE  0.6962784 0.24312726 FALSE
## 8  0.60666058     FALSE  0.4741578 0.31769368 FALSE
## 9  0.63224759     FALSE -0.5726832 0.71657040 FALSE
## 10 0.74126197     FALSE -0.3993470 0.65518123 FALSE
## 11 0.93133480     FALSE -0.2612337 0.60304386 FALSE
## 12 0.94447273     FALSE -0.1217934 0.54846869 FALSE
## 
## 
## Summary statistics:
## 
## Total Chi squared         =  17.44777 
## Total degrees of freedom  =  11 
## p                         =  2.9531e-05 
## Sum of counts             =  1120 
## 
## Levels:
## 
## Variety     Age  Gender   Class 
##       2       2       2       2</code></pre>
</div>
<div id="hierarchical-configural-frequency-analysis-hcfa" class="section level2 unnumbered">
<h2>Hierarchical Configural Frequency Analysis (HCFA)</h2>
<p>A hierarchical alternative to CFA is Hierarchical Configural Frequency Analysis (HCFA). In contrast to CFA, in HCFA, the data is assumed to be nested! We begin by defining the configurations and separate them from the counts.</p>
<pre class="r"><code># load library
library(dplyr)
# define configurations
configs &lt;- cfadata %&gt;%
  select(Variety, Age, Gender, Class)
# define counts
counts &lt;- cfadata$Frequency</code></pre>
<p>Now that configurations and counts are separated, we can perform the hierarchical configural frequency analysis.</p>
<pre class="r"><code># perform cfa
hcfa(configs,counts) </code></pre>
<pre><code>## 
## *** Hierarchical CFA ***
## 
##                      Overall chi squared df          p order
## Variety Age Class              12.218696  4 0.01579696     3
## Variety Gender Class            8.773578  4 0.06701496     3
## Variety Age Gender              7.974102  4 0.09253149     3
## Variety Class                   6.078225  1 0.01368582     2
## Variety Class                   6.078225  1 0.01368582     2
## Age Gender Class                5.164357  4 0.27084537     3
## Variety Age                     4.466643  1 0.03456284     2
## Variety Age                     4.466643  1 0.03456284     2
## Age Gender                      1.934543  1 0.16426233     2
## Age Gender                      1.934543  1 0.16426233     2
## Age Class                       1.673538  1 0.19578534     2
## Age Class                       1.673538  1 0.19578534     2
## Gender Class                    1.546666  1 0.21362833     2
## Gender Class                    1.546666  1 0.21362833     2
## Variety Gender                  1.120155  1 0.28988518     2
## Variety Gender                  1.120155  1 0.28988518     2</code></pre>
<p>According to the HCFA, only a single configuration (Variety : Age : Class) is significant (X2 = 12.21, p = .016).</p>
</div>
</div>
<div id="simple-linear-regression" class="section level1">
<h1><span class="header-section-number">5</span> Simple Linear Regression</h1>
<p>This section focuses on a very widely used statistical method which is called regression. Regressions are used when we try to understand how independent variables correlate with a dependent or outcome variable. So, if you want to investigate how a certain factor affects an outcome, then a regression is the way to go. We will have a look at two simple examples to understand what the concepts underlying a regression mean and how a regression works. The R-Code, that we will use, is adapted from <span class="citation">Field, Miles, and Field (<a href="#ref-field2012discovering" role="doc-biblioref">2012</a>)</span> - which is highly recommended for understanding regression analyses! In addition to <span class="citation">Field, Miles, and Field (<a href="#ref-field2012discovering" role="doc-biblioref">2012</a>)</span>, there are various introductions which also focus on regression (among other types of analyses), for example, <span class="citation">Gries (<a href="#ref-gries2009statistics" role="doc-biblioref">2009</a>)</span>, <span class="citation">Levshina (<a href="#ref-levshina2015linguistics" role="doc-biblioref">2015</a>)</span>, <span class="citation">Wilcox (<a href="#ref-wilcox2009basic" role="doc-biblioref">2009</a>)</span> - <span class="citation">Baayen (<a href="#ref-baayen2008analyzing" role="doc-biblioref">2008</a>)</span> is also very good but probably not the first book one should read about statistics.</p>
<div id="introduction-1" class="section level2 unnumbered">
<h2>Introduction</h2>
<p>Although the basic logic underlying regressions is identical to the conceptual underpinnings of <em>analysis of variance</em> (ANOVA), a related method, sociolinguistists have traditionally favoured regression analysis in their studies while ANOVAs have been the method of choice in psycholinguistics. The preference for either method is grounded in historical happenstances and the culture of these subdisciplines rather than in methodological reasoning.</p>
<p>A minor difference between regressions and ANOVA lies in the fact that regressions are based on the <span class="math inline">\(t\)</span>-distribution while ANOVAs use the <span class="math inline">\(F\)</span>-distribution (however, the <span class="math inline">\(F\)</span>-value is simply the value of <span class="math inline">\(t\)</span> squared or t<sup>2</sup>). Both <span class="math inline">\(t\)</span>- and <span class="math inline">\(F\)</span>-values report on the ratio between explained and unexplained variance.</p>
<p>The idea behind regression analysis is expressed formally in the equation below where<span class="math inline">\(f_{(x)}\)</span> is the <span class="math inline">\(y\)</span>-value we want to predict, <span class="math inline">\(\alpha\)</span> is the intercept (the point where the regression line crosses the <span class="math inline">\(y\)</span>-axis), <span class="math inline">\(\beta\)</span> is the coefficient (the slope of the regression line).</p>
<p><span class="math inline">\(f_{(x)} = \alpha + \beta_{1}x_{i} + \epsilon\)</span></p>
<p>In other words, to estimate how much some weights who is 180cm tall, we would multiply the coefficent (slope of the line) with 180 (<span class="math inline">\(x\)</span>) and add the value of the intercept (point where line crosses the <span class="math inline">\(y\)</span>-axis).</p>
<p>However, the idea behind regressions can best be described graphically: imagine a cloud of points (like the points in the scatterplot below). Regressions aim to find that line which has the minimal summed distance between points and the line (like the line in the right panel). Technically speaking, the aim of a regression is to find the line with the minimal deviance (or the line with the minimal sum of residuals). Residuals are the distance between the line and the points (the red lines) and it is also called <em>variance</em>.</p>
<p>Thus, regression lines are those lines where the sum of the red lines should be minimal. The slope of the regression line is called <em>coefficient</em> and the point where the regression line crosses the y-axis is called the <em>intercept</em>.</p>
<p><img src="basicstatz_files/figure-html/slr1-1.png" width="672" /></p>
<p>A word about standard errors (SE) is in order here because most commonly used statistics programs will provide SE values when reporting regression models. The SE is a measure that tells us how much the coefficients were to vary if the same regression were applied to many samples from the same population. A relatively small SE value therefore indicates that the coefficients will remain very stable if the same regression model is fitted to many different samples with identical parameters. In contrast, a large SE tells you that the model is volatile and not very stable or reliable as the coefficients vary substantially if the model is applied to many samples.</p>
</div>
<div id="example-1-preposition-use-across-real-time" class="section level2 unnumbered">
<h2>Example 1: Preposition Use across Real-Time</h2>
<p>We will now turn to our first example. In this example, we will investigate whether the frequency of prepositions has changed from Middle English to Late Modern English. The reasoning behind this example is that Old English was highly synthetic compared with Present-Day English which comparatively analytic. In other words, while Old English speakers used case to indicate syntactic relations, speakers of Present-Day English use word order and prepositions to indicate syntactic relationships. This means that the loss of case had to be compensated by different strategies and maybe these strategies continued to develop and increase in frequency even after the change from synthetic to analytic had been mostly accomplished. And this prolonged change in compensatory strategies is what this example will focus on.</p>
<p>The analysis is based on data extracted from the <em>Penn Corpora of Historical English</em> (see <a href="http://www.ling.upenn.edu/hist-corpora/" class="uri">http://www.ling.upenn.edu/hist-corpora/</a>), that consists of 603 texts written between 1125 and 1900. In preparation of this example, all elements that were part-of-speech tagged as prepositions were extracted from the PennCorpora.</p>
<p>Then, the relative frequencies (per 1,000 words) of prepositions per text were calculated. This frequency of prepositions per 1,000 words represents our dependent variable. In a next step, the date when each letter had been written was extracted. The resulting two vectors were combined into a table which thus contained for each text, when it was written (independent variable) and its relative frequency of prepositions (dependent or outcome variable).</p>
<p>A regression analysis will follow the steps described below: 1. Extraction and processing of the data 2. Data visualization 3. Applying the regression analysis to the data 4. Diagnosing the regression model and checking whether or not basic model assumptions have been violated.</p>
<p>In a first step, we load the libraries and functions.</p>
<pre class="r"><code># load libraries
library(car)
library(dplyr)
library(ggplot2)
library(knitr)         
library(QuantPsyc)  
library(DT)
# load functions
source(&quot;https://slcladal.github.io/rscripts/multiplot.r&quot;)
source(&quot;https://slcladal.github.io/rscripts/slrsummary.r&quot;)</code></pre>
<p>After preparing our session, we can now load and inspect the data to get a first impression of its properties.</p>
<pre class="r"><code># load data
slrdata &lt;- read.delim(&quot;https://slcladal.github.io/data/lmmdata.txt&quot;, header = TRUE)
# inspect data
datatable(slrdata, rownames = FALSE, filter=&quot;none&quot;, options = list(pageLength = 10, scrollX=T))</code></pre>
<div id="htmlwidget-7f0f22f0097972cdc90b" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-7f0f22f0097972cdc90b">{"x":{"filter":"none","data":[[1736,1711,1808,1878,1743,1908,1906,1897,1785,1776,1905,1711,1762,1726,1835,1837,1774,1776,1712,1719,1837,1747,1718,1881,1885,1859,1749,1886,1830,1763,1776,1873,1805,1780,1808,1797,1707,1749,1749,1775,1742,1806,1866,1830,1747,1895,1718,1836,1796,1889,1890,1744,1895,1769,1764,1764,1866,1863,1777,1835,1716,1813,1716,1745,1775,1805,1835,1865,1895,1890,1901,1872,1814,1746,1882,1799,1800,1865,1747,1817,1913,1718,1815,1745,1837,1793,1865,1530,1700,1687,1525,1695,1695,1695,1504,1504,1552,1608,1608,1608,1568,1568,1568,1675,1675,1675,1611,1611,1611,1611,1611,1611,1605,1605,1605,1537,1668,1668,1668,1597,1597,1597,1556,1556,1556,1593,1593,1593,1695,1695,1695,1664,1664,1676,1676,1627,1627,1627,1713,1713,1713,1680,1680,1680,1677,1677,1677,1506,1657,1679,1679,1692,1692,1692,1673,1623,1675,1688,1612,1612,1612,1537,1530,1530,1513,1538,1688,1597,1597,1597,1647,1509,1690,1547,1599,1599,1599,1552,1552,1552,1666,1690,1566,1575,1588,1600,1600,1531,1531,1531,1658,1667,1667,1680,1504,1600,1600,1600,1600,1689,1689,1689,1629,1516,1516,1516,1677,1698,1698,1698,1521,1521,1521,1534,1534,1534,1602,1602,1602,1538,1681,1681,1681,1503,1515,1589,1589,1589,1538,1538,1538,1593,1593,1593,1505,1515,1523,1633,1633,1638,1638,1568,1568,1568,1580,1612,1612,1612,1515,1528,1528,1532,1601,1601,1601,1665,1665,1665,1614,1614,1614,1614,1614,1614,1660,1660,1660,1648,1652,1663,1520,1509,1655,1630,1630,1630,1673,1673,1662,1662,1688,1688,1688,1630,1630,1630,1636,1636,1686,1686,1630,1630,1590,1627,1627,1639,1637,1637,1637,1625,1625,1625,1536,1585,1696,1696,1696,1549,1549,1549,1543,1543,1543,1685,1685,1685,1685,1685,1685,1539,1559,1559,1559,1582,1582,1582,1535,1537,1615,1615,1615,1630,1630,1630,1675,1675,1526,1526,1526,1678,1536,1670,1670,1670,1681,1529,1535,1535,1513,1513,1513,1524,1524,1524,1642,1642,1534,1534,1534,1515,1515,1528,1505,1631,1631,1631,1706,1706,1719,1685,1685,1685,1676,1676,1687,1687,1687,1667,1667,1667,1603,1603,1603,1625,1625,1628,1632,1685,1685,1685,1677,1626,1636,1600,1600,1598,1598,1598,1551,1551,1551,1633,1692,1658,1673,1673,1688,1703,1712,1624,1555,1555,1555,1607,1627,1627,1631,1631,1536,1536,1503,1503,1538,1546,1591,1591,1591,1697,1686,1688,1708,1708,1512,1515,1536,1545,1545,1555,1563,1572,1572,1589,1589,1593,1610,1610,1627,1640,1666,1671,1699,1699,1580,1580,1580,1665,1665,1549,1593,1630,1630,1630,1520,1520,1571,1571,1554,1671,1671,1679,1679,1680,1517,1517,1517,1594,1521,1521,1568,1568,1568,1562,1534,1534,1534,1530,1530,1530,1676,1676,1676,1565,1588,1588,1529,1529,1529,1625,1503,1516,1516,1649,1450,1230,1230,1340,1380,1400,1464,1452,1400,1390,1350,1390,1495,1440,1475,1200,1396,1497,1400,1450,1400,1415,1388,1382,1150,1387,1388,1481,1500,1348,1349,1425,1200,1400,1400],["Science","Education","PrivateLetter","Education","Education","Education","Diary","Philosophy","Philosophy","Diary","Travel","Education","Sermon","Sermon","PrivateLetter","History","Education","Travel","Travel","Fiction","Fiction","Biography","Handbook","Bible","Bible","Science","Fiction","Handbook","Sermon","PublicLetter","History","PublicLetter","Fiction","Handbook","Diary","Science","History","PublicLetter","TrialProceeding","PrivateLetter","History","Education","History","Science","Handbook","Fiction","PrivateLetter","Travel","Bible","PrivateLetter","PrivateLetter","Travel","History","Science","Bible","Bible","Sermon","Travel","Fiction","Diary","Diary","Biography","Law","Law","Law","Law","Law","Law","Law","Science","Sermon","Diary","Handbook","TrialProceeding","Biography","History","Travel","PrivateLetter","PrivateLetter","TrialProceeding","Handbook","Education","PublicLetter","Diary","Education","Sermon","Fiction","PrivateLetter","PrivateLetter","PrivateLetter","PublicLetter","PrivateLetter","PrivateLetter","PrivateLetter","PrivateLetter","PrivateLetter","PrivateLetter","Fiction","Fiction","Fiction","Education","Education","Education","PublicLetter","PublicLetter","PublicLetter","Bible","Bible","Bible","Bible","Bible","Bible","Education","Education","Education","PublicLetter","Fiction","Fiction","Fiction","Science","Science","Science","Philosophy","Philosophy","Philosophy","Philosophy","Philosophy","Philosophy","Philosophy","Philosophy","Philosophy","Science","Science","Science","Science","Education","Education","Education","History","History","History","Biography","Biography","Biography","PublicLetter","PublicLetter","PublicLetter","Travel","PublicLetter","PublicLetter","PublicLetter","PrivateLetter","PrivateLetter","PrivateLetter","PublicLetter","PublicLetter","PublicLetter","PublicLetter","Travel","Travel","Travel","PublicLetter","PublicLetter","PublicLetter","PublicLetter","PrivateLetter","PublicLetter","Fiction","Fiction","Fiction","PrivateLetter","PrivateLetter","PublicLetter","PrivateLetter","PublicLetter","PublicLetter","PublicLetter","Diary","Diary","Diary","PrivateLetter","PrivateLetter","PublicLetter","PublicLetter","PublicLetter","PublicLetter","PublicLetter","Education","Education","Education","PrivateLetter","PrivateLetter","PrivateLetter","PrivateLetter","PrivateLetter","TrialProceeding","TrialProceeding","TrialProceeding","TrialProceeding","Diary","Diary","Diary","PrivateLetter","History","History","History","PrivateLetter","Travel","Travel","Travel","Sermon","Sermon","Sermon","Handbook","Handbook","Handbook","Diary","Diary","Diary","PublicLetter","Travel","Travel","Travel","PrivateLetter","PrivateLetter","PrivateLetter","PrivateLetter","PrivateLetter","PrivateLetter","PrivateLetter","PrivateLetter","Handbook","Handbook","Handbook","PrivateLetter","PrivateLetter","PrivateLetter","PrivateLetter","PrivateLetter","PrivateLetter","PrivateLetter","Fiction","Fiction","Fiction","PublicLetter","History","History","History","PublicLetter","PublicLetter","PublicLetter","PublicLetter","Diary","Diary","Diary","Science","Science","Science","Sermon","Sermon","Sermon","Sermon","Sermon","Sermon","Education","Education","Education","PrivateLetter","PrivateLetter","PrivateLetter","PublicLetter","PrivateLetter","PrivateLetter","PrivateLetter","PrivateLetter","PrivateLetter","Sermon","Sermon","Education","Education","PrivateLetter","PrivateLetter","PrivateLetter","Travel","Travel","Travel","PrivateLetter","PrivateLetter","PrivateLetter","PrivateLetter","PrivateLetter","PrivateLetter","TrialProceeding","PrivateLetter","PrivateLetter","PrivateLetter","PrivateLetter","PrivateLetter","PrivateLetter","PrivateLetter","PrivateLetter","PrivateLetter","PrivateLetter","PrivateLetter","Handbook","Handbook","Handbook","Sermon","Sermon","Sermon","Travel","Travel","Travel","TrialProceeding","TrialProceeding","TrialProceeding","Education","Education","Education","PublicLetter","Diary","Diary","Diary","Diary","Diary","Diary","PrivateLetter","PublicLetter","Handbook","Handbook","Handbook","PrivateLetter","PrivateLetter","PrivateLetter","PublicLetter","PublicLetter","Fiction","Fiction","Fiction","PrivateLetter","PrivateLetter","History","History","History","PrivateLetter","PrivateLetter","PrivateLetter","PrivateLetter","History","History","History","PublicLetter","PublicLetter","PublicLetter","PrivateLetter","PrivateLetter","PrivateLetter","PrivateLetter","PrivateLetter","PublicLetter","PublicLetter","PublicLetter","PrivateLetter","PrivateLetter","PrivateLetter","PrivateLetter","PrivateLetter","PrivateLetter","PrivateLetter","TrialProceeding","TrialProceeding","TrialProceeding","PublicLetter","PublicLetter","Fiction","Fiction","Fiction","Diary","Diary","Diary","Biography","Biography","Biography","PrivateLetter","PrivateLetter","PrivateLetter","PrivateLetter","PrivateLetter","PrivateLetter","PrivateLetter","PublicLetter","PrivateLetter","PrivateLetter","TrialProceeding","TrialProceeding","PublicLetter","PublicLetter","PublicLetter","Science","Science","Science","PrivateLetter","PrivateLetter","PrivateLetter","PrivateLetter","PrivateLetter","PrivateLetter","PrivateLetter","PrivateLetter","PublicLetter","Biography","Biography","Biography","PrivateLetter","PrivateLetter","PrivateLetter","PrivateLetter","PrivateLetter","PrivateLetter","PrivateLetter","PrivateLetter","PrivateLetter","PublicLetter","PrivateLetter","Sermon","Sermon","Sermon","PublicLetter","PrivateLetter","PublicLetter","PublicLetter","PublicLetter","Law","Law","Law","Law","Law","Law","Law","Law","Law","Law","Law","Law","Law","Law","Law","Law","Law","Law","Law","Law","History","History","History","PrivateLetter","PrivateLetter","PublicLetter","PublicLetter","PrivateLetter","PrivateLetter","PrivateLetter","PublicLetter","PublicLetter","TrialProceeding","TrialProceeding","TrialProceeding","Sermon","Sermon","Sermon","Sermon","Sermon","Travel","Travel","Travel","PublicLetter","PublicLetter","PublicLetter","Handbook","Handbook","Handbook","Handbook","Bible","Bible","Bible","Bible","Bible","Bible","Handbook","Handbook","Handbook","PublicLetter","PublicLetter","PublicLetter","PublicLetter","PublicLetter","PublicLetter","PrivateLetter","PrivateLetter","PrivateLetter","PrivateLetter","PrivateLetter","Religion","Religion","Religion","Religion","Philosophy","History","History","Sermon","Religion","Religion","Bible","Religion","Sermon","Sermon","History","Religion","Religion","Sermon","Religion","Religion","Travel","Sermon","Bible","Bible","History","History","Religion","Fiction","Handbook","Religion","Religion","Sermon","Religion","Religion","Sermon"],["albin","anon","austen","bain","barclay","benson","benson","boethja","boethri","boswell","bradley","brightland","burton","butler","carlyle","carlyle","chapman","cook","cooke","defoe","dickens","doddridge","drummond","erv","erv","faraday","fielding","fleming","froude","george","gibbon","gladstone","godwin","grafting","haydon","herschel","hind","holmes","holmes","johnson","kimber","lancaster","long","lyell","maxwell","meredith","montagu","montefiore","newcome","nightingale","nightingale","officer","oman","priestley","purver","purver","pusey","reade","reeve","ruskin","ryder","southey","statutes","statutes","statutes","statutes","statutes","statutes","statutes","strutt","talbot","thring","tindall","townley","trollope","turner","turner","victoria","walpole","watson","weathers","webster","wellesley","wesley","whewell","wollaston","yonge","abott","alhatton","alhatton","ambass","anhatton","anhatton","anhatton","aplumpt","aplumpt","apoole","armin","armin","armin","asch","asch","asch","aungier","aungier","aungier","authnew","authnew","authnew","authold","authold","authold","bacon","bacon","bacon","bedyll","behn","behn","behn","blundev","blundev","blundev","boethco","boethco","boethco","boethel","boethel","boethel","boethpr","boethpr","boethpr","boylecol","boylecol","boyle","boyle","brinsley","brinsley","brinsley","burnetcha","burnetcha","burnetcha","burnetroc","burnetroc","burnetroc","capel","capel","capel","chaplain","charles","charles","charles","chatton","chatton","chatton","commiss","conway","conway","counc","coverte","coverte","coverte","cromwell","cromwell","cromwell","dacre","delapole","dell","deloney","deloney","deloney","dering","dplumpt","drummond","ecumberl","edmondes","edmondes","edmondes","edward","edward","edward","ehatton","ehatton","eliz","eliz","eliz","eliz","eliz","elyot","elyot","elyot","eoxinden","eoxinden","eoxinden","eoxinden","epoole","essex","essex","essexstate","essexstate","evelyn","evelyn","evelyn","everard","fabyan","fabyan","fabyan","fhatton","fiennes","fiennes","fiennes","fisher","fisher","fisher","fitzh","fitzh","fitzh","forman","forman","forman","friar","fryer","fryer","fryer","gascoigne","gascoigne","gawdy","gawdy","gawdy","gcromw","gcromw","gcromw","gifford","gifford","gifford","gpoole","gpoole","grey","harley","harley","harleyedw","harleyedw","harman","harman","harman","hatcher","hayward","hayward","hayward","henry","henry","henry","henry","hoby","hoby","hoby","hooke","hooke","hooke","hooker","hooker","hooker","hooker","hooker","hooker","hoole","hoole","hoole","hoxinden","hoxinden","hoxinden","interview","iplumpt","jackson","jbarring","jbarring","jbarring","jetaylor","jetaylor","jetaylormeas","jetaylormeas","jopinney","jopinney","jopinney","jotaylor","jotaylor","jotaylor","joxinden","joxinden","jpinney","jpinney","jubarring","jubarring","judall","knyvett","knyvett","knyvett","koxinden","koxinden","koxinden","kpaston","kpaston","kpaston","kscrope","kscrope","langf","langf","langf","latimer","latimer","latimer","leland","leland","leland","lisle","lisle","lisle","locke","locke","locke","lords","machyn","machyn","machyn","madox","madox","madox","manners","marches","markham","markham","markham","masham","masham","masham","memo","memo","merrytal","merrytal","merrytal","mhatton","mhoward","milton","milton","milton","montague","morelet","morelet","morelet","moreric","moreric","moreric","morewol","morewol","morewol","moxinden","moxinden","mroper","mroper","mroper","mtudor","mtudor","mtudor","nevill","nferrar","nferrar","nferrar","nhadd","nhadd","nhadd","oates","oates","oates","osborne","osborne","penny","penny","penny","pepys","pepys","pepys","perrott","perrott","perrott","pettit","pettit","pettit","peyton","phenry","phenry","phenry","proposals","proud","proud","raleigh","raleigh","rcecil","rcecil","rcecil","record","record","record","rferrar","rhaddjr","rhaddsr","rhaddsr","rhaddsr","rhaddsr","rhaddsr","rhaddsr","rich","roper","roper","roper","roxinden","roxinden","roxinden","roxinden","roxinden","rplumpt","rplumpt","rplumpt","rplumpt","russell","savill","smith","smith","smith","somers","southard","spencer","spencer","spencer","stat","stat","stat","stat","stat","stat","stat","stat","stat","stat","stat","stat","stat","stat","stat","stat","stat","stat","stat","stat","stow","stow","stow","strype","strype","surety","talbot","tbarring","tbarring","tbarring","thoward","thoward","thoward","thoward","throckm","tillots","tillots","tillots","tillots","tillots","torkingt","torkingt","torkingt","trincoll","tunstall","tunstall","turner","turner","turner","turnerherb","tyndnew","tyndnew","tyndnew","tyndold","tyndold","tyndold","walton","walton","walton","wcecil","wcecil","wcecil","wolsey","wolsey","wolsey","wpaston","wplumpt","wplumpt","wplumpt","zouch","cmaelr","cmancriw","cmancriw","cmayenbi","cmboeth","cmbrut","cmcapchr","cmcapser","cmcloud","cmctpars","cmearlps","cmedvern","cmfitzja","cmgaytry","cmgregor","cmhali","cmhilton","cminnoce","cmjulnor","cmkempe","cmmandev","cmmirk","cmntest","cmotest","cmpeterb","cmpolych","cmpurvey","cmreynar","cmreynes","cmrollep","cmrolltr","cmroyal","cmvices","cmvices","cmwycser"],[166.01,139.86,130.78,151.29,145.72,120.77,119.17,132.96,130.49,135.94,154.2,149.14,159.71,157.49,124.16,134.48,153.54,140.22,150,131.48,126.63,152.89,150.66,107.17,134.67,134.34,128.82,155.57,150.1,137.02,172.42,138.88,146.74,145.89,127.51,125.93,158,144.76,129.18,103.31,159.36,150.75,141.11,176.07,128.52,141.39,139.77,142.62,109.4,121.14,119.34,135.96,143.15,152.17,108.66,143.58,158.94,130.87,123.09,159.18,139.27,137.96,158.13,165.67,168.19,170.23,170.54,185.17,181.49,160.07,137.01,116.23,141.86,129.06,140.25,146.06,165.54,100.66,119.95,110.15,153.37,138.32,149.68,143.32,157.58,161.31,123.06,116.67,96.23,107.9,149.43,97.38,130.37,104.84,111.78,132.49,148.51,88.33,112.05,108.38,138.82,129.98,133.84,144.19,152.32,144.82,102.96,112.42,129.91,131.44,125.34,127.91,147.5,152.59,142.91,115.74,122.16,130.67,131.69,139.84,143.32,145.22,105.61,122.07,99.92,115.26,121.21,98.12,121.99,132.93,110.52,154.79,150.01,139.43,147.33,121.58,134.34,122.84,135.61,141.34,139.68,142.76,141.31,142.56,130.71,116.2,110.31,154.24,137.97,160,176.47,105.62,114.36,99.83,195.86,138.66,122.3,164.95,126.73,135.74,113.6,147.13,152.94,127.59,149.6,115.38,161.79,103.78,106.98,128.62,125,122.14,116.56,117.65,118.99,134.73,125,148.03,142.08,147.8,132.96,93.97,144.69,143.26,136.12,144.5,120,139.06,136.59,134.98,113.37,114.62,113.46,119.64,133.55,133.67,135.87,126.95,125.02,151.72,146.6,147.91,88.93,159.18,175.74,174.17,63.97,128.92,132.09,135.12,125.49,115.7,130.87,99.16,106.26,104.45,151.36,142.73,129.7,150.88,150.18,151.44,146.57,129.77,153.85,134.74,122.45,118.64,154.31,134.36,103.73,95.76,110.4,108.64,121.17,137.93,147.73,99.18,103.07,107.95,113.09,114.77,115.28,114.85,126.28,159.63,150.23,159.39,127.45,169.38,165.46,174.6,143.64,136.52,147.88,140.81,138.97,145.17,145.38,134.27,145.37,137.55,128.52,140.35,140.25,138.41,131.84,133.23,129.45,126.53,146.15,107.34,128.57,108.9,134.22,129.75,128.12,131.45,115.08,115.75,142.14,122.37,133.7,133.16,131.19,140.82,124.02,115.36,91.94,102.15,120.31,104.17,115.96,112.37,108.48,105.8,101.19,98.04,102.36,114.08,112.93,113.57,132.81,112,140.48,142.61,137.22,115.23,116.74,113.45,171.11,177.7,173.55,87.79,88.78,85.7,124.78,124.42,132.76,101.85,149.83,148.4,147.85,119.83,136.83,122.38,112.28,152.75,130.78,129.01,128.86,109.03,118.86,108.5,146.85,162.22,113.66,106.91,109.44,127.7,115.62,155.1,162.07,165.42,111.69,123.26,123.39,131.34,134.23,132.12,134.41,151.56,137.37,123.91,99.4,108.87,141.84,117.84,134.8,135.72,139.83,101.52,98.04,112.9,121.43,118.75,143.97,153.55,122.84,95.82,112.45,102,147.21,142.25,90.45,116.1,112.12,128.7,126.75,128.2,135.64,139.44,146.14,124.71,118.28,155.32,105.88,124.36,133.41,114.86,151.11,74.71,117.02,112.03,122.62,140.28,160.43,124.89,130.33,132.82,126.99,95.96,115.94,153.3,122.42,125.89,116.5,137.04,128.34,125.3,142.94,142.93,137.27,134.38,120.27,106.67,141.51,114.68,128.94,121.21,150.18,109.76,147.85,131.38,118.27,125.24,118.44,148.1,144.28,132.59,132.58,140.44,170.21,176.6,149.2,176.03,156.88,154.64,154.27,150,140.2,130.75,149.45,158.93,150.06,159.5,149.68,161.17,159.85,160.66,155.92,159.49,162.34,147.42,163.91,127.08,107.91,110.39,134.83,109.09,105.54,135.23,150.62,144.3,140.42,134.57,121.23,141.61,129.71,132.86,131.75,145.02,141.51,142.78,141.09,142.16,144.64,143.49,122.26,119.82,138.61,136.55,106.1,117.48,136.31,135.89,117.76,124.22,106.73,118.35,115,111.75,163.18,119.47,141.93,117.5,139.45,101.62,73.17,160,139.94,126.54,133.62,110.22,109.5,120.36,111.28,121.27,138.38,160.65,145.91,139.17,129.58,138.13,166.49,117.6,141.67,131.37,148.5,130.98,125.7,129.16,142.08,129.3,113.08,148.78,134.08,137.2,153.39,93.22,128.57,133.24,144.86,126.95,105.18,113.02,130.62],["North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","North","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South","South"]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th>Date<\/th>\n      <th>Genre<\/th>\n      <th>Text<\/th>\n      <th>Prepositions<\/th>\n      <th>Region<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"pageLength":10,"scrollX":true,"columnDefs":[{"className":"dt-right","targets":[0,3]}],"order":[],"autoWidth":false,"orderClasses":false}},"evals":[],"jsHooks":[]}</script>
<p>Inspecting the data is very important because it can happen that a data set may not load completely or that variables which should be numeric have been converted to character variables. If unchecked, then such issues could go unnoticed and cause trouble.</p>
<p>We will now plot the data to get a better understanding of what the data looks like.</p>
<pre class="r"><code>ggplot(slrdata, aes(Date, Prepositions)) +
  geom_point() +
  theme_bw() +
  labs(x = &quot;Year&quot;) +
  labs(y = &quot;Prepositions per 1,000 words&quot;) +
  geom_smooth()</code></pre>
<p><img src="basicstatz_files/figure-html/slr5-1.png" width="672" /></p>
<pre class="r"><code>ggplot(slrdata, aes(Date, Prepositions)) +
  geom_point() +
  theme_bw() +
  labs(x = &quot;Year&quot;) +
  labs(y = &quot;Prepositions per 1,000 words&quot;) +
  geom_smooth(method = &quot;lm&quot;) # with linear model smoothing!</code></pre>
<p><img src="basicstatz_files/figure-html/slr6-1.png" width="672" /></p>
<p>Before beginning with the regression analysis, we will scale the year. We scale by subtracting each value from the mean of year. This can be useful when dealing with numeric variables because if we did not scale year, we would get estimated values for year 0 (a year when English did not even exist yet). If a variable is scaled, the regression provides estimates of the model refer to the mean of that numeric variable. In other words, scaling can eb very helpful, especially with respect to the interpretation of the results that regression models report.</p>
<pre class="r"><code># scale date
slrdata$Date &lt;- slrdata$Date - mean(slrdata$Date) </code></pre>
<p>We will now begin the regression analysis by generating a first regression model. and inspect its results.</p>
<pre class="r"><code># create initial model
m1.lm &lt;- lm(Prepositions ~ Date, data = slrdata)
# inspect results
summary(m1.lm)</code></pre>
<pre><code>## 
## Call:
## lm(formula = Prepositions ~ Date, data = slrdata)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -69.101 -13.855   0.578  13.321  62.858 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 1.322e+02  8.386e-01 157.625   &lt;2e-16 ***
## Date        1.732e-02  7.267e-03   2.383   0.0175 *  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 19.43 on 535 degrees of freedom
## Multiple R-squared:  0.01051,    Adjusted R-squared:  0.008657 
## F-statistic: 5.681 on 1 and 535 DF,  p-value: 0.0175</code></pre>
<p>The summary output starts by repeating the regression equation. Then, the model provides the distribution of the residuals. The residuals should be sitributed normally with the Min and Max as well as the 1Q (first quartile) and 3Q (third quartile) being similar or ideally identical. In our case, the values are very similar which suggests that the residuals are distributed evenly and follow a normal distribution. The next part of the repost is the coefficients table. The Estimate for the intercept is the value where the regression line crosses the y-axis. The estimate for Date represents the slope of the regression line and tells us that with each year, the predicted frequency of prepositions incerase by .01732 prepositions. The t-value is the Estimate divided by the standard error (Std. Error). Based on the t-value, the p-value can be calculated manually as shown below.</p>
<pre class="r"><code># use pt function (which uses t-values and the degrees of freedom)
2*pt(-2.383, nrow(slrdata)-1)</code></pre>
<pre><code>## [1] 0.01751964</code></pre>
<p>The R<sup>2</sup>-values tell us how much variance is explained by our model compared to the overall variance (0.0105 means that our model explains only 1.05 percent of the variance - which is a tiny amount). The adjusted R<sup>2</sup>-value tell us how much variance the model would explain if we applied the model to new data of the same nature (which data points taken from the same population). Or, to be more precise, the adjusted R<sup>2</sup> takes the number of predictors into account: if a model contains predictors that do not explain much variance, then the adjusted R<sup>2</sup> will be lower than the multiple R<sup>2</sup> as the former penalizes models for having superfluous predictors. If there is a big difference between the two R<sup>2</sup>-values, then the model is overfitted which is not good. The F-statistic and the associated p-value tell us that the model, despite explaining almost no variance, is still significantly better than an intercept-only base-line model (or using the overall mean to predict the frequency of prepositions per text).</p>
<p>We can test this and also see where the F-values comes from by comparing the</p>
<pre class="r"><code># create intercept-only base-line model
m0.lm &lt;- lm(Prepositions ~ 1, data = slrdata)
# compare the base-line and the more saturated model
anova(m1.lm, m0.lm, test = &quot;F&quot;)</code></pre>
<pre><code>## Analysis of Variance Table
## 
## Model 1: Prepositions ~ Date
## Model 2: Prepositions ~ 1
##   Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)  
## 1    535 202058                             
## 2    536 204204 -1   -2145.6 5.6809 0.0175 *
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>The F- and p-values are exactly those reported by the summary which shows where the F-values comes from and what it means; namely it denote the difference between the base-line and the more saturated model.</p>
<p>The degrees of freedom associated with the residual standard error are the number of cases in the model minus the number of predictors (including the intercept). The residual standard error is square root of the sum of the squared residuals of the model divided by the degrees of freedom. Have a look at he following to clear this up:</p>
<pre class="r"><code># DF = N - number of predictors (including intercept)
DegreesOfFreedom &lt;- nrow(slrdata)-length(coef(m1.lm))
# sum of the squared residuals
SumSquaredResiduals &lt;- sum(resid(m1.lm)^2)
# Residual Standard Error
sqrt(SumSquaredResiduals/DegreesOfFreedom); DegreesOfFreedom</code></pre>
<pre><code>## [1] 19.43396</code></pre>
<pre><code>## [1] 535</code></pre>
<p>We will now check if mathematical assumptions have been violated (homogeneity of variance) or whether the data contains outliers. We check this using diagnostic plots.</p>
<pre class="r"><code># plot model: 3 plots per row in one window
par(mfrow = c(1, 3))
plot(resid(m1.lm))
plot(rstandard(m1.lm))
plot(rstudent(m1.lm)); par(mfrow = c(1, 1)) # restore default parameters</code></pre>
<p><img src="basicstatz_files/figure-html/slr9-1.png" width="672" /></p>
<p>The left graph shows the residuals of the model (i.e., the differences between the observed and the values predicted by the regression model). The problem with this plot is that the residuals are not standardized and so they cannot be compared to the residuals of other models. To remedy this deficiency, residuals are normalized by dividing the residuals by their standard deviation. Then, the normalized residuals can be plotted against the observed values (centre panel). In this way, not only are standardized residuals obtained, but the values of the residuals are transformed into z-values, and one can use the z-distribution to find problematic data points. There are three rules of thumb regarding finding problematic data points through standardized residuals <span class="citation">(Field, Miles, and Field <a href="#ref-field2012discovering" role="doc-biblioref">2012</a>, 268–69)</span>:</p>
<ul>
<li><p>Points with values higher than 3.29 should be removed from the data.</p></li>
<li><p>If more than 1% of the data points have values higher than 2.58, then the error rate of our model is too high.</p></li>
<li><p>If more than 5% of the data points have values greater than 1.96, then the error rate of our model is too high.</p></li>
</ul>
<p>The right panel shows the * studentized residuals* (adjusted predicted values: each data point is divided by the standard error of the residuals). In this way, it is possible to use Student’s t-distribution to diagnose our model.</p>
<p>Adjusted predicted values are residuals of a special kind: the model is calculated without a data point and then used to predict this data point. The difference between the observed data point and its predicted value is then called the adjusted predicted value. In summary, studentized residuals are very useful because they allow us to identify influential data points.</p>
<p>The plots show that there are two potentially problematic data points (the top-most and bottom-most point). These two points are clearly different from the other data points and may therefore be outliers. We will test later if these points need to be removed.</p>
<p>We will now generate more diagnostic plots.</p>
<pre class="r"><code>par(mfrow = c(2, 2)) # plot window: 2 plots/row, 2 plots/column
plot(m1.lm); par(mfrow = c(1, 1)) # restore normal plot window</code></pre>
<p><img src="basicstatz_files/figure-html/slr10-1.png" width="672" /></p>
<p>The diagnostic plots are very positive and we will go through why this is so for each panel. The graph in the upper left panel is useful for finding outliers or for determining the correlation between residuals and predicted values: when a trend becomes visible in the line or points (e.g., a rising trend or a zigzag line), then this would indicate that the model would be problematic (in such cases, it can help to remove data points that are too influential (outliers)).</p>
<p>The graphic in the upper right panel indicates whether the residuals are normally distributed (which is desirable) or whether the residuals do not follow a normal distribution. If the points lie on the line, the residuals follow a normal distribution. For example, if the points are not on the line at the top and bottom, it shows that the model does not predict small and large values well and that it therefore does not have a good fit.</p>
<p>The graphic in the lower left panel provides information about <em>homoscedasticity</em>. Homoscedasticity means that the variance of the residuals remains constant and does not correlate with any independent variable. In unproblematic cases, the graphic shows a flat line. If there is a trend in the line, we are dealing with heteroscedasticity, that is, a correlation between independent variables and the residuals, which is very problematic for regressions.</p>
<p>The graph in the lower right panel shows problematic influential data points that disproportionately affect the regression (this would be problematic). If such influential data points are present, they should be either weighted (one could generate a robust rather than a simple linear regression) or they must be removed. The graph displays Cook’s distance, which shows how the regression changes when a model without this data point is calculated. The cook distance thus shows the influence a data point has on the regression as a whole. Data points that have a Cook’s distance value greater than 1 are problematic <span class="citation">(Field, Miles, and Field <a href="#ref-field2012discovering" role="doc-biblioref">2012</a>, 269)</span>.</p>
<p>The so-called leverage is also a measure that indicates how strongly a data point affects the accuracy of the regression. Leverage values range between 0 (no influence) and 1 (strong influence: suboptimal!). To test whether a specific data point has a high leverage value, we calculate a cut-off point that indicates whether the leverage is too strong or still acceptable. The following two formulas are used for this:</p>
<p><span class="math display">\[\begin{equation}

\frac{3(k + 1)}{n}

\end{equation}\]</span></p>
<p>or</p>
<p><span class="math display">\[\begin{equation}

\frac{2(k + 1)}{n}

\end{equation}\]</span></p>
<p>We will look more closely at leverage in the context of multiple linear regression and will therefore end the current analysis by summarizing the results of the regression analysis in a table.</p>
<pre class="r"><code># create summary table
slrresults &lt;- slrsummary(m1.lm)  
# show summary table
slrresults</code></pre>
<table>
<caption>
Results of a simple linear regression analysis.
</caption>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:left;">
Estimate
</th>
<th style="text-align:left;">
Pearson’s r
</th>
<th style="text-align:left;">
Std. Error
</th>
<th style="text-align:left;">
t value
</th>
<th style="text-align:left;">
Pr(&gt;|t|)
</th>
<th style="text-align:left;">
P-value sig.
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
(Intercept)
</td>
<td style="text-align:left;">
132.19
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
0.84
</td>
<td style="text-align:left;">
157.62
</td>
<td style="text-align:left;">
0
</td>
<td style="text-align:left;">
p &lt; .001***
</td>
</tr>
<tr>
<td style="text-align:left;">
Date
</td>
<td style="text-align:left;">
0.02
</td>
<td style="text-align:left;">
0.1
</td>
<td style="text-align:left;">
0.01
</td>
<td style="text-align:left;">
2.38
</td>
<td style="text-align:left;">
0.0175
</td>
<td style="text-align:left;">
p &lt; .05*
</td>
</tr>
<tr>
<td style="text-align:left;">
Model statistics
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
Value
</td>
</tr>
<tr>
<td style="text-align:left;">
Number of cases in model
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
537
</td>
</tr>
<tr>
<td style="text-align:left;">
Residual standard error on 535 DF
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
19.43
</td>
</tr>
<tr>
<td style="text-align:left;">
Multiple R-squared
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
0.0105
</td>
</tr>
<tr>
<td style="text-align:left;">
Adjusted R-squared
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
0.0087
</td>
</tr>
<tr>
<td style="text-align:left;">
F-statistic (1, 535)
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
5.68
</td>
</tr>
<tr>
<td style="text-align:left;">
Model p-value
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
0.0175
</td>
</tr>
</tbody>
</table>
<hr />
<p>Typically, the results of regression analyses are presented in such tables as they include all important measures of model quality and significance, as well as the magnitude of the effects.</p>
<p>In addition, the results of simple linear regressions should be summarized in writing. An example of how the results of a regression analysis can be written up is provided below.</p>
<p>A simple linear regression has been fitted to the data. A visual assessment of the model diagnostic graphics did not indicate any problematic or disproportionately influential data points (outliers) and performed significantly better compared to an intercept-only base line model but only explained .87 percent of the vraiance (Adjusted R<sup>2</sup>: .0087, F-statistic (1, 535): 5,68, p-value: 0.0175*). The final minimal adequate linear regression model is based on 537 data points and confirms a significant and positive correlation between the year in which the text was written and the relative frequency of prepositions (coefficient estimate: .02, SE: 0.01, t-value: 2.38, p-value: .0175*).</p>
</div>
<div id="example-2-teaching-styles" class="section level2 unnumbered">
<h2>Example 2: Teaching Styles</h2>
<p>In the previous example, we dealt with two numeric variables, while the following example deals with a categorical independent variable and a numeric dependent variable. The ability for regressions to handle very different types of variables makes regressions a widely used and robust method of analysis.</p>
<p>In this example, we are dealing with two groups of students that have been randomly assigned to be exposed to different teaching methods. Both groups undergo a language learning test after the lesson with a maximum score of 20 points.</p>
<p>The question that we will try to answer is whether the students in group A have performed significantly better than those in group B which would indicate that the teaching method to which group A was exposed works better than the teaching method to which group B was exposed.</p>
<p>Let’s move on to implementing the regression in “R”. In a first step, we load the data set and inspect its structure.</p>
<pre class="r"><code># load data
slrdata2 &lt;- read.delim(&quot;https://slcladal.github.io/data/slrdata2.txt&quot;, sep = &quot;\t&quot;, header = T)
# inspect data
head(slrdata2)</code></pre>
<pre><code>##   Group Score
## 1     A    15
## 2     A    12
## 3     A    11
## 4     A    18
## 5     A    15
## 6     A    15</code></pre>
<p>Now, we graphically display the data. In this case, a boxplot represents a good way to visualize the data.</p>
<pre class="r"><code># extract means
means &lt;- slrdata2 %&gt;%
  dplyr::group_by(Group) %&gt;%
  dplyr::summarise(Mean = round(mean(Score), 1), SD = round(sd(Score), 1))</code></pre>
<pre><code>## `summarise()` ungrouping output (override with `.groups` argument)</code></pre>
<pre class="r"><code># start plot
ggplot(slrdata2, aes(Group, Score)) + 
  geom_boxplot(fill=c(&quot;orange&quot;, &quot;darkgray&quot;)) +
  geom_text(data = means, aes(label = paste(&quot;M = &quot;, Mean, sep = &quot;&quot;), y = 1)) +
  geom_text(data = means, aes(label = paste(&quot;SD = &quot;, SD, sep = &quot;&quot;), y = 0)) +
  theme_bw(base_size = 15) +
  labs(x = &quot;Group&quot;) +                      
  labs(y = &quot;Test score (Points)&quot;, cex = .75) +   
  coord_cartesian(ylim = c(0, 20)) +  
  guides(fill = FALSE)                </code></pre>
<div class="figure">
<img src="basicstatz_files/figure-html/slr14-1.png" alt="Language test data" width="672" />
<p class="caption">
Language test data
</p>
</div>
<p>The data indicate that group A did significantly better than group B. We will test this impression by generating the regression model and creating the model and extracting the model summary.</p>
<pre class="r"><code># generate regression model
m2.lm &lt;- lm(Score ~ Group, data = slrdata2) 
# inspect results
summary(m2.lm)                             </code></pre>
<pre><code>## 
## Call:
## lm(formula = Score ~ Group, data = slrdata2)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -6.767 -1.933  0.150  2.067  6.233 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  14.9333     0.5346  27.935  &lt; 2e-16 ***
## GroupB       -3.1667     0.7560  -4.189 9.67e-05 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2.928 on 58 degrees of freedom
## Multiple R-squared:  0.2322, Adjusted R-squared:  0.219 
## F-statistic: 17.55 on 1 and 58 DF,  p-value: 9.669e-05</code></pre>
<p>The model summary reports that Groups B performed significantly (Pr(&gt;|t|) is smaller than .001 as indicated by the three * after the p-values) comapred with Groups A (the Estmate is negative). We will now create the diagnostic graphics.</p>
<pre class="r"><code>par(mfrow = c(1, 3))        # plot window: 1 plot/row, 3 plots/column
plot(resid(m2.lm))     # generate diagnostic plot
plot(rstandard(m2.lm)) # generate diagnostic plot
plot(rstudent(m2.lm)); par(mfrow = c(1, 1))  # restore normal plot window</code></pre>
<p><img src="basicstatz_files/figure-html/slr16-1.png" width="672" /></p>

<p>The graphics do not indicate outliers or other issues, so we can continue with more diagnostic graphics.</p>
<pre class="r"><code>par(mfrow = c(2, 2)) # generate a plot window with 2x2 panels
plot(m2.lm); par(mfrow = c(1, 1)) # restore normal plot window</code></pre>
<p><img src="basicstatz_files/figure-html/slr17-1.png" width="672" /></p>

<p>These graphics also show no problems. In this case, the data can be summarized in the next step.</p>
<pre class="r"><code># tabulate results
slrresults2 &lt;- slrsummary(m2.lm)
slrresults2</code></pre>
<table>
<caption>
Results of the regression model.
</caption>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:left;">
Estimate
</th>
<th style="text-align:left;">
Pearson’s r
</th>
<th style="text-align:left;">
Std. Error
</th>
<th style="text-align:left;">
t value
</th>
<th style="text-align:left;">
Pr(&gt;|t|)
</th>
<th style="text-align:left;">
P-value sig.
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
(Intercept)
</td>
<td style="text-align:left;">
14.93
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
0.53
</td>
<td style="text-align:left;">
27.94
</td>
<td style="text-align:left;">
0
</td>
<td style="text-align:left;">
p &lt; .001***
</td>
</tr>
<tr>
<td style="text-align:left;">
GroupB
</td>
<td style="text-align:left;">
-3.17
</td>
<td style="text-align:left;">
0.48
</td>
<td style="text-align:left;">
0.76
</td>
<td style="text-align:left;">
-4.19
</td>
<td style="text-align:left;">
1e-04
</td>
<td style="text-align:left;">
p &lt; .001***
</td>
</tr>
<tr>
<td style="text-align:left;">
Model statistics
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
Value
</td>
</tr>
<tr>
<td style="text-align:left;">
Number of cases in model
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
60
</td>
</tr>
<tr>
<td style="text-align:left;">
Residual standard error on 58 DF
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
2.93
</td>
</tr>
<tr>
<td style="text-align:left;">
Multiple R-squared
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
0.2322
</td>
</tr>
<tr>
<td style="text-align:left;">
Adjusted R-squared
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
0.219
</td>
</tr>
<tr>
<td style="text-align:left;">
F-statistic (1, 58)
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
17.55
</td>
</tr>
<tr>
<td style="text-align:left;">
Model p-value
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
1e-04
</td>
</tr>
</tbody>
</table>
<hr />
<p>The results of this second simple linear regressions can be summarized as follows:</p>
<p>A simple linear regression was fitted to the data. A visual assessment of the model diagnostics did not indicate any problematic or disproportionately influential data points (outliers). The final linear regression model is based on 60 data points, performed significantly better than an intercept-only base line model (F (1, 58): 17.55, p-value &lt;. 001***), and reported that the model explained 21.9 percent of variance which confirmed a good model fit. According to this final model, group A scored significantly better on the language learning test than group B (coefficient: -3.17, SE: 0.48, t-value: -4.19, p-value &lt;. 001 ***).</p>
</div>
</div>
<div id="citation-session-info" class="section level1 unnumbered">
<h1>Citation &amp; Session Info</h1>
<p>Schweinberger, Martin. 2020. <em>Basic Inferential Statistics with R</em>. Brisbane: The University of Queensland. url: <a href="https://slcladal.github.io/basicstatzchi.html" class="uri">https://slcladal.github.io/basicstatzchi.html</a> (Version 2020.09.27).</p>
<pre><code>@manual{schweinberger2020basicstatz,
  author = {Schweinberger, Martin},
  title = {Basic Inferential Statistics using R},
  note = {https://slcladal.github.io/basicstatzchi.html},
  year = {2020},
  organization = &quot;The University of Queensland, School of Languages and Cultures},
  address = {Brisbane},
  edition = {2020/09/24}
}</code></pre>
<pre class="r"><code>sessionInfo()</code></pre>
<pre><code>## R version 4.0.2 (2020-06-22)
## Platform: x86_64-w64-mingw32/x64 (64-bit)
## Running under: Windows 10 x64 (build 18362)
## 
## Matrix products: default
## 
## locale:
## [1] LC_COLLATE=German_Germany.1252  LC_CTYPE=German_Germany.1252   
## [3] LC_MONETARY=German_Germany.1252 LC_NUMERIC=C                   
## [5] LC_TIME=German_Germany.1252    
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] QuantPsyc_1.5       MASS_7.3-51.6       boot_1.3-25        
##  [4] car_3.0-9           carData_3.0-4       dplyr_1.0.2        
##  [7] cfa_0.10-0          ggplot2_3.3.2       gridExtra_2.3      
## [10] fGarch_3042.83.2    fBasics_3042.89.1   timeSeries_3062.100
## [13] timeDate_3043.102   kableExtra_1.2.1    knitr_1.30         
## [16] DT_0.15            
## 
## loaded via a namespace (and not attached):
##  [1] Rcpp_1.0.5        lattice_0.20-41   digest_0.6.25     R6_2.4.1         
##  [5] cellranger_1.1.0  evaluate_0.14     highr_0.8         httr_1.4.2       
##  [9] pillar_1.4.6      rlang_0.4.7       spatial_7.3-12    curl_4.3         
## [13] readxl_1.3.1      rstudioapi_0.11   data.table_1.13.0 Matrix_1.2-18    
## [17] rmarkdown_2.3     splines_4.0.2     labeling_0.3      webshot_0.5.2    
## [21] stringr_1.4.0     foreign_0.8-80    htmlwidgets_1.5.1 munsell_0.5.0    
## [25] compiler_4.0.2    xfun_0.16         pkgconfig_2.0.3   mgcv_1.8-31      
## [29] htmltools_0.5.0   tidyselect_1.1.0  tibble_3.0.3      rio_0.5.16       
## [33] viridisLite_0.3.0 crayon_1.3.4      withr_2.3.0       grid_4.0.2       
## [37] nlme_3.1-148      jsonlite_1.7.1    gtable_0.3.0      lifecycle_0.2.0  
## [41] magrittr_1.5      scales_1.1.1      zip_2.1.1         stringi_1.5.3    
## [45] farver_2.0.3      xml2_1.3.2        ellipsis_0.3.1    generics_0.0.2   
## [49] vctrs_0.3.4       openxlsx_4.2.2    tools_4.0.2       forcats_0.5.0    
## [53] glue_1.4.2        purrr_0.3.4       hms_0.5.3         crosstalk_1.1.0.1
## [57] abind_1.4-5       yaml_2.2.1        colorspace_1.4-1  rvest_0.3.6      
## [61] haven_2.3.1</code></pre>
</div>
<div id="references" class="section level1 unnumbered">
<h1>References</h1>
<hr />
<p><a href="https://slcladal.github.io/index.html">Main page</a></p>
<hr />
<div id="refs" class="references">
<div id="ref-baayen2008analyzing">
<p>Baayen, R Harald. 2008. <em>Analyzing Linguistic Data. A Practical Introduction to Statistics Using R</em>. Cambridge: Cambridge University press.</p>
</div>
<div id="ref-buehner2009statistik">
<p>Bühner, Markus, and Matthias Ziegler. 2009. <em>Statistik Für Psychologen Und Sozialwissenschaftler</em>. München: Pearson Studium.</p>
</div>
<div id="ref-cochran1954somemethods">
<p>Cochran, W. G. 1954. “Some Methods for Strengthening the Common $$^2tests.” <em>Biometrics</em> 10: 417–51.</p>
</div>
<div id="ref-field2012discovering">
<p>Field, Andy, Jeremy Miles, and Zoe Field. 2012. <em>Discovering Statistics Using R</em>. Sage.</p>
</div>
<div id="ref-gries2009statistics">
<p>Gries, Stefan Th. 2009. <em>Statistics for Linguistics Using R: A Practical Introduction</em>. Berlin &amp; New York: Mouton de Gruyter.</p>
</div>
<div id="ref-gries2014frequency">
<p>Gries, Stefan Thomas. 2014. “Frequency Tables: Tests, Effect Sizes, and Explorations.” In <em>Polysemy and Synonymy: Corpus Methods and Applications in Cognitive Linguistics.</em>, edited by Dylan Glynn and Justyna Robinson, 365–89. Amsterdam: John Benjamins.</p>
</div>
<div id="ref-levshina2015linguistics">
<p>Levshina, Natalia. 2015. <em>How to Do Linguistics with R: Data Exploration and Statistical Analysis</em>. Amsterdam: John Benjamins.</p>
</div>
<div id="ref-wilcox2009basic">
<p>Wilcox, Rand R. 2009. <em>Basic Statistics: Understanding Conventional Methods and Modern Insights</em>. Oxford University Press.</p>
</div>
</div>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
