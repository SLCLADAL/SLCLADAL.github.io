{
  "hash": "321d8ba57322c9f8c0bd124a39f10bc9",
  "result": {
    "markdown": "---\ntitle: \"Reinforcement Learning and Text Summarization in R\"\nauthor: \"Dattatreya Majumdar\"\ndate: \"2022-08-31\"\n---\n\n::: {.cell}\n::: {.cell-output-display}\n![](https://slcladal.github.io/images/uq1.jpg){width=100%}\n:::\n:::\n\n\n# Introduction{-}\n\nThis tutorial introduces the concept of Reinforcement Learning (RL) [see @sutton2018reinforcement; @wu2018study; @paulus2017deep], and how it can be applied in the domain of Natural Language Processing (NLP) and linguistics.\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](https://slcladal.github.io/images/gy_chili.jpg){width=15% style=\"float:right; padding:10px\"}\n:::\n:::\n\n\nThis tutorial is aimed at beginners and intermediate users of R with the aim of showcasing how RI works. The aim is not to provide a fully-fledged analysis but rather to show and exemplify selected useful methods associated with RI. \n\n\n<div class=\"warning\" style='padding:0.1em; background-color:#f2f2f2; color:#51247a'>\n<span>\n<p style='margin-top:1em; text-align:center'>\nThe entire R Notebook for the tutorial can be downloaded [**here**](https://slcladal.github.io/content/reinfnlp.Rmd).  If you want to render the R Notebook on your machine, i.e. knitting the document to html or a pdf, you need to make sure that you have R and RStudio installed and you also need to download the [**bibliography file**](https://slcladal.github.io/content/bibliography.bib) and store it in the same folder where you store the Rmd file. <br></p>\n<p style='margin-left:1em;'>\n</p></span>\n</div>\n\n<br>\n\n\n## Preparation and session set up{-}\n\nThis tutorial is based on R. If you have not installed R or are new to it, you will find an introduction to and more information how to use R [here](https://slcladal.github.io/IntroR_workshop.html). For this tutorials, we need to install certain *packages* from an R *library* so that the scripts in this tutorial are executed without errors. Before continuing, please install the packages by running the code below this paragraph. If you have already installed the packages mentioned below, then you can skip ahead ignore this section. To install the necessary packages, simply run the following code - it may take some time (between 1 and 5 minutes to install all of the libraries so you do not need to worry if it takes some time).\n\nFor this tutorial we will be primarily requiring four packages: *tidytext* for text manipulations, *tidyverse* for general tasks, *textrank* for the implementation of the TextRank algorithm and *rvest* to scrape through an article to use as an example. For this analysis an article for Time has been selected.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# set options\noptions(stringsAsFactors = F)\n# install libraries\ninstall.packages(c(\"tidytext\",\"tidyverse\",\"textrank\",\"rvest\",\"ggplot2\"))\n# install klippy for copy-to-clipboard button in code chunks\ninstall.packages(\"remotes\")\nremotes::install_github(\"rlesur/klippy\")\n```\n:::\n\n\nNow that we have installed the packages, we activate them as shown below.\n\n\n::: {.cell}\n\n```{.r .klippy .cell-code}\n# set options\noptions(stringsAsFactors = F)          # no automatic data transformation\noptions(\"scipen\" = 100, \"digits\" = 12) # suppress math annotation\n# activate packages\nlibrary(tidytext)\nlibrary(tidyverse)\nlibrary(textrank)\nlibrary(rvest)\nlibrary(ggplot2)\n# activate klippy for copy-to-clipboard button\nklippy::klippy()\n```\n\n::: {.cell-output-display}\n```{=html}\n<script>\n  addClassKlippyTo(\"pre.r, pre.markdown\");\n  addKlippy('left', 'top', 'auto', '1', 'Copy code', 'Copied!');\n</script>\n```\n:::\n:::\n\n\nOnce you have installed R and RStudio and also initiated the session by executing the code shown above, you are good to go.\n\n\n# Reinforment Learning{-}\n\nReinforcement Learning enables a machines and software agents to independently determine the optimal behavior depending on a specific concept to enhance the overall performance. The system requires a reward feedback to learn its behavior which is known as reinforcement signal. The schematic diagram of Reinforcement Learning is provided below: -\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](https://slcladal.github.io/images/Reinforcement.PNG){width=60% style=\"float:center; padding:10px\"}\n:::\n:::\n\n\nAny RL framework comprises of 3 major components:\n\n- *Action* determines all possible moves that the agent can make which is normally expressed as a mathematical function.\n- *State* it is an explicit and quick circumstance that the agent can find itself in posed by the environment or any future circumstance\n- *Reward* it is the feedback input from the environment which measure the achievement or failure of the agent's activities.\n\nThe are three broad categories of RL:\n\n- *Value Based* which determines the optimal value function and it is the maximum value achievable under any policy.\n- *Policy Based* which identifies the optimal policy achieving maximum future reward\n- *Model Based* involves a model which predicts attributes or provides representation of the environment\n\nWithout going into the mathematical intricacies of RL we will focus on possible applications of deep RL to linguistic data this tutorial. In its current form, RL plays a pivotal role in various Natural Language Processing (NLP) applications some of which are:\n\n- Article Summarisation\n- Question Answering (QA)\n- Dialogue Generation\n- Dialogue System\n- Knowledge-based QA\n- Machine Translation\n- Text Generation\n\nIn the following sections we will explore some use cases of RL and interpret how deep RL can implement them.\n\n## Text Summarisation{-}\n\nA deep reinforced model for text summarisation involves sequence of input tokens *x={x~1~,x~2~,...,x~n~}* and produces a sequence of output (summary) tokens. A schematic presentation of the process is shown below:\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](https://slcladal.github.io/images/deeprlartsumm.PNG){width=100% style=\"float:center; padding:10px\"}\n:::\n:::\n\n\nFor the article summarisation objective the deep RL has the following components:\n\n- *Action* which involves a function *u~t~* which copies and generates summary output *y~t~*\n- *State* it encapsulates the hidden states of encoder and previous outputs\n- *Reward* which generates a rough score determining the performance of the summarisation\n\nText summarisation [see @mihalcea2004textrank] is highly critical in extracting important information from large texts. \n\nIn case of text summarisation there are broadly two categories:\n\n- Extractive Summarisation\n- Abstractive Summarisation\n\nIn case of *Extractive Summarisation* words and sentences are scored according to a specific metric and then utilizing that information for summarizing based copying or pasting the most informative parts of the text.\n\nOn the other hand *Abstractive Summarisation* involves building a semantic representation of the text and then incorporating natural language generation techniques to generate text highlighting the informative parts of the parent text document.\n\nHere, we will be focusing on an extractive summarisation method called *TextRank* which is hinged upon the *PageRank* algorithm which was developed by Google to rank websites based on their importance.\n\n**The TextRank Algorithm**\n\nTextRank is a graph-based ranking algorithm for NLP. Graph-based ranking algorithms evaluate the importance of a vertex within a graph, based on global information extracted recursively from the entire graph. When one vertex is associated with another it is actually casting a vote for that vertex. The higher the number of votes cast for a vertex, the higher importance of that vertex.\n\nIn the NLP case it is necessary to define vertices and edges. In this tutorial we will be using sentences as vertices and words as edges. Thus sentences with words present in many other sentences will have higher priority\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(tidytext)\nlibrary(textrank)\nlibrary(rvest)\n# define url\nurl <- \"http://time.com/5196761/fitbit-ace-kids-fitness-tracker/\"\n# read in data\narticle <- read_html(url) %>%\n  html_nodes('div[class=\"padded\"]') %>%\n  html_text()\n```\n:::\n\n\nNext the article is loaded into a tibble. Then tokenisation is implemented according to sentences. Although this tokenisation is fully perfect it has a lower number of dependencies and is suitable for this case. Finally we add column for sentence number and switch the order of the columns. \n\n\n::: {.cell}\n\n```{.r .cell-code}\narticle_sentences <- tibble(text = article) %>%\n  tidytext::unnest_tokens(sentence, text, token = \"sentences\") %>%\n  dplyr::mutate(sentence_id = row_number()) %>%\n  dplyr::select(sentence_id, sentence)\narticle_sentences\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 21 × 2\n   sentence_id sentence                                                         \n         <int> <chr>                                                            \n 1           1 \"fitbit is launching a new fitness tracker designed for children…\n 2           2 \"the [tempo-ecommerce src=”http://www.amazon.com/fitbit-activity…\n 3           3 \"the most important of which is fitbit’s new family account opti…\n 4           4 \"parents must approve who their child can connect with via the f…\n 5           5 \"but while fitbit’s default move goal is 30 minutes for adult us…\n 6           6 \"fitbit says the tracker is designed for children eight years ol…\n 7           7 \"fitbit will also be introducing a family faceoff feature that l…\n 8           8 \"the app also will reward children with in-app badges for achiev…\n 9           9 \"fitbit’s new child-friendly fitness band will be available in b…\n10          10 \"the ace launch is part of fitbit’s broader goal of branching ou…\n# … with 11 more rows\n```\n:::\n:::\n\n\nNext we will tokenize based on words.\n\n\n::: {.cell}\n\n```{.r .cell-code}\narticle_words <- article_sentences %>%\n  tidytext::unnest_tokens(word, sentence)\narticle_words\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 587 × 2\n   sentence_id word     \n         <int> <chr>    \n 1           1 fitbit   \n 2           1 is       \n 3           1 launching\n 4           1 a        \n 5           1 new      \n 6           1 fitness  \n 7           1 tracker  \n 8           1 designed \n 9           1 for      \n10           1 children \n# … with 577 more rows\n```\n:::\n:::\n\n\nWe have one last step left is to remove the stop words in *article_words* as they are prone to result in redundancy.\n\n\n::: {.cell}\n\n```{.r .cell-code}\narticle_words <- article_words %>%\n  dplyr::anti_join(stop_words, by = \"word\")\narticle_words\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 297 × 2\n   sentence_id word     \n         <int> <chr>    \n 1           1 fitbit   \n 2           1 launching\n 3           1 fitness  \n 4           1 tracker  \n 5           1 designed \n 6           1 children \n 7           1 called   \n 8           1 fitbit   \n 9           1 ace      \n10           1 sale     \n# … with 287 more rows\n```\n:::\n:::\n\n\nUsing the textrank package it is really easy to implement the TextRank algorithm. The *textrank_sentences* function requires only 2 inputs:\n\n- A data frame with sentences\n- A data frame with tokens which are part of each sentence\n\n\n::: {.cell}\n\n```{.r .cell-code}\narticle_summary <- textrank_sentences(data = article_sentences, \n                                      terminology = article_words)\n# inspect the summary\narticle_summary\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTextrank on sentences, showing top 5 most important sentences found:\n  1. fitbit is launching a new fitness tracker designed for children called the fitbit ace, which will go on sale for $99.95 in the second quarter of this year.\n  2. fitbit says the tracker is designed for children eight years old and up.\n  3. above all else, the ace is an effort to get children up and moving.\n  4. but while fitbit’s default move goal is 30 minutes for adult users, the ace’s will be 60 minutes, in line with the world health organization’s recommendation that children between the ages of five and 17 get an hour of daily physical activity per day.\n  5. the most important of which is fitbit’s new family account option, which gives parents control over how their child uses their tracker and is compliant with the children’s online privacy protection act, or coppa.\n```\n:::\n:::\n\n\nLets have a look where these important sentences appear in the article:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\narticle_summary[[\"sentences\"]] %>%\n  ggplot(aes(textrank_id, textrank, fill = textrank_id)) +\n  geom_col() +\n  theme_minimal() +\n  scale_fill_viridis_c() +\n  guides(fill = \"none\") +\n  labs(x = \"Sentence\",\n       y = \"TextRank score\",\n       title = \"Most informative sentences appear within first half of sentences\",\n       subtitle = 'In article \"Fitbits Newest Fitness Tracker Is Just for Kids\"',\n       caption = \"Source: http://time.com/5196761/fitbit-ace-kids-fitness-tracker/\")\n```\n\n::: {.cell-output-display}\n![Position of Important Sentences in the Article](reinfnlp_files/figure-html/impsent-1.png){width=576}\n:::\n:::\n\n\n## Other Applications of RL{-}\n\n### Dialogue Generation{-}\n\nIn today's digital world dialogue generation is a widely used application especially in chatbots. One widely used model in this regard is the Long Short Term Memory (LSTM) sequence-to-sequence (SEQ2SEQ) model. It is a neural generative model that maximizes the probability of generating a response given the previous dialogue. However SEQ2SEQ model has some constraints:\n\n- They tend to generate highly generic responses\n- Often they are stuck in an infinite loop of repetitive responses\n\nThis is where deep RL is much more efficient as it can integrate developer-defined rewards which efficiently mimics the true goal of chatbot development. In case of dialogue generation the component:\n\n- *Action* which involves a function that generates sequences of arbitrary lengths\n- *State* it comprises of previous 2 dialogue turns [p~i~,q~i~]\n- *Reward* which determines the ease of answering, information flow and semantic coherence\n\nThe schematic diagram highlighting the dialogue simulation between 2 agents using deep RL is shown below:\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](https://slcladal.github.io/images/dlsimrl.PNG){width=100% style=\"float:center; padding:10px\"}\n:::\n:::\n\n\n### Neural Machine Translation{-}\n\nMost of Neural Machine Translation (NMT) models are based encoder-decoder framework with attention mechanism. The encoder initially maps a source sentence *x={x~1~,x~2~,...,x~n~}* to a set of continuous representations *z={z~1~,z~2~,...,z~n~}* . Given *z* the decoder then generates a target sentence *y={y~1~,y~2~,...,y~m~}* of word tokens one by one. RL is used to bridge the gap between training and inference of of NMT by directly optimizing the loss function at training time. In this scenario the NMT model acts as the *agent* which interacts with the *environment* which in this case are the previous words and the context vector *z* available at each step *t*. This is a a policy based RL and in place of a state a policy will be assigned in every iteration. The critical components of the RL for NMT are discussed below:\n\n- *Policy* which is a conditional probability defined by the parameters of the agent\n- *Action* is decided by the agent based on the policy and it will pick up a candidate word from the vocabulary\n- *Reward* is evaluated once the agent generates a complete sequence which in case of machine translation is *Bilingual Evaluation Understudy (BLEU)*.BLEU is defined by comparing the generated sequence with the ground truth sequence.\n\nThe schematic of the overall process is depicted below:\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](https://slcladal.github.io/images/NMT.PNG){width=60% style=\"float:center; padding:10px\"}\n:::\n:::\n\n\n\n# Citation & Session Info {-}\n\nMajumdar, Dattatreya. 2022. *Reinforcement Learning in NLP*. Brisbane: The University of Queensland. url: https://slcladal.github.io/reinfnlp.html (Version 2022.08.31).\n\n```\n@manual{Majumdar2022ta,\n  author = {Majumdar, Dattatreya},\n  title = {Reinforcement Learning in NLP},\n  note = {https://slcladal.github.io/reinfnlp.html},\n  year = {2022},\n  organization = \"The University of Queensland, Australia. School of Languages and Cultures},\n  address = {Brisbane},\n  edition = {2022.08.31}\n}\n```\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsessionInfo()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nR version 4.2.1 (2022-06-23)\nPlatform: x86_64-pc-linux-gnu (64-bit)\nRunning under: Ubuntu 22.04.1 LTS\n\nMatrix products: default\nBLAS:   /usr/lib/x86_64-linux-gnu/blas/libblas.so.3.10.0\nLAPACK: /usr/lib/x86_64-linux-gnu/lapack/liblapack.so.3.10.0\n\nlocale:\n [1] LC_CTYPE=en_AU.UTF-8       LC_NUMERIC=C              \n [3] LC_TIME=en_AU.UTF-8        LC_COLLATE=en_AU.UTF-8    \n [5] LC_MONETARY=en_AU.UTF-8    LC_MESSAGES=en_AU.UTF-8   \n [7] LC_PAPER=en_AU.UTF-8       LC_NAME=C                 \n [9] LC_ADDRESS=C               LC_TELEPHONE=C            \n[11] LC_MEASUREMENT=en_AU.UTF-8 LC_IDENTIFICATION=C       \n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] rvest_1.0.2     textrank_0.3.1  forcats_0.5.1   stringr_1.4.0  \n [5] dplyr_1.0.9     purrr_0.3.4     readr_2.1.2     tidyr_1.2.0    \n [9] tibble_3.1.7    ggplot2_3.3.6   tidyverse_1.3.2 tidytext_0.3.3 \n\nloaded via a namespace (and not attached):\n [1] Rcpp_1.0.8.3        lubridate_1.8.0     lattice_0.20-45    \n [4] assertthat_0.2.1    digest_0.6.29       utf8_1.2.2         \n [7] R6_2.5.1            cellranger_1.1.0    backports_1.4.1    \n[10] reprex_2.0.1        evaluate_0.15       httr_1.4.3         \n[13] pillar_1.7.0        rlang_1.0.4         curl_4.3.2         \n[16] googlesheets4_1.0.0 readxl_1.4.0        data.table_1.14.2  \n[19] rstudioapi_0.13     Matrix_1.4-1        klippy_0.0.0.9500  \n[22] rmarkdown_2.14      labeling_0.4.2      googledrive_2.0.0  \n[25] selectr_0.4-2       htmlwidgets_1.5.4   igraph_1.3.2       \n[28] munsell_0.5.0       broom_1.0.0         compiler_4.2.1     \n[31] janeaustenr_0.1.5   modelr_0.1.8        xfun_0.31          \n[34] pkgconfig_2.0.3     htmltools_0.5.2     tidyselect_1.1.2   \n[37] viridisLite_0.4.0   fansi_1.0.3         crayon_1.5.1       \n[40] tzdb_0.3.0          dbplyr_2.2.1        withr_2.5.0        \n[43] SnowballC_0.7.0     grid_4.2.1          jsonlite_1.8.0     \n[46] gtable_0.3.0        lifecycle_1.0.1     DBI_1.1.3          \n[49] magrittr_2.0.3      scales_1.2.0        tokenizers_0.2.1   \n[52] cli_3.3.0           stringi_1.7.8       farver_2.1.1       \n[55] fs_1.5.2            xml2_1.3.3          ellipsis_0.3.2     \n[58] generics_0.1.3      vctrs_0.4.1         tools_4.2.1        \n[61] glue_1.6.2          hms_1.1.1           fastmap_1.1.0      \n[64] yaml_2.3.5          colorspace_2.0-3    gargle_1.2.0       \n[67] knitr_1.39          haven_2.5.0        \n```\n:::\n:::\n\n\n\n***\n\n[Back to top](#introduction)\n\n[Back to HOME](https://slcladal.github.io/index.html)\n\n***\n\n# References {-}\n\n\n",
    "supporting": [
      "reinfnlp_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"site_libs/clipboard-1.7.1/clipboard.min.js\"></script>\n<link href=\"site_libs/primer-tooltips-1.4.0/build.css\" rel=\"stylesheet\" />\n<link href=\"site_libs/klippy-0.0.0.9500/css/klippy.min.css\" rel=\"stylesheet\" />\n<script src=\"site_libs/klippy-0.0.0.9500/js/klippy.min.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}