{
  "hash": "fe714dede3a58c5a91954432ba499609",
  "result": {
    "markdown": "---\ntitle: \"Topic Modeling with R\"\nauthor: \"Martin Schweinberger\"\ndate: \"2022-08-31\"\n---\n\n::: {.cell}\n::: {.cell-output-display}\n![](https://slcladal.github.io/images/uq1.jpg){width=100%}\n:::\n:::\n\n\n# Introduction{-}\n\nThis tutorial introduces topic modeling using R. \n\n\n::: {.cell}\n::: {.cell-output-display}\n![](https://slcladal.github.io/images/gy_chili.jpg){width=15% style=\"float:right; padding:10px\"}\n:::\n:::\n\n\nThis tutorial is aimed at beginners and intermediate users of R with the aim of showcasing how to perform basic topic modeling on textual data using R and how to visualize the results of such a model. The aim is not to provide a fully-fledged analysis but rather to show and exemplify selected useful methods associated with topic modeling. \n\n\n\n<div class=\"warning\" style='padding:0.1em; background-color:#f2f2f2; color:#51247a'>\n<span>\n<p style='margin-top:1em; text-align:center'>\nThe entire R Notebook for the tutorial can be downloaded [**here**](https://slcladal.github.io/content/topicmodels.Rmd).  If you want to render the R Notebook on your machine, i.e. knitting the document to html or a pdf, you need to make sure that you have R and RStudio installed and you also need to download the [**bibliography file**](https://slcladal.github.io/content/bibliography.bib) and store it in the same folder where you store the Rmd file. <br><br>\n**[Here](https://colab.research.google.com/drive/1yjAdudj3bQig0i3isBSaTi3mohRv0cUk?usp=sharing)** is a **link to an interactive version of this tutorial on Google Colab**. The interactive tutorial is based on a Jupyter notebook of this tutorial. This interactive Jupyter notebook allows you to execute code yourself and - if you copy the Jupyter notebook - you can also change and edit the notebook, e.g. you can change code and upload your own data.<br></p>\n<p style='margin-left:1em;'>\n</p></span>\n</div>\n\n<br>\n\n\nThis tutorial builds heavily on and uses materials from  [this tutorial](https://tm4ss.github.io/docs/Tutorial_6_Topic_Models.html) on web crawling and scraping using R by Andreas Niekler and Gregor Wiedemann [see @WN17]. [The tutorial](https://tm4ss.github.io/docs/index.html) by Andreas Niekler and Gregor Wiedemann is more thorough, goes into more detail than this tutorial, and covers many more very useful text mining methods. an alternative and equally recommendable introduction to topic modeling with R is, of course, @silge2017text.\n\n<div class=\"warning\" style='padding:0.1em; background-color:#f2f2f2; color:#51247a'>\n<span>\n<p style='margin-top:1em; text-align:center'>\n**Topic models aim to find topics (which are operationalized as bundles of correlating terms) in documents to see what the texts are about.**</p>\n<p style='margin-left:1em;'>\n</p></span>\n</div>\n\n<br>\n\n\nTopic models are a common procedure in In machine learning and natural language processing. Topic models represent a type of statistical model that is use to discover more or less abstract *topics* in a given selection of documents. Topic models are particularly common in text mining  to unearth hidden semantic structures in textual data. Topics can be conceived of as networks of collocation terms that, because of the co-occurrence across documents, can be assumed to refer to the same semantic domain (or topic). This assumes that, if a document is about a certain topic, one would expect words, that are related to that topic, to appear in the document more often than in documents that deal with other topics. For instance, *dog* and *bone* will appear more often in documents about dogs whereas *cat* and *meow* will appear in documents about cats. Terms like *the* and *is* will, however, appear approximately equally in both. \n\nTopic models are also referred to as probabilistic topic models, which refers to statistical algorithms for discovering the latent semantic structures of an extensive text body. Given the availability of vast amounts of textual data, topic models can help to organize and offer insights and assist in understanding large collections of unstructured text. \n\n## Preparation and session set up{-}\n\nThis tutorial is based on R. If you have not installed R or are new to it, you will find an introduction to and more information how to use R [here](https://slcladal.github.io/intror.html). For this tutorials, we need to install certain *packages* from an R *library* so that the scripts shown below are executed without errors. Before turning to the code below, please install the packages by running the code below this paragraph. If you have already installed the packages mentioned below, then you can skip ahead and ignore this section. To install the necessary packages, simply run the following code - it may take some time (between 1 and 5 minutes to install all of the packages so you do not need to worry if it takes some time).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# install packages\ninstall.packages(\"tm\")\ninstall.packages(\"topicmodels\")\ninstall.packages(\"reshape2\")\ninstall.packages(\"ggplot2\")\ninstall.packages(\"wordcloud\")\ninstall.packages(\"pals\")\ninstall.packages(\"SnowballC\")\ninstall.packages(\"lda\")\ninstall.packages(\"ldatuning\")\ninstall.packages(\"kableExtra\")\ninstall.packages(\"DT\")\ninstall.packages(\"flextable\")\n# install klippy for copy-to-clipboard button in code chunks\ninstall.packages(\"remotes\")\nremotes::install_github(\"rlesur/klippy\")\n```\n:::\n\n\nNext, we activate the packages. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# set options\noptions(stringsAsFactors = F)         # no automatic data transformation\noptions(\"scipen\" = 100, \"digits\" = 4) # suppress math annotation\n# load packages\nlibrary(knitr) \nlibrary(kableExtra) \nlibrary(DT)\nlibrary(tm)\nlibrary(topicmodels)\nlibrary(reshape2)\nlibrary(ggplot2)\nlibrary(wordcloud)\nlibrary(pals)\nlibrary(SnowballC)\nlibrary(lda)\nlibrary(ldatuning)\nlibrary(flextable)\n# activate klippy for copy-to-clipboard button\nklippy::klippy()\n```\n\n::: {.cell-output-display}\n```{=html}\n<script>\n  addClassKlippyTo(\"pre.r, pre.markdown\");\n  addKlippy('left', 'top', 'auto', '1', 'Copy code', 'Copied!');\n</script>\n```\n:::\n:::\n\n\nOnce you have installed R and RStudio and once you have initiated the session by executing the code shown above, you are good to go.\n\n\n# Topic Modelling{-}\n\nThe process starts as usual with the reading of the corpus data. For this tutorial we will analyze *State of the Union Addresses* (SOTU) by US presidents and investigate how the topics that were addressed in the SOTU speeches changeover time. The 231 SOTU addresses are rather long documents. Documents lengths clearly affects the results of topic modeling. For very short texts (e.g. Twitter posts) or very long texts (e.g. books), it can make sense to concatenate/split single documents to receive longer/shorter textual units for modeling.\n\nFor the SOTU speeches for instance, we infer the model based on paragraphs instead of entire speeches. By manual inspection / qualitative inspection of the results you can check if this procedure yields better (interpretable) topics. In `sotu_paragraphs.csv`, we provide a paragraph separated version of the speeches.\n\nFor text preprocessing, we remove stopwords, since they tend to occur as \"noise\" in the estimated topics of the LDA model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# load data\ntextdata <- base::readRDS(url(\"https://slcladal.github.io/data/sotu_paragraphs.rda\", \"rb\"))\n# load stopwords\nenglish_stopwords <- readLines(\"https://slcladal.github.io/resources/stopwords_en.txt\", encoding = \"UTF-8\")\n# create corpus object\ncorpus <- Corpus(DataframeSource(textdata))\n# Preprocessing chain\nprocessedCorpus <- tm_map(corpus, content_transformer(tolower))\nprocessedCorpus <- tm_map(processedCorpus, removeWords, english_stopwords)\nprocessedCorpus <- tm_map(processedCorpus, removePunctuation, preserve_intra_word_dashes = TRUE)\nprocessedCorpus <- tm_map(processedCorpus, removeNumbers)\nprocessedCorpus <- tm_map(processedCorpus, stemDocument, language = \"en\")\nprocessedCorpus <- tm_map(processedCorpus, stripWhitespace)\n```\n:::\n\n\n## Model calculation{-}\n\nAfter the preprocessing, we have two corpus objects: `processedCorpus`, on which we calculate an LDA topic model [@blei2003lda]. To this end, *stopwords*, i.e. function words that have relational rather than content meaning,  were removed, words were stemmed and converted to lowercase letters and special characters were removed. The second corpus object `corpus` serves to be able to view the original texts and thus to facilitate a qualitative control of the topic model results.\n\nWe now calculate a topic model on the `processedCorpus`. For this purpose, a DTM of the corpus is created. In this case, we only want to consider terms that occur with a certain minimum frequency in the body. This is primarily used to speed up the model calculation.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# compute document term matrix with terms >= minimumFrequency\nminimumFrequency <- 5\nDTM <- DocumentTermMatrix(processedCorpus, control = list(bounds = list(global = c(minimumFrequency, Inf))))\n# have a look at the number of documents and terms in the matrix\ndim(DTM)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 8833 4278\n```\n:::\n\n```{.r .cell-code}\n# due to vocabulary pruning, we have empty rows in our DTM\n# LDA does not like this. So we remove those docs from the\n# DTM and the metadata\nsel_idx <- slam::row_sums(DTM) > 0\nDTM <- DTM[sel_idx, ]\ntextdata <- textdata[sel_idx, ]\n```\n:::\n\n\nAs an unsupervised machine learning method, topic models are suitable for the exploration of data. The calculation of topic models aims to determine the proportionate composition of a fixed number of topics in the documents of a collection. It is useful to experiment with different parameters in order to find the most suitable parameters for your own analysis needs.\n\nFor parameterized models such as Latent Dirichlet Allocation (LDA), the number of topics `K` is the most important parameter to define in advance. How an optimal `K` should be selected depends on various factors. If `K` is too small, the collection is divided into a few very general semantic contexts. If `K` is too large, the collection is divided into too many topics of which some may overlap and others are hardly interpretable.\n\nAn alternative to deciding on a set number of topics is to extract parameters form a models using a rage of number of topics. This approach can be useful when the number of topics is not theoretically motivated or based on closer, qualitative inspection of the data. In the example below, the determination of the optimal number of topics follows @murzintcev2020idealtopics, but we only use two metrics (*CaoJuan2009* and *Deveaud2014*) - it is highly recommendable to inspect the results of the four metrics available for the `FindTopicsNumber` function (*Griffiths2004*, *CaoJuan2009*, *Arun2010*, and *Deveaud2014*). \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# create models with different number of topics\nresult <- ldatuning::FindTopicsNumber(\n  DTM,\n  topics = seq(from = 2, to = 20, by = 1),\n  metrics = c(\"CaoJuan2009\",  \"Deveaud2014\"),\n  method = \"Gibbs\",\n  control = list(seed = 77),\n  verbose = TRUE\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nfit models... done.\ncalculate metrics:\n  CaoJuan2009... done.\n  Deveaud2014... done.\n```\n:::\n:::\n\n\n\nWe can now plot the results. In this case, we have only use two methods *CaoJuan2009* and *Griffith2004*. The best number of topics shows low values for *CaoJuan2009* and high values for *Griffith2004* (optimally, several methods should converge and show peaks and dips respectively for a certain number of topics).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nFindTopicsNumber_plot(result)\n```\n\n::: {.cell-output-display}\n![](topicmodels_files/figure-html/tm3c-1.png){width=672}\n:::\n:::\n\n\n\nFor our first analysis, however, we choose a thematic \"resolution\" of `K = 20` topics. In contrast to a resolution of 100 or more, this number of topics can be evaluated qualitatively very easy.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# number of topics\nK <- 20\n# set random number generator seed\nset.seed(9161)\n# compute the LDA model, inference via 1000 iterations of Gibbs sampling\ntopicModel <- LDA(DTM, K, method=\"Gibbs\", control=list(iter = 500, verbose = 25))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nK = 20; V = 4278; M = 8810\nSampling 500 iterations!\nIteration 25 ...\nIteration 50 ...\nIteration 75 ...\nIteration 100 ...\nIteration 125 ...\nIteration 150 ...\nIteration 175 ...\nIteration 200 ...\nIteration 225 ...\nIteration 250 ...\nIteration 275 ...\nIteration 300 ...\nIteration 325 ...\nIteration 350 ...\nIteration 375 ...\nIteration 400 ...\nIteration 425 ...\nIteration 450 ...\nIteration 475 ...\nIteration 500 ...\nGibbs sampling completed!\n```\n:::\n:::\n\n\nDepending on the size of the vocabulary, the collection size and the number K, the inference of topic models can take a very long time. This calculation may take several minutes. If it takes too long, reduce the vocabulary in the DTM by increasing the minimum frequency in the previous step.\n\nThe topic model inference results in two (approximate) posterior probability distributions: a distribution `theta` over K topics within each document and a distribution `beta` over V terms within each topic, where V represents the length of the vocabulary of the collection (V = 4278). Let's take a closer look at these results:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# have a look a some of the results (posterior distributions)\ntmResult <- posterior(topicModel)\n# format of the resulting object\nattributes(tmResult)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$names\n[1] \"terms\"  \"topics\"\n```\n:::\n\n```{.r .cell-code}\nnTerms(DTM)              # lengthOfVocab\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 4278\n```\n:::\n\n```{.r .cell-code}\n# topics are probability distributions over the entire vocabulary\nbeta <- tmResult$terms   # get beta from results\ndim(beta)                # K distributions over nTerms(DTM) terms\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1]   20 4278\n```\n:::\n\n```{.r .cell-code}\nrowSums(beta)            # rows in beta sum to 1\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 \n 1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 \n```\n:::\n\n```{.r .cell-code}\nnDocs(DTM)               # size of collection\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 8810\n```\n:::\n\n```{.r .cell-code}\n# for every document we have a probability distribution of its contained topics\ntheta <- tmResult$topics \ndim(theta)               # nDocs(DTM) distributions over K topics\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 8810   20\n```\n:::\n\n```{.r .cell-code}\nrowSums(theta)[1:10]     # rows in theta sum to 1\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n 1  2  3  4  5  6  7  8  9 10 \n 1  1  1  1  1  1  1  1  1  1 \n```\n:::\n:::\n\n\nLet's take a look at the 10 most likely terms within the term probabilities `beta` of the inferred topics (only the first 8 are shown below).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nterms(topicModel, 10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      Topic 1     Topic 2     Topic 3    Topic 4    Topic 5     Topic 6     \n [1,] \"land\"      \"recommend\" \"measur\"   \"citizen\"  \"great\"     \"claim\"     \n [2,] \"indian\"    \"report\"    \"interest\" \"law\"      \"line\"      \"govern\"    \n [3,] \"territori\" \"congress\"  \"view\"     \"case\"     \"part\"      \"question\"  \n [4,] \"larg\"      \"attent\"    \"subject\"  \"person\"   \"coast\"     \"commiss\"   \n [5,] \"tribe\"     \"secretari\" \"time\"     \"court\"    \"pacif\"     \"spain\"     \n [6,] \"limit\"     \"depart\"    \"present\"  \"properti\" \"construct\" \"island\"    \n [7,] \"popul\"     \"subject\"   \"object\"   \"protect\"  \"import\"    \"made\"      \n [8,] \"portion\"   \"consider\"  \"reason\"   \"natur\"    \"river\"     \"adjust\"    \n [9,] \"general\"   \"present\"   \"adopt\"    \"justic\"   \"complet\"   \"commission\"\n[10,] \"public\"    \"import\"    \"regard\"   \"demand\"   \"south\"     \"final\"     \n      Topic 7     Topic 8     Topic 9      Topic 10     Topic 11   Topic 12   \n [1,] \"public\"    \"state\"     \"govern\"     \"year\"       \"nation\"   \"constitut\"\n [2,] \"offic\"     \"unit\"      \"relat\"      \"amount\"     \"power\"    \"power\"    \n [3,] \"duti\"      \"govern\"    \"receiv\"     \"expenditur\" \"peac\"     \"state\"    \n [4,] \"execut\"    \"mexico\"    \"minist\"     \"increas\"    \"govern\"   \"peopl\"    \n [5,] \"general\"   \"part\"      \"friend\"     \"treasuri\"   \"war\"      \"union\"    \n [6,] \"administr\" \"territori\" \"republ\"     \"end\"        \"foreign\"  \"repres\"   \n [7,] \"give\"      \"texa\"      \"continu\"    \"estim\"      \"independ\" \"govern\"   \n [8,] \"respect\"   \"mexican\"   \"intercours\" \"fiscal\"     \"maintain\" \"presid\"   \n [9,] \"direct\"    \"republ\"    \"hope\"       \"revenu\"     \"polici\"   \"hous\"     \n[10,] \"proper\"    \"author\"    \"inform\"     \"june\"       \"intern\"   \"elect\"    \n      Topic 13   Topic 14   Topic 15    Topic 16   Topic 17     Topic 18  \n [1,] \"great\"    \"treati\"   \"made\"      \"congress\" \"duti\"       \"war\"     \n [2,] \"countri\"  \"great\"    \"appropri\"  \"act\"      \"import\"     \"forc\"    \n [3,] \"peopl\"    \"british\"  \"improv\"    \"law\"      \"increas\"    \"servic\"  \n [4,] \"labor\"    \"britain\"  \"work\"      \"author\"   \"countri\"    \"militari\"\n [5,] \"interest\" \"convent\"  \"purpos\"    \"provis\"   \"foreign\"    \"armi\"    \n [6,] \"condit\"   \"trade\"    \"provid\"    \"session\"  \"product\"    \"navi\"    \n [7,] \"good\"     \"vessel\"   \"make\"      \"legisl\"   \"produc\"     \"men\"     \n [8,] \"system\"   \"port\"     \"establish\" \"execut\"   \"manufactur\" \"offic\"   \n [9,] \"busi\"     \"negoti\"   \"secur\"     \"effect\"   \"revenu\"     \"ship\"    \n[10,] \"individu\" \"american\" \"object\"    \"pass\"     \"larg\"       \"command\" \n      Topic 19   Topic 20  \n [1,] \"nation\"   \"public\"  \n [2,] \"countri\"  \"bank\"    \n [3,] \"peopl\"    \"govern\"  \n [4,] \"prosper\"  \"money\"   \n [5,] \"great\"    \"issu\"    \n [6,] \"institut\" \"treasuri\"\n [7,] \"preserv\"  \"gold\"    \n [8,] \"honor\"    \"note\"    \n [9,] \"happi\"    \"debt\"    \n[10,] \"spirit\"   \"interest\"\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nexampleTermData <- terms(topicModel, 10)\nexampleTermData[, 1:8]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      Topic 1     Topic 2     Topic 3    Topic 4    Topic 5     Topic 6     \n [1,] \"land\"      \"recommend\" \"measur\"   \"citizen\"  \"great\"     \"claim\"     \n [2,] \"indian\"    \"report\"    \"interest\" \"law\"      \"line\"      \"govern\"    \n [3,] \"territori\" \"congress\"  \"view\"     \"case\"     \"part\"      \"question\"  \n [4,] \"larg\"      \"attent\"    \"subject\"  \"person\"   \"coast\"     \"commiss\"   \n [5,] \"tribe\"     \"secretari\" \"time\"     \"court\"    \"pacif\"     \"spain\"     \n [6,] \"limit\"     \"depart\"    \"present\"  \"properti\" \"construct\" \"island\"    \n [7,] \"popul\"     \"subject\"   \"object\"   \"protect\"  \"import\"    \"made\"      \n [8,] \"portion\"   \"consider\"  \"reason\"   \"natur\"    \"river\"     \"adjust\"    \n [9,] \"general\"   \"present\"   \"adopt\"    \"justic\"   \"complet\"   \"commission\"\n[10,] \"public\"    \"import\"    \"regard\"   \"demand\"   \"south\"     \"final\"     \n      Topic 7     Topic 8    \n [1,] \"public\"    \"state\"    \n [2,] \"offic\"     \"unit\"     \n [3,] \"duti\"      \"govern\"   \n [4,] \"execut\"    \"mexico\"   \n [5,] \"general\"   \"part\"     \n [6,] \"administr\" \"territori\"\n [7,] \"give\"      \"texa\"     \n [8,] \"respect\"   \"mexican\"  \n [9,] \"direct\"    \"republ\"   \n[10,] \"proper\"    \"author\"   \n```\n:::\n:::\n\n\nFor the next steps, we want to give the topics more descriptive names than just numbers. Therefore, we simply concatenate the five most likely terms of each topic to a string that represents a pseudo-name for each topic.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntop5termsPerTopic <- terms(topicModel, 5)\ntopicNames <- apply(top5termsPerTopic, 2, paste, collapse=\" \")\n```\n:::\n\n\n## Visualization of Words and Topics{-}\n\nAlthough wordclouds may not be optimal for scientific purposes they can provide a quick visual overview of a set of terms. Let's look at some topics as wordcloud.\n\nIn the following code, you can change the variable **topicToViz** with values between 1 and 20 to display other topics.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# visualize topics as word cloud\ntopicToViz <- 11 # change for your own topic of interest\ntopicToViz <- grep('mexico', topicNames)[1] # Or select a topic by a term contained in its name\n# select to 40 most probable terms from the topic by sorting the term-topic-probability vector in decreasing order\ntop40terms <- sort(tmResult$terms[topicToViz,], decreasing=TRUE)[1:40]\nwords <- names(top40terms)\n# extract the probabilites of each of the 40 terms\nprobabilities <- sort(tmResult$terms[topicToViz,], decreasing=TRUE)[1:40]\n# visualize the terms as wordcloud\nmycolors <- brewer.pal(8, \"Dark2\")\nwordcloud(words, probabilities, random.order = FALSE, color = mycolors)\n```\n\n::: {.cell-output-display}\n![](topicmodels_files/figure-html/unnamed-chunk-1-1.png){fig-align='center' width=384}\n:::\n:::\n\n\nLet us now look more closely at the distribution of topics within individual documents. To this end, we visualize the distribution in 3 sample documents.\n\nLet us first take a look at the contents of three sample documents:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nexampleIds <- c(2, 100, 200)\nlapply(corpus[exampleIds], as.character)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$`2`\n[1] \"I embrace with great satisfaction the opportunity which now presents itself\\nof congratulating you on the present favorable prospects of our public\\naffairs. The recent accession of the important state of North Carolina to\\nthe Constitution of the United States (of which official information has\\nbeen received), the rising credit and respectability of our country, the\\ngeneral and increasing good will toward the government of the Union, and\\nthe concord, peace, and plenty with which we are blessed are circumstances\\nauspicious in an eminent degree to our national prosperity.\"\n\n$`100`\n[1] \"Provision is likewise requisite for the reimbursement of the loan which has\\nbeen made of the Bank of the United States, pursuant to the eleventh\\nsection of the act by which it is incorporated. In fulfilling the public\\nstipulations in this particular it is expected a valuable saving will be\\nmade.\"\n\n$`200`\n[1] \"After many delays and disappointments arising out of the European war, the\\nfinal arrangements for fulfilling the engagements made to the Dey and\\nRegency of Algiers will in all present appearance be crowned with success,\\nbut under great, though inevitable, disadvantages in the pecuniary\\ntransactions occasioned by that war, which will render further provision\\nnecessary. The actual liberation of all our citizens who were prisoners in\\nAlgiers, while it gratifies every feeling of heart, is itself an earnest of\\na satisfactory termination of the whole negotiation. Measures are in\\noperation for effecting treaties with the Regencies of Tunis and Tripoli.\"\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nexampleIds <- c(2, 100, 200)\nprint(paste0(exampleIds[1], \": \", substr(content(corpus[[exampleIds[1]]]), 0, 400), '...'))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"2: I embrace with great satisfaction the opportunity which now presents itself\\nof congratulating you on the present favorable prospects of our public\\naffairs. The recent accession of the important state of North Carolina to\\nthe Constitution of the United States (of which official information has\\nbeen received), the rising credit and respectability of our country, the\\ngeneral and increasing good will ...\"\n```\n:::\n\n```{.r .cell-code}\nprint(paste0(exampleIds[2], \": \", substr(content(corpus[[exampleIds[2]]]), 0, 400), '...'))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"100: Provision is likewise requisite for the reimbursement of the loan which has\\nbeen made of the Bank of the United States, pursuant to the eleventh\\nsection of the act by which it is incorporated. In fulfilling the public\\nstipulations in this particular it is expected a valuable saving will be\\nmade....\"\n```\n:::\n\n```{.r .cell-code}\nprint(paste0(exampleIds[3], \": \", substr(content(corpus[[exampleIds[3]]]), 0, 400), '...'))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"200: After many delays and disappointments arising out of the European war, the\\nfinal arrangements for fulfilling the engagements made to the Dey and\\nRegency of Algiers will in all present appearance be crowned with success,\\nbut under great, though inevitable, disadvantages in the pecuniary\\ntransactions occasioned by that war, which will render further provision\\nnecessary. The actual liberation of all ...\"\n```\n:::\n:::\n\n\nAfter looking into the documents, we visualize the topic distributions within the documents.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nN <- length(exampleIds)\n# get topic proportions form example documents\ntopicProportionExamples <- theta[exampleIds,]\ncolnames(topicProportionExamples) <- topicNames\nvizDataFrame <- melt(cbind(data.frame(topicProportionExamples), document = factor(1:N)), variable.name = \"topic\", id.vars = \"document\")  \nggplot(data = vizDataFrame, aes(topic, value, fill = document), ylab = \"proportion\") + \n  geom_bar(stat=\"identity\") +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +  \n  coord_flip() +\n  facet_wrap(~ document, ncol = N)\n```\n\n::: {.cell-output-display}\n![](topicmodels_files/figure-html/unnamed-chunk-2-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n## Topic distributions{-}\n\nThe figure above shows how topics within a document are distributed according to the model. In the current model all three documents show at least a small percentage of each topic. However, two to three topics dominate each document.\n\nThe topic distribution within a document can be controlled with the *Alpha*-parameter of the model. Higher alpha priors for topics result in an even distribution of topics within a document. Low alpha priors ensure that the inference process distributes the probability mass on a few topics for each document. \n\nIn the previous model calculation the alpha-prior was automatically estimated in order to fit to the data (highest overall probability of the model). However, this automatic estimate does not necessarily correspond to the results that one would like to have as an analyst. Depending on our analysis interest, we might be interested in a more peaky/more even distribution of topics in the model. \n\nNow let us change the alpha prior to a lower value to see how this affects the topic distributions in the model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# see alpha from previous model\nattr(topicModel, \"alpha\") \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2.5\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntopicModel2 <- LDA(DTM, K, method=\"Gibbs\", control=list(iter = 500, verbose = 25, alpha = 0.2))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nK = 20; V = 4278; M = 8810\nSampling 500 iterations!\nIteration 25 ...\nIteration 50 ...\nIteration 75 ...\nIteration 100 ...\nIteration 125 ...\nIteration 150 ...\nIteration 175 ...\nIteration 200 ...\nIteration 225 ...\nIteration 250 ...\nIteration 275 ...\nIteration 300 ...\nIteration 325 ...\nIteration 350 ...\nIteration 375 ...\nIteration 400 ...\nIteration 425 ...\nIteration 450 ...\nIteration 475 ...\nIteration 500 ...\nGibbs sampling completed!\n```\n:::\n\n```{.r .cell-code}\ntmResult <- posterior(topicModel2)\ntheta <- tmResult$topics\nbeta <- tmResult$terms\ntopicNames <- apply(terms(topicModel2, 5), 2, paste, collapse = \" \")  # reset topicnames\n```\n:::\n\n\nNow visualize the topic distributions in the three documents again. What are the differences in the distribution structure?\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# get topic proportions form example documents\ntopicProportionExamples <- theta[exampleIds,]\ncolnames(topicProportionExamples) <- topicNames\nvizDataFrame <- melt(cbind(data.frame(topicProportionExamples), document = factor(1:N)), variable.name = \"topic\", id.vars = \"document\")  \nggplot(data = vizDataFrame, aes(topic, value, fill = document), ylab = \"proportion\") + \n  geom_bar(stat=\"identity\") +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +  \n  coord_flip() +\n  facet_wrap(~ document, ncol = N)\n```\n\n::: {.cell-output-display}\n![](topicmodels_files/figure-html/unnamed-chunk-3-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n## Topic ranking{-}\n\nFirst, we try to get a more meaningful order of top terms per topic by re-ranking them with a specific score [@Chang2009]. The idea of re-ranking terms is similar to the idea of TF-IDF. The more a term appears in top levels w.r.t. its probability, the less meaningful it is to describe the topic. Hence, the scoring advanced favors terms to describe a topic.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# re-rank top topic terms for topic names\ntopicNames <- apply(lda::top.topic.words(beta, 5, by.score = T), 2, paste, collapse = \" \")\n```\n:::\n\n\nWhat are the defining topics within a collection? There are different approaches to find out which can be used to bring the topics into a certain order.\n\n### Approach 1{-}\n\nWe sort topics according to their probability within the entire collection:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# What are the most probable topics in the entire collection?\ntopicProportions <- colSums(theta) / nDocs(DTM)  # mean probabilities over all paragraphs\nnames(topicProportions) <- topicNames     # assign the topic names we created before\nsort(topicProportions, decreasing = TRUE) # show summed proportions in decreased order\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       public object system consider great \n                                   0.06485 \n         nation peopl countri prosper peac \n                                   0.06293 \n         claim govern adjust treati negoti \n                                   0.05979 \ncongress attent report recommend secretari \n                                   0.05888 \n        congress senat state treati repres \n                                   0.05527 \n      year amount treasuri expenditur debt \n                                   0.05500 \n           govern relat state island spain \n                                   0.05493 \n             war mexico peac state citizen \n                                   0.05265 \n         constitut state power peopl union \n                                   0.05128 \n              peopl man labor polit condit \n                                   0.04965 \n        offic servic depart appoint public \n                                   0.04889 \n   product manufactur tariff duti industri \n                                   0.04537 \n              state unit trade vessel port \n                                   0.04491 \n               law court state unit person \n                                   0.04416 \n          year increas mail pension number \n                                   0.04386 \n           bank gold currenc silver circul \n                                   0.04337 \n          navi vessel ship naval construct \n                                   0.04236 \n             armi war forc militari servic \n                                   0.04122 \n          line state territori river pacif \n                                   0.04117 \n           land indian tribe territori acr \n                                   0.03949 \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsoP <- sort(topicProportions, decreasing = TRUE)\npaste(round(soP, 5), \":\", names(soP))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] \"0.06485 : public object system consider great\"       \n [2] \"0.06293 : nation peopl countri prosper peac\"         \n [3] \"0.05979 : claim govern adjust treati negoti\"         \n [4] \"0.05888 : congress attent report recommend secretari\"\n [5] \"0.05527 : congress senat state treati repres\"        \n [6] \"0.055 : year amount treasuri expenditur debt\"        \n [7] \"0.05493 : govern relat state island spain\"           \n [8] \"0.05265 : war mexico peac state citizen\"             \n [9] \"0.05128 : constitut state power peopl union\"         \n[10] \"0.04965 : peopl man labor polit condit\"              \n[11] \"0.04889 : offic servic depart appoint public\"        \n[12] \"0.04537 : product manufactur tariff duti industri\"   \n[13] \"0.04491 : state unit trade vessel port\"              \n[14] \"0.04416 : law court state unit person\"               \n[15] \"0.04386 : year increas mail pension number\"          \n[16] \"0.04337 : bank gold currenc silver circul\"           \n[17] \"0.04236 : navi vessel ship naval construct\"          \n[18] \"0.04122 : armi war forc militari servic\"             \n[19] \"0.04117 : line state territori river pacif\"          \n[20] \"0.03949 : land indian tribe territori acr\"           \n```\n:::\n:::\n\n\nWe recognize some topics that are way more likely to occur in the corpus than others. These describe rather general thematic coherence. Other topics correspond more to specific contents. \n\n### Approach 2{-}\n\nWe count how often a topic appears as a primary topic within a paragraph This method is also called Rank-1.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncountsOfPrimaryTopics <- rep(0, K)\nnames(countsOfPrimaryTopics) <- topicNames\nfor (i in 1:nDocs(DTM)) {\n  topicsPerDoc <- theta[i, ] # select topic distribution for document i\n  # get first element position from ordered list\n  primaryTopic <- order(topicsPerDoc, decreasing = TRUE)[1] \n  countsOfPrimaryTopics[primaryTopic] <- countsOfPrimaryTopics[primaryTopic] + 1\n}\nsort(countsOfPrimaryTopics, decreasing = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n         claim govern adjust treati negoti \n                                       623 \n           govern relat state island spain \n                                       594 \n         nation peopl countri prosper peac \n                                       576 \n       public object system consider great \n                                       525 \n      year amount treasuri expenditur debt \n                                       524 \n        congress senat state treati repres \n                                       521 \n             war mexico peac state citizen \n                                       476 \ncongress attent report recommend secretari \n                                       461 \n           bank gold currenc silver circul \n                                       428 \n        offic servic depart appoint public \n                                       420 \n         constitut state power peopl union \n                                       414 \n               law court state unit person \n                                       383 \n              state unit trade vessel port \n                                       373 \n   product manufactur tariff duti industri \n                                       373 \n          navi vessel ship naval construct \n                                       370 \n              peopl man labor polit condit \n                                       369 \n           land indian tribe territori acr \n                                       368 \n          year increas mail pension number \n                                       357 \n             armi war forc militari servic \n                                       336 \n          line state territori river pacif \n                                       319 \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nso <- sort(countsOfPrimaryTopics, decreasing = TRUE)\npaste(so, \":\", names(so))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] \"623 : claim govern adjust treati negoti\"         \n [2] \"594 : govern relat state island spain\"           \n [3] \"576 : nation peopl countri prosper peac\"         \n [4] \"525 : public object system consider great\"       \n [5] \"524 : year amount treasuri expenditur debt\"      \n [6] \"521 : congress senat state treati repres\"        \n [7] \"476 : war mexico peac state citizen\"             \n [8] \"461 : congress attent report recommend secretari\"\n [9] \"428 : bank gold currenc silver circul\"           \n[10] \"420 : offic servic depart appoint public\"        \n[11] \"414 : constitut state power peopl union\"         \n[12] \"383 : law court state unit person\"               \n[13] \"373 : state unit trade vessel port\"              \n[14] \"373 : product manufactur tariff duti industri\"   \n[15] \"370 : navi vessel ship naval construct\"          \n[16] \"369 : peopl man labor polit condit\"              \n[17] \"368 : land indian tribe territori acr\"           \n[18] \"357 : year increas mail pension number\"          \n[19] \"336 : armi war forc militari servic\"             \n[20] \"319 : line state territori river pacif\"          \n```\n:::\n:::\n\n\nWe see that sorting topics by the  Rank-1 method places topics with rather specific thematic coherences in upper ranks of the list. \n\nThis sorting of topics can be used for further analysis steps such as the semantic interpretation of topics found in the collection, the analysis of time series of the most important topics or the filtering of the original collection based on specific sub-topics.\n\n## Filtering documents{-}\n\nThe fact that a topic model conveys of topic probabilities for each document, resp. paragraph in our case, makes it possible to use it for thematic filtering of a collection. AS filter we select only those documents which exceed a certain threshold of their probability value for certain topics (for example, each document which contains topic `X` to more than 20 percent).\n\nIn the following, we will select documents based on their topic content and display the resulting document quantity over time.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntopicToFilter <- 6  # you can set this manually ...\n# ... or have it selected by a term in the topic name (e.g. 'children')\ntopicToFilter <- grep('children', topicNames)[1] \ntopicThreshold <- 0.2\nselectedDocumentIndexes <- which(theta[, topicToFilter] >= topicThreshold)\nfilteredCorpus <- corpus[selectedDocumentIndexes]\n# show length of filtered corpus\nfilteredCorpus\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<<SimpleCorpus>>\nMetadata:  corpus specific: 1, document level (indexed): 4\nContent:  documents: 0\n```\n:::\n:::\n\n\nOur filtered corpus contains 0 documents related to the topic NA to at least 20 %.\n\n## Topic proportions over time{-}\n\nIn a last step, we provide a distant view on the topics in the data over time. For this, we aggregate mean topic proportions per decade of all SOTU speeches. These aggregated topic proportions can then be visualized, e.g. as a bar plot. \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# append decade information for aggregation\ntextdata$decade <- paste0(substr(textdata$date, 0, 3), \"0\")\n# get mean topic proportions per decade\ntopic_proportion_per_decade <- aggregate(theta, by = list(decade = textdata$decade), mean)\n# set topic names to aggregated columns\ncolnames(topic_proportion_per_decade)[2:(K+1)] <- topicNames\n# reshape data frame\nvizDataFrame <- melt(topic_proportion_per_decade, id.vars = \"decade\")\n# plot topic proportions per decade as bar plot\nggplot(vizDataFrame, aes(x=decade, y=value, fill=variable)) + \n  geom_bar(stat = \"identity\") + ylab(\"proportion\") + \n  scale_fill_manual(values = paste0(alphabet(20), \"FF\"), name = \"decade\") + \n  theme(axis.text.x = element_text(angle = 90, hjust = 1))\n```\n\n::: {.cell-output-display}\n![](topicmodels_files/figure-html/unnamed-chunk-4-1.png){fig-align='center' width=864}\n:::\n:::\n\n\nThe visualization shows that topics around the relation between the federal government and the states as well as inner conflicts clearly dominate the first decades. Security issues and the economy are the most important topics of recent SOTU addresses.\n\n# Citation & Session Info {-}\n\nSchweinberger, Martin. 2022. *Topic Modeling with R*. Brisbane: The University of Queensland. url: https://slcladal.github.io/topicmodels.html (Version 2022.08.31).\n\n\n```\n@manual{schweinberger2022topic,\n  author = {Schweinberger, Martin},\n  title = {Topic Modeling with R},\n  note = {https://slcladal.github.io/topicmodels.html},\n  year = {2022},\n  organization = \"The University of Queensland, Australia. School of Languages and Cultures},\n  address = {Brisbane},\n  edition = {2022.08.31}\n}\n```\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsessionInfo()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nR version 4.2.1 (2022-06-23)\nPlatform: x86_64-pc-linux-gnu (64-bit)\nRunning under: Ubuntu 22.04.1 LTS\n\nMatrix products: default\nBLAS:   /usr/lib/x86_64-linux-gnu/blas/libblas.so.3.10.0\nLAPACK: /usr/lib/x86_64-linux-gnu/lapack/liblapack.so.3.10.0\n\nlocale:\n [1] LC_CTYPE=en_AU.UTF-8       LC_NUMERIC=C              \n [3] LC_TIME=en_AU.UTF-8        LC_COLLATE=en_AU.UTF-8    \n [5] LC_MONETARY=en_AU.UTF-8    LC_MESSAGES=en_AU.UTF-8   \n [7] LC_PAPER=en_AU.UTF-8       LC_NAME=C                 \n [9] LC_ADDRESS=C               LC_TELEPHONE=C            \n[11] LC_MEASUREMENT=en_AU.UTF-8 LC_IDENTIFICATION=C       \n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] flextable_0.7.3    ldatuning_1.0.2    lda_1.4.2          SnowballC_0.7.0   \n [5] pals_1.7           wordcloud_2.6      RColorBrewer_1.1-3 ggplot2_3.3.6     \n [9] reshape2_1.4.4     topicmodels_0.2-12 tm_0.7-8           NLP_0.2-1         \n[13] DT_0.24            kableExtra_1.3.4   knitr_1.39        \n\nloaded via a namespace (and not attached):\n [1] httr_1.4.3        maps_3.4.0        jsonlite_1.8.0    viridisLite_0.4.0\n [5] assertthat_0.2.1  stats4_4.2.1      yaml_2.3.5        slam_0.1-50      \n [9] gdtools_0.2.4     pillar_1.7.0      glue_1.6.2        uuid_1.1-0       \n[13] digest_0.6.29     rvest_1.0.2       colorspace_2.0-3  htmltools_0.5.2  \n[17] plyr_1.8.7        pkgconfig_2.0.3   purrr_0.3.4       scales_1.2.0     \n[21] webshot_0.5.3     svglite_2.1.0     officer_0.4.3     tibble_3.1.7     \n[25] farver_2.1.1      generics_0.1.3    ellipsis_0.3.2    withr_2.5.0      \n[29] klippy_0.0.0.9500 cli_3.3.0         magrittr_2.0.3    crayon_1.5.1     \n[33] evaluate_0.15     fansi_1.0.3       xml2_1.3.3        tools_4.2.1      \n[37] data.table_1.14.2 lifecycle_1.0.1   stringr_1.4.0     munsell_0.5.0    \n[41] zip_2.2.0         compiler_4.2.1    systemfonts_1.0.4 rlang_1.0.4      \n[45] grid_4.2.1        dichromat_2.0-0.1 rstudioapi_0.13   htmlwidgets_1.5.4\n[49] base64enc_0.1-3   labeling_0.4.2    rmarkdown_2.14    gtable_0.3.0     \n[53] DBI_1.1.3         R6_2.5.1          dplyr_1.0.9       fastmap_1.1.0    \n[57] utf8_1.2.2        modeltools_0.2-23 stringi_1.7.8     parallel_4.2.1   \n[61] Rcpp_1.0.8.3      vctrs_0.4.1       mapproj_1.2.8     tidyselect_1.1.2 \n[65] xfun_0.31        \n```\n:::\n:::\n\n\n\n\n***\n\n[Back to top](#introduction)\n\n[Back to HOME](https://slcladal.github.io/index.html)\n\n***\n\n\n# References{-}\n\n\n",
    "supporting": [
      "topicmodels_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"site_libs/clipboard-1.7.1/clipboard.min.js\"></script>\n<link href=\"site_libs/primer-tooltips-1.4.0/build.css\" rel=\"stylesheet\" />\n<link href=\"site_libs/klippy-0.0.0.9500/css/klippy.min.css\" rel=\"stylesheet\" />\n<script src=\"site_libs/klippy-0.0.0.9500/js/klippy.min.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}