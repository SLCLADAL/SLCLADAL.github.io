{
  "hash": "58bb99d2bb7218f5284e707f285690da",
  "result": {
    "markdown": "---\ntitle: \"Automated Text Summarization with R\"\nauthor: \"Martin Schweinberger\"\ndate: \"2022-08-31\"\n---\n\n::: {.cell}\n::: {.cell-output-display}\n![](https://slcladal.github.io/images/uq1.jpg){width=100%}\n:::\n:::\n\n\n# Introduction{-}\n\nThis tutorial shows how to summarize texts automatically using R by extracting the most prototypical sentences. \n\n\n::: {.cell}\n::: {.cell-output-display}\n![](https://slcladal.github.io/images/gy_chili.jpg){width=15% style=\"float:right; padding:10px\"}\n:::\n:::\n\n\nThis tutorial is aimed at beginners and intermediate users of R with the aim of showcasing how to summarize textual data  using R. The aim is not to provide a fully-fledged analysis but rather to show and exemplify selected useful methods associated with summarizing texts.\n\n\n<div class=\"warning\" style='padding:0.1em; background-color:#f2f2f2; color:#51247a'>\n<span>\n<p style='margin-top:1em; text-align:center'>\nThe entire R Notebook for the tutorial can be downloaded [**here**](https://slcladal.github.io/content/txtsum.Rmd).  If you want to render the R Notebook on your machine, i.e. knitting the document to html or a pdf, you need to make sure that you have R and RStudio installed and you also need to download the [**bibliography file**](https://slcladal.github.io/content/bibliography.bib) and store it in the same folder where you store the Rmd file. <br></p>\n<p style='margin-left:1em;'>\n</p></span>\n</div>\n\n<br>\n\n**Preparation and session set up**\n\nThis tutorial is based on R. If you have not installed R or are new to it, you will find an introduction to and more information how to use R [here](https://slcladal.github.io/intror.html). For this tutorials, we need to install certain *packages* from an R *library* so that the scripts shown below are executed without errors. Before turning to the code below, please install the packages by running the code below this paragraph. If you have already installed the packages mentioned below, then you can skip ahead and ignore this section. To install the necessary packages, simply run the following code - it may take some time (between 1 and 5 minutes to install all of the libraries so you do not need to worry if it takes some time).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# set options\noptions(stringsAsFactors = F)          # no automatic data transformation\noptions(\"scipen\" = 100, \"digits\" = 12) # suppress math annotation\n# install packages\ninstall.packages(\"xml2\")\ninstall.packages(\"rvest\")\ninstall.packages(\"lexRankr\")\ninstall.packages(\"textmineR\")\ninstall.packages(\"tidyverse\")\ninstall.packages(\"quanteda\")\ninstall.packages(\"igraph\")\ninstall.packages(\"here\")\n# install klippy for copy-to-clipboard button in code chunks\ninstall.packages(\"remotes\")\nremotes::install_github(\"rlesur/klippy\")\n```\n:::\n\n\nNext we activate the packages.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# activate packages\nlibrary(xml2)\nlibrary(rvest)\nlibrary(lexRankr)\nlibrary(textmineR)\nlibrary(tidyverse)\nlibrary(quanteda)\nlibrary(igraph)\nlibrary(here)\n# activate klippy for copy-to-clipboard button\nklippy::klippy()\n```\n\n::: {.cell-output-display}\n```{=html}\n<script>\n  addClassKlippyTo(\"pre.r, pre.markdown\");\n  addKlippy('left', 'top', 'auto', '1', 'Copy code', 'Copied!');\n</script>\n```\n:::\n:::\n\n\nOnce you have installed RStudio and have also initiated the session by executing the code shown above, you are good to go.\n\n# Basic Text summarization{-}\n\nThis section shows an easy to use text summarizing method which extracts the most prototypical sentences from a text. As such, this text summarizer does not generate sentences based on prototypical words but evaluates how prototypical or central sentences are and then orders the sentences in a text according to their prototypicality (or centrality).\n\nFor this example, we will download text from a Guardian article about a meeting between Angela Merkel and Donald Trump at the G20 summit in 2017. In a first step, we define the url of the webpage hosting the article.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# url to scrape\nurl = \"https://www.theguardian.com/world/2017/jun/26/angela-merkel-and-donald-trump-head-for-clash-at-g20-summit\"\n```\n:::\n\n\nNext, we extract the text of the article using the`xml2  and the `rvest` packages.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# read page html\npage = xml2::read_html(url)\n# extract text from page html using selector\npage %>%\n  # extract paragraphs\n  rvest::html_nodes(\"p\") %>%\n  # extract text\n  rvest::html_text() %>%\n  # remove empty elements\n  .[. != \"\"] -> text\n# inspect data\nhead(text)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"German chancellor plans to make climate change, free trade and mass migration key themes in Hamburg, putting her on collision course with US\"                                                                                                        \n[2] \"A clash between Angela Merkel and Donald Trump appears unavoidable after Germany signalled that it will make climate change, free trade and the management of forced mass global migration the key themes of the G20 summit in Hamburg next week.\"   \n[3] \"The G20 summit brings together the world’s biggest economies, representing 85% of global gross domestic product (GDP), and Merkel’s chosen agenda looks likely to maximise American isolation while attempting to minimise disunity amongst others. \"\n[4] \"The meeting, which is set to be the scene of large-scale street protests, will also mark the first meeting between Trump and the Russian president, Vladimir Putin, as world leaders.\"                                                               \n[5] \"Trump has already rowed with Europe once over climate change and refugees at the G7 summit in Italy, and now looks set to repeat the experience in Hamburg but on a bigger stage, as India and China join in the criticism of Washington. \"          \n[6] \"Last week, the new UN secretary-general, António Guterres, warned the Trump team if the US disengages from too many issues confronting the international community it will be replaced as world leader.\"                                             \n```\n:::\n:::\n\n\nNow that we have the text, we apply the `lexRank` function from the `lexRankr` package to determine the prototypicality (or centrality) and extract the three most central sentences.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# perform lexrank for top 3 sentences\ntop3sentences = lexRankr::lexRank(text,\n                          # only 1 article; repeat same docid for all of input vector\n                          docId = rep(1, length(text)),\n                          # return 3 sentences\n                          n = 3,\n                          continuous = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nParsing text into sentences and tokens...DONE\nCalculating pairwise sentence similarities...DONE\nApplying LexRank...DONE\nFormatting Output...DONE\n```\n:::\n\n```{.r .cell-code}\n# inspect\ntop3sentences\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  docId sentenceId\n1     1        1_2\n2     1        1_5\n3     1       1_16\n                                                                                                                                                                                                                                                       sentence\n1             A clash between Angela Merkel and Donald Trump appears unavoidable after Germany signalled that it will make climate change, free trade and the management of forced mass global migration the key themes of the G20 summit in Hamburg next week.\n2                     Trump has already rowed with Europe once over climate change and refugees at the G7 summit in Italy, and now looks set to repeat the experience in Hamburg but on a bigger stage, as India and China join in the criticism of Washington.\n3 But the G7, and Trump’s subsequent decision to shun the Paris climate change treaty, clearly left a permanent mark on her, leading to her famous declaration of independence four days later at a Christian Social Union (CSU) rally in a Bavarian beer tent.\n       value\n1 0.06017053\n2 0.05656337\n3 0.04974733\n```\n:::\n:::\n\n\nNext, we extract and display the sentences from the table. \n\n\n::: {.cell}\n\n```{.r .cell-code}\ntop3sentences$sentence\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"A clash between Angela Merkel and Donald Trump appears unavoidable after Germany signalled that it will make climate change, free trade and the management of forced mass global migration the key themes of the G20 summit in Hamburg next week.\"            \n[2] \"Trump has already rowed with Europe once over climate change and refugees at the G7 summit in Italy, and now looks set to repeat the experience in Hamburg but on a bigger stage, as India and China join in the criticism of Washington.\"                    \n[3] \"But the G7, and Trump’s subsequent decision to shun the Paris climate change treaty, clearly left a permanent mark on her, leading to her famous declaration of independence four days later at a Christian Social Union (CSU) rally in a Bavarian beer tent.\"\n```\n:::\n:::\n\n\nThe output show the three most prototypical (or central) sentences of the article. The articles are already in chronological order - if the sentences were not in chronological order, we could also have ordered them by *sentenceId* before displaying them using `dplyr` and `stringr` package functions as shown below (in our case the order does not change as the prototypicality and the chronological order are identical).\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntop3sentences %>%\n  dplyr::mutate(sentenceId = as.numeric(stringr::str_remove_all(sentenceId, \".*_\"))) %>%\n  dplyr::arrange(sentenceId) %>%\n  dplyr::pull(sentence)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"A clash between Angela Merkel and Donald Trump appears unavoidable after Germany signalled that it will make climate change, free trade and the management of forced mass global migration the key themes of the G20 summit in Hamburg next week.\"            \n[2] \"Trump has already rowed with Europe once over climate change and refugees at the G7 summit in Italy, and now looks set to repeat the experience in Hamburg but on a bigger stage, as India and China join in the criticism of Washington.\"                    \n[3] \"But the G7, and Trump’s subsequent decision to shun the Paris climate change treaty, clearly left a permanent mark on her, leading to her famous declaration of independence four days later at a Christian Social Union (CSU) rally in a Bavarian beer tent.\"\n```\n:::\n:::\n\n\n***\n\n<div class=\"warning\" style='padding:0.1em; background-color:#51247a; color:#f2f2f2'>\n<span>\n<p style='margin-top:1em; text-align:center'>\n<b>EXERCISE TIME!</b></p>\n<p style='margin-left:1em;'>\n</p></span>\n</div>\n\n<div class=\"question\">` \n\n\n\n1. Extract the top 10 sentences from every chapter of Charles Darwin's *On the Origin of Species*. You can download the text using this command: `darwin <- base::readRDS(url(\"https://slcladal.github.io/data/origindarwin.rda\", \"rb\"))`. You will then have to paste the whole text together, split it into chapters, create a list of sentences in each chapter, and then apply text summarization to each element in the list. <br>\n\n<details>\n  <summary>Answer</summary>\n\n  ::: {.cell}\n  \n  ```{.r .cell-code}\n  darwin <- base::readRDS(url(\"https://slcladal.github.io/data/origindarwin.rda\", \"rb\")) %>%\n  # collapse into single document\n  paste0(collapse = \" \") %>%\n  # split into chapters\n  stringr::str_split(\"CHAPTER\")\n  \n  # split chapters into sentences\n  chapters <- sapply(darwin, function(x){\n    x <- stringi::stri_split_boundaries(x, type = \"sentence\")\n  })\n  \n  chapters_clean <- lapply(chapters, function(x){\n    # remove chapter headings\n    x <- stringr::str_remove_all(x, \"[A-Z]{2,} {0,1}[0-9]{0,}\")\n  })\n  \n  # extract top 3 sentences from each chapter\n  top3s <- lapply(chapters_clean, function(x){\n    x <- lexRankr::lexRank(x,\n                          # only 1 article; repeat same docid for all of input vector\n                          #docId = rep(1, length(text)),\n                          # return 3 sentences\n                          n = 3,\n                          continuous = TRUE) %>%\n                          dplyr::pull(sentence) %>%\n    # remove special characters\n    stringr::str_remove_all(\"[^[:alnum:] ]\") %>%\n    # remove superfluous white spaces\n    stringr::str_squish()\n  })\n  \n  # inspect top 3 sentences of first 5 chapters\n  top3s[1:5]\n  ```\n  :::\n\n</details>\n\n\n</div>`\n\n***\n\nYou can go ahead and play with the text summarization and see if it is useful for you or if you can trust the results based on your data. \n\n\n# Citation & Session Info {-}\n\nSchweinberger, Martin. 2022. *Automated text summarization with R*. Brisbane: The University of Queensland. url: https://slcladal.github.io/txtsum.html (Version 2022.08.31).\n\n\n```\n@manual{schweinberger2022txtsum,\n  author = {Schweinberger, Martin},\n  title = {Automated Text Summarization with R},\n  note = {https://slcladal.github.io/txtsum.html},\n  year = {2022},\n  organization = \"The University of Queensland, Australia. School of Languages and Cultures},\n  address = {Brisbane},\n  edition = {2022.08.31}\n}\n```\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsessionInfo()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nR version 4.2.1 (2022-06-23)\nPlatform: x86_64-pc-linux-gnu (64-bit)\nRunning under: Ubuntu 22.04.1 LTS\n\nMatrix products: default\nBLAS:   /usr/lib/x86_64-linux-gnu/blas/libblas.so.3.10.0\nLAPACK: /usr/lib/x86_64-linux-gnu/lapack/liblapack.so.3.10.0\n\nlocale:\n [1] LC_CTYPE=en_AU.UTF-8       LC_NUMERIC=C              \n [3] LC_TIME=en_AU.UTF-8        LC_COLLATE=en_AU.UTF-8    \n [5] LC_MONETARY=en_AU.UTF-8    LC_MESSAGES=en_AU.UTF-8   \n [7] LC_PAPER=en_AU.UTF-8       LC_NAME=C                 \n [9] LC_ADDRESS=C               LC_TELEPHONE=C            \n[11] LC_MEASUREMENT=en_AU.UTF-8 LC_IDENTIFICATION=C       \n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] here_1.0.1      igraph_1.3.2    quanteda_3.2.1  forcats_0.5.1  \n [5] stringr_1.4.0   dplyr_1.0.9     purrr_0.3.4     readr_2.1.2    \n [9] tidyr_1.2.0     tibble_3.1.7    ggplot2_3.3.6   tidyverse_1.3.2\n[13] textmineR_3.0.5 Matrix_1.4-1    lexRankr_0.5.2  rvest_1.0.2    \n[17] xml2_1.3.3     \n\nloaded via a namespace (and not attached):\n [1] httr_1.4.3          jsonlite_1.8.0      modelr_0.1.8       \n [4] RcppParallel_5.1.5  assertthat_0.2.1    selectr_0.4-2      \n [7] googlesheets4_1.0.0 cellranger_1.1.0    yaml_2.3.5         \n[10] pillar_1.7.0        backports_1.4.1     lattice_0.20-45    \n[13] glue_1.6.2          digest_0.6.29       colorspace_2.0-3   \n[16] htmltools_0.5.2     pkgconfig_2.0.3     broom_1.0.0        \n[19] haven_2.5.0         scales_1.2.0        tzdb_0.3.0         \n[22] googledrive_2.0.0   generics_0.1.3      ellipsis_0.3.2     \n[25] withr_2.5.0         klippy_0.0.0.9500   cli_3.3.0          \n[28] magrittr_2.0.3      crayon_1.5.1        readxl_1.4.0       \n[31] evaluate_0.15       stopwords_2.3       fs_1.5.2           \n[34] fansi_1.0.3         SnowballC_0.7.0     RcppProgress_0.4.2 \n[37] tools_4.2.1         hms_1.1.1           gargle_1.2.0       \n[40] lifecycle_1.0.1     munsell_0.5.0       reprex_2.0.1       \n[43] compiler_4.2.1      rlang_1.0.4         grid_4.2.1         \n[46] rstudioapi_0.13     htmlwidgets_1.5.4   rmarkdown_2.14     \n[49] gtable_0.3.0        DBI_1.1.3           curl_4.3.2         \n[52] R6_2.5.1            lubridate_1.8.0     knitr_1.39         \n[55] fastmap_1.1.0       utf8_1.2.2          fastmatch_1.1-3    \n[58] rprojroot_2.0.3     stringi_1.7.8       Rcpp_1.0.8.3       \n[61] vctrs_0.4.1         dbplyr_2.2.1        tidyselect_1.1.2   \n[64] xfun_0.31          \n```\n:::\n:::\n\n\n\n\n***\n\n[Back to top](#introduction)\n\n[Back to HOME](https://slcladal.github.io/index.html)\n\n***\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"site_libs/clipboard-1.7.1/clipboard.min.js\"></script>\n<link href=\"site_libs/primer-tooltips-1.4.0/build.css\" rel=\"stylesheet\" />\n<link href=\"site_libs/klippy-0.0.0.9500/css/klippy.min.css\" rel=\"stylesheet\" />\n<script src=\"site_libs/klippy-0.0.0.9500/js/klippy.min.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}