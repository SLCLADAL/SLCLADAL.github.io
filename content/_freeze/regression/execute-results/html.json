{
  "hash": "b8f4541f2b5722a36a53ceac588617b0",
  "result": {
    "markdown": "---\ntitle: \"Fixed- and Mixed-Effects Regression Models in R\"\nauthor: \"Martin Schweinberger\"\ndate: \"2022-08-31\"\n---\n\n::: {.cell}\n::: {.cell-output-display}\n![](https://slcladal.github.io/images/uq1.jpg){width=100%}\n:::\n:::\n\n\n# Introduction{-}\n\nThis tutorial introduces regression analyses (also called regression modeling) using R.^[I'm extremely grateful to Stefan Thomas Gries who provided very helpful feedback and pointed out many errors in previous versions of this tutorial. All remaining errors are, of course, my own.] Regression models are among the most widely used quantitative methods in the language sciences to assess if and how predictors (variables or interactions between variables) correlate with a certain response. \n\n\n::: {.cell}\n::: {.cell-output-display}\n![](https://slcladal.github.io/images/yr_chili.jpg){width=15% style=\"float:right; padding:10px\"}\n:::\n:::\n\n\nThis tutorial is aimed at intermediate and advanced users of R with the aim of showcasing how to perform regression analysis using R. The aim is not to provide a fully-fledged analysis but rather to show and exemplify common regression types, model diagnostics, and model fitting using R. \n\n<br>\n<div class=\"warning\" style='padding:0.1em; background-color:#f2f2f2; color:#51247a'>\n<span>\n<p style='margin-top:1em; text-align:center'>\nThe entire R Notebook for the tutorial can be downloaded [**here**](https://slcladal.github.io/content/regression.Rmd).  If you want to render the R Notebook on your machine, i.e. knitting the document to html or a pdf, you need to make sure that you have R and RStudio installed and you also need to download the [**bibliography file**](https://slcladal.github.io/content/bibliography.bib) and store it in the same folder where you store the Rmd or the Rproj file. <br></p>\n<p style='margin-left:1em;'>\n</p></span>\n</div>\n\n<br>\n\nRegression models are so popular because they can\n\n* incorporate many predictors in a single model (multivariate: allows to test the impact of one predictor while the impact of (all) other predictors is controlled for)\n\n* extremely flexible and and can be fitted to different types of predictors and dependent variables\n\n* provide output that can be easily interpreted\n\n* conceptually relative simple and not overly complex from a  mathematical perspective\n\nR offers various ready-made functions with which implementing different types of regression models is very easy.\n\nThe most widely use regression models are\n\n* linear regression (dependent variable is numeric, no outliers)\n\n* logistic regression (dependent variable is binary)\n\n* ordinal regression (dependent variable represents an ordered factor, e.g. Likert items)\n\n* multinomial regression (dependent variable is categorical)\n\n\nThe major difference between these types of models is that they take different types of dependent variables: linear regressions take numeric, logistic regressions take nominal variables, ordinal regressions take ordinal variables, and Poisson regressions take dependent variables that reflect counts of (rare) events. Robust regression, in contrast, is a simple multiple linear regression that is able to handle outliers due to a weighing procedure.\n\nIf regression models contain a random effect structure which is used to model nestedness or dependence among data points, the regression models are called *mixed-effect models*. regressions that do not have a random effect component to model  nestedness or dependence are referred to as fixed-effect regressions (we will have a closer look at the difference between fixed and random effects below).\n\nThere are two basic types of regression models: \n\n* fixed-effects regression models \n\n* mixed-effects regression models (which are fitted using the `lme4` package [@lme4] in this tutorial). \n\nFixed-effects regression models are models that assume a non-hierarchical data structure, i.e. data where data points are not nested or grouped in higher order categories (e.g. students within classes). The first part of this tutorial focuses on fixed-effects regression models while the second part focuses on mixed-effects regression models.\n\nThere exists a wealth of literature focusing on  regression analysis and the concepts it is based on.  For instance, there are @achen1982interpreting, @bortz2006statistik, @crawley2005statistics, @faraway2002practical, @field2012discovering (my personal favorite), @gries2021statistics, @levshina2015linguistics,  and @wilcox2009basic to name just a few. Introductions to regression modeling in R are @baayen2008analyzing, @crawley2012r, @gries2021statistics, or @levshina2015linguistics.\n\n**The basic principle**\n\nThe idea behind regression analysis is expressed formally in the equation below where $f_{(x)}$ is the $y$-value we want to predict, $\\alpha$ is the intercept (the point where the regression line crosses the $y$-axis), $\\beta$ is the coefficient (the slope of the regression line). \n\n\\begin{equation}\nf_{(x)} = \\alpha + \\beta_{i}x + \\epsilon\n\\end{equation}\n\nTo understand what this means, let us imagine that we have collected information about the how tall people are and what they weigh. Now we want to predict the weight of people of a certain height - let's say 180cm.\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n```{=html}\n<div class=\"tabwid\"><style>.cl-b2300ef2{table-layout:auto;width:75%;}.cl-b22c808e{font-family:'DejaVu Sans';font-size:12pt;font-weight:bold;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-b22c8098{font-family:'DejaVu Sans';font-size:12pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-b22c8e3a{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-b22cad0c{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b22cad0d{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b22cad16{background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b22cad17{background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b22cad18{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b22cad20{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b22cad21{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b22cad22{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}</style><table class='cl-b2300ef2'>\n```\n<caption class=\"Table Caption\">\n\nWeigth and height of a random sample of people.\n\n</caption>\n```{=html}\n<thead><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-b22cad21\"><p class=\"cl-b22c8e3a\"><span class=\"cl-b22c808e\">Height</span></p></td><td class=\"cl-b22cad22\"><p class=\"cl-b22c8e3a\"><span class=\"cl-b22c808e\">Weight</span></p></td></tr></thead><tbody><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-b22cad0c\"><p class=\"cl-b22c8e3a\"><span class=\"cl-b22c8098\">173</span></p></td><td class=\"cl-b22cad0d\"><p class=\"cl-b22c8e3a\"><span class=\"cl-b22c8098\">80</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-b22cad18\"><p class=\"cl-b22c8e3a\"><span class=\"cl-b22c8098\">169</span></p></td><td class=\"cl-b22cad20\"><p class=\"cl-b22c8e3a\"><span class=\"cl-b22c8098\">68</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-b22cad0c\"><p class=\"cl-b22c8e3a\"><span class=\"cl-b22c8098\">176</span></p></td><td class=\"cl-b22cad0d\"><p class=\"cl-b22c8e3a\"><span class=\"cl-b22c8098\">72</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-b22cad18\"><p class=\"cl-b22c8e3a\"><span class=\"cl-b22c8098\">166</span></p></td><td class=\"cl-b22cad20\"><p class=\"cl-b22c8e3a\"><span class=\"cl-b22c8098\">75</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-b22cad0c\"><p class=\"cl-b22c8e3a\"><span class=\"cl-b22c8098\">161</span></p></td><td class=\"cl-b22cad0d\"><p class=\"cl-b22c8e3a\"><span class=\"cl-b22c8098\">70</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-b22cad18\"><p class=\"cl-b22c8e3a\"><span class=\"cl-b22c8098\">164</span></p></td><td class=\"cl-b22cad20\"><p class=\"cl-b22c8e3a\"><span class=\"cl-b22c8098\">65</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-b22cad0c\"><p class=\"cl-b22c8e3a\"><span class=\"cl-b22c8098\">160</span></p></td><td class=\"cl-b22cad0d\"><p class=\"cl-b22c8e3a\"><span class=\"cl-b22c8098\">62</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-b22cad18\"><p class=\"cl-b22c8e3a\"><span class=\"cl-b22c8098\">158</span></p></td><td class=\"cl-b22cad20\"><p class=\"cl-b22c8e3a\"><span class=\"cl-b22c8098\">60</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-b22cad0c\"><p class=\"cl-b22c8e3a\"><span class=\"cl-b22c8098\">180</span></p></td><td class=\"cl-b22cad0d\"><p class=\"cl-b22c8e3a\"><span class=\"cl-b22c8098\">85</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-b22cad16\"><p class=\"cl-b22c8e3a\"><span class=\"cl-b22c8098\">187</span></p></td><td class=\"cl-b22cad17\"><p class=\"cl-b22c8e3a\"><span class=\"cl-b22c8098\">92</span></p></td></tr></tbody></table></div>\n```\n:::\n:::\n\n\nWe can run a simple linear regression on the data and we get the following output:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# model for upper panels\nsummary(glm(Weight ~ 1, data = df))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nglm(formula = Weight ~ 1, data = df)\n\nDeviance Residuals: \n   Min      1Q  Median      3Q     Max  \n-12.90   -7.15   -1.90    5.85   19.10  \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   72.900      3.244   22.48 3.24e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 105.2111)\n\n    Null deviance: 946.9  on 9  degrees of freedom\nResidual deviance: 946.9  on 9  degrees of freedom\nAIC: 77.885\n\nNumber of Fisher Scoring iterations: 2\n```\n:::\n:::\n\n\nTo estimate how much some weights who is 180cm tall, we would multiply the coefficient (slope of the line) with 180 ($x$) and add the value of the intercept (point where line crosses the $y$-axis). If we plug in the numbers from the regression model below, we get\n\n\\begin{equation}\n-93.77 + 0.98 âˆ— 180 = 83.33 (kg)\n\\end{equation}\n\nA person who is 180cm tall is predicted to weigh 83.33kg. Thus, the predictions of the weights are visualized as the red line in the figure below. Such lines are called *regression lines*. Regression lines are those lines where the sum of the red lines should be minimal. The slope of the regression line is called *coefficient* and the point where the regression line crosses the y-axis at x = 0 is called the *intercept*. Other important concepts in regression analysis are *variance* and *residuals*. *Residuals* are the distance between the line and the points (the red lines) and it is also called *variance*.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](regression_files/figure-html/intro01-1.png){width=672}\n:::\n:::\n\n\nSome words about the plots in the figure above: the upper left panel shows the raw observed data (black dots). The upper right panel shows the mean weight (blue line) and the residuals (in red). residuals are the distances from the expected or predicted values to the observed values (in this case the mean is the most most basic model which we use to predict values while the observed values simply represent the actual data points). The lower left panel shows observed values and the regression line, i.e, that line which, when drawn through the data points, will have the lowest sum of residuals. The lower right panel shows the regression line and the residuals, i.e. the distances between the expected or predicted values to the actual observed values (in red). Note that the sum of residuals in the lower right panel is much smaller than the sum of residuals in the upper right panel. This suggest that considering Height is a good idea as it explains a substantive amount of residual error and reduces the sum of residuals (or variance).^[I'm very grateful to Antonio Dello Iacono who pointed out that the plots require additional discussion.]\n\nNow that we are familiar with the basic principle of regression modeling - i.e. finding the line through data that has the smallest sum of residuals, we will apply this to a linguistic example.\n\n**Preparation and session set up**\n\nThis tutorial is based on R. If you have not installed R or are new to it, you will find an introduction to and more information how to use R [here](https://slcladal.github.io/intror.html). For this tutorials, we need to install certain *packages* from an R *library* so that the scripts shown below are executed without errors. Before turning to the code below, please install the packages by running the code below this paragraph. If you have already installed the packages mentioned below, then you can skip ahead and ignore this section. To install the necessary packages, simply run the following code - it may take some time (between 1 and 5 minutes to install all of the libraries so you do not need to worry if it takes some time).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# install\ninstall.packages(\"Boruta\")\ninstall.packages(\"car\")\ninstall.packages(\"emmeans\")\ninstall.packages(\"effects\")\ninstall.packages(\"flextable\")\ninstall.packages(\"ggplot2\")\ninstall.packages(\"ggpubr\")\ninstall.packages(\"Hmisc\")\ninstall.packages(\"knitr\")\ninstall.packages(\"lme4\")\ninstall.packages(\"MASS\")\ninstall.packages(\"mclogit\")\ninstall.packages(\"MuMIn\")\ninstall.packages(\"nlme\")\ninstall.packages(\"ordinal\")\ninstall.packages(\"rms\")\ninstall.packages(\"robustbase\")\ninstall.packages(\"sjPlot\")\ninstall.packages(\"stringr\")\ninstall.packages(\"tibble\")\ninstall.packages(\"dplyr\")\ninstall.packages(\"vcd\")\ninstall.packages(\"vip\")\n# install klippy for copy-to-clipboard button in code chunks\ninstall.packages(\"remotes\")\nremotes::install_github(\"rlesur/klippy\")\n```\n:::\n\n\nNow that we have installed the packages, we activate them as shown below.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# set options\noptions(stringsAsFactors = F)          # no automatic data transformation\noptions(\"scipen\" = 100, \"digits\" = 12) # suppress math annotation\n# load packages\nlibrary(Boruta)\nlibrary(car)\nlibrary(effects)\nlibrary(emmeans)\nlibrary(flextable)\nlibrary(ggfortify)\nlibrary(ggplot2)\nlibrary(ggpubr)\nlibrary(Hmisc)\nlibrary(knitr)\nlibrary(lme4)\nlibrary(MASS)\nlibrary(mclogit)\nlibrary(MuMIn)\nlibrary(nlme)\nlibrary(ordinal)\nlibrary(rms)\nlibrary(robustbase)\nlibrary(sjPlot)\nlibrary(stringr)\nlibrary(tibble)\nlibrary(vcd)\nlibrary(vip)\n# activate klippy for copy-to-clipboard button\nklippy::klippy()\n```\n\n::: {.cell-output-display}\n```{=html}\n<script>\n  addClassKlippyTo(\"pre.r, pre.markdown\");\n  addKlippy('left', 'top', 'auto', '1', 'Copy code', 'Copied!');\n</script>\n```\n:::\n:::\n\n\n\nOnce you have installed R and RStudio and initiated the session by executing the code shown above, you are good to go.\n\n# Fixed Effects Regression\n\nBefore turning to mixed-effects models which are able to represent hierarchical data structures, we will focus on traditional fixed effects regression models and begin with multiple linear regression. \n\n\n## Simple Linear Regression{-}\n \nThis section focuses on a very widely used statistical method which is called regression. Regressions are used when we try to understand how independent variables correlate with a dependent or outcome variable. So, if you want to investigate how a certain factor affects an outcome, then a regression is the way to go. We will have a look at two simple examples to understand what the concepts underlying a regression mean and how a regression works. The R code, that we will use, is adapted from many highly recommendable introductions which also focus on regression (among other types of analyses), for example, @gries2021statistics,  @winter2019statistics, @levshina2015linguistics, @winter2019statistics or @wilcox2009basic. @baayen2008analyzing is also very good but probably not the first book one should read about statistics but it is highly recommendable for advanced learners.\n\nAlthough the basic logic underlying regressions is identical to the conceptual underpinnings of *analysis of variance* (ANOVA), a related method, sociolinguists have traditionally favored regression analysis in their studies while ANOVAs have been the method of choice in psycholinguistics. The preference for either method is grounded in historical happenstances and the culture of these subdisciplines rather than in methodological reasoning. However, ANOVA are more restricted in that they can only take numeric dependent variables and they have stricter model assumptions that are violated more readily. In addition, a minor difference between regressions and ANOVA lies in the fact that regressions are based on the $t$-distribution while ANOVAs use the F-distribution (however, the F-value is simply the value of t squared or t^2^). Both t- and F-values report on the ratio between explained and unexplained variance.\n\nThe idea behind regression analysis is expressed formally in the equation below where$f_{(x)}$ is the y-value we want to predict, $\\alpha$ is the intercept (the point where the regression line crosses the y-axis at x = 0), $\\beta$ is the coefficient (the slope of the regression line). \n\n\\begin{equation}\nf_{(x)} = \\alpha + \\beta_{i}x + \\epsilon\n\\end{equation}\n\nIn other words, to estimate how much some weights who is 180cm tall, we would multiply the coefficient (slope of the line) with 180 (x) and add the value of the intercept (point where line crosses the y-axis at x = 0). \n\nHowever, the idea behind regressions can best be described graphically: imagine a cloud of points (like the points in the scatterplot in the upper left panel below). Regressions aim to find that line which has the minimal summed distance between points and the line (like the line in the lower panels). Technically speaking, the aim of a regression is to find the line with the minimal deviance (or the line with the minimal sum of residuals). Residuals are the distance between the line and the points (the red lines) and it is also called *variance*. \n\nThus, regression lines are those lines where the sum of the red lines should be minimal. The slope of the regression line is called *coefficient* and the point where the regression line crosses the y-axis at x = 0 is called the *intercept*.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](regression_files/figure-html/slr1-1.png){width=672}\n:::\n:::\n\n\nA word about standard errors (SE) is in order here because most commonly used statistics programs will provide SE values when reporting regression models. The SE is a measure that tells us how much the coefficients were to vary if the same regression were applied to many samples from the same population. A relatively small SE value therefore indicates that the coefficients will remain very stable if the same regression model is fitted to many different samples with identical parameters. In contrast, a large SE tells you that the model is volatile and not very stable or reliable as the coefficients vary substantially if the model is applied to many samples. \n\n\nMathematically, the SE is the standard deviation (SD) divided by the square root of the sample size (N) (see below).The SD is the square root of the deviance (that is, the SD is the square root of the sum of the mean $\\bar{x}$ minus each data point (x~i~) squared divided by the sample size (N) minus 1).\n\n\\begin{equation}\nStandard Error (SE) = \\frac{\\sum (\\bar{x}-x_{i})^2/N-1}{\\sqrt{N}} = \\frac{SD}{\\sqrt{N}}\n\\end{equation}\n\n### Example 1: Preposition Use across Real-Time{-}\n\nWe will now turn to our first example. In this example, we will investigate whether the frequency of prepositions has changed from Middle English to Late Modern English. The reasoning behind this example is that Old English was highly synthetic compared with Present-Day English which comparatively analytic. In other words, while Old English speakers used case to indicate syntactic relations, speakers of Present-Day English use word order and prepositions to indicate syntactic relationships. This means that the loss of case had to be compensated by different strategies and maybe these strategies continued to develop and increase in frequency even after the change from synthetic to analytic had been mostly accomplished. And this prolonged change in compensatory strategies is what this example will focus on. \n\nThe analysis is based on data extracted from the *Penn Corpora of Historical English* (see http://www.ling.upenn.edu/hist-corpora/), that consists of 603 texts written between 1125 and 1900. In preparation of this example, all elements that were part-of-speech tagged as prepositions were extracted from the PennCorpora. \n\nThen, the relative frequencies (per 1,000 words) of prepositions per text were calculated. This frequency of prepositions per 1,000 words represents our dependent variable. In a next step, the date when each letter had been written was extracted. The resulting two vectors were combined into a table which thus contained for each text, when it was written (independent variable) and its relative frequency of prepositions (dependent or outcome variable).\n\nA regression analysis will follow the steps described below: \n\n1. Extraction and processing of the data\n\n2. Data visualization\n\n3. Applying the regression analysis to the data\n\n4. Diagnosing the regression model and checking whether or not basic model assumptions have been violated.\n\nIn a first step, we load functions that we may need (which in this case is a function that we will use to summarize the results of the analysis).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# load functions\nsource(\"https://slcladal.github.io/rscripts/slrsummary.r\")\n```\n:::\n\n\nAfter preparing our session, we can now load and inspect the data to get a first impression of its properties.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# load data\nslrdata  <- base::readRDS(url(\"https://slcladal.github.io/data/sld.rda\", \"rb\"))\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n```{=html}\n<div class=\"tabwid\"><style>.cl-b4d124b6{table-layout:auto;width:75%;}.cl-b4cb17b0{font-family:'DejaVu Sans';font-size:12pt;font-weight:bold;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-b4cb17c4{font-family:'DejaVu Sans';font-size:12pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-b4cb2d90{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-b4cb2d9a{margin:0;text-align:center;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-b4cb86aa{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b4cb86b4{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b4cb86be{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b4cb86c8{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b4cb86d2{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b4cb86dc{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b4cb86e6{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b4cb86e7{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b4cb86f0{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b4cb86fa{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b4cb86fb{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b4cb870e{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b4cb870f{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b4cb8718{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b4cb8719{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b4cb8722{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}</style><table class='cl-b4d124b6'>\n```\n<caption class=\"Table Caption\">\n\nFirst 15 rows of slrdata.\n\n</caption>\n```{=html}\n<thead><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-b4cb870f\"><p class=\"cl-b4cb2d90\"><span class=\"cl-b4cb17b0\">Date</span></p></td><td class=\"cl-b4cb8718\"><p class=\"cl-b4cb2d9a\"><span class=\"cl-b4cb17b0\">Genre</span></p></td><td class=\"cl-b4cb8718\"><p class=\"cl-b4cb2d9a\"><span class=\"cl-b4cb17b0\">Text</span></p></td><td class=\"cl-b4cb8719\"><p class=\"cl-b4cb2d90\"><span class=\"cl-b4cb17b0\">Prepositions</span></p></td><td class=\"cl-b4cb8722\"><p class=\"cl-b4cb2d9a\"><span class=\"cl-b4cb17b0\">Region</span></p></td></tr></thead><tbody><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-b4cb86c8\"><p class=\"cl-b4cb2d90\"><span class=\"cl-b4cb17c4\">1,736</span></p></td><td class=\"cl-b4cb86aa\"><p class=\"cl-b4cb2d9a\"><span class=\"cl-b4cb17c4\">Science</span></p></td><td class=\"cl-b4cb86aa\"><p class=\"cl-b4cb2d9a\"><span class=\"cl-b4cb17c4\">albin</span></p></td><td class=\"cl-b4cb86b4\"><p class=\"cl-b4cb2d90\"><span class=\"cl-b4cb17c4\">166.01</span></p></td><td class=\"cl-b4cb86be\"><p class=\"cl-b4cb2d9a\"><span class=\"cl-b4cb17c4\">North</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-b4cb86e6\"><p class=\"cl-b4cb2d90\"><span class=\"cl-b4cb17c4\">1,711</span></p></td><td class=\"cl-b4cb86d2\"><p class=\"cl-b4cb2d9a\"><span class=\"cl-b4cb17c4\">Education</span></p></td><td class=\"cl-b4cb86d2\"><p class=\"cl-b4cb2d9a\"><span class=\"cl-b4cb17c4\">anon</span></p></td><td class=\"cl-b4cb86e7\"><p class=\"cl-b4cb2d90\"><span class=\"cl-b4cb17c4\">139.86</span></p></td><td class=\"cl-b4cb86dc\"><p class=\"cl-b4cb2d9a\"><span class=\"cl-b4cb17c4\">North</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-b4cb86c8\"><p class=\"cl-b4cb2d90\"><span class=\"cl-b4cb17c4\">1,808</span></p></td><td class=\"cl-b4cb86aa\"><p class=\"cl-b4cb2d9a\"><span class=\"cl-b4cb17c4\">PrivateLetter</span></p></td><td class=\"cl-b4cb86aa\"><p class=\"cl-b4cb2d9a\"><span class=\"cl-b4cb17c4\">austen</span></p></td><td class=\"cl-b4cb86b4\"><p class=\"cl-b4cb2d90\"><span class=\"cl-b4cb17c4\">130.78</span></p></td><td class=\"cl-b4cb86be\"><p class=\"cl-b4cb2d9a\"><span class=\"cl-b4cb17c4\">North</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-b4cb86e6\"><p class=\"cl-b4cb2d90\"><span class=\"cl-b4cb17c4\">1,878</span></p></td><td class=\"cl-b4cb86d2\"><p class=\"cl-b4cb2d9a\"><span class=\"cl-b4cb17c4\">Education</span></p></td><td class=\"cl-b4cb86d2\"><p class=\"cl-b4cb2d9a\"><span class=\"cl-b4cb17c4\">bain</span></p></td><td class=\"cl-b4cb86e7\"><p class=\"cl-b4cb2d90\"><span class=\"cl-b4cb17c4\">151.29</span></p></td><td class=\"cl-b4cb86dc\"><p class=\"cl-b4cb2d9a\"><span class=\"cl-b4cb17c4\">North</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-b4cb86c8\"><p class=\"cl-b4cb2d90\"><span class=\"cl-b4cb17c4\">1,743</span></p></td><td class=\"cl-b4cb86aa\"><p class=\"cl-b4cb2d9a\"><span class=\"cl-b4cb17c4\">Education</span></p></td><td class=\"cl-b4cb86aa\"><p class=\"cl-b4cb2d9a\"><span class=\"cl-b4cb17c4\">barclay</span></p></td><td class=\"cl-b4cb86b4\"><p class=\"cl-b4cb2d90\"><span class=\"cl-b4cb17c4\">145.72</span></p></td><td class=\"cl-b4cb86be\"><p class=\"cl-b4cb2d9a\"><span class=\"cl-b4cb17c4\">North</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-b4cb86e6\"><p class=\"cl-b4cb2d90\"><span class=\"cl-b4cb17c4\">1,908</span></p></td><td class=\"cl-b4cb86d2\"><p class=\"cl-b4cb2d9a\"><span class=\"cl-b4cb17c4\">Education</span></p></td><td class=\"cl-b4cb86d2\"><p class=\"cl-b4cb2d9a\"><span class=\"cl-b4cb17c4\">benson</span></p></td><td class=\"cl-b4cb86e7\"><p class=\"cl-b4cb2d90\"><span class=\"cl-b4cb17c4\">120.77</span></p></td><td class=\"cl-b4cb86dc\"><p class=\"cl-b4cb2d9a\"><span class=\"cl-b4cb17c4\">North</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-b4cb86c8\"><p class=\"cl-b4cb2d90\"><span class=\"cl-b4cb17c4\">1,906</span></p></td><td class=\"cl-b4cb86aa\"><p class=\"cl-b4cb2d9a\"><span class=\"cl-b4cb17c4\">Diary</span></p></td><td class=\"cl-b4cb86aa\"><p class=\"cl-b4cb2d9a\"><span class=\"cl-b4cb17c4\">benson</span></p></td><td class=\"cl-b4cb86b4\"><p class=\"cl-b4cb2d90\"><span class=\"cl-b4cb17c4\">119.17</span></p></td><td class=\"cl-b4cb86be\"><p class=\"cl-b4cb2d9a\"><span class=\"cl-b4cb17c4\">North</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-b4cb86e6\"><p class=\"cl-b4cb2d90\"><span class=\"cl-b4cb17c4\">1,897</span></p></td><td class=\"cl-b4cb86d2\"><p class=\"cl-b4cb2d9a\"><span class=\"cl-b4cb17c4\">Philosophy</span></p></td><td class=\"cl-b4cb86d2\"><p class=\"cl-b4cb2d9a\"><span class=\"cl-b4cb17c4\">boethja</span></p></td><td class=\"cl-b4cb86e7\"><p class=\"cl-b4cb2d90\"><span class=\"cl-b4cb17c4\">132.96</span></p></td><td class=\"cl-b4cb86dc\"><p class=\"cl-b4cb2d9a\"><span class=\"cl-b4cb17c4\">North</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-b4cb86c8\"><p class=\"cl-b4cb2d90\"><span class=\"cl-b4cb17c4\">1,785</span></p></td><td class=\"cl-b4cb86aa\"><p class=\"cl-b4cb2d9a\"><span class=\"cl-b4cb17c4\">Philosophy</span></p></td><td class=\"cl-b4cb86aa\"><p class=\"cl-b4cb2d9a\"><span class=\"cl-b4cb17c4\">boethri</span></p></td><td class=\"cl-b4cb86b4\"><p class=\"cl-b4cb2d90\"><span class=\"cl-b4cb17c4\">130.49</span></p></td><td class=\"cl-b4cb86be\"><p class=\"cl-b4cb2d9a\"><span class=\"cl-b4cb17c4\">North</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-b4cb86e6\"><p class=\"cl-b4cb2d90\"><span class=\"cl-b4cb17c4\">1,776</span></p></td><td class=\"cl-b4cb86d2\"><p class=\"cl-b4cb2d9a\"><span class=\"cl-b4cb17c4\">Diary</span></p></td><td class=\"cl-b4cb86d2\"><p class=\"cl-b4cb2d9a\"><span class=\"cl-b4cb17c4\">boswell</span></p></td><td class=\"cl-b4cb86e7\"><p class=\"cl-b4cb2d90\"><span class=\"cl-b4cb17c4\">135.94</span></p></td><td class=\"cl-b4cb86dc\"><p class=\"cl-b4cb2d9a\"><span class=\"cl-b4cb17c4\">North</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-b4cb86c8\"><p class=\"cl-b4cb2d90\"><span class=\"cl-b4cb17c4\">1,905</span></p></td><td class=\"cl-b4cb86aa\"><p class=\"cl-b4cb2d9a\"><span class=\"cl-b4cb17c4\">Travel</span></p></td><td class=\"cl-b4cb86aa\"><p class=\"cl-b4cb2d9a\"><span class=\"cl-b4cb17c4\">bradley</span></p></td><td class=\"cl-b4cb86b4\"><p class=\"cl-b4cb2d90\"><span class=\"cl-b4cb17c4\">154.20</span></p></td><td class=\"cl-b4cb86be\"><p class=\"cl-b4cb2d9a\"><span class=\"cl-b4cb17c4\">North</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-b4cb86e6\"><p class=\"cl-b4cb2d90\"><span class=\"cl-b4cb17c4\">1,711</span></p></td><td class=\"cl-b4cb86d2\"><p class=\"cl-b4cb2d9a\"><span class=\"cl-b4cb17c4\">Education</span></p></td><td class=\"cl-b4cb86d2\"><p class=\"cl-b4cb2d9a\"><span class=\"cl-b4cb17c4\">brightland</span></p></td><td class=\"cl-b4cb86e7\"><p class=\"cl-b4cb2d90\"><span class=\"cl-b4cb17c4\">149.14</span></p></td><td class=\"cl-b4cb86dc\"><p class=\"cl-b4cb2d9a\"><span class=\"cl-b4cb17c4\">North</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-b4cb86c8\"><p class=\"cl-b4cb2d90\"><span class=\"cl-b4cb17c4\">1,762</span></p></td><td class=\"cl-b4cb86aa\"><p class=\"cl-b4cb2d9a\"><span class=\"cl-b4cb17c4\">Sermon</span></p></td><td class=\"cl-b4cb86aa\"><p class=\"cl-b4cb2d9a\"><span class=\"cl-b4cb17c4\">burton</span></p></td><td class=\"cl-b4cb86b4\"><p class=\"cl-b4cb2d90\"><span class=\"cl-b4cb17c4\">159.71</span></p></td><td class=\"cl-b4cb86be\"><p class=\"cl-b4cb2d9a\"><span class=\"cl-b4cb17c4\">North</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-b4cb86e6\"><p class=\"cl-b4cb2d90\"><span class=\"cl-b4cb17c4\">1,726</span></p></td><td class=\"cl-b4cb86d2\"><p class=\"cl-b4cb2d9a\"><span class=\"cl-b4cb17c4\">Sermon</span></p></td><td class=\"cl-b4cb86d2\"><p class=\"cl-b4cb2d9a\"><span class=\"cl-b4cb17c4\">butler</span></p></td><td class=\"cl-b4cb86e7\"><p class=\"cl-b4cb2d90\"><span class=\"cl-b4cb17c4\">157.49</span></p></td><td class=\"cl-b4cb86dc\"><p class=\"cl-b4cb2d9a\"><span class=\"cl-b4cb17c4\">North</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-b4cb86f0\"><p class=\"cl-b4cb2d90\"><span class=\"cl-b4cb17c4\">1,835</span></p></td><td class=\"cl-b4cb86fa\"><p class=\"cl-b4cb2d9a\"><span class=\"cl-b4cb17c4\">PrivateLetter</span></p></td><td class=\"cl-b4cb86fa\"><p class=\"cl-b4cb2d9a\"><span class=\"cl-b4cb17c4\">carlyle</span></p></td><td class=\"cl-b4cb86fb\"><p class=\"cl-b4cb2d90\"><span class=\"cl-b4cb17c4\">124.16</span></p></td><td class=\"cl-b4cb870e\"><p class=\"cl-b4cb2d9a\"><span class=\"cl-b4cb17c4\">North</span></p></td></tr></tbody></table></div>\n```\n:::\n:::\n\n\n\nInspecting the data is very important because it can happen that a data set may not load completely or that variables which should be numeric have been converted to character variables. If unchecked, then such issues could go unnoticed and cause trouble.\n\nWe will now plot the data to get a better understanding of what the data looks like.\n\n\n::: {.cell}\n\n```{.r .cell-code}\np1 <- ggplot(slrdata, aes(Date, Prepositions)) +\n  geom_point() +\n  theme_bw() +\n  labs(x = \"Year\") +\n  labs(y = \"Prepositions per 1,000 words\") +\n  geom_smooth()\np2 <- ggplot(slrdata, aes(Date, Prepositions)) +\n  geom_point() +\n  theme_bw() +\n  labs(x = \"Year\") +\n  labs(y = \"Prepositions per 1,000 words\") +\n  geom_smooth(method = \"lm\") # with linear model smoothing!\n# display plots\nggpubr::ggarrange(p1, p2, ncol = 2, nrow = 1)\n```\n\n::: {.cell-output-display}\n![](regression_files/figure-html/slr6-1.png){width=672}\n:::\n:::\n\n\nBefore beginning with the regression analysis, we will center the year. We center the values of year by subtracting each value from the mean of year. This can be useful when dealing with numeric variables because if we did not center year, we would get estimated values for year 0 (a year when English did not even exist yet). If a variable is centered, the regression provides estimates of the model refer to the mean of that numeric variable. In other words, centering can be very helpful, especially with respect to the interpretation of the results that regression models report.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# center date\nslrdata$Date <- slrdata$Date - mean(slrdata$Date) \n```\n:::\n\n\nWe will now begin the regression analysis by generating a first regression model and inspect its results. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# create initial model\nm1.lm <- lm(Prepositions ~ Date, data = slrdata)\n# inspect results\nsummary(m1.lm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = Prepositions ~ Date, data = slrdata)\n\nResiduals:\n        Min          1Q      Median          3Q         Max \n-69.1012471 -13.8549421   0.5779091  13.3208913  62.8580401 \n\nCoefficients:\n                   Estimate      Std. Error   t value             Pr(>|t|)    \n(Intercept) 132.19009310987   0.83863748040 157.62483 < 0.0000000000000002 ***\nDate          0.01732180307   0.00726746646   2.38347             0.017498 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 19.4339648 on 535 degrees of freedom\nMultiple R-squared:  0.010507008,\tAdjusted R-squared:  0.00865748837 \nF-statistic: 5.68093894 on 1 and 535 DF,  p-value: 0.017498081\n```\n:::\n:::\n\n\nThe summary output starts by repeating the regression equation. Then, the model provides the distribution of the residuals. The residuals should be distributed normally with the absolute values of the Min and Max as well as the 1Q (first quartile) and 3Q (third quartile) being similar or ideally identical. In our case, the values are very similar which suggests that the residuals are distributed evenly and follow a normal distribution. The next part of the report is the coefficients table. The estimate for the intercept is the value of y at x = 0. The estimate for Date represents the slope of the regression line and tells us that with each year, the predicted frequency of prepositions increase by .01732 prepositions. The t-value is the Estimate divided by the standard error (Std. Error). Based on the t-value, the p-value can be calculated manually as shown below.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# use pt function (which uses t-values and the degrees of freedom)\n2*pt(-2.383, nrow(slrdata)-1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.0175196401501\n```\n:::\n:::\n\n\nThe R^2^-values tell us how much variance is explained by our model. The baseline value represents a model that uses merely the mean. 0.0105 means that our model explains only 1.05 percent of the variance (0.010 x 100) - which is a tiny amount. The problem of the multiple R^2^ is that it will increase even if we add variables that explain almost no variance. Hence, multiple R^2^ encourages the inclusion of *junk* variables.\n\n\\begin{equation}\nR^2 = R^2_{multiple} = 1 - \\frac{\\sum (y_i - \\hat{y_i})^2}{\\sum (y_i - \\bar y)^2}\n\\end{equation}\n\nThe adjusted R^2^-value takes the number of predictors into account and, thus, the adjusted R^2^ will always be lower than the multiple R^2^. This is so because the adjusted R^2^ penalizes models for having predictors. The equation for the adjusted R^2^ below shows that the amount of variance that is explained by all the variables in the model (the top part of the fraction) must outweigh the inclusion of the number of variables (k) (lower part of the fraction). Thus, the  adjusted R^2^ will decrease when variables are added that explain little or even no variance while it will increase if variables are added that explain a lot of variance.\n\n\\begin{equation}\nR^2_{adjusted} = 1 - (\\frac{(1 - R^2)(n - 1)}{n - k - 1})\n\\end{equation}\n\nIf there is a big difference between the two R^2^-values, then the model contains (many) predictors that do not explain much variance which is not good. The F-statistic and the associated p-value tell us that the model, despite explaining almost no variance, is still significantly better than an intercept-only base-line model (or using the overall mean to predict the frequency of prepositions per text).\n\nWe can test this and also see where the F-values comes from by comparing the \n \n\n::: {.cell}\n\n```{.r .cell-code}\n# create intercept-only base-line model\nm0.lm <- lm(Prepositions ~ 1, data = slrdata)\n# compare the base-line and the more saturated model\nanova(m1.lm, m0.lm, test = \"F\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Variance Table\n\nModel 1: Prepositions ~ Date\nModel 2: Prepositions ~ 1\n  Res.Df         RSS Df   Sum of Sq       F   Pr(>F)  \n1    535 202058.2576                                  \n2    536 204203.8289 -1 -2145.57126 5.68094 0.017498 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\nThe F- and p-values are exactly those reported by the summary which shows where the F-values comes from and what it means; namely it denote the difference between the base-line and the more saturated model.\n\n\nThe degrees of freedom associated with the residual standard error are the number of cases in the model minus the number of predictors (including the intercept). The residual standard error is square root of the sum of the squared residuals of the model divided by the degrees of freedom. Have a look at he following to clear this up:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# DF = N - number of predictors (including intercept)\nDegreesOfFreedom <- nrow(slrdata)-length(coef(m1.lm))\n# sum of the squared residuals\nSumSquaredResiduals <- sum(resid(m1.lm)^2)\n# Residual Standard Error\nsqrt(SumSquaredResiduals/DegreesOfFreedom); DegreesOfFreedom\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 19.4339647585\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 535\n```\n:::\n:::\n\n\nWe will now check if mathematical assumptions have been violated (homogeneity of variance) or whether the data contains outliers. We check this using diagnostic plots.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# generate data\ndf2 <- data.frame(id = 1:length(resid(m1.lm)),\n                 residuals = resid(m1.lm),\n                 standard = rstandard(m1.lm),\n                 studend = rstudent(m1.lm))\n# generate plots\np1 <- ggplot(df2, aes(x = id, y = residuals)) + \n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +\n  geom_point() +\n  labs(y = \"Residuals\", x = \"Index\")\np2 <- ggplot(df2, aes(x = id, y = standard)) + \n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +\n  geom_point() +\n  labs(y = \"Standardized Residuals\", x = \"Index\")\np3 <- ggplot(df2, aes(x = id, y = studend)) + \n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +\n  geom_point() +\n  labs(y = \"Studentized Residuals\", x = \"Index\")\n# display plots\nggpubr::ggarrange(p1, p2, p3, ncol = 3, nrow = 1)\n```\n\n::: {.cell-output-display}\n![](regression_files/figure-html/slr12-1.png){width=672}\n:::\n:::\n\n\n\nThe left graph shows the residuals of the model (i.e., the differences between the observed and the values predicted by the regression model). The problem with this plot is that the residuals are not standardized and so they cannot be compared to the residuals of other models. To remedy this deficiency, residuals are normalized by dividing the residuals by their standard deviation. Then, the normalized residuals can be plotted against the observed values (center panel). In this way, not only are standardized residuals obtained, but the values of the residuals are transformed into z-values, and one can use the z-distribution to find problematic data points. There are three rules of thumb regarding finding problematic data points through standardized residuals [@field2012discovering 268-269]:\n\n* Points with values higher than 3.29 should be removed from the data.\n\n* If more than 1% of the data points have values higher than 2.58, then the error rate of our model is too high.\n\n* If more than 5% of the data points have values greater than 1.96, then the error rate of our model is too high.\n\n\nThe right panel shows the * studentized residuals* (adjusted predicted values: each data point is divided by the standard error of the residuals). In this way, it is possible to use Student's t-distribution to diagnose our model.\n\nAdjusted predicted values are residuals of a special kind: the model is calculated without a data point and then used to predict this data point. The difference between the observed data point and its predicted value is then called the adjusted predicted value. In summary, studentized residuals are very useful because they allow us to identify influential data points.\n\nThe plots show that there are two potentially problematic data points (the top-most and bottom-most point). These two points are clearly different from the other data points and may therefore be outliers. We will test later if these points need to be removed.\n\nWe will now generate more diagnostic plots.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# generate plots\nautoplot(m1.lm) + \n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) \n```\n\n::: {.cell-output-display}\n![](regression_files/figure-html/slr13-1.png){width=672}\n:::\n:::\n\n\n\nThe diagnostic plots are very positive and we will go through why this is so for each panel. The graph in the upper left panel is useful for finding outliers or for determining the correlation between residuals and predicted values: when a trend becomes visible in the line or points (e.g., a rising trend or a zigzag line), then this would indicate that the model would be problematic (in such cases, it can help to remove data points that are too influential (outliers)).\n\nThe graphic in the upper right panel indicates whether the residuals are normally distributed (which is desirable) or whether the residuals do not follow a normal distribution. If the points lie on the line, the residuals follow a normal distribution. For example, if the points are not on the line at the top and bottom, it shows that the model does not predict small and large values well and that it therefore does not have a good fit.\n\nThe graphic in the lower left panel provides information about *homoscedasticity*. Homoscedasticity means that the variance of the residuals remains constant and does not correlate with any independent variable. In unproblematic cases, the graphic shows a flat line. If there is a trend in the line, we are dealing with heteroscedasticity, that is, a correlation between independent variables and the residuals, which is very problematic for regressions.\n\nThe graph in the lower right panel shows problematic influential data points that disproportionately affect the regression (this would be problematic). If such influential data points are present, they should be either weighted (one could generate a robust rather than a simple linear regression) or they must be removed. The graph displays Cook's distance, which shows how the regression changes when a model without this data point is calculated. The cook distance thus shows the influence a data point has on the regression as a whole. Data points that have a Cook's distance value greater than 1 are problematic [@field2012discovering 269].\n\nThe so-called leverage is also a measure that indicates how strongly a data point affects the accuracy of the regression. Leverage values range between 0 (no influence) and 1 (strong influence: suboptimal!). To test whether a specific data point has a high leverage value, we calculate a cut-off point that indicates whether the leverage is too strong or still acceptable. The following two formulas are used for this:\n\n\\begin{equation}\nLeverage = \\frac{3(k + 1)}{n} |  \\frac{2(k + 1)}{n}\n\\end{equation}\n\nWe will look more closely at leverage in the context of multiple linear regression and will therefore end the current analysis by summarizing the results of the regression analysis in a table.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# create summary table\nslrsummary(m1.lm)  \n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n```{=html}\n<div class=\"tabwid\"><style>.cl-b5b77984{table-layout:auto;width:75%;}.cl-b5b32aa0{font-family:'DejaVu Sans';font-size:12pt;font-weight:bold;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-b5b32aaa{font-family:'DejaVu Sans';font-size:12pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-b5b33748{margin:0;text-align:center;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-b5b361f0{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b5b361fa{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b5b361fb{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b5b361fc{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b5b36204{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b5b3620e{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b5b3620f{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b5b36218{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b5b36219{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b5b36222{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b5b36223{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b5b3622c{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}</style><table class='cl-b5b77984'>\n```\n<caption class=\"Table Caption\">\n\nResults of a simple linear regression analysis.\n\n</caption>\n```{=html}\n<thead><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-b5b3622c\"><p class=\"cl-b5b33748\"><span class=\"cl-b5b32aa0\">Parameters</span></p></td><td class=\"cl-b5b36222\"><p class=\"cl-b5b33748\"><span class=\"cl-b5b32aa0\">Estimate</span></p></td><td class=\"cl-b5b36222\"><p class=\"cl-b5b33748\"><span class=\"cl-b5b32aa0\">Pearson's r</span></p></td><td class=\"cl-b5b36222\"><p class=\"cl-b5b33748\"><span class=\"cl-b5b32aa0\">Std. Error</span></p></td><td class=\"cl-b5b36222\"><p class=\"cl-b5b33748\"><span class=\"cl-b5b32aa0\">t value</span></p></td><td class=\"cl-b5b36222\"><p class=\"cl-b5b33748\"><span class=\"cl-b5b32aa0\">Pr(&gt;|t|)</span></p></td><td class=\"cl-b5b36223\"><p class=\"cl-b5b33748\"><span class=\"cl-b5b32aa0\">P-value sig.</span></p></td></tr></thead><tbody><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-b5b361fb\"><p class=\"cl-b5b33748\"><span class=\"cl-b5b32aaa\">(Intercept)</span></p></td><td class=\"cl-b5b361f0\"><p class=\"cl-b5b33748\"><span class=\"cl-b5b32aaa\">132.19</span></p></td><td class=\"cl-b5b361f0\"><p class=\"cl-b5b33748\"><span class=\"cl-b5b32aaa\"></span></p></td><td class=\"cl-b5b361f0\"><p class=\"cl-b5b33748\"><span class=\"cl-b5b32aaa\">0.84</span></p></td><td class=\"cl-b5b361f0\"><p class=\"cl-b5b33748\"><span class=\"cl-b5b32aaa\">157.62</span></p></td><td class=\"cl-b5b361f0\"><p class=\"cl-b5b33748\"><span class=\"cl-b5b32aaa\">0</span></p></td><td class=\"cl-b5b361fa\"><p class=\"cl-b5b33748\"><span class=\"cl-b5b32aaa\">p &lt; .001***</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-b5b361fc\"><p class=\"cl-b5b33748\"><span class=\"cl-b5b32aaa\">Date</span></p></td><td class=\"cl-b5b36204\"><p class=\"cl-b5b33748\"><span class=\"cl-b5b32aaa\">0.02</span></p></td><td class=\"cl-b5b36204\"><p class=\"cl-b5b33748\"><span class=\"cl-b5b32aaa\">0.1</span></p></td><td class=\"cl-b5b36204\"><p class=\"cl-b5b33748\"><span class=\"cl-b5b32aaa\">0.01</span></p></td><td class=\"cl-b5b36204\"><p class=\"cl-b5b33748\"><span class=\"cl-b5b32aaa\">2.38</span></p></td><td class=\"cl-b5b36204\"><p class=\"cl-b5b33748\"><span class=\"cl-b5b32aaa\">0.0175</span></p></td><td class=\"cl-b5b3620e\"><p class=\"cl-b5b33748\"><span class=\"cl-b5b32aaa\">p &lt; .05*</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-b5b361fb\"><p class=\"cl-b5b33748\"><span class=\"cl-b5b32aaa\">Model statistics</span></p></td><td class=\"cl-b5b361f0\"><p class=\"cl-b5b33748\"><span class=\"cl-b5b32aaa\"></span></p></td><td class=\"cl-b5b361f0\"><p class=\"cl-b5b33748\"><span class=\"cl-b5b32aaa\"></span></p></td><td class=\"cl-b5b361f0\"><p class=\"cl-b5b33748\"><span class=\"cl-b5b32aaa\"></span></p></td><td class=\"cl-b5b361f0\"><p class=\"cl-b5b33748\"><span class=\"cl-b5b32aaa\"></span></p></td><td class=\"cl-b5b361f0\"><p class=\"cl-b5b33748\"><span class=\"cl-b5b32aaa\"></span></p></td><td class=\"cl-b5b361fa\"><p class=\"cl-b5b33748\"><span class=\"cl-b5b32aaa\">Value</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-b5b361fc\"><p class=\"cl-b5b33748\"><span class=\"cl-b5b32aaa\">Number of cases in model</span></p></td><td class=\"cl-b5b36204\"><p class=\"cl-b5b33748\"><span class=\"cl-b5b32aaa\"></span></p></td><td class=\"cl-b5b36204\"><p class=\"cl-b5b33748\"><span class=\"cl-b5b32aaa\"></span></p></td><td class=\"cl-b5b36204\"><p class=\"cl-b5b33748\"><span class=\"cl-b5b32aaa\"></span></p></td><td class=\"cl-b5b36204\"><p class=\"cl-b5b33748\"><span class=\"cl-b5b32aaa\"></span></p></td><td class=\"cl-b5b36204\"><p class=\"cl-b5b33748\"><span class=\"cl-b5b32aaa\"></span></p></td><td class=\"cl-b5b3620e\"><p class=\"cl-b5b33748\"><span class=\"cl-b5b32aaa\">537</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-b5b361fb\"><p class=\"cl-b5b33748\"><span class=\"cl-b5b32aaa\">Residual standard error on 535 DF</span></p></td><td class=\"cl-b5b361f0\"><p class=\"cl-b5b33748\"><span class=\"cl-b5b32aaa\"></span></p></td><td class=\"cl-b5b361f0\"><p class=\"cl-b5b33748\"><span class=\"cl-b5b32aaa\"></span></p></td><td class=\"cl-b5b361f0\"><p class=\"cl-b5b33748\"><span class=\"cl-b5b32aaa\"></span></p></td><td class=\"cl-b5b361f0\"><p class=\"cl-b5b33748\"><span class=\"cl-b5b32aaa\"></span></p></td><td class=\"cl-b5b361f0\"><p class=\"cl-b5b33748\"><span class=\"cl-b5b32aaa\"></span></p></td><td class=\"cl-b5b361fa\"><p class=\"cl-b5b33748\"><span class=\"cl-b5b32aaa\">19.43</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-b5b361fc\"><p class=\"cl-b5b33748\"><span class=\"cl-b5b32aaa\">Multiple R-squared</span></p></td><td class=\"cl-b5b36204\"><p class=\"cl-b5b33748\"><span class=\"cl-b5b32aaa\"></span></p></td><td class=\"cl-b5b36204\"><p class=\"cl-b5b33748\"><span class=\"cl-b5b32aaa\"></span></p></td><td class=\"cl-b5b36204\"><p class=\"cl-b5b33748\"><span class=\"cl-b5b32aaa\"></span></p></td><td class=\"cl-b5b36204\"><p class=\"cl-b5b33748\"><span class=\"cl-b5b32aaa\"></span></p></td><td class=\"cl-b5b36204\"><p class=\"cl-b5b33748\"><span class=\"cl-b5b32aaa\"></span></p></td><td class=\"cl-b5b3620e\"><p class=\"cl-b5b33748\"><span class=\"cl-b5b32aaa\">0.0105</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-b5b361fb\"><p class=\"cl-b5b33748\"><span class=\"cl-b5b32aaa\">Adjusted R-squared</span></p></td><td class=\"cl-b5b361f0\"><p class=\"cl-b5b33748\"><span class=\"cl-b5b32aaa\"></span></p></td><td class=\"cl-b5b361f0\"><p class=\"cl-b5b33748\"><span class=\"cl-b5b32aaa\"></span></p></td><td class=\"cl-b5b361f0\"><p class=\"cl-b5b33748\"><span class=\"cl-b5b32aaa\"></span></p></td><td class=\"cl-b5b361f0\"><p class=\"cl-b5b33748\"><span class=\"cl-b5b32aaa\"></span></p></td><td class=\"cl-b5b361f0\"><p class=\"cl-b5b33748\"><span class=\"cl-b5b32aaa\"></span></p></td><td class=\"cl-b5b361fa\"><p class=\"cl-b5b33748\"><span class=\"cl-b5b32aaa\">0.0087</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-b5b361fc\"><p class=\"cl-b5b33748\"><span class=\"cl-b5b32aaa\">F-statistic (1, 535)</span></p></td><td class=\"cl-b5b36204\"><p class=\"cl-b5b33748\"><span class=\"cl-b5b32aaa\"></span></p></td><td class=\"cl-b5b36204\"><p class=\"cl-b5b33748\"><span class=\"cl-b5b32aaa\"></span></p></td><td class=\"cl-b5b36204\"><p class=\"cl-b5b33748\"><span class=\"cl-b5b32aaa\"></span></p></td><td class=\"cl-b5b36204\"><p class=\"cl-b5b33748\"><span class=\"cl-b5b32aaa\"></span></p></td><td class=\"cl-b5b36204\"><p class=\"cl-b5b33748\"><span class=\"cl-b5b32aaa\"></span></p></td><td class=\"cl-b5b3620e\"><p class=\"cl-b5b33748\"><span class=\"cl-b5b32aaa\">5.68</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-b5b36219\"><p class=\"cl-b5b33748\"><span class=\"cl-b5b32aaa\">Model p-value</span></p></td><td class=\"cl-b5b3620f\"><p class=\"cl-b5b33748\"><span class=\"cl-b5b32aaa\"></span></p></td><td class=\"cl-b5b3620f\"><p class=\"cl-b5b33748\"><span class=\"cl-b5b32aaa\"></span></p></td><td class=\"cl-b5b3620f\"><p class=\"cl-b5b33748\"><span class=\"cl-b5b32aaa\"></span></p></td><td class=\"cl-b5b3620f\"><p class=\"cl-b5b33748\"><span class=\"cl-b5b32aaa\"></span></p></td><td class=\"cl-b5b3620f\"><p class=\"cl-b5b33748\"><span class=\"cl-b5b32aaa\"></span></p></td><td class=\"cl-b5b36218\"><p class=\"cl-b5b33748\"><span class=\"cl-b5b32aaa\">0.0175</span></p></td></tr></tbody></table></div>\n```\n:::\n:::\n\n\n\nAn alternative but less informative summary table of the results of a regression analysis can be generated using the `tab_model` function from the `sjPlot` package [@sjPlot] (as is shown below).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# generate summary table\nsjPlot::tab_model(m1.lm) \n```\n\n::: {.cell-output-display}\n`````{=html}\n<table style=\"border-collapse:collapse; border:none;\">\n<tr>\n<th style=\"border-top: double; text-align:center; font-style:normal; font-weight:bold; padding:0.2cm;  text-align:left; \">&nbsp;</th>\n<th colspan=\"3\" style=\"border-top: double; text-align:center; font-style:normal; font-weight:bold; padding:0.2cm; \">Prepositions</th>\n</tr>\n<tr>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  text-align:left; \">Predictors</td>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  \">Estimates</td>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  \">CI</td>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  \">p</td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; \">(Intercept)</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">132.19</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">130.54&nbsp;&ndash;&nbsp;133.84</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"><strong>&lt;0.001</strong></td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; \">Date</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.02</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.00&nbsp;&ndash;&nbsp;0.03</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"><strong>0.017</strong></td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; padding-top:0.1cm; padding-bottom:0.1cm; border-top:1px solid;\">Observations</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left; border-top:1px solid;\" colspan=\"3\">537</td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; padding-top:0.1cm; padding-bottom:0.1cm;\">R<sup>2</sup> / R<sup>2</sup> adjusted</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left;\" colspan=\"3\">0.011 / 0.009</td>\n</tr>\n\n</table>\n\n`````\n:::\n:::\n\n\n***\n\nTypically, the results of regression analyses are presented in such tables as they include all important measures of model quality and significance, as well as the magnitude of the effects.\n\nIn addition, the results of simple linear regressions should be summarized in writing. \n\nWe can use the `reports` package [@report] to summarize the analysis.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nreport::report(m1.lm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nWe fitted a linear model (estimated using OLS) to predict Prepositions with Date (formula: Prepositions ~ Date). The model explains a statistically significant and very weak proportion of variance (R2 = 0.01, F(1, 535) = 5.68, p = 0.017, adj. R2 = 8.66e-03). The model's intercept, corresponding to Date = 0, is at 132.19 (95% CI [130.54, 133.84], t(535) = 157.62, p < .001). Within this model:\n\n  - The effect of Date is statistically significant and positive (beta = 0.02, 95% CI [3.05e-03, 0.03], t(535) = 2.38, p = 0.017; Std. beta = 0.10, 95% CI [0.02, 0.19])\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using the Wald approximation.\n```\n:::\n:::\n\n\nWe can use this output to write up a final report: \n\nA simple linear regression has been fitted to the data. A visual assessment of the model diagnostic graphics did not indicate any problematic or disproportionately influential data points (outliers) and performed significantly better compared to an intercept-only base line model but only explained .87 percent of the variance (adjusted R^2^: .0087, F-statistic (1, 535): 5,68, p-value: 0.0175\\*). The final minimal adequate linear regression model is based on 537 data points and confirms a significant and positive correlation between the year in which the text was written and the relative frequency of prepositions (coefficient estimate: .02 (standardized \\beta: 0.10, 95% CI [0.02, 0.19]), SE: 0.01, t-value~535~: 2.38, p-value: .0175\\*). Standardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using the Wald approximation.\n\n### Example 2: Teaching Styles{-}\n\nIn the previous example, we dealt with two numeric variables, while the following example deals with a categorical independent variable and a numeric dependent variable. The ability for regressions to handle very different types of variables makes regressions a widely used and robust method of analysis.\n\nIn this example, we are dealing with two groups of students that have been randomly assigned to be exposed to different teaching methods. Both groups undergo a language learning test after the lesson with a maximum score of 20 points. \n\nThe question that we will try to answer is whether the students in group A have performed significantly better than those in group B which would indicate that the teaching method to which group A was exposed works better than the teaching method to which group B was exposed.\n\nLet's move on to implementing the regression in R. In a first step, we load the data set and inspect its structure.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# load data\nslrdata2  <- base::readRDS(url(\"https://slcladal.github.io/data/sgd.rda\", \"rb\"))\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n```{=html}\n<div class=\"tabwid\"><style>.cl-b63857ca{table-layout:auto;width:75%;}.cl-b634c416{font-family:'DejaVu Sans';font-size:12pt;font-weight:bold;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-b634c420{font-family:'DejaVu Sans';font-size:12pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-b634cf88{margin:0;text-align:center;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-b634cf89{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-b634f17a{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b634f184{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b634f18e{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b634f198{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b634f199{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b634f1a2{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b634f1a3{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b634f1a4{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}</style><table class='cl-b63857ca'>\n```\n<caption class=\"Table Caption\">\n\nFirst 15 rows of the slrdata2 data.\n\n</caption>\n```{=html}\n<thead><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-b634f1a3\"><p class=\"cl-b634cf88\"><span class=\"cl-b634c416\">Group</span></p></td><td class=\"cl-b634f1a4\"><p class=\"cl-b634cf89\"><span class=\"cl-b634c416\">Score</span></p></td></tr></thead><tbody><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-b634f17a\"><p class=\"cl-b634cf88\"><span class=\"cl-b634c420\">A</span></p></td><td class=\"cl-b634f184\"><p class=\"cl-b634cf89\"><span class=\"cl-b634c420\">15</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-b634f18e\"><p class=\"cl-b634cf88\"><span class=\"cl-b634c420\">A</span></p></td><td class=\"cl-b634f198\"><p class=\"cl-b634cf89\"><span class=\"cl-b634c420\">12</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-b634f17a\"><p class=\"cl-b634cf88\"><span class=\"cl-b634c420\">A</span></p></td><td class=\"cl-b634f184\"><p class=\"cl-b634cf89\"><span class=\"cl-b634c420\">11</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-b634f18e\"><p class=\"cl-b634cf88\"><span class=\"cl-b634c420\">A</span></p></td><td class=\"cl-b634f198\"><p class=\"cl-b634cf89\"><span class=\"cl-b634c420\">18</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-b634f17a\"><p class=\"cl-b634cf88\"><span class=\"cl-b634c420\">A</span></p></td><td class=\"cl-b634f184\"><p class=\"cl-b634cf89\"><span class=\"cl-b634c420\">15</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-b634f18e\"><p class=\"cl-b634cf88\"><span class=\"cl-b634c420\">A</span></p></td><td class=\"cl-b634f198\"><p class=\"cl-b634cf89\"><span class=\"cl-b634c420\">15</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-b634f17a\"><p class=\"cl-b634cf88\"><span class=\"cl-b634c420\">A</span></p></td><td class=\"cl-b634f184\"><p class=\"cl-b634cf89\"><span class=\"cl-b634c420\">9</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-b634f18e\"><p class=\"cl-b634cf88\"><span class=\"cl-b634c420\">A</span></p></td><td class=\"cl-b634f198\"><p class=\"cl-b634cf89\"><span class=\"cl-b634c420\">19</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-b634f17a\"><p class=\"cl-b634cf88\"><span class=\"cl-b634c420\">A</span></p></td><td class=\"cl-b634f184\"><p class=\"cl-b634cf89\"><span class=\"cl-b634c420\">14</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-b634f18e\"><p class=\"cl-b634cf88\"><span class=\"cl-b634c420\">A</span></p></td><td class=\"cl-b634f198\"><p class=\"cl-b634cf89\"><span class=\"cl-b634c420\">13</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-b634f17a\"><p class=\"cl-b634cf88\"><span class=\"cl-b634c420\">A</span></p></td><td class=\"cl-b634f184\"><p class=\"cl-b634cf89\"><span class=\"cl-b634c420\">11</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-b634f18e\"><p class=\"cl-b634cf88\"><span class=\"cl-b634c420\">A</span></p></td><td class=\"cl-b634f198\"><p class=\"cl-b634cf89\"><span class=\"cl-b634c420\">12</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-b634f17a\"><p class=\"cl-b634cf88\"><span class=\"cl-b634c420\">A</span></p></td><td class=\"cl-b634f184\"><p class=\"cl-b634cf89\"><span class=\"cl-b634c420\">18</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-b634f18e\"><p class=\"cl-b634cf88\"><span class=\"cl-b634c420\">A</span></p></td><td class=\"cl-b634f198\"><p class=\"cl-b634cf89\"><span class=\"cl-b634c420\">15</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-b634f199\"><p class=\"cl-b634cf88\"><span class=\"cl-b634c420\">A</span></p></td><td class=\"cl-b634f1a2\"><p class=\"cl-b634cf89\"><span class=\"cl-b634c420\">16</span></p></td></tr></tbody></table></div>\n```\n:::\n:::\n\n\nNow, we graphically display the data. In this case, a boxplot represents a good way to visualize the data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# extract means\nslrdata2 %>%\n  dplyr::group_by(Group) %>%\n  dplyr::mutate(Mean = round(mean(Score), 1), SD = round(sd(Score), 1)) %>%\n  ggplot(aes(Group, Score)) + \n  geom_boxplot(fill=c(\"orange\", \"darkgray\")) +\n  geom_text(aes(label = paste(\"M = \", Mean, sep = \"\"), y = 1)) +\n  geom_text(aes(label = paste(\"SD = \", SD, sep = \"\"), y = 0)) +\n  theme_bw(base_size = 15) +\n  labs(x = \"Group\") +                      \n  labs(y = \"Test score (Points)\", cex = .75) +   \n  coord_cartesian(ylim = c(0, 20)) +  \n  guides(fill = FALSE)                \n```\n\n::: {.cell-output-display}\n![](regression_files/figure-html/slr22-1.png){width=672}\n:::\n:::\n\n\nThe data indicate that group A did significantly better than group B. We will test this impression by generating the regression model and creating the model and extracting the model summary. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# generate regression model\nm2.lm <- lm(Score ~ Group, data = slrdata2) \n# inspect results\nsummary(m2.lm)                             \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = Score ~ Group, data = slrdata2)\n\nResiduals:\n        Min          1Q      Median          3Q         Max \n-6.76666667 -1.93333333  0.15000000  2.06666667  6.23333333 \n\nCoefficients:\n                Estimate   Std. Error  t value               Pr(>|t|)    \n(Intercept) 14.933333333  0.534571121 27.93517 < 0.000000000000000222 ***\nGroupB      -3.166666667  0.755997730 -4.18873            0.000096692 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.92796662 on 58 degrees of freedom\nMultiple R-squared:  0.232249929,\tAdjusted R-squared:  0.219012859 \nF-statistic:  17.545418 on 1 and 58 DF,  p-value: 0.0000966923559\n```\n:::\n:::\n\n\nThe model summary reports that Group A performed significantly better compared with Group B. This is shown by the fact that the p-value (the value in the column with the header (Pr(>|t|)) is smaller than .001 as indicated by the three \\* after the p-values). Also, the negative Estimate for Group B indicates that Group B has lower scores than Group A. We will now generate the diagnostic graphics.^[I'm very grateful to Antonio Dello Iacono who pointed out that a previous version of this tutorial misinterpreted the results as I erroneously reversed the group results.]\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow = c(1, 3))        # plot window: 1 plot/row, 3 plots/column\nplot(resid(m2.lm))     # generate diagnostic plot\nplot(rstandard(m2.lm)) # generate diagnostic plot\nplot(rstudent(m2.lm)); par(mfrow = c(1, 1))  # restore normal plot window\n```\n\n::: {.cell-output-display}\n![](regression_files/figure-html/slr24-1.png){width=672}\n:::\n:::\n\n\nThe graphics do not indicate outliers or other issues, so we can continue with more diagnostic graphics.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow = c(2, 2)) # generate a plot window with 2x2 panels\nplot(m2.lm); par(mfrow = c(1, 1)) # restore normal plot window\n```\n\n::: {.cell-output-display}\n![](regression_files/figure-html/slr25-1.png){width=672}\n:::\n:::\n\n\nThese graphics also show no problems. In this case, the data can be summarized in the next step.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# tabulate results\nslrsummary(m2.lm)\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n```{=html}\n<div class=\"tabwid\"><style>.cl-b6a6956e{table-layout:auto;width:75%;}.cl-b6a21b24{font-family:'DejaVu Sans';font-size:12pt;font-weight:bold;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-b6a21b2e{font-family:'DejaVu Sans';font-size:12pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-b6a22786{margin:0;text-align:center;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-b6a266ce{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b6a266cf{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b6a266d8{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b6a266e2{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b6a266ec{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b6a266ed{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b6a266ee{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b6a266f6{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b6a266f7{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b6a26700{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b6a26701{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b6a2670a{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}</style><table class='cl-b6a6956e'>\n```\n<caption class=\"Table Caption\">\n\nResults of the regression analysis.\n\n</caption>\n```{=html}\n<thead><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-b6a2670a\"><p class=\"cl-b6a22786\"><span class=\"cl-b6a21b24\">Parameters</span></p></td><td class=\"cl-b6a26700\"><p class=\"cl-b6a22786\"><span class=\"cl-b6a21b24\">Estimate</span></p></td><td class=\"cl-b6a26700\"><p class=\"cl-b6a22786\"><span class=\"cl-b6a21b24\">Pearson's r</span></p></td><td class=\"cl-b6a26700\"><p class=\"cl-b6a22786\"><span class=\"cl-b6a21b24\">Std. Error</span></p></td><td class=\"cl-b6a26700\"><p class=\"cl-b6a22786\"><span class=\"cl-b6a21b24\">t value</span></p></td><td class=\"cl-b6a26700\"><p class=\"cl-b6a22786\"><span class=\"cl-b6a21b24\">Pr(&gt;|t|)</span></p></td><td class=\"cl-b6a26701\"><p class=\"cl-b6a22786\"><span class=\"cl-b6a21b24\">P-value sig.</span></p></td></tr></thead><tbody><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-b6a266d8\"><p class=\"cl-b6a22786\"><span class=\"cl-b6a21b2e\">(Intercept)</span></p></td><td class=\"cl-b6a266ce\"><p class=\"cl-b6a22786\"><span class=\"cl-b6a21b2e\">14.93</span></p></td><td class=\"cl-b6a266ce\"><p class=\"cl-b6a22786\"><span class=\"cl-b6a21b2e\"></span></p></td><td class=\"cl-b6a266ce\"><p class=\"cl-b6a22786\"><span class=\"cl-b6a21b2e\">0.53</span></p></td><td class=\"cl-b6a266ce\"><p class=\"cl-b6a22786\"><span class=\"cl-b6a21b2e\">27.94</span></p></td><td class=\"cl-b6a266ce\"><p class=\"cl-b6a22786\"><span class=\"cl-b6a21b2e\">0</span></p></td><td class=\"cl-b6a266cf\"><p class=\"cl-b6a22786\"><span class=\"cl-b6a21b2e\">p &lt; .001***</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-b6a266e2\"><p class=\"cl-b6a22786\"><span class=\"cl-b6a21b2e\">GroupB</span></p></td><td class=\"cl-b6a266ec\"><p class=\"cl-b6a22786\"><span class=\"cl-b6a21b2e\">-3.17</span></p></td><td class=\"cl-b6a266ec\"><p class=\"cl-b6a22786\"><span class=\"cl-b6a21b2e\">0.48</span></p></td><td class=\"cl-b6a266ec\"><p class=\"cl-b6a22786\"><span class=\"cl-b6a21b2e\">0.76</span></p></td><td class=\"cl-b6a266ec\"><p class=\"cl-b6a22786\"><span class=\"cl-b6a21b2e\">-4.19</span></p></td><td class=\"cl-b6a266ec\"><p class=\"cl-b6a22786\"><span class=\"cl-b6a21b2e\">0.0001</span></p></td><td class=\"cl-b6a266ed\"><p class=\"cl-b6a22786\"><span class=\"cl-b6a21b2e\">p &lt; .001***</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-b6a266d8\"><p class=\"cl-b6a22786\"><span class=\"cl-b6a21b2e\">Model statistics</span></p></td><td class=\"cl-b6a266ce\"><p class=\"cl-b6a22786\"><span class=\"cl-b6a21b2e\"></span></p></td><td class=\"cl-b6a266ce\"><p class=\"cl-b6a22786\"><span class=\"cl-b6a21b2e\"></span></p></td><td class=\"cl-b6a266ce\"><p class=\"cl-b6a22786\"><span class=\"cl-b6a21b2e\"></span></p></td><td class=\"cl-b6a266ce\"><p class=\"cl-b6a22786\"><span class=\"cl-b6a21b2e\"></span></p></td><td class=\"cl-b6a266ce\"><p class=\"cl-b6a22786\"><span class=\"cl-b6a21b2e\"></span></p></td><td class=\"cl-b6a266cf\"><p class=\"cl-b6a22786\"><span class=\"cl-b6a21b2e\">Value</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-b6a266e2\"><p class=\"cl-b6a22786\"><span class=\"cl-b6a21b2e\">Number of cases in model</span></p></td><td class=\"cl-b6a266ec\"><p class=\"cl-b6a22786\"><span class=\"cl-b6a21b2e\"></span></p></td><td class=\"cl-b6a266ec\"><p class=\"cl-b6a22786\"><span class=\"cl-b6a21b2e\"></span></p></td><td class=\"cl-b6a266ec\"><p class=\"cl-b6a22786\"><span class=\"cl-b6a21b2e\"></span></p></td><td class=\"cl-b6a266ec\"><p class=\"cl-b6a22786\"><span class=\"cl-b6a21b2e\"></span></p></td><td class=\"cl-b6a266ec\"><p class=\"cl-b6a22786\"><span class=\"cl-b6a21b2e\"></span></p></td><td class=\"cl-b6a266ed\"><p class=\"cl-b6a22786\"><span class=\"cl-b6a21b2e\">60</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-b6a266d8\"><p class=\"cl-b6a22786\"><span class=\"cl-b6a21b2e\">Residual standard error on 58 DF</span></p></td><td class=\"cl-b6a266ce\"><p class=\"cl-b6a22786\"><span class=\"cl-b6a21b2e\"></span></p></td><td class=\"cl-b6a266ce\"><p class=\"cl-b6a22786\"><span class=\"cl-b6a21b2e\"></span></p></td><td class=\"cl-b6a266ce\"><p class=\"cl-b6a22786\"><span class=\"cl-b6a21b2e\"></span></p></td><td class=\"cl-b6a266ce\"><p class=\"cl-b6a22786\"><span class=\"cl-b6a21b2e\"></span></p></td><td class=\"cl-b6a266ce\"><p class=\"cl-b6a22786\"><span class=\"cl-b6a21b2e\"></span></p></td><td class=\"cl-b6a266cf\"><p class=\"cl-b6a22786\"><span class=\"cl-b6a21b2e\">2.93</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-b6a266e2\"><p class=\"cl-b6a22786\"><span class=\"cl-b6a21b2e\">Multiple R-squared</span></p></td><td class=\"cl-b6a266ec\"><p class=\"cl-b6a22786\"><span class=\"cl-b6a21b2e\"></span></p></td><td class=\"cl-b6a266ec\"><p class=\"cl-b6a22786\"><span class=\"cl-b6a21b2e\"></span></p></td><td class=\"cl-b6a266ec\"><p class=\"cl-b6a22786\"><span class=\"cl-b6a21b2e\"></span></p></td><td class=\"cl-b6a266ec\"><p class=\"cl-b6a22786\"><span class=\"cl-b6a21b2e\"></span></p></td><td class=\"cl-b6a266ec\"><p class=\"cl-b6a22786\"><span class=\"cl-b6a21b2e\"></span></p></td><td class=\"cl-b6a266ed\"><p class=\"cl-b6a22786\"><span class=\"cl-b6a21b2e\">0.2322</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-b6a266d8\"><p class=\"cl-b6a22786\"><span class=\"cl-b6a21b2e\">Adjusted R-squared</span></p></td><td class=\"cl-b6a266ce\"><p class=\"cl-b6a22786\"><span class=\"cl-b6a21b2e\"></span></p></td><td class=\"cl-b6a266ce\"><p class=\"cl-b6a22786\"><span class=\"cl-b6a21b2e\"></span></p></td><td class=\"cl-b6a266ce\"><p class=\"cl-b6a22786\"><span class=\"cl-b6a21b2e\"></span></p></td><td class=\"cl-b6a266ce\"><p class=\"cl-b6a22786\"><span class=\"cl-b6a21b2e\"></span></p></td><td class=\"cl-b6a266ce\"><p class=\"cl-b6a22786\"><span class=\"cl-b6a21b2e\"></span></p></td><td class=\"cl-b6a266cf\"><p class=\"cl-b6a22786\"><span class=\"cl-b6a21b2e\">0.219</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-b6a266e2\"><p class=\"cl-b6a22786\"><span class=\"cl-b6a21b2e\">F-statistic (1, 58)</span></p></td><td class=\"cl-b6a266ec\"><p class=\"cl-b6a22786\"><span class=\"cl-b6a21b2e\"></span></p></td><td class=\"cl-b6a266ec\"><p class=\"cl-b6a22786\"><span class=\"cl-b6a21b2e\"></span></p></td><td class=\"cl-b6a266ec\"><p class=\"cl-b6a22786\"><span class=\"cl-b6a21b2e\"></span></p></td><td class=\"cl-b6a266ec\"><p class=\"cl-b6a22786\"><span class=\"cl-b6a21b2e\"></span></p></td><td class=\"cl-b6a266ec\"><p class=\"cl-b6a22786\"><span class=\"cl-b6a21b2e\"></span></p></td><td class=\"cl-b6a266ed\"><p class=\"cl-b6a22786\"><span class=\"cl-b6a21b2e\">17.55</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-b6a266f7\"><p class=\"cl-b6a22786\"><span class=\"cl-b6a21b2e\">Model p-value</span></p></td><td class=\"cl-b6a266ee\"><p class=\"cl-b6a22786\"><span class=\"cl-b6a21b2e\"></span></p></td><td class=\"cl-b6a266ee\"><p class=\"cl-b6a22786\"><span class=\"cl-b6a21b2e\"></span></p></td><td class=\"cl-b6a266ee\"><p class=\"cl-b6a22786\"><span class=\"cl-b6a21b2e\"></span></p></td><td class=\"cl-b6a266ee\"><p class=\"cl-b6a22786\"><span class=\"cl-b6a21b2e\"></span></p></td><td class=\"cl-b6a266ee\"><p class=\"cl-b6a22786\"><span class=\"cl-b6a21b2e\"></span></p></td><td class=\"cl-b6a266f6\"><p class=\"cl-b6a22786\"><span class=\"cl-b6a21b2e\">0.0001</span></p></td></tr></tbody></table></div>\n```\n:::\n:::\n\n\n\nWe can use the `reports` package [@report] to summarize the analysis.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nreport::report(m2.lm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nWe fitted a linear model (estimated using OLS) to predict Score with Group (formula: Score ~ Group). The model explains a statistically significant and moderate proportion of variance (R2 = 0.23, F(1, 58) = 17.55, p < .001, adj. R2 = 0.22). The model's intercept, corresponding to Group = A, is at 14.93 (95% CI [13.86, 16.00], t(58) = 27.94, p < .001). Within this model:\n\n  - The effect of Group [B] is statistically significant and negative (beta = -3.17, 95% CI [-4.68, -1.65], t(58) = -4.19, p < .001; Std. beta = -0.96, 95% CI [-1.41, -0.50])\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using the Wald approximation.\n```\n:::\n:::\n\n\nWe can use this output to write up a final report: \n \n\nA simple linear regression was fitted to the data. A visual assessment of the model diagnostics did not indicate any problematic or disproportionately influential data points (outliers). The final linear regression model is based on 60 data points, performed significantly better than an intercept-only base line model (F (1, 58): 17.55, p-value <. 001^$***$^), and reported that the model explained 21.9 percent of variance which confirmed a good model fit. According to this final model, group A scored significantly better on the language learning test than group B (coefficient: -3.17, 95% CI [-4.68, -1.65], Std. \\beta: -0.96, 95% CI [-1.41, -0.50], SE: 0.48, t-value~58~: -4.19, p-value <. 001^$***$^). Standardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using the Wald approximation.\n\n\n## Multiple Linear Regression{-}\n\nIn contrast to simple linear regression, which estimates the effect of a single predictor, multiple linear regression estimates the effect of various predictor (see the equation below). A multiple linear regression can thus test the effects of various predictors simultaneously.\n\n\\begin{equation}\n\nf_{(x)} = \\alpha + \\beta_{1}x_{i} + \\beta_{2}x_{i+1} + \\dots + \\beta_{n}x_{i+n} + \\epsilon\n\n\\end{equation}\n\nThere exists a wealth of literature focusing on multiple linear regressions and the concepts it is based on.  For instance, there are @achen1982interpreting, @bortz2006statistik, @crawley2005statistics, @faraway2002practical, @field2012discovering, @gries2021statistics, @levshina2015linguistics, @winter2019statistics and @wilcox2009basic to name just a few. Introductions to regression modeling in R are @baayen2008analyzing, @crawley2012r, @gries2021statistics, or @levshina2015linguistics.\n\nThe model diagnostics we are dealing with here are partly identical to the diagnostic methods discussed in the section on simple linear regression. Because of this overlap, diagnostics will only be described in more detail if they have not been described in the section on simple linear regression.\n\n***\n\n<div class=\"warning\" style='padding:0.1em; background-color:#f2f2f2; color:#51247a'>\n<span>\n<p style='margin-top:1em; text-align:center'>\n<b>EXCURSION</b></p>\n<p style='margin-left:1em;'>\n</p></span>\n</div>\n\n<div class=\"question\">` \n\n<details>\n  <summary>A note on sample size and power</summary>\n  A brief note on minimum necessary sample or data set size appears necessary here. Although there appears to be a general assumption that 25 data points per group are sufficient, this is not necessarily correct (it is merely a general rule of thumb that is actually often incorrect). Such rules of thumb are inadequate because the required sample size depends on the number of variables in a given model, the size of the effect and the variance of the effect - in other words, the minimum necessary sample size relates to *statistical power* (see [here](https://slcladal.github.io/pwr.html) for a tutorial on power). If a model contains many variables, then this requires a larger sample size than a model which only uses very few predictors. \n\n  Also, to detect an effect with a very minor effect size, one needs a substantially larger sample compared to cases where the effect is very strong. In fact, when dealing with small effects, model require a minimum of 600 cases to reliably detect these effects. Finally, effects that are very robust and do not vary much require a much smaller sample size compared with effects that are spurious and vary substantially. Since the sample size depends on the effect size and variance as well as the number of variables, there is no final one-size-fits-all answer to what the best sample size is.\n  \n  Another, slightly better but still incorrect, rule of thumb is that the more data, the better. This is not correct because models based on too many cases are prone for overfitting and thus report correlations as being significant that are not. However, given that there are procedures that can correct for overfitting, larger data sets are still preferable to data sets that are simply too small to warrant reliable results. In conclusion, it remains true that the sample size depends on the effect under investigation.\n</details>\n\n</div>`\n\n***\n\nDespite there being no ultimate rule of thumb, @field2012discovering[273-275], based on @green1991many, provide data-driven suggestions for the minimal size of data required for regression models that aim to find medium sized effects (k = number of predictors; categorical variables with more than two levels should be transformed into dummy variables):\n\n* If one is merely interested in the overall model fit (something I have not encountered), then the sample size should be at least 50 + k (k = number of predictors in model).\n\n*  If one is only interested in the effect of specific variables, then the sample size should be at least 104 + k (k = number of predictors in model).\n\n*  If one is only interested in both model fit and the effect of specific variables, then the sample size should be at least the higher value of 50 + k or 104 + k (k = number of predictors in model).\n\nYou will see in the R code below that there is already a function that tests whether the sample size is sufficient.\n\n### Example: Gifts and Availability{-}\n\nThe example we will go through here is taken from @field2012discovering. In this example, the research question is if the money that men spend on presents for women depends on the women's attractiveness and their relationship status. To answer this research question, we will implement a multiple linear regression and start by loading the data and inspect its structure and properties.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# load data\nmlrdata  <- base::readRDS(url(\"https://slcladal.github.io/data/mld.rda\", \"rb\"))\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n```{=html}\n<div class=\"tabwid\"><style>.cl-b6f4e264{table-layout:auto;width:75%;}.cl-b6f1179c{font-family:'DejaVu Sans';font-size:12pt;font-weight:bold;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-b6f117b0{font-family:'DejaVu Sans';font-size:12pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-b6f1264c{margin:0;text-align:center;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-b6f12656{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-b6f14cd0{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b6f14cda{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b6f14cdb{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b6f14ce4{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b6f14ce5{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b6f14cee{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b6f14cef{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b6f14cf8{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b6f14cf9{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b6f14cfa{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b6f14cfb{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b6f14d02{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}</style><table class='cl-b6f4e264'>\n```\n<caption class=\"Table Caption\">\n\nFirst 15 rows of the mlrdata.\n\n</caption>\n```{=html}\n<thead><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-b6f14cfb\"><p class=\"cl-b6f1264c\"><span class=\"cl-b6f1179c\">status</span></p></td><td class=\"cl-b6f14cfa\"><p class=\"cl-b6f1264c\"><span class=\"cl-b6f1179c\">attraction</span></p></td><td class=\"cl-b6f14d02\"><p class=\"cl-b6f12656\"><span class=\"cl-b6f1179c\">money</span></p></td></tr></thead><tbody><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-b6f14cdb\"><p class=\"cl-b6f1264c\"><span class=\"cl-b6f117b0\">Relationship</span></p></td><td class=\"cl-b6f14cd0\"><p class=\"cl-b6f1264c\"><span class=\"cl-b6f117b0\">NotInterested</span></p></td><td class=\"cl-b6f14cda\"><p class=\"cl-b6f12656\"><span class=\"cl-b6f117b0\">86.33</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-b6f14cee\"><p class=\"cl-b6f1264c\"><span class=\"cl-b6f117b0\">Relationship</span></p></td><td class=\"cl-b6f14ce4\"><p class=\"cl-b6f1264c\"><span class=\"cl-b6f117b0\">NotInterested</span></p></td><td class=\"cl-b6f14ce5\"><p class=\"cl-b6f12656\"><span class=\"cl-b6f117b0\">45.58</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-b6f14cdb\"><p class=\"cl-b6f1264c\"><span class=\"cl-b6f117b0\">Relationship</span></p></td><td class=\"cl-b6f14cd0\"><p class=\"cl-b6f1264c\"><span class=\"cl-b6f117b0\">NotInterested</span></p></td><td class=\"cl-b6f14cda\"><p class=\"cl-b6f12656\"><span class=\"cl-b6f117b0\">68.43</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-b6f14cee\"><p class=\"cl-b6f1264c\"><span class=\"cl-b6f117b0\">Relationship</span></p></td><td class=\"cl-b6f14ce4\"><p class=\"cl-b6f1264c\"><span class=\"cl-b6f117b0\">NotInterested</span></p></td><td class=\"cl-b6f14ce5\"><p class=\"cl-b6f12656\"><span class=\"cl-b6f117b0\">52.93</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-b6f14cdb\"><p class=\"cl-b6f1264c\"><span class=\"cl-b6f117b0\">Relationship</span></p></td><td class=\"cl-b6f14cd0\"><p class=\"cl-b6f1264c\"><span class=\"cl-b6f117b0\">NotInterested</span></p></td><td class=\"cl-b6f14cda\"><p class=\"cl-b6f12656\"><span class=\"cl-b6f117b0\">61.86</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-b6f14cee\"><p class=\"cl-b6f1264c\"><span class=\"cl-b6f117b0\">Relationship</span></p></td><td class=\"cl-b6f14ce4\"><p class=\"cl-b6f1264c\"><span class=\"cl-b6f117b0\">NotInterested</span></p></td><td class=\"cl-b6f14ce5\"><p class=\"cl-b6f12656\"><span class=\"cl-b6f117b0\">48.47</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-b6f14cdb\"><p class=\"cl-b6f1264c\"><span class=\"cl-b6f117b0\">Relationship</span></p></td><td class=\"cl-b6f14cd0\"><p class=\"cl-b6f1264c\"><span class=\"cl-b6f117b0\">NotInterested</span></p></td><td class=\"cl-b6f14cda\"><p class=\"cl-b6f12656\"><span class=\"cl-b6f117b0\">32.79</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-b6f14cee\"><p class=\"cl-b6f1264c\"><span class=\"cl-b6f117b0\">Relationship</span></p></td><td class=\"cl-b6f14ce4\"><p class=\"cl-b6f1264c\"><span class=\"cl-b6f117b0\">NotInterested</span></p></td><td class=\"cl-b6f14ce5\"><p class=\"cl-b6f12656\"><span class=\"cl-b6f117b0\">35.91</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-b6f14cdb\"><p class=\"cl-b6f1264c\"><span class=\"cl-b6f117b0\">Relationship</span></p></td><td class=\"cl-b6f14cd0\"><p class=\"cl-b6f1264c\"><span class=\"cl-b6f117b0\">NotInterested</span></p></td><td class=\"cl-b6f14cda\"><p class=\"cl-b6f12656\"><span class=\"cl-b6f117b0\">30.98</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-b6f14cee\"><p class=\"cl-b6f1264c\"><span class=\"cl-b6f117b0\">Relationship</span></p></td><td class=\"cl-b6f14ce4\"><p class=\"cl-b6f1264c\"><span class=\"cl-b6f117b0\">NotInterested</span></p></td><td class=\"cl-b6f14ce5\"><p class=\"cl-b6f12656\"><span class=\"cl-b6f117b0\">44.82</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-b6f14cdb\"><p class=\"cl-b6f1264c\"><span class=\"cl-b6f117b0\">Relationship</span></p></td><td class=\"cl-b6f14cd0\"><p class=\"cl-b6f1264c\"><span class=\"cl-b6f117b0\">NotInterested</span></p></td><td class=\"cl-b6f14cda\"><p class=\"cl-b6f12656\"><span class=\"cl-b6f117b0\">35.05</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-b6f14cee\"><p class=\"cl-b6f1264c\"><span class=\"cl-b6f117b0\">Relationship</span></p></td><td class=\"cl-b6f14ce4\"><p class=\"cl-b6f1264c\"><span class=\"cl-b6f117b0\">NotInterested</span></p></td><td class=\"cl-b6f14ce5\"><p class=\"cl-b6f12656\"><span class=\"cl-b6f117b0\">64.49</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-b6f14cdb\"><p class=\"cl-b6f1264c\"><span class=\"cl-b6f117b0\">Relationship</span></p></td><td class=\"cl-b6f14cd0\"><p class=\"cl-b6f1264c\"><span class=\"cl-b6f117b0\">NotInterested</span></p></td><td class=\"cl-b6f14cda\"><p class=\"cl-b6f12656\"><span class=\"cl-b6f117b0\">54.50</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-b6f14cee\"><p class=\"cl-b6f1264c\"><span class=\"cl-b6f117b0\">Relationship</span></p></td><td class=\"cl-b6f14ce4\"><p class=\"cl-b6f1264c\"><span class=\"cl-b6f117b0\">NotInterested</span></p></td><td class=\"cl-b6f14ce5\"><p class=\"cl-b6f12656\"><span class=\"cl-b6f117b0\">61.48</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-b6f14cf9\"><p class=\"cl-b6f1264c\"><span class=\"cl-b6f117b0\">Relationship</span></p></td><td class=\"cl-b6f14cf8\"><p class=\"cl-b6f1264c\"><span class=\"cl-b6f117b0\">NotInterested</span></p></td><td class=\"cl-b6f14cef\"><p class=\"cl-b6f12656\"><span class=\"cl-b6f117b0\">55.51</span></p></td></tr></tbody></table></div>\n```\n:::\n:::\n\n\nThe data set consist of three variables stored in three columns. The first column contains the relationship status of the present giver (in this study this were men), the second whether the man is interested in the woman (the present receiver in this study), and the third column represents the money spend on the present. The data set represents 100 cases and the mean amount of money spend on a present is 88.38 dollars. In a next step, we visualize the data to get a more detailed impression of the relationships between variables.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# create plots\np1 <- ggplot(mlrdata, aes(status, money)) +\n  geom_boxplot() + \n  theme_bw()\n# plot 2\np2 <- ggplot(mlrdata, aes(attraction, money)) +\n  geom_boxplot() +\n  theme_bw()\n# plot 3\np3 <- ggplot(mlrdata, aes(x = money)) +\n  geom_histogram(aes(y=..density..)) +            \n  theme_bw() +         \n  geom_density(alpha=.2, fill = \"gray50\") \n# plot 4\np4 <- ggplot(mlrdata, aes(status, money)) +\n  geom_boxplot(aes(fill = factor(status))) + \n  scale_fill_manual(values = c(\"grey30\", \"grey70\")) + \n  facet_wrap(~ attraction) + \n  guides(fill = \"none\") +\n  theme_bw()\n# show plots\nvip::grid.arrange(grobs = list(p1, p2, p3, p4), widths = c(1, 1), layout_matrix = rbind(c(1, 2), c(3, 4)))\n```\n\n::: {.cell-output-display}\n![](regression_files/figure-html/mlr3-1.png){width=672}\n:::\n:::\n\n\nThe upper left figure consists of a boxplot which shows how much money was spent by relationship status. The figure suggests that men spend more on women if they are not in a relationship. The next figure shows the relationship between the money spend on presents and whether or not the men were interested in the women.\n\nThe boxplot in the upper right panel suggests that men spend substantially more on women if the men are interested in them. The next figure depicts the distribution of the amounts of money spend on the presents for the women. In addition, the figure indicates the existence of two outliers (dots in the boxplot)\n\nThe histogram in the lower left panel shows that, although the mean amount of money spent on presents is 88.38 dollars, the distribution peaks around 50 dollars indicating that on average, men spend about 50 dollars on presents. Finally, we will plot the amount of money spend on presents against relationship status by attraction in order to check whether the money spent on presents is affected by an interaction between attraction and relationship status.\n\nThe boxplot in the lower right panel confirms the existence of an interaction (a non-additive term) as men only spend more money on  women if the men single *and* they are interested in the women. If men are not interested in the women, then the relationship has no effect as they spend an equal amount of money on the women regardless of whether they are in a relationship or not.\n\nWe will now start to implement the regression model. In a first step, we create two saturated models that contain all possible predictors (main effects and interactions). The two models are identical but one is generated with the `lm` and the other with the `glm` function as these functions offer different model parameters in their output.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm1.mlr = lm(                      # generate lm regression object\n  money ~ 1 + attraction*status,  # def. regression formula (1 = intercept)\n  data = mlrdata)                 # def. data\nm1.glm = glm(                     # generate glm regression object\n  money ~ 1 + attraction*status,  # def. regression formula (1 = intercept)\n  family = gaussian,              # def. linkage function\n  data = mlrdata)                 # def. data\n```\n:::\n\n\nAfter generating the saturated models we can now start with the model fitting. Model fitting refers to a process that aims at find the model that explains a maximum of variance with a minimum of predictors [see @field2012discovering 318]. Model fitting is therefore based on the *principle of parsimony* which is related to Occam's razor according to which explanations that require fewer assumptions are more likely to be true.\n\n### Automatic Model Fitting and Why You Should Not Use It{-}\n\nIn this section, we will use a step-wise step-down procedure that uses decreases in AIC (*Akaike Information Criterion*) as the criterion to minimize the model in a step-wise manner. This procedure aims at finding the model with the lowest AIC values by evaluating - step-by-step - whether the removal of a predictor (term) leads to a lower AIC value.\n\nWe use this method here just so that you know it exists and how to implement it but you should rather avoid using automated model fitting. The reason for avoiding automated model fitting is that the algorithm only checks if the AIC has decreased but not if the model is stable or reliable. Thus, automated model fitting has the problem that you can never be sure that the way that lead you to the final model is reliable and that all models were indeed stable. Imagine you want to climb down from a roof top and you have a ladder. The problem is that you do not know if and how many steps are broken. This is similar to using automated model fitting. In other sections, we will explore better methods to fit models (manual step-wise step-up and step-down procedures, for example).\n\nThe AIC is calculated using the equation below. The lower the AIC value, the better the balance between explained variance and the number of predictors. AIC values can and should only be compared for models that are fit on the same data set with the same (number of) cases (*LL* stands for *logged likelihood* or *LogLikelihood* and *k* represents the number of predictors in the model (including the intercept); the LL represents a measure of how good the model fits the data).\n\n\\begin{equation}\nAkaike Information Criterion (AIC) = -2LL + 2k\n\\end{equation}\n\nAn alternative to the AIC is the BIC (Bayesian Information Criterion). Both AIC and BIC penalize models for including variables in a model. The penalty of the BIC is bigger than the penalty of the AIC and it includes the number of cases in the model (*LL* stands for *logged likelihood* or *LogLikelihood*, *k* represents the number of predictors in the model (including the intercept), and *N* represents the number of cases in the model). \n\n\\begin{equation}\nBayesian Information Criterion (BIC) = -2LL + 2k * log(N)\n\\end{equation}\n\nInteractions are evaluated first and only if all insignificant interactions have been removed would the procedure start removing insignificant main effects (that are not part of significant interactions). Other model fitting procedures (forced entry, step-wise step up, hierarchical) are discussed during the implementation of other regression models. We cannot discuss all procedures here as model fitting is rather complex and a discussion of even the most common procedures would to lengthy and time consuming at this point. It is important to note though that there is not perfect model fitting procedure and automated approaches should be handled with care as they are likely to ignore violations of model parameters that can be detected during manual - but time consuming - model fitting procedures. As a general rule of thumb, it is advisable to fit models as carefully and deliberately as possible. We will now begin to fit the model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# automated AIC based model fitting\nstep(m1.mlr, direction = \"both\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nStart:  AIC=592.52\nmoney ~ 1 + attraction * status\n\n                    Df   Sum of Sq         RSS         AIC\n<none>                             34557.56428 592.5211556\n- attraction:status  1 24947.25481 59504.81909 644.8642395\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = money ~ 1 + attraction * status, data = mlrdata)\n\nCoefficients:\n                         (Intercept)               attractionNotInterested  \n                             99.1548                              -47.6628  \n                        statusSingle  attractionNotInterested:statusSingle  \n                             57.6928                              -63.1788  \n```\n:::\n:::\n\n\nThe automated model fitting procedure informs us that removing predictors has not caused a decrease in the AIC. The saturated model is thus also the final minimal adequate model. We will now inspect the final minimal model and go over the model report.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm2.mlr = lm(                       # generate lm regression object\n  money ~ (status + attraction)^2, # def. regression formula\n  data = mlrdata)                  # def. data\nm2.glm = glm(                      # generate glm regression object\n  money ~ (status + attraction)^2, # def. regression formula\n  family = gaussian,               # def. linkage function\n  data = mlrdata)                  # def. data\n# inspect final minimal model\nsummary(m2.mlr)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = money ~ (status + attraction)^2, data = mlrdata)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-45.0760 -14.2580   0.4596  11.9315  44.1424 \n\nCoefficients:\n                                         Estimate   Std. Error  t value\n(Intercept)                           99.15480000   3.79459947 26.13050\nstatusSingle                          57.69280000   5.36637403 10.75080\nattractionNotInterested              -47.66280000   5.36637403 -8.88175\nstatusSingle:attractionNotInterested -63.17880000   7.58919893 -8.32483\n                                                   Pr(>|t|)    \n(Intercept)                          < 0.000000000000000222 ***\nstatusSingle                         < 0.000000000000000222 ***\nattractionNotInterested                 0.00000000000003751 ***\nstatusSingle:attractionNotInterested    0.00000000000058085 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 18.9729973 on 96 degrees of freedom\nMultiple R-squared:  0.852041334,\tAdjusted R-squared:  0.847417626 \nF-statistic: 184.276619 on 3 and 96 DF,  p-value: < 0.0000000000000002220446\n```\n:::\n:::\n\n\nThe first element of the report is called *Call* and it reports the regression formula of the model. Then, the report provides the residual distribution (the range, median and quartiles of the residuals) which allows drawing inferences about the distribution of differences between observed and expected values. If the residuals are distributed non-normally, then this is a strong indicator that the model is unstable and unreliable because mathematical assumptions on which the model is based are violated.\n\nNext, the model summary reports the most important part: a table with model statistics of the fixed-effects structure of the model. The table contains the estimates (coefficients of the predictors), standard errors, t-values, and the p-values which show whether a predictor significantly correlates with the dependent variable that the model investigates.\n\nAll main effects (status and attraction) as well as the interaction between status and attraction is reported as being significantly correlated with the dependent variable (money). An interaction occurs if a correlation between the dependent variable and a predictor is affected by another predictor.\n\nThe top most term is called intercept and has a value of 99.15 which represents the base estimate to which all other estimates refer. To exemplify what this means, let us consider what the model would predict that a man would spend on a present if he interested in the woman but he is also in a relationship. The amount he would spend (based on the model would be 99.15 dollars (which is the intercept). This means that the intercept represents the predicted value if all predictors take the base or reference level. And since being in relationship but being interested are the case, and because the interaction does not apply, the predicted value in our example is exactly the intercept (see below). \n\n\n::: {.cell}\n\n```{.r .cell-code}\n#intercept  Single  NotInterested  Single:NotInterested\n99.15     + 57.69  + 0           + 0     # 156.8 single + interested\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 156.84\n```\n:::\n\n```{.r .cell-code}\n99.15     + 57.69  - 47.66       - 63.18 # 46.00 single + not interested\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 46\n```\n:::\n\n```{.r .cell-code}\n99.15     - 0      + 0           - 0     # 99.15 relationship + interested\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 99.15\n```\n:::\n\n```{.r .cell-code}\n99.15     - 0      - 47.66       - 0     # 51.49 relationship + not interested\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 51.49\n```\n:::\n:::\n\n\nNow, let us consider what a man would spend if he is in a relationship and he is not attracted to the women. In that case, the model predicts that the man would spend only  51.49 dollars on a present: the intercept (99.15) minus 47.66 because the man is not interested (and no additional subtraction because the interaction does not apply). \n\nWe can derive the same results easier using the `predict` function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# make prediction based on the model for original data\nprediction <- predict(m2.mlr, newdata = mlrdata)\n# inspect predictions\ntable(round(prediction,2))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n 46.01  51.49  99.15 156.85 \n    25     25     25     25 \n```\n:::\n:::\n\n\nBelow the table of coefficients, the regression summary reports model statistics that provide information about how well the model performs. The difference between the values and the values in the coefficients table is that the model statistics refer to the model as a whole rather than focusing on individual predictors.\n\nThe multiple R^2^-value is a measure of how much variance the model explains. A multiple R^2^-value of 0 would inform us that the model does not explain any variance while a value of .852 mean that the model explains 85.2 percent of the variance. A value of 1 would inform us that the model explains 100 percent of the variance and that the predictions of the model match the observed values perfectly. Multiplying the multiple R^2^-value thus provides the percentage of explained variance. Models that have a multiple R^2^-value equal or higher than .05 are deemed substantially significant [see @szmrecsanyi2006morphosyntactic 55]. It has been claimed that models should explain a minimum of 5 percent of variance but this is problematic as it is not uncommon for models to have very low explanatory power while still performing significantly and systematically better than chance. In addition, the total amount of variance is negligible in cases where one is interested in very weak but significant effects. It is much more important for model to perform significantly better than minimal base-line models because if this is not the case, then the model does not have any predictive and therefore no explanatory power.\n\nThe adjusted R^2^-value considers the amount of explained variance in light of the number of predictors in the model (it is thus somewhat similar to the AIC and BIC) and informs about how well the model would perform if it were applied to the population that the sample is drawn from. Ideally, the difference between multiple and adjusted R^2^-value should be very small as this means that the model is not overfitted. If, however, the difference between multiple and adjusted R^2^-value is substantial, then this would strongly suggest that the model is unstable and overfitted to the data while being inadequate for drawing inferences about the population. Differences between multiple and adjusted R^2^-values indicate that the data contains outliers that cause the distribution of the data on which the model is based to differ from the distributions that the model mathematically requires to provide reliable estimates. The difference between multiple and adjusted R^2^-value in our model is very small (85.2-84.7=.05) and should not cause concern.\n\nBefore continuing, we will calculate the confidence intervals of the coefficients.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# extract confidence intervals of the coefficients\nconfint(m2.mlr)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                                              2.5 %         97.5 %\n(Intercept)                           91.6225795890 106.6870204110\nstatusSingle                          47.0406317400  68.3449682600\nattractionNotInterested              -58.3149682600 -37.0106317400\nstatusSingle:attractionNotInterested -78.2432408219 -48.1143591781\n```\n:::\n\n```{.r .cell-code}\n# create and compare baseline- and minimal adequate model\nm0.mlr <- lm(money ~1, data = mlrdata)\nanova(m0.mlr, m2.mlr)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Variance Table\n\nModel 1: money ~ 1\nModel 2: money ~ (status + attraction)^2\n  Res.Df          RSS Df   Sum of Sq         F                 Pr(>F)    \n1     99 233562.28650                                                    \n2     96  34557.56428  3 199004.7222 184.27662 < 0.000000000000000222 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\nNow, we compare the final minimal adequate model to the base-line model to test whether then final model significantly outperforms the baseline model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# compare baseline- and minimal adequate model\nAnova(m0.mlr, m2.mlr, type = \"III\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnova Table (Type III tests)\n\nResponse: money\n                 Sum Sq Df    F value                 Pr(>F)    \n(Intercept) 781015.8300  1 2169.64133 < 0.000000000000000222 ***\nResiduals    34557.5643 96                                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\nThe comparison between the two model confirms that the minimal adequate model performs significantly better (makes significantly more accurate estimates of the outcome variable) compared with the baseline model.\n\n### Outlier Detection{-}\n\nAfter implementing the multiple regression, we now need to look for outliers and perform the model diagnostics by testing whether removing data points disproportionately decreases model fit. To begin with, we generate diagnostic plots.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# generate plots\nautoplot(m2.mlr) + \n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](regression_files/figure-html/mlr11-1.png){width=672}\n:::\n:::\n\n\nThe plots do not show severe problems such as funnel shaped patterns or drastic deviations from the diagonal line in Normal Q-Q plot (have a look at the explanation of what to look for and how to interpret these diagnostic plots in the section on simple linear regression) but data points 52, 64, and 83 are repeatedly indicated as potential outliers. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# determine a cutoff for data points that have D-values higher than 4/(n-k-1)\ncutoff <- 4/((nrow(mlrdata)-length(m2.mlr$coefficients)-2))\n# start plotting\npar(mfrow = c(1, 2))           # display plots in 3 rows/2 columns\nqqPlot(m2.mlr, main=\"QQ Plot\") # create qq-plot\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 52 83\n```\n:::\n\n```{.r .cell-code}\nplot(m2.mlr, which=4, cook.levels = cutoff); par(mfrow = c(1, 1))\n```\n\n::: {.cell-output-display}\n![](regression_files/figure-html/mlr12-1.png){width=672}\n:::\n:::\n\n\nThe graphs indicate that data points 52, 64, and 83 may be problematic. We will therefore statistically evaluate whether these data points need to be removed. In order to find out which data points require removal, we extract the influence measure statistics and add them to out data set.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# extract influence statistics\ninfl <- influence.measures(m2.mlr)\n# add infl. statistics to data\nmlrdata <- data.frame(mlrdata, infl[[1]], infl[[2]])\n# annotate too influential data points\nremove <- apply(infl$is.inf, 1, function(x) {\n  ifelse(x == TRUE, return(\"remove\"), return(\"keep\")) } )\n# add annotation to data\nmlrdata <- data.frame(mlrdata, remove)\n# number of rows before removing outliers\nnrow(mlrdata)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 100\n```\n:::\n\n```{.r .cell-code}\n# remove outliers\nmlrdata <- mlrdata[mlrdata$remove == \"keep\", ]\n# number of rows after removing outliers\nnrow(mlrdata)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 98\n```\n:::\n:::\n\n\nThe difference in row in the data set before and after removing data points indicate that two data points which represented outliers have been removed.\n\n\n\n<div class=\"warning\" style='padding:0.1em; background-color:#f2f2f2; color:#51247a'>\n<span>\n<p style='margin-top:1em; text-align:center'>\n<b>NOTE</b><br><br>In general, outliers should not simply be removed unless there are good reasons for it (this could be that the outliers represent measurement errors). If a data set contains outliers, one should rather switch to methods that are better at handling outliers, e.g. by using weights to account for data points with high leverage. One alternative would be to switch to a robust regression (see [here](https://slcladal.github.io/regression.html#16_Robust_Regression)). However, here we show how to proceed by removing outliers as this is a common, though potentially problematic, method of dealing with outliers. </p>\n<p style='margin-left:1em;'>\n</p></span>\n</div>\n\n<br>\n\n\n### Rerun Regression{-}\n\nAs we have decided to remove the outliers which means that we are now dealing with a different data set, we need to rerun the regression analysis. As the steps are identical to the regression analysis performed above, the steps will not be described in greater detail.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# recreate regression models on new data\nm0.mlr = lm(money ~ 1, data = mlrdata)\nm0.glm = glm(money ~ 1, family = gaussian, data = mlrdata)\nm1.mlr = lm(money ~ (status + attraction)^2, data = mlrdata)\nm1.glm = glm(money ~ status * attraction, family = gaussian,\n             data = mlrdata)\n# automated AIC based model fitting\nstep(m1.mlr, direction = \"both\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nStart:  AIC=570.29\nmoney ~ (status + attraction)^2\n\n                    Df   Sum of Sq         RSS         AIC\n<none>                             30411.31714 570.2850562\n- status:attraction  1 21646.86199 52058.17914 620.9646729\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = money ~ (status + attraction)^2, data = mlrdata)\n\nCoefficients:\n                         (Intercept)                          statusSingle  \n                          99.1548000                            55.8535333  \n             attractionNotInterested  statusSingle:attractionNotInterested  \n                         -47.6628000                           -59.4613667  \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# create new final models\nm2.mlr = lm(money ~ (status + attraction)^2, data = mlrdata)\nm2.glm = glm(money ~ status * attraction, family = gaussian,\n             data = mlrdata)\n# inspect final minimal model\nsummary(m2.mlr)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = money ~ (status + attraction)^2, data = mlrdata)\n\nResiduals:\n         Min           1Q       Median           3Q          Max \n-35.76416667 -13.50520000  -0.98948333  10.59887500  38.77166667 \n\nCoefficients:\n                                         Estimate   Std. Error  t value\n(Intercept)                           99.15480000   3.59735820 27.56323\nstatusSingle                          55.85353333   5.14015367 10.86612\nattractionNotInterested              -47.66280000   5.08743275 -9.36873\nstatusSingle:attractionNotInterested -59.46136667   7.26927504 -8.17982\n                                                   Pr(>|t|)    \n(Intercept)                          < 0.000000000000000222 ***\nstatusSingle                         < 0.000000000000000222 ***\nattractionNotInterested               0.0000000000000040429 ***\nstatusSingle:attractionNotInterested  0.0000000000013375166 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 17.986791 on 94 degrees of freedom\nMultiple R-squared:  0.857375902,\tAdjusted R-squared:  0.852824069 \nF-statistic: 188.358387 on 3 and 94 DF,  p-value: < 0.0000000000000002220446\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# extract confidence intervals of the coefficients\nconfint(m2.mlr)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                                              2.5 %         97.5 %\n(Intercept)                           92.0121609656 106.2974390344\nstatusSingle                          45.6476377202  66.0594289465\nattractionNotInterested              -57.7640169936 -37.5615830064\nstatusSingle:attractionNotInterested -73.8946826590 -45.0280506744\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# compare baseline with final model\nanova(m0.mlr, m2.mlr)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Variance Table\n\nModel 1: money ~ 1\nModel 2: money ~ (status + attraction)^2\n  Res.Df          RSS Df   Sum of Sq         F                 Pr(>F)    \n1     97 213227.06081                                                    \n2     94  30411.31714  3 182815.7437 188.35839 < 0.000000000000000222 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# compare baseline with final model\nAnova(m0.mlr, m2.mlr, type = \"III\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnova Table (Type III tests)\n\nResponse: money\n                 Sum Sq Df    F value                 Pr(>F)    \n(Intercept) 760953.2107  1 2352.07181 < 0.000000000000000222 ***\nResiduals    30411.3171 94                                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\n### Additional Model Diagnostics{-}\n\nAfter rerunning the regression analysis on the updated data set, we again create diagnostic plots in order to check whether there are potentially problematic data points.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# generate plots\nautoplot(m2.mlr) + \n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](regression_files/figure-html/mlr19-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# determine a cutoff for data points that have\n# D-values higher than 4/(n-k-1)\ncutoff <- 4/((nrow(mlrdata)-length(m2.mlr$coefficients)-2))\n# start plotting\npar(mfrow = c(1, 2))           # display plots in 1 row/2 columns\nqqPlot(m2.mlr, main=\"QQ Plot\") # create qq-plot\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n84 88 \n82 86 \n```\n:::\n\n```{.r .cell-code}\nplot(m2.mlr, which=4, cook.levels = cutoff); par(mfrow = c(1, 1))\n```\n\n::: {.cell-output-display}\n![](regression_files/figure-html/mlr20-1.png){width=672}\n:::\n:::\n\n\nAlthough the diagnostic plots indicate that additional points may be problematic, but these data points deviate substantially less from the trend than was the case with the data points that have already been removed. To make sure that retaining the data points that are deemed potentially problematic by the diagnostic plots, is acceptable, we extract diagnostic statistics and add them to the data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# add model diagnostics to the data\nmlrdata <- mlrdata %>%\n  dplyr::mutate(residuals = resid(m2.mlr),\n                standardized.residuals = rstandard(m2.mlr),\n                studentized.residuals = rstudent(m2.mlr),\n                cooks.distance = cooks.distance(m2.mlr),\n                dffit = dffits(m2.mlr),\n                leverage = hatvalues(m2.mlr),\n                covariance.ratios = covratio(m2.mlr),\n                fitted = m2.mlr$fitted.values)\n```\n:::\n\n\nWe can now use these diagnostic statistics to create more precise diagnostic plots.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# plot 5\np5 <- ggplot(mlrdata,\n             aes(studentized.residuals)) +\n  theme(legend.position = \"none\")+\n  geom_histogram(aes(y=..density..),\n                 binwidth = .2,\n                 colour=\"black\",\n                 fill=\"gray90\") +\n  labs(x = \"Studentized Residual\", y = \"Density\") +\n  stat_function(fun = dnorm,\n                args = list(mean = mean(mlrdata$studentized.residuals, na.rm = TRUE),\n                            sd = sd(mlrdata$studentized.residuals, na.rm = TRUE)),\n                colour = \"red\", size = 1) +\n  theme_bw(base_size = 8)\n# plot 6\np6 <- ggplot(mlrdata, aes(fitted, studentized.residuals)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", colour = \"Red\")+\n  theme_bw(base_size = 8)+\n  labs(x = \"Fitted Values\",\n       y = \"Studentized Residual\")\n# plot 7\np7 <- qplot(sample = mlrdata$studentized.residuals, stat=\"qq\") +\n  theme_bw(base_size = 8) +\n  labs(x = \"Theoretical Values\",\n       y = \"Observed Values\")\nvip::grid.arrange(p5, p6, p7, nrow = 1)\n```\n\n::: {.cell-output-display}\n![](regression_files/figure-html/mlr22-1.png){width=672}\n:::\n:::\n\n\nThe new diagnostic plots do not indicate outliers that require removal. With respect to such data points the following parameters should be considered:\n\n1) Data points with standardized residuals > 3.29 should be removed [@field2012discovering 269]\n\n2) If more than 1 percent of data points have standardized residuals exceeding values > 2.58, then the error rate of the model is unacceptable [@field2012discovering 269].\n\n3) If more than 5 percent of data points have standardized residuals exceeding values   > 1.96, then the error rate of the model is unacceptable [@field2012discovering 269]\n\n4) In addition, data points with Cook's D-values > 1 should be removed [@field2012discovering 269]\n\n5) Also, data points with leverage values higher than $3(k + 1)/N$ or $2(k + 1)/N$ (k = Number of predictors, N = Number of cases in model) should be removed [@field2012discovering 270]\n\n6) There should not be (any) autocorrelation among predictors. This means that independent variables cannot be correlated with itself (for instance, because data points come from the same subject). If there is autocorrelation among predictors, then a Repeated Measures Design or a (hierarchical) mixed-effects model should be implemented instead.\n\n7) Predictors cannot substantially correlate with each other (multicollinearity) (see the [subsection on (multi-)collinearity](https://slcladal.github.io/regression.html#Multicollinearity) in the section of multiple binomial logistic regression for more details about (multi-)collinearity). If a model contains predictors that have variance inflation factors (VIF) > 10 the model is unreliable [@myers1990classical] and predictors causing such VIFs should be removed. Indeed, even VIFs of 2.5 can be problematic [@szmrecsanyi2006morphosyntactic 215] Indeed, @zuur2010protocol propose that variables with VIFs exceeding 3 should be removed! \n\n\n<div class=\"warning\" style='padding:0.1em; background-color:#f2f2f2; color:#51247a'>\n<span>\n<p style='margin-top:1em; text-align:center'>\n<b>NOTE</b><br><br>However, (multi-)collinearity is only an issue if one is interested in interpreting regression results!  If the interpretation is irrelevant because what is relevant is prediction(!), then it does not matter if the model contains collinear predictors! See @gries2021statistics for a more elaborate explanation.</p>\n<p style='margin-left:1em;'>\n</p></span>\n</div>\n\n<br>\n\n8) The mean value of VIFs should be ~ 1 [@bowerman1990linear].\n\nThe following code chunk evaluates these criteria.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# 1: optimal = 0\n# (listed data points should be removed)\nwhich(mlrdata$standardized.residuals > 3.29)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nnamed integer(0)\n```\n:::\n\n```{.r .cell-code}\n# 2: optimal = 1\n# (listed data points should be removed)\nstdres_258 <- as.vector(sapply(mlrdata$standardized.residuals, function(x) {\nifelse(sqrt((x^2)) > 2.58, 1, 0) } ))\n(sum(stdres_258) / length(stdres_258)) * 100\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0\n```\n:::\n\n```{.r .cell-code}\n# 3: optimal = 5\n# (listed data points should be removed)\nstdres_196 <- as.vector(sapply(mlrdata$standardized.residuals, function(x) {\nifelse(sqrt((x^2)) > 1.96, 1, 0) } ))\n(sum(stdres_196) / length(stdres_196)) * 100\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 6.12244897959\n```\n:::\n\n```{.r .cell-code}\n# 4: optimal = 0\n# (listed data points should be removed)\nwhich(mlrdata$cooks.distance > 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nnamed integer(0)\n```\n:::\n\n```{.r .cell-code}\n# 5: optimal = 0\n# (data points should be removed if cooks distance is close to 1)\nwhich(mlrdata$leverage >= (3*mean(mlrdata$leverage)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nnamed integer(0)\n```\n:::\n\n```{.r .cell-code}\n# 6: checking autocorrelation:\n# Durbin-Watson test (optimal: high p-value)\ndwt(m2.mlr)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n lag  Autocorrelation D-W Statistic p-value\n   1 -0.0143324675649  1.9680423527   0.672\n Alternative hypothesis: rho != 0\n```\n:::\n\n```{.r .cell-code}\n# 7: test multicollinearity 1\nvif(m2.mlr)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                        statusSingle              attractionNotInterested \n                                2.00                                 1.96 \nstatusSingle:attractionNotInterested \n                                2.96 \n```\n:::\n\n```{.r .cell-code}\n# 8: test multicollinearity 2\n1/vif(m2.mlr)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                        statusSingle              attractionNotInterested \n                      0.500000000000                       0.510204081633 \nstatusSingle:attractionNotInterested \n                      0.337837837838 \n```\n:::\n\n```{.r .cell-code}\n# 9: mean vif should not exceed 1\nmean(vif(m2.mlr))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2.30666666667\n```\n:::\n:::\n\n\nExcept for the mean VIF value (2.307) which should not exceed 1, all diagnostics are acceptable. We will now test whether the sample size is sufficient for our model. With respect to the minimal sample size and based on @green1991many, @field2012discovering[273-274] offer the following rules of thumb for an adequate sample size (k = number of predictors; categorical predictors with more than two levels should be recoded as dummy variables):\n\n* if you are interested in the overall model: 50 + 8k (k = number of predictors)\n\n* if you are interested in individual predictors: 104 + k\n\n* if you are interested in both: take the higher value!\n\n### Evaluation of Sample Size{-}\n\nAfter performing the diagnostics, we will now test whether the sample size is adequate and what the values of R would be based on a random distribution in order to be able to estimate how likely a $\\beta$-error is given the present sample size [see @field2012discovering 274]. Beta errors (or $\\beta$-errors) refer to the erroneous assumption that a predictor is not significant (based on the analysis and given the sample) although it does have an effect in the population. In other words, $\\beta$-error means to overlook a significant effect because of weaknesses of the analysis. The test statistics ranges between 0 and 1 where lower values are better. If the values approximate 1, then there is serious concern as the model is not reliable given the sample size. In such cases, unfortunately, the best option is to increase the sample size.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# load functions\nsource(\"https://slcladal.github.io/rscripts/SampleSizeMLR.r\")\nsource(\"https://slcladal.github.io/rscripts/ExpR.r\")\n# check if sample size is sufficient\nsmplesz(m2.mlr)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Sample too small: please increase your sample by  9  data points\"\n```\n:::\n\n```{.r .cell-code}\n# check beta-error likelihood\nexpR(m2.mlr)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Based on the sample size expect a false positive correlation of 0.0309 between the predictors and the predicted\"\n```\n:::\n:::\n\n\nThe function `smplesz` reports that the sample size is insufficient by 9 data points according to @green1991many. The likelihood of $\\beta$-errors, however, is very small (0.0309). As a last step, we summarize the results of the regression analysis.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# tabulate model results\nsjPlot::tab_model(m0.glm, m2.glm)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table style=\"border-collapse:collapse; border:none;\">\n<tr>\n<th style=\"border-top: double; text-align:center; font-style:normal; font-weight:bold; padding:0.2cm;  text-align:left; \">&nbsp;</th>\n<th colspan=\"3\" style=\"border-top: double; text-align:center; font-style:normal; font-weight:bold; padding:0.2cm; \">money</th>\n<th colspan=\"3\" style=\"border-top: double; text-align:center; font-style:normal; font-weight:bold; padding:0.2cm; \">money</th>\n</tr>\n<tr>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  text-align:left; \">Predictors</td>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  \">Estimates</td>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  \">CI</td>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  \">p</td>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  \">Estimates</td>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  \">CI</td>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  col7\">p</td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; \">(Intercept)</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">88.12</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">78.72&nbsp;&ndash;&nbsp;97.52</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"><strong>&lt;0.001</strong></td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">99.15</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">92.10&nbsp;&ndash;&nbsp;106.21</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  col7\"><strong>&lt;0.001</strong></td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; \">status [Single]</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"></td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"></td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"></td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">55.85</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">45.78&nbsp;&ndash;&nbsp;65.93</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  col7\"><strong>&lt;0.001</strong></td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; \">attraction<br>[NotInterested]</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"></td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"></td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"></td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">&#45;47.66</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">&#45;57.63&nbsp;&ndash;&nbsp;-37.69</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  col7\"><strong>&lt;0.001</strong></td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; \">status [Single] *<br>attraction<br>[NotInterested]</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"></td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"></td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"></td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">&#45;59.46</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">&#45;73.71&nbsp;&ndash;&nbsp;-45.21</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  col7\"><strong>&lt;0.001</strong></td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; padding-top:0.1cm; padding-bottom:0.1cm; border-top:1px solid;\">Observations</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left; border-top:1px solid;\" colspan=\"3\">98</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left; border-top:1px solid;\" colspan=\"3\">98</td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; padding-top:0.1cm; padding-bottom:0.1cm;\">R<sup>2</sup></td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left;\" colspan=\"3\">0.000</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left;\" colspan=\"3\">0.857</td>\n</tr>\n\n</table>\n\n`````\n:::\n:::\n\n\n***\n\n<div class=\"warning\" style='padding:0.1em; background-color:#f2f2f2; color:#51247a'>\n<span>\n<p style='margin-top:1em; text-align:center'>\n<b>NOTE</b><br><br>The R^2^ values in this report is incorrect! As we have seen above, and is also shown in the table below, the correct R^2^ values are: multiple R^2^ 0.8574, adjusted R^2^ 0.8528.<br></p>\n<p style='margin-left:1em;'>\n</p></span>\n</div>\n\n<br>\n\nAdditionally, we can inspect the summary of the regression model as shown below to extract additional information. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(m2.mlr)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = money ~ (status + attraction)^2, data = mlrdata)\n\nResiduals:\n         Min           1Q       Median           3Q          Max \n-35.76416667 -13.50520000  -0.98948333  10.59887500  38.77166667 \n\nCoefficients:\n                                         Estimate   Std. Error  t value\n(Intercept)                           99.15480000   3.59735820 27.56323\nstatusSingle                          55.85353333   5.14015367 10.86612\nattractionNotInterested              -47.66280000   5.08743275 -9.36873\nstatusSingle:attractionNotInterested -59.46136667   7.26927504 -8.17982\n                                                   Pr(>|t|)    \n(Intercept)                          < 0.000000000000000222 ***\nstatusSingle                         < 0.000000000000000222 ***\nattractionNotInterested               0.0000000000000040429 ***\nstatusSingle:attractionNotInterested  0.0000000000013375166 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 17.986791 on 94 degrees of freedom\nMultiple R-squared:  0.857375902,\tAdjusted R-squared:  0.852824069 \nF-statistic: 188.358387 on 3 and 94 DF,  p-value: < 0.0000000000000002220446\n```\n:::\n:::\n\n\nAlthough @field2012discovering suggest that the main effects of the predictors involved in the interaction should not be interpreted, they are interpreted here to illustrate how the results of a multiple linear regression can be reported. \n\n\nWe can use the `reports` package [@report] to summarize the analysis.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nreport::report(m2.mlr)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nWe fitted a linear model (estimated using OLS) to predict money with status and attraction (formula: money ~ (status + attraction)^2). The model explains a statistically significant and substantial proportion of variance (R2 = 0.86, F(3, 94) = 188.36, p < .001, adj. R2 = 0.85). The model's intercept, corresponding to status = Relationship and attraction = Interested, is at 99.15 (95% CI [92.01, 106.30], t(94) = 27.56, p < .001). Within this model:\n\n  - The effect of status [Single] is statistically significant and positive (beta = 55.85, 95% CI [45.65, 66.06], t(94) = 10.87, p < .001; Std. beta = 1.19, 95% CI [0.97, 1.41])\n  - The effect of attraction [NotInterested] is statistically significant and negative (beta = -47.66, 95% CI [-57.76, -37.56], t(94) = -9.37, p < .001; Std. beta = -1.02, 95% CI [-1.23, -0.80])\n  - The interaction effect of attraction [NotInterested] on status [Single] is statistically significant and negative (beta = -59.46, 95% CI [-73.89, -45.03], t(94) = -8.18, p < .001; Std. beta = -1.27, 95% CI [-1.58, -0.96])\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using the Wald approximation.\n```\n:::\n:::\n\n\nWe can use this output to write up a final report: \n\n\nA multiple linear regression was fitted to the data using an automated, step-wise, AIC-based (Akaike's Information Criterion) procedure. The model fitting arrived at a final minimal model. During the model diagnostics, two outliers were detected and removed. Further diagnostics did not find other issues after the removal.\n\nThe final minimal adequate regression model is based on 98 data points and performs highly significantly better than a minimal baseline model (multiple R^2^: .857, adjusted R^2^: .853, F-statistic (3, 94): 154.4, AIC: 850.4, BIC: 863.32, p<.001^$***$^). The final minimal adequate regression model reports *attraction* and *status* as significant main effects. The relationship status of men correlates highly significantly and positively with the amount of money spend on the women's presents (SE: 5.14, t-value: 10.87, p<.001^$***$^). This shows that men spend 156.8 dollars on presents if they are single while they spend 99,15 dollars if they are in a relationship. Whether men are attracted to women also correlates highly significantly and positively with the money they spend on women (SE: 5.09, t-values: -9.37, p<.001^$***$^). If men are not interested in women, they spend 47.66 dollar less on a present for women compared with women the men are interested in.\n\nFurthermore, the final minimal adequate regression model reports a highly significant interaction between relationship *status* and *attraction* (SE: 7.27, t-value: -8.18, p<.001^$***$^): If men are single but they are not interested in a women, a man would spend only 59.46 dollars on a present compared to all other constellations.\n\n## Multiple Binomial Logistic Regression{-}\n\nLogistic regression is a multivariate analysis technique that builds on and is very similar in terms of its implementation to linear regression but logistic regressions take dependent variables that represent nominal rather than numeric scaling [@harrell2015regression]. The difference requires that the linear regression must be modified in certain ways to avoid producing non-sensical outcomes. The most fundamental difference between logistic and linear regressions is that logistic regression work on the probabilities of an outcome (the likelihood), rather than the outcome itself. In addition, the likelihoods on which the logistic regression works must be logged (logarithmized) in order to avoid produce predictions that produce values greater than 1 (instance occurs) and 0 (instance does not occur). You can check this by logging the values from -10 to 10 using the `plogis` function as shown below.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nround(plogis(-10:10), 5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] 0.00005 0.00012 0.00034 0.00091 0.00247 0.00669 0.01799 0.04743 0.11920\n[10] 0.26894 0.50000 0.73106 0.88080 0.95257 0.98201 0.99331 0.99753 0.99909\n[19] 0.99966 0.99988 0.99995\n```\n:::\n:::\n\n\nIf we visualize these logged values, we get an S-shaped curve which reflects the logistic function.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](regression_files/figure-html/blm0b-1.png){width=672}\n:::\n:::\n\n\n\nTo understand what this mean, we will use a very simple example. In this example, we want to see whether the height of men affect their likelihood of being in a relationship. The data we use represents a data set consisting of two variables: height and relationship.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](regression_files/figure-html/blm1-1.png){width=672}\n:::\n:::\n\n\nThe left panel of the Figure above shows that a linear model would predict values for the relationship status, which represents a factor (0 = Single and 1 = In a Relationship), that are nonsensical because values above 1 or below 0 do not make sense. In contrast to a linear regression, which predicts actual values, such as the frequencies of prepositions in a certain text, a logistic regression predicts *probabilities* of events (for example, being in a relationship) rather than actual values. The center panel shows the predictions of a logistic regression and we see that a logistic regression also has an intercept and a (very steep) slope but that the regression line also predicts values that are above 1 and below 0. However, when we log the predicted values we these predicted values are transformed into probabilities with values between 0 and 1. And the logged regression line has a S-shape which reflects the logistic function. Furthermore, we can then find the optimal line (the line with the lowest residual deviance) by comparing the sum of residuals - just as we did for a simple linear model and that way, we find the regression line for a logistic regression. \n\n### Example 1: EH in Kiwi English{-}\n\nTo exemplify how to implement a logistic regression in R [see @agresti1996introduction; @agresti2011categorical] for very good and thorough introductions to this topic], we will analyze the use of the discourse particle *eh* in New Zealand English and test which factors correlate with its occurrence. The data set represents speech units in a corpus that were coded for the speaker who uttered a given speech unit, the gender, ethnicity, and age of that speaker and whether or not the speech unit contained an *eh*. To begin with, we clean the current work space, set option, install and activate relevant packages, load customized functions, and load the example data set.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# load data\nblrdata  <- base::readRDS(url(\"https://slcladal.github.io/data/bld.rda\", \"rb\"))\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n```{=html}\n<div class=\"tabwid\"><style>.cl-ba38864c{table-layout:auto;width:75%;}.cl-ba33f58c{font-family:'DejaVu Sans';font-size:12pt;font-weight:bold;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-ba33f596{font-family:'DejaVu Sans';font-size:12pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-ba34048c{margin:0;text-align:center;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-ba340496{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-ba344244{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ba34424e{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ba344258{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ba344259{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ba344262{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ba344263{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ba34426c{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ba344276{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ba344277{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ba344280{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ba344281{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ba34428a{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}</style><table class='cl-ba38864c'>\n```\n<caption class=\"Table Caption\">\n\nFirst 15 rows of the blrdata.\n\n</caption>\n```{=html}\n<thead><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-ba344280\"><p class=\"cl-ba34048c\"><span class=\"cl-ba33f58c\">ID</span></p></td><td class=\"cl-ba344281\"><p class=\"cl-ba34048c\"><span class=\"cl-ba33f58c\">Gender</span></p></td><td class=\"cl-ba344281\"><p class=\"cl-ba34048c\"><span class=\"cl-ba33f58c\">Age</span></p></td><td class=\"cl-ba344281\"><p class=\"cl-ba34048c\"><span class=\"cl-ba33f58c\">Ethnicity</span></p></td><td class=\"cl-ba34428a\"><p class=\"cl-ba340496\"><span class=\"cl-ba33f58c\">EH</span></p></td></tr></thead><tbody><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-ba344258\"><p class=\"cl-ba34048c\"><span class=\"cl-ba33f596\">&lt;S1A-001#M&gt;</span></p></td><td class=\"cl-ba34424e\"><p class=\"cl-ba34048c\"><span class=\"cl-ba33f596\">Men</span></p></td><td class=\"cl-ba34424e\"><p class=\"cl-ba34048c\"><span class=\"cl-ba33f596\">Young</span></p></td><td class=\"cl-ba34424e\"><p class=\"cl-ba34048c\"><span class=\"cl-ba33f596\">Pakeha</span></p></td><td class=\"cl-ba344244\"><p class=\"cl-ba340496\"><span class=\"cl-ba33f596\">0</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-ba344263\"><p class=\"cl-ba34048c\"><span class=\"cl-ba33f596\">&lt;S1A-001#M&gt;</span></p></td><td class=\"cl-ba344259\"><p class=\"cl-ba34048c\"><span class=\"cl-ba33f596\">Men</span></p></td><td class=\"cl-ba344259\"><p class=\"cl-ba34048c\"><span class=\"cl-ba33f596\">Young</span></p></td><td class=\"cl-ba344259\"><p class=\"cl-ba34048c\"><span class=\"cl-ba33f596\">Pakeha</span></p></td><td class=\"cl-ba344262\"><p class=\"cl-ba340496\"><span class=\"cl-ba33f596\">1</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-ba344258\"><p class=\"cl-ba34048c\"><span class=\"cl-ba33f596\">&lt;S1A-001#M&gt;</span></p></td><td class=\"cl-ba34424e\"><p class=\"cl-ba34048c\"><span class=\"cl-ba33f596\">Men</span></p></td><td class=\"cl-ba34424e\"><p class=\"cl-ba34048c\"><span class=\"cl-ba33f596\">Young</span></p></td><td class=\"cl-ba34424e\"><p class=\"cl-ba34048c\"><span class=\"cl-ba33f596\">Pakeha</span></p></td><td class=\"cl-ba344244\"><p class=\"cl-ba340496\"><span class=\"cl-ba33f596\">0</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-ba344263\"><p class=\"cl-ba34048c\"><span class=\"cl-ba33f596\">&lt;S1A-001#M&gt;</span></p></td><td class=\"cl-ba344259\"><p class=\"cl-ba34048c\"><span class=\"cl-ba33f596\">Men</span></p></td><td class=\"cl-ba344259\"><p class=\"cl-ba34048c\"><span class=\"cl-ba33f596\">Young</span></p></td><td class=\"cl-ba344259\"><p class=\"cl-ba34048c\"><span class=\"cl-ba33f596\">Pakeha</span></p></td><td class=\"cl-ba344262\"><p class=\"cl-ba340496\"><span class=\"cl-ba33f596\">0</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-ba344258\"><p class=\"cl-ba34048c\"><span class=\"cl-ba33f596\">&lt;S1A-001#M&gt;</span></p></td><td class=\"cl-ba34424e\"><p class=\"cl-ba34048c\"><span class=\"cl-ba33f596\">Men</span></p></td><td class=\"cl-ba34424e\"><p class=\"cl-ba34048c\"><span class=\"cl-ba33f596\">Young</span></p></td><td class=\"cl-ba34424e\"><p class=\"cl-ba34048c\"><span class=\"cl-ba33f596\">Pakeha</span></p></td><td class=\"cl-ba344244\"><p class=\"cl-ba340496\"><span class=\"cl-ba33f596\">1</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-ba344263\"><p class=\"cl-ba34048c\"><span class=\"cl-ba33f596\">&lt;S1A-001#M&gt;</span></p></td><td class=\"cl-ba344259\"><p class=\"cl-ba34048c\"><span class=\"cl-ba33f596\">Men</span></p></td><td class=\"cl-ba344259\"><p class=\"cl-ba34048c\"><span class=\"cl-ba33f596\">Young</span></p></td><td class=\"cl-ba344259\"><p class=\"cl-ba34048c\"><span class=\"cl-ba33f596\">Pakeha</span></p></td><td class=\"cl-ba344262\"><p class=\"cl-ba340496\"><span class=\"cl-ba33f596\">1</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-ba344258\"><p class=\"cl-ba34048c\"><span class=\"cl-ba33f596\">&lt;S1A-001#M&gt;</span></p></td><td class=\"cl-ba34424e\"><p class=\"cl-ba34048c\"><span class=\"cl-ba33f596\">Men</span></p></td><td class=\"cl-ba34424e\"><p class=\"cl-ba34048c\"><span class=\"cl-ba33f596\">Young</span></p></td><td class=\"cl-ba34424e\"><p class=\"cl-ba34048c\"><span class=\"cl-ba33f596\">Pakeha</span></p></td><td class=\"cl-ba344244\"><p class=\"cl-ba340496\"><span class=\"cl-ba33f596\">0</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-ba344263\"><p class=\"cl-ba34048c\"><span class=\"cl-ba33f596\">&lt;S1A-001#M&gt;</span></p></td><td class=\"cl-ba344259\"><p class=\"cl-ba34048c\"><span class=\"cl-ba33f596\">Men</span></p></td><td class=\"cl-ba344259\"><p class=\"cl-ba34048c\"><span class=\"cl-ba33f596\">Young</span></p></td><td class=\"cl-ba344259\"><p class=\"cl-ba34048c\"><span class=\"cl-ba33f596\">Pakeha</span></p></td><td class=\"cl-ba344262\"><p class=\"cl-ba340496\"><span class=\"cl-ba33f596\">0</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-ba344258\"><p class=\"cl-ba34048c\"><span class=\"cl-ba33f596\">&lt;S1A-001#M&gt;</span></p></td><td class=\"cl-ba34424e\"><p class=\"cl-ba34048c\"><span class=\"cl-ba33f596\">Men</span></p></td><td class=\"cl-ba34424e\"><p class=\"cl-ba34048c\"><span class=\"cl-ba33f596\">Young</span></p></td><td class=\"cl-ba34424e\"><p class=\"cl-ba34048c\"><span class=\"cl-ba33f596\">Pakeha</span></p></td><td class=\"cl-ba344244\"><p class=\"cl-ba340496\"><span class=\"cl-ba33f596\">0</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-ba344263\"><p class=\"cl-ba34048c\"><span class=\"cl-ba33f596\">&lt;S1A-001#M&gt;</span></p></td><td class=\"cl-ba344259\"><p class=\"cl-ba34048c\"><span class=\"cl-ba33f596\">Men</span></p></td><td class=\"cl-ba344259\"><p class=\"cl-ba34048c\"><span class=\"cl-ba33f596\">Young</span></p></td><td class=\"cl-ba344259\"><p class=\"cl-ba34048c\"><span class=\"cl-ba33f596\">Pakeha</span></p></td><td class=\"cl-ba344262\"><p class=\"cl-ba340496\"><span class=\"cl-ba33f596\">1</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-ba344258\"><p class=\"cl-ba34048c\"><span class=\"cl-ba33f596\">&lt;S1A-001#M&gt;</span></p></td><td class=\"cl-ba34424e\"><p class=\"cl-ba34048c\"><span class=\"cl-ba33f596\">Men</span></p></td><td class=\"cl-ba34424e\"><p class=\"cl-ba34048c\"><span class=\"cl-ba33f596\">Young</span></p></td><td class=\"cl-ba34424e\"><p class=\"cl-ba34048c\"><span class=\"cl-ba33f596\">Pakeha</span></p></td><td class=\"cl-ba344244\"><p class=\"cl-ba340496\"><span class=\"cl-ba33f596\">0</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-ba344263\"><p class=\"cl-ba34048c\"><span class=\"cl-ba33f596\">&lt;S1A-001#M&gt;</span></p></td><td class=\"cl-ba344259\"><p class=\"cl-ba34048c\"><span class=\"cl-ba33f596\">Men</span></p></td><td class=\"cl-ba344259\"><p class=\"cl-ba34048c\"><span class=\"cl-ba33f596\">Young</span></p></td><td class=\"cl-ba344259\"><p class=\"cl-ba34048c\"><span class=\"cl-ba33f596\">Pakeha</span></p></td><td class=\"cl-ba344262\"><p class=\"cl-ba340496\"><span class=\"cl-ba33f596\">0</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-ba344258\"><p class=\"cl-ba34048c\"><span class=\"cl-ba33f596\">&lt;S1A-001#M&gt;</span></p></td><td class=\"cl-ba34424e\"><p class=\"cl-ba34048c\"><span class=\"cl-ba33f596\">Men</span></p></td><td class=\"cl-ba34424e\"><p class=\"cl-ba34048c\"><span class=\"cl-ba33f596\">Young</span></p></td><td class=\"cl-ba34424e\"><p class=\"cl-ba34048c\"><span class=\"cl-ba33f596\">Pakeha</span></p></td><td class=\"cl-ba344244\"><p class=\"cl-ba340496\"><span class=\"cl-ba33f596\">1</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-ba344263\"><p class=\"cl-ba34048c\"><span class=\"cl-ba33f596\">&lt;S1A-001#M&gt;</span></p></td><td class=\"cl-ba344259\"><p class=\"cl-ba34048c\"><span class=\"cl-ba33f596\">Men</span></p></td><td class=\"cl-ba344259\"><p class=\"cl-ba34048c\"><span class=\"cl-ba33f596\">Young</span></p></td><td class=\"cl-ba344259\"><p class=\"cl-ba34048c\"><span class=\"cl-ba33f596\">Pakeha</span></p></td><td class=\"cl-ba344262\"><p class=\"cl-ba340496\"><span class=\"cl-ba33f596\">1</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-ba344276\"><p class=\"cl-ba34048c\"><span class=\"cl-ba33f596\">&lt;S1A-001#M&gt;</span></p></td><td class=\"cl-ba34426c\"><p class=\"cl-ba34048c\"><span class=\"cl-ba33f596\">Men</span></p></td><td class=\"cl-ba34426c\"><p class=\"cl-ba34048c\"><span class=\"cl-ba33f596\">Young</span></p></td><td class=\"cl-ba34426c\"><p class=\"cl-ba34048c\"><span class=\"cl-ba33f596\">Pakeha</span></p></td><td class=\"cl-ba344277\"><p class=\"cl-ba340496\"><span class=\"cl-ba33f596\">0</span></p></td></tr></tbody></table></div>\n```\n:::\n:::\n\n\nThe summary of the data show that the data set contains 25,821 observations of five variables. The variable `ID` contains strings that represent a combination file and speaker of a speech unit. The second variable represents the gender, the third the age, and the fourth the ethnicity of speakers. The fifth variable represents whether or not a speech unit contained the discourse particle *eh*. \n\nNext, we factorize the variables in our data set. In other words, we specify that the strings represent variable levels and define new reference levels because as a default `R` will use the variable level which first occurs in alphabet ordering as the reference level for each variable, we redefine the variable levels for Age and Ethnicity.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nblrdata <- blrdata %>%\n  # factorize variables\n  dplyr::mutate(Age = factor(Age),\n                Gender = factor(Gender),\n                Ethnicity = factor(Ethnicity),\n                ID = factor(ID),\n                EH = factor(EH)) %>%\n  # relevel Age (Reference = Young) and Ethnicity (Reference= Pakeha))\n  dplyr::mutate(Age = relevel(Age, \"Young\"),\n                Ethnicity = relevel(Ethnicity, \"Pakeha\"))\n```\n:::\n\n\nAfter preparing the data, we will now plot the data to get an overview of potential relationships between variables.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nblrdata %>%\n  dplyr::mutate(EH = ifelse(EH == \"0\", 0, 1)) %>%\n  ggplot(aes(Age, EH, color = Gender)) +\n  facet_wrap(~Ethnicity) +\n  stat_summary(fun = mean, geom = \"point\") +\n  stat_summary(fun.data = mean_cl_boot, geom = \"errorbar\", width = 0.2) +\n  theme_set(theme_bw(base_size = 10)) +\n  theme(legend.position = \"top\") +\n  labs(x = \"\", y = \"Observed Probabilty of eh\") +\n  scale_color_manual(values = c(\"gray20\", \"gray70\"))\n```\n\n::: {.cell-output-display}\n![](regression_files/figure-html/blm6-1.png){width=672}\n:::\n:::\n\n\nWith respect to main effects, the Figure above indicates that men use *eh* more frequently than women, that young speakers use it more frequently compared with old speakers, and that speakers that are descendants of European settlers (Pakeha) use *eh* more frequently compared with Maori (the native inhabitants of New Zealand).\n\nThe plots in the lower panels do not indicate significant interactions between use of *eh* and the Age, Gender, and Ethnicity of speakers. In a next step, we will start building the logistic regression model.\n\n### Model Building{-}\n\nAs a first step, we need to define contrasts and use the `datadist` function to store aspects of our variables that can be accessed later when plotting and summarizing the model. Contrasts define what and how variable levels should be compared and therefore influences how the results of the regression analysis are presented. In this case, we use treatment contrasts which are in-built. Treatment contrasts mean that we assess the significance of levels of a predictor against a baseline which is the reference level of a predictor. @field2012discovering [414-427] and @gries2021statistics provide very good and accessible explanations of contrasts and how to manually define contrasts if you would like to know more. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# set contrasts\noptions(contrasts  =c(\"contr.treatment\", \"contr.poly\"))\n# create distance matrix\nblrdata.dist <- datadist(blrdata)\n# include distance matrix in options\noptions(datadist = \"blrdata.dist\")\n```\n:::\n\n\nNext, we generate a minimal model that predicts the use of *eh* solely based on the intercept.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# baseline glm model\nm0.blr = glm(EH ~ 1, family = binomial, data = blrdata)\n```\n:::\n\n\n### Model fitting{-}\n\nWe will now start with the model fitting procedure. In the present case, we will use a manual step-wise step-up procedure during which predictors are added to the model if they significantly improve the model fit. In addition, we will perform diagnostics as we fit the model at each step of the model fitting process rather than after the fitting.\n\nWe will test two things in particular: whether the data has incomplete information or complete separation and if the model suffers from (multi-)collinearity. \n\nIncomplete information or complete separation means that the data does not contain all combinations of the predictor or the dependent variable. This is important because if the data does not contain cases of all combinations, the model will assume that it has found a perfect predictor. In such cases the model overestimates the effect of that that predictor and the results of that model are no longer reliable. For example, if *eh* was only used by young speakers in the data, the model would jump on that fact and say *Ha! If there is an old speaker, that means that that speaker will never ever and under no circumstances say *eh* - I can therefore ignore all other factors!* \n\nMulticollinearity means that predictors correlate and have shared variance. This means that whichever predictor is included first will take all the variance that it can explain and the remaining part of the variable that is shared will not be attributed to the other predictor. This may lead to reporting that a factor is not significant because all of the variance it can explain is already accounted for. However, if the other predictor were included first, then the original predictor would be returned as insignificant. This means that- depending on the order in which predictors are added - the results of the regression can differ dramatically and the model is therefore not reliable. Multicollinearity is actually a very common problem and there are various ways to deal with it but it cannot be ignored (at least in regression analyses).\n\nWe will start by adding *Age* to the minimal adequate model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# check incomplete information\nifelse(min(ftable(blrdata$Age, blrdata$EH)) == 0, \"not possible\", \"possible\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"possible\"\n```\n:::\n\n```{.r .cell-code}\n# add age to the model\nm1.blr = glm(EH ~ Age, family = binomial, data = blrdata)\n# check multicollinearity (vifs should have values of 3 or lower for main effects)\nifelse(max(vif(m1.blr)) <= 3,  \"vifs ok\", \"WARNING: high vifs!\") # VIFs ok\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"vifs ok\"\n```\n:::\n\n```{.r .cell-code}\n# check if adding Age significantly improves model fit\nanova(m1.blr, m0.blr, test = \"Chi\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Deviance Table\n\nModel 1: EH ~ Age\nModel 2: EH ~ 1\n  Resid. Df  Resid. Dev Df     Deviance               Pr(>Chi)    \n1     25819 32376.86081                                           \n2     25820 33007.75469 -1 -630.8938871 < 0.000000000000000222 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\nAs the data does not contain incomplete information, the vif values are below 3, and adding *Age* has significantly improved the model fit (the p-value of the ANOVA is lower than .05). We therefore proceed with *Age* included.\n\nWe continue by adding *Gender*. We add a second ANOVA test to see if including Gender affects the significance of other predictors in the model. If this were the case - if adding Gender would cause Age to become insignificant - then we could change the ordering in which we include predictors into our model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nifelse(min(ftable(blrdata$Gender, blrdata$EH)) == 0, \"not possible\", \"possible\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"possible\"\n```\n:::\n\n```{.r .cell-code}\nm2.blr <- update(m1.blr, . ~ . +Gender)\nifelse(max(vif(m2.blr)) <= 3,  \"vifs ok\", \"WARNING: high vifs!\") # VIFs ok\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"vifs ok\"\n```\n:::\n\n```{.r .cell-code}\nanova(m2.blr, m1.blr, test = \"Chi\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Deviance Table\n\nModel 1: EH ~ Age + Gender\nModel 2: EH ~ Age\n  Resid. Df  Resid. Dev Df    Deviance               Pr(>Chi)    \n1     25818 32139.54089                                          \n2     25819 32376.86081 -1 -237.319914 < 0.000000000000000222 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n\n```{.r .cell-code}\nAnova(m2.blr, test = \"LR\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Deviance Table (Type II tests)\n\nResponse: EH\n          LR Chisq Df             Pr(>Chisq)    \nAge    668.6350712  1 < 0.000000000000000222 ***\nGender 237.3199140  1 < 0.000000000000000222 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\n\nAgain, including *Gender* significantly improves model fit and the data does not contain incomplete information or complete separation. Also, including *Gender* does not affect the significance of *Age*. Now, we include *Ethnicity*.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nifelse(min(ftable(blrdata$Ethnicity, blrdata$EH)) == 0, \"not possible\", \"possible\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"possible\"\n```\n:::\n\n```{.r .cell-code}\nm3.blr <- update(m2.blr, . ~ . +Ethnicity)\nifelse(max(vif(m3.blr)) <= 3,  \"vifs ok\", \"WARNING: high vifs!\") # VIFs ok\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"vifs ok\"\n```\n:::\n\n```{.r .cell-code}\nanova(m3.blr, m2.blr, test = \"Chi\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Deviance Table\n\nModel 1: EH ~ Age + Gender + Ethnicity\nModel 2: EH ~ Age + Gender\n  Resid. Df  Resid. Dev Df      Deviance Pr(>Chi)\n1     25817 32139.27988                          \n2     25818 32139.54089 -1 -0.2610145387  0.60942\n```\n:::\n:::\n\n\nSince adding *Ethnicity* does not significantly improve the model fit, we do not need to test if its inclusion affects the significance of other predictors. We continue without *Ethnicity* and include the interaction between *Age* and *Gender*.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nifelse(min(ftable(blrdata$Age, blrdata$Gender, blrdata$EH)) == 0, \"not possible\", \"possible\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"possible\"\n```\n:::\n\n```{.r .cell-code}\nm4.blr <- update(m2.blr, . ~ . +Age*Gender)\nifelse(max(vif(m4.blr)) <= 3,  \"vifs ok\", \"WARNING: high vifs!\") # VIFs ok\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"vifs ok\"\n```\n:::\n\n```{.r .cell-code}\nanova(m4.blr, m2.blr, test = \"Chi\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Deviance Table\n\nModel 1: EH ~ Age + Gender + Age:Gender\nModel 2: EH ~ Age + Gender\n  Resid. Df  Resid. Dev Df     Deviance Pr(>Chi)\n1     25817 32139.41665                         \n2     25818 32139.54089 -1 -0.124239923  0.72448\n```\n:::\n:::\n\n\nThe interaction between *Age* and *Gender* is not significant which means that men and women do not behave differently with respect to their use of `EH` as they age. Also, the data does not contain incomplete information and the model does not suffer from multicollinearity - the predictors are not collinear. We can now include if there is a significant interaction between *Age* and *Ethnicity*.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nifelse(min(ftable(blrdata$Age, blrdata$Ethnicity, blrdata$EH)) == 0, \"not possible\", \"possible\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"possible\"\n```\n:::\n\n```{.r .cell-code}\nm5.blr <- update(m2.blr, . ~ . +Age*Ethnicity)\nifelse(max(vif(m5.blr)) <= 3,  \"vifs ok\", \"WARNING: high vifs!\") # VIFs ok\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"vifs ok\"\n```\n:::\n\n```{.r .cell-code}\nanova(m5.blr, m2.blr, test = \"Chi\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Deviance Table\n\nModel 1: EH ~ Age + Gender + Ethnicity + Age:Ethnicity\nModel 2: EH ~ Age + Gender\n  Resid. Df  Resid. Dev Df     Deviance Pr(>Chi)\n1     25816 32136.47224                         \n2     25818 32139.54089 -2 -3.068654514   0.2156\n```\n:::\n:::\n\n\nAgain, no incomplete information or multicollinearity and no significant interaction. Now, we test if there exists a significant interaction between *Gender* and *Ethnicity*.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nifelse(min(ftable(blrdata$Gender, blrdata$Ethnicity, blrdata$EH)) == 0, \"not possible\", \"possible\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"possible\"\n```\n:::\n\n```{.r .cell-code}\nm6.blr <- update(m2.blr, . ~ . +Gender*Ethnicity)\nifelse(max(vif(m6.blr)) <= 3,  \"vifs ok\", \"WARNING: high vifs!\") # VIFs ok\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"vifs ok\"\n```\n:::\n\n```{.r .cell-code}\nanova(m6.blr, m2.blr, test = \"Chi\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Deviance Table\n\nModel 1: EH ~ Age + Gender + Ethnicity + Gender:Ethnicity\nModel 2: EH ~ Age + Gender\n  Resid. Df  Resid. Dev Df      Deviance Pr(>Chi)\n1     25816 32139.26864                          \n2     25818 32139.54089 -2 -0.2722521835  0.87273\n```\n:::\n:::\n\n\n\nAs the interaction between *Gender* and *Ethnicity* is not significant, we continue without it. In a final step, we include the three-way interaction between *Age*, *Gender*, and *Ethnicity*.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nifelse(min(ftable(blrdata$Age, blrdata$Gender, blrdata$Ethnicity, blrdata$EH)) == 0, \"not possible\", \"possible\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"possible\"\n```\n:::\n\n```{.r .cell-code}\nm7.blr <- update(m2.blr, . ~ . +Gender*Ethnicity)\nifelse(max(vif(m7.blr)) <= 3,  \"vifs ok\", \"WARNING: high vifs!\") # VIFs ok\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"vifs ok\"\n```\n:::\n\n```{.r .cell-code}\nanova(m7.blr, m2.blr, test = \"Chi\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Deviance Table\n\nModel 1: EH ~ Age + Gender + Ethnicity + Gender:Ethnicity\nModel 2: EH ~ Age + Gender\n  Resid. Df  Resid. Dev Df      Deviance Pr(>Chi)\n1     25816 32139.26864                          \n2     25818 32139.54089 -2 -0.2722521835  0.87273\n```\n:::\n:::\n\n\nWe have found our final minimal adequate model because the 3-way interaction is also insignificant. As we have now arrived at the final minimal adequate model (m2.blr), we generate a final minimal model using the lrm function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm2.lrm <- lrm(EH ~ Age+Gender, data = blrdata, x = T, y = T, linear.predictors = T)\nm2.lrm\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLogistic Regression Model\n \n lrm(formula = EH ~ Age + Gender, data = blrdata, x = T, y = T, \n     linear.predictors = T)\n \n                        Model Likelihood        Discrimination    Rank Discrim.    \n                              Ratio Test               Indexes          Indexes    \n Obs         25821    LR chi2     868.21        R2       0.046    C       0.602    \n  0          17114    d.f.             2      R2(2,25821)0.033    Dxy     0.203    \n  1           8707    Pr(> chi2) <0.0001    R2(2,17312.8)0.049    gamma   0.302    \n max |deriv| 3e-10                              Brier    0.216    tau-a   0.091    \n \n              Coef    S.E.   Wald Z Pr(>|Z|)\n Intercept    -0.2324 0.0223 -10.44 <0.0001 \n Age=Old      -0.8305 0.0335 -24.78 <0.0001 \n Gender=Women -0.4201 0.0273 -15.42 <0.0001 \n \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nanova(m2.lrm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                Wald Statistics          Response: EH \n\n Factor     Chi-Square d.f. P     \n Age        614.04     1    <.0001\n Gender     237.65     1    <.0001\n TOTAL      802.65     2    <.0001\n```\n:::\n:::\n\n\nAfter fitting the model, we validate the model to avoid arriving at a final minimal model that is overfitted to the data at hand.\n\n### Model Validation{-}\n\nTo validate a model, you can apply the `validate` function and apply it to a saturated model. The output of the `validate` function shows how often predictors are retained if the sample is re-selected with the same size but with placing back drawn data points. The execution of the function requires some patience as it is rather computationally expensive and it is, therefore, commented out below.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# model validation (remove # to activate: output too long for website)\nm7.lrm <- lrm(EH ~ (Age+Gender+Ethnicity)^3, data = blrdata, x = T, y = T, linear.predictors = T)\n#validate(m7.lrm, bw = T, B = 200)\n```\n:::\n\n\nThe `validate` function shows that retaining two predictors (Age and Gender) is the best option and thereby confirms our final minimal adequate model as the best minimal model. In addition, we check whether we need to include a penalty for data points because they have too strong of an impact of the model fit. To see whether a penalty is warranted, we apply the `pentrace` function to the final minimal adequate model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npentrace(m2.lrm, seq(0, 0.8, by = 0.05)) # determine penalty\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nBest penalty:\n\n penalty            df\n     0.8 1.99925395138\n\n penalty            df           aic           bic         aic.c\n    0.00 2.00000000000 864.213801108 847.895914321 864.213336316\n    0.05 1.99995335085 864.213893816 847.896387637 864.213429042\n    0.10 1.99990670452 864.213985335 847.896859740 864.213520579\n    0.15 1.99986006100 864.214075641 847.897330609 864.213610904\n    0.20 1.99981342030 864.214164764 847.897800270 864.213700044\n    0.25 1.99976678241 864.214252710 847.898268733 864.213788009\n    0.30 1.99972014734 864.214339446 847.898735961 864.213874762\n    0.35 1.99967351509 864.214424993 847.899201978 864.213960327\n    0.40 1.99962688564 864.214509360 847.899666792 864.214044712\n    0.45 1.99958025902 864.214592526 847.900130382 864.214127896\n    0.50 1.99953363520 864.214674504 847.900592761 864.214209892\n    0.55 1.99948701420 864.214755279 847.901053914 864.214290685\n    0.60 1.99944039601 864.214834874 847.901513865 864.214370299\n    0.65 1.99939378063 864.214913276 847.901972599 864.214448719\n    0.70 1.99934716807 864.214990480 847.902430112 864.214525941\n    0.75 1.99930055832 864.215066506 847.902886425 864.214601985\n    0.80 1.99925395138 864.215141352 847.903341534 864.214676849\n```\n:::\n:::\n\n\nThe values are so similar  that a penalty is unnecessary. In a next step, we rename the final models.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlr.glm <- m2.blr  # rename final minimal adequate glm model\nlr.lrm <- m2.lrm  # rename final minimal adequate lrm model\n```\n:::\n\n\nNow, we calculate a Model Likelihood Ratio Test to check if the final model performs significantly better than the initial minimal base-line model. The result of this test is provided as a default if we call a summary of the lrm object.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodelChi <- lr.glm$null.deviance - lr.glm$deviance\nchidf <- lr.glm$df.null - lr.glm$df.residual\nchisq.prob <- 1 - pchisq(modelChi, chidf)\nmodelChi; chidf; chisq.prob\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 868.21380111\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0\n```\n:::\n:::\n\n\nThe code above provides three values: a $\\chi$^2^, the degrees of freedom, and a p-value. The p-value is lower than .05 and the results of the Model Likelihood Ratio Test therefore confirm that the final minimal adequate model performs significantly better than the initial minimal base-line model. Another way to extract the model likelihood test statistics is to use an ANOVA to compare the final minimal adequate model to the minimal base-line model.\n\nA handier way to get these statistics is by performing an ANOVA on the final minimal model which, if used this way, is identical to a Model Likelihood Ratio test.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nanova(m0.glm, lr.glm, test = \"Chi\") # Model Likelihood Ratio Test\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in anova.glmlist(c(list(object), dotargs), dispersion = dispersion, :\nmodels with response '\"EH\"' removed because response differs from model 1\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Deviance Table\n\nModel: gaussian, link: identity\n\nResponse: money\n\nTerms added sequentially (first to last)\n\n     Df Deviance Resid. Df  Resid. Dev Pr(>Chi)\nNULL                    97 213227.0608         \n```\n:::\n:::\n\n\nIn a next step, we calculate pseudo-R^2^ values which represent the amount of residual variance that is explained by the final minimal adequate model. We cannot use the ordinary R^2^ because the model works on the logged probabilities rather than the values of the dependent variable.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# calculate pseudo R^2\n# number of cases\nncases <- length(fitted(lr.glm))\nR2.hl <- modelChi/lr.glm$null.deviance\nR.cs <- 1 - exp ((lr.glm$deviance - lr.glm$null.deviance)/ncases)\nR.n <- R.cs /( 1- ( exp (-(lr.glm$null.deviance/ ncases))))\n# function for extracting pseudo-R^2\nlogisticPseudoR2s <- function(LogModel) {\n  dev <- LogModel$deviance\n\tnullDev <- LogModel$null.deviance\n\tmodelN <-  length(LogModel$fitted.values)\n\tR.l <-  1 -  dev / nullDev\n\tR.cs <- 1- exp ( -(nullDev - dev) / modelN)\n\tR.n <- R.cs / ( 1 - ( exp (-(nullDev / modelN))))\n\tcat(\"Pseudo R^2 for logistic regression\\n\")\n\tcat(\"Hosmer and Lemeshow R^2  \", round(R.l, 3), \"\\n\")\n\tcat(\"Cox and Snell R^2        \", round(R.cs, 3), \"\\n\")\n\tcat(\"Nagelkerke R^2           \", round(R.n, 3),    \"\\n\") }\nlogisticPseudoR2s(lr.glm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPseudo R^2 for logistic regression\nHosmer and Lemeshow R^2   0.026 \nCox and Snell R^2         0.033 \nNagelkerke R^2            0.046 \n```\n:::\n:::\n\n\nThe low pseudo-R^2^ values show that our model has very low explanatory power. For instance, the value of Hosmer and Lemeshow R^2^ (0.026) \"is the proportional reduction in the absolute value of the log-likelihood measure and as such it is a measure of how much the badness of fit improves as a result of the inclusion of the predictor variables\" [@field2012discovering 317]. In essence, all the pseudo-R^2^ values are measures of how substantive the model is (how much better it is compared to a baseline model). Next, we extract the confidence intervals for the coefficients of the model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# extract the confidence intervals for the coefficients\nconfint(lr.glm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                      2.5 %          97.5 %\n(Intercept) -0.276050866670 -0.188778707810\nAgeOld      -0.896486392279 -0.765095825382\nGenderWomen -0.473530977637 -0.366703827307\n```\n:::\n:::\n\n\nDespite having low explanatory and predictive power, the age of speakers and their gender are significant as the confidence intervals of the coefficients do not overlap with 0.\n\n### Effect Size{-}\n\nIn a next step, we compute odds ratios and their confidence intervals. Odds Ratios represent a common measure of effect size and can be used to compare effect sizes across models. Odds ratios rang between 0 and infinity. Values of 1 indicate that there is no effect. The further away the values are from 1, the stronger the effect. If the values are lower than 1, then the variable level correlates negatively with the occurrence of the outcome (the probability decreases) while values above 1 indicate a positive correlation and show that the variable level causes an increase in the probability of the outcome (the occurrence of EH).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nexp(lr.glm$coefficients) # odds ratios\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   (Intercept)         AgeOld    GenderWomen \n0.792642499264 0.435815384592 0.656972294902 \n```\n:::\n\n```{.r .cell-code}\nexp(confint(lr.glm))     # confidence intervals of the odds ratios\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWaiting for profiling to be done...\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                     2.5 %         97.5 %\n(Intercept) 0.758774333456 0.827969709653\nAgeOld      0.408000698619 0.465289342309\nGenderWomen 0.622799290871 0.693014866732\n```\n:::\n:::\n\n\nThe odds ratios confirm that older speakers use *eh* significantly less often compared with younger speakers and that women use *eh* less frequently than men as the confidence intervals of the odds rations do not overlap with 1. In a next step, we calculate the prediction accuracy of the model.\n\n### Prediction Accuracy{-}\n\nIn order to calculate the prediction accuracy of the model, we generate a variable called *Prediction* that contains the predictions of pour model and which we add to the data. Then, we use the `confusionMatrix` function from the `caret` package [@caret] to extract the prediction accuracy. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# create variable with contains the prediction of the model\nblrdata <- blrdata %>%\n  dplyr::mutate(Prediction = predict(lr.glm, type = \"response\"),\n                Prediction = ifelse(Prediction > .5, 1, 0),\n                Prediction = factor(Prediction, levels = c(\"0\", \"1\")),\n                EH = factor(EH, levels = c(\"0\", \"1\")))\n# create a confusion matrix with compares observed against predicted values\ncaret::confusionMatrix(blrdata$Prediction, blrdata$EH)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nConfusion Matrix and Statistics\n\n          Reference\nPrediction     0     1\n         0 17114  8707\n         1     0     0\n                                                    \n               Accuracy : 0.66279385                \n                 95% CI : (0.656990096, 0.668560948)\n    No Information Rate : 0.66279385                \n    P-Value [Acc > NIR] : 0.5029107                 \n                                                    \n                  Kappa : 0                         \n                                                    \n Mcnemar's Test P-Value : < 0.00000000000000022     \n                                                    \n            Sensitivity : 1.00000000                \n            Specificity : 0.00000000                \n         Pos Pred Value : 0.66279385                \n         Neg Pred Value :        NaN                \n             Prevalence : 0.66279385                \n         Detection Rate : 0.66279385                \n   Detection Prevalence : 1.00000000                \n      Balanced Accuracy : 0.50000000                \n                                                    \n       'Positive' Class : 0                         \n                                                    \n```\n:::\n:::\n\n\nWe can see that out model has never predicted the use of *eh* which is common when dealing with rare phenomena. This is expected as the event s so rare that the probability of it not occurring substantively outweighs the probability of it occurring. As such, the prediction accuracy of our model is not significantly better compared to the prediction accuracy of the baseline model which is the no-information rate (NIR)) (p = 0.5029).\n\nWe can use the `plot_model` function from the `sjPlot` package [@sjPlot] to visualize the effects.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# predicted probability\nefp1 <- plot_model(lr.glm, type = \"pred\", terms = c(\"Age\"), axis.lim = c(0, 1)) \n# predicted percentage\nefp2 <- plot_model(lr.glm, type = \"pred\", terms = c(\"Gender\"), axis.lim = c(0, 1)) \ngrid.arrange(efp1, efp2, nrow = 1)\n```\n\n::: {.cell-output-display}\n![](regression_files/figure-html/blm27-1.png){width=672}\n:::\n:::\n\n\nAnd we can also combine the visualization of the effects in a single plot as shown below.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsjPlot::plot_model(lr.glm, type = \"pred\", terms = c(\"Age\", \"Gender\"), axis.lim = c(0, 1)) +\n  theme(legend.position = \"top\") +\n  labs(x = \"\", y = \"Predicted Probabilty of eh\", title = \"\") +\n  scale_color_manual(values = c(\"gray20\", \"gray70\"))\n```\n\n::: {.cell-output-display}\n![](regression_files/figure-html/blm28-1.png){width=672}\n:::\n:::\n\n\n\n### Model Diagnostics{-}\n\nWe are now in a position to perform model diagnostics and test if the model violates distributional requirements. In a first step, we test for the existence of multicollinearity.\n\n### Multicollinearity{-}\n\nMulticollinearity means that predictors in a model can be predicted by other predictors in the model (this means that they *share* variance with other predictors). If this is the case, the model results are unreliable because the presence of absence of one predictor has substantive effects on at least one other predictor. \n\nTo check whether the final minimal model contains predictors that correlate with each other, we extract variance inflation factors (VIF). If a model contains predictors that have variance inflation factors (VIF) > 10 the model is unreliable [@myers1990classical]. @gries2021statistics shows that a VIF of 10 means that that predictor is explainable (predictable) from the other predictors in the model with an R^2^ of .9 (a VIF of 5 means that predictor is explainable (predictable) from the other predictors in the model with an R^2^ of .8).Indeed, predictors with VIF values greater than 4 are usually already problematic but, for large data sets, even VIFs greater than 2 can lead inflated standard errors ([Jaeger 2013](http://wiki.bcs.rochester.edu/HlpLab/LSA2013Regression?action=AttachFile&do=view&target=LSA13-Lecture6-CommonIssuesAndSolutions.pdf)). Also, VIFs of 2.5 can be problematic [@szmrecsanyi2006morphosyntactic 215] and [@zuur2010protocol] proposes that variables with VIFs exceeding 3 should be removed.\n\n\n<div class=\"warning\" style='padding:0.1em; background-color:#f2f2f2; color:#51247a'>\n<span>\n<p style='margin-top:1em; text-align:center'>\n<b>NOTE</b><br><br>However, (multi-)collinearity is only an issue if one is interested in interpreting regression results!  If the interpretation is irrelevant because what is relevant is prediction(!), then it does not matter if the model contains collinear predictors! See @gries2021statistics or the excursion below for a more elaborate explanation.</p>\n<p style='margin-left:1em;'>\n</p></span>\n</div>\n\n<br>\n\n\n***\n\n<div class=\"warning\" style='padding:0.1em; background-color:#f2f2f2; color:#51247a'>\n<span>\n<p style='margin-top:1em; text-align:center'>\n<b>EXCURSION</b></p>\n<p style='margin-left:1em;'>\n</p></span>\n</div>\n\n<div class=\"question\">` \n\nWhat is multicollinearity?<br>\n\n<details>\n  <summary>Answer</summary>\n  \n  <br>\n  \n  During the workshop on mixed-effects modeling, we talked about (multi-)collinearity and someone asked if collinearity reflected shared variance (what I thought) or predictability of variables (what the other person thought). Both answers are correct! We will see below why...\n  \n  ***\n  \n  <div class=\"warning\" style='padding:0.1em; background-color:#f2f2f2; color:#51247a'>\n  <span>\n  <p style='margin-top:1em; text-align:center'>\n  (Multi-)collinearity reflects the predictability of predictors based on the values of other predictors!</p></span></div>\n  \n  \n  ***\n  \n  To test this, I generate a data set with 4 independent variables `a`, `b`, `c`, and `d` as well as two potential response variables `r1` (which is random) and `r2` (where the first 50 data points are the same as in `r1` but for the second 50 data points I have added a value of 50 to the data points 51 to 100 from `r1`). This means that the predictors `a` and `d` should both strongly correlate with `r2`. \n  \n\n  ::: {.cell}\n  \n  ```{.r .cell-code}\n  # load packages\n  library(dplyr)\n  library(rms)\n  # create data set\n  # responses\n  # 100 random numbers\n  r1 <- rnorm(100, 50, 10)\n  # 50 smaller + 50 larger numbers\n  r2 <- c(r1[1:50], r1[51:100] + 50)\n  # predictors\n  a <- c(rep(\"1\", 50), rep (\"0\", 50))\n  b <- rep(c(rep(\"1\", 25), rep (\"0\", 25)), 2)\n  c <- rep(c(rep(\"1\", 10), rep(\"0\", 10)), 5)\n  d <- c(rep(\"1\", 47), rep (\"0\", 3), rep (\"0\", 47), rep (\"1\", 3))\n  # create data set\n  df <- data.frame(r1, r2, a, b, c, d)\n  ```\n  :::\n\n  ::: {.cell}\n  ::: {.cell-output-display}\n  `````{=html}\n  <table class=\"table table-striped table-hover table-condensed table-responsive\" style=\"width: auto !important; margin-left: auto; margin-right: auto;\">\n  <caption>First 10 rows of df data.</caption>\n   <thead>\n    <tr>\n     <th style=\"text-align:right;\"> r1 </th>\n     <th style=\"text-align:right;\"> r2 </th>\n     <th style=\"text-align:left;\"> a </th>\n     <th style=\"text-align:left;\"> b </th>\n     <th style=\"text-align:left;\"> c </th>\n     <th style=\"text-align:left;\"> d </th>\n    </tr>\n   </thead>\n  <tbody>\n    <tr>\n     <td style=\"text-align:right;\"> 55.3805120133 </td>\n     <td style=\"text-align:right;\"> 55.3805120133 </td>\n     <td style=\"text-align:left;\"> 1 </td>\n     <td style=\"text-align:left;\"> 1 </td>\n     <td style=\"text-align:left;\"> 1 </td>\n     <td style=\"text-align:left;\"> 1 </td>\n    </tr>\n    <tr>\n     <td style=\"text-align:right;\"> 50.5169933942 </td>\n     <td style=\"text-align:right;\"> 50.5169933942 </td>\n     <td style=\"text-align:left;\"> 1 </td>\n     <td style=\"text-align:left;\"> 1 </td>\n     <td style=\"text-align:left;\"> 1 </td>\n     <td style=\"text-align:left;\"> 1 </td>\n    </tr>\n    <tr>\n     <td style=\"text-align:right;\"> 67.5135734932 </td>\n     <td style=\"text-align:right;\"> 67.5135734932 </td>\n     <td style=\"text-align:left;\"> 1 </td>\n     <td style=\"text-align:left;\"> 1 </td>\n     <td style=\"text-align:left;\"> 1 </td>\n     <td style=\"text-align:left;\"> 1 </td>\n    </tr>\n    <tr>\n     <td style=\"text-align:right;\"> 60.1733973728 </td>\n     <td style=\"text-align:right;\"> 60.1733973728 </td>\n     <td style=\"text-align:left;\"> 1 </td>\n     <td style=\"text-align:left;\"> 1 </td>\n     <td style=\"text-align:left;\"> 1 </td>\n     <td style=\"text-align:left;\"> 1 </td>\n    </tr>\n    <tr>\n     <td style=\"text-align:right;\"> 72.3678161278 </td>\n     <td style=\"text-align:right;\"> 72.3678161278 </td>\n     <td style=\"text-align:left;\"> 1 </td>\n     <td style=\"text-align:left;\"> 1 </td>\n     <td style=\"text-align:left;\"> 1 </td>\n     <td style=\"text-align:left;\"> 1 </td>\n    </tr>\n    <tr>\n     <td style=\"text-align:right;\"> 52.4309002788 </td>\n     <td style=\"text-align:right;\"> 52.4309002788 </td>\n     <td style=\"text-align:left;\"> 1 </td>\n     <td style=\"text-align:left;\"> 1 </td>\n     <td style=\"text-align:left;\"> 1 </td>\n     <td style=\"text-align:left;\"> 1 </td>\n    </tr>\n    <tr>\n     <td style=\"text-align:right;\"> 55.6904236277 </td>\n     <td style=\"text-align:right;\"> 55.6904236277 </td>\n     <td style=\"text-align:left;\"> 1 </td>\n     <td style=\"text-align:left;\"> 1 </td>\n     <td style=\"text-align:left;\"> 1 </td>\n     <td style=\"text-align:left;\"> 1 </td>\n    </tr>\n    <tr>\n     <td style=\"text-align:right;\"> 44.2366484882 </td>\n     <td style=\"text-align:right;\"> 44.2366484882 </td>\n     <td style=\"text-align:left;\"> 1 </td>\n     <td style=\"text-align:left;\"> 1 </td>\n     <td style=\"text-align:left;\"> 1 </td>\n     <td style=\"text-align:left;\"> 1 </td>\n    </tr>\n    <tr>\n     <td style=\"text-align:right;\"> 55.5987335816 </td>\n     <td style=\"text-align:right;\"> 55.5987335816 </td>\n     <td style=\"text-align:left;\"> 1 </td>\n     <td style=\"text-align:left;\"> 1 </td>\n     <td style=\"text-align:left;\"> 1 </td>\n     <td style=\"text-align:left;\"> 1 </td>\n    </tr>\n    <tr>\n     <td style=\"text-align:right;\"> 58.3120761737 </td>\n     <td style=\"text-align:right;\"> 58.3120761737 </td>\n     <td style=\"text-align:left;\"> 1 </td>\n     <td style=\"text-align:left;\"> 1 </td>\n     <td style=\"text-align:left;\"> 1 </td>\n     <td style=\"text-align:left;\"> 1 </td>\n    </tr>\n  </tbody>\n  </table>\n  \n  `````\n  :::\n  :::\n\n  \n  \n  Here are the visualizations of r1 and r2\n  \n\n  ::: {.cell}\n  ::: {.cell-output-display}\n  ![](regression_files/figure-html/data2-1.png){width=672}\n  :::\n  \n  ::: {.cell-output-display}\n  ![](regression_files/figure-html/data2-2.png){width=672}\n  :::\n  :::\n\n  \n  **Fit first model**\n  \n  Now, I fit a first model. As the response is random, we do not expect any of the predictors to have a significant effect and we expect the R^2^ to be rather low.\n  \n\n  ::: {.cell}\n  \n  ```{.r .cell-code}\n  m1 <- lm(r1 ~ a + b + c + d, data = df)\n  # inspect model\n  summary(m1)\n  ```\n  \n  ::: {.cell-output .cell-output-stdout}\n  ```\n  \n  Call:\n  lm(formula = r1 ~ a + b + c + d, data = df)\n  \n  Residuals:\n            Min            1Q        Median            3Q           Max \n  -25.076373713  -7.774694324   0.933388851   6.630369791  22.021652947 \n  \n  Coefficients:\n                  Estimate   Std. Error  t value             Pr(>|t|)    \n  (Intercept) 46.077236782  2.059313248 22.37505 < 0.0000000000000002 ***\n  a1           5.957479586  4.593681927  1.29689             0.197812    \n  b1           5.148440925  2.098541787  2.45334             0.015977 *  \n  c1          -0.213502165  2.188893729 -0.09754             0.922504    \n  d1          -5.232737413  4.515342995 -1.15888             0.249410    \n  ---\n  Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n  \n  Residual standard error: 10.4927089 on 95 degrees of freedom\n  Multiple R-squared:  0.075627922,\tAdjusted R-squared:  0.0367069924 \n  F-statistic: 1.94311705 on 4 and 95 DF,  p-value: 0.109586389\n  ```\n  :::\n  :::\n\n  \n  We now check for (multi-)collinearity using the `vif` function from the `rms` package [@rms]. Variables `a` and `d` should have high variance inflation factor values (vif-values) because they overlap very much!\n  \n\n  ::: {.cell}\n  \n  ```{.r .cell-code}\n  # extract vifs\n  rms::vif(m1)\n  ```\n  \n  ::: {.cell-output .cell-output-stdout}\n  ```\n             a1            b1            c1            d1 \n  4.79166666667 1.00000000000 1.08796296296 4.62962962963 \n  ```\n  :::\n  :::\n\n  \n  Variables `a` and `d` do indeed have high vif-values.\n  \n  **Fit second model**\n  \n  We now fit a second model to the response which has higher values for the latter part of the response. Both `a` and `d` strongly correlate with the response. **But** because `a` and `d` are collinear, `d` should not be reported as being significant by the model. The R^2^ of the model should be rather high (given the correlation between the response r2 and `a` and `d`).\n  \n\n  ::: {.cell}\n  \n  ```{.r .cell-code}\n  m2 <- lm(r2 ~ a + b + c + d, data = df)\n  # inspect model\n  summary(m2)\n  ```\n  \n  ::: {.cell-output .cell-output-stdout}\n  ```\n  \n  Call:\n  lm(formula = r2 ~ a + b + c + d, data = df)\n  \n  Residuals:\n            Min            1Q        Median            3Q           Max \n  -25.076373713  -7.774694324   0.933388851   6.630369791  22.021652947 \n  \n  Coefficients:\n                   Estimate    Std. Error  t value               Pr(>|t|)    \n  (Intercept)  96.077236782   2.059313248 46.65499 < 0.000000000000000222 ***\n  a1          -44.042520414   4.593681927 -9.58763  0.0000000000000012574 ***\n  b1            5.148440925   2.098541787  2.45334               0.015977 *  \n  c1           -0.213502165   2.188893729 -0.09754               0.922504    \n  d1           -5.232737413   4.515342995 -1.15888               0.249410    \n  ---\n  Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n  \n  Residual standard error: 10.4927089 on 95 degrees of freedom\n  Multiple R-squared:  0.851726565,\tAdjusted R-squared:  0.845483473 \n  F-statistic: 136.427041 on 4 and 95 DF,  p-value: < 0.0000000000000002220446\n  ```\n  :::\n  :::\n\n  \n  Again, we extract the vif-values.\n  \n\n  ::: {.cell}\n  \n  ```{.r .cell-code}\n  # extract vifs\n  rms::vif(m2)\n  ```\n  \n  ::: {.cell-output .cell-output-stdout}\n  ```\n             a1            b1            c1            d1 \n  4.79166666667 1.00000000000 1.08796296296 4.62962962963 \n  ```\n  :::\n  :::\n\n  \n  The vif-values are identical which shows that what matters is if the variables are predictable. To understand how we arrive at vif-values, we inspect the model matrix.\n  \n\n  ::: {.cell}\n  \n  ```{.r .cell-code}\n  # inspect model matrix\n  mm <- model.matrix(m2)\n  ```\n  :::\n\n  ::: {.cell}\n  ::: {.cell-output-display}\n  `````{=html}\n  <table class=\"table table-striped table-hover table-condensed table-responsive\" style=\"width: auto !important; margin-left: auto; margin-right: auto;\">\n  <caption>First 15 rows of the model matrix.</caption>\n   <thead>\n    <tr>\n     <th style=\"text-align:right;\"> (Intercept) </th>\n     <th style=\"text-align:right;\"> a1 </th>\n     <th style=\"text-align:right;\"> b1 </th>\n     <th style=\"text-align:right;\"> c1 </th>\n     <th style=\"text-align:right;\"> d1 </th>\n    </tr>\n   </thead>\n  <tbody>\n    <tr>\n     <td style=\"text-align:right;\"> 1 </td>\n     <td style=\"text-align:right;\"> 1 </td>\n     <td style=\"text-align:right;\"> 1 </td>\n     <td style=\"text-align:right;\"> 1 </td>\n     <td style=\"text-align:right;\"> 1 </td>\n    </tr>\n    <tr>\n     <td style=\"text-align:right;\"> 1 </td>\n     <td style=\"text-align:right;\"> 1 </td>\n     <td style=\"text-align:right;\"> 1 </td>\n     <td style=\"text-align:right;\"> 1 </td>\n     <td style=\"text-align:right;\"> 1 </td>\n    </tr>\n    <tr>\n     <td style=\"text-align:right;\"> 1 </td>\n     <td style=\"text-align:right;\"> 1 </td>\n     <td style=\"text-align:right;\"> 1 </td>\n     <td style=\"text-align:right;\"> 1 </td>\n     <td style=\"text-align:right;\"> 1 </td>\n    </tr>\n    <tr>\n     <td style=\"text-align:right;\"> 1 </td>\n     <td style=\"text-align:right;\"> 1 </td>\n     <td style=\"text-align:right;\"> 1 </td>\n     <td style=\"text-align:right;\"> 1 </td>\n     <td style=\"text-align:right;\"> 1 </td>\n    </tr>\n    <tr>\n     <td style=\"text-align:right;\"> 1 </td>\n     <td style=\"text-align:right;\"> 1 </td>\n     <td style=\"text-align:right;\"> 1 </td>\n     <td style=\"text-align:right;\"> 1 </td>\n     <td style=\"text-align:right;\"> 1 </td>\n    </tr>\n    <tr>\n     <td style=\"text-align:right;\"> 1 </td>\n     <td style=\"text-align:right;\"> 1 </td>\n     <td style=\"text-align:right;\"> 1 </td>\n     <td style=\"text-align:right;\"> 1 </td>\n     <td style=\"text-align:right;\"> 1 </td>\n    </tr>\n    <tr>\n     <td style=\"text-align:right;\"> 1 </td>\n     <td style=\"text-align:right;\"> 1 </td>\n     <td style=\"text-align:right;\"> 1 </td>\n     <td style=\"text-align:right;\"> 1 </td>\n     <td style=\"text-align:right;\"> 1 </td>\n    </tr>\n    <tr>\n     <td style=\"text-align:right;\"> 1 </td>\n     <td style=\"text-align:right;\"> 1 </td>\n     <td style=\"text-align:right;\"> 1 </td>\n     <td style=\"text-align:right;\"> 1 </td>\n     <td style=\"text-align:right;\"> 1 </td>\n    </tr>\n    <tr>\n     <td style=\"text-align:right;\"> 1 </td>\n     <td style=\"text-align:right;\"> 1 </td>\n     <td style=\"text-align:right;\"> 1 </td>\n     <td style=\"text-align:right;\"> 1 </td>\n     <td style=\"text-align:right;\"> 1 </td>\n    </tr>\n    <tr>\n     <td style=\"text-align:right;\"> 1 </td>\n     <td style=\"text-align:right;\"> 1 </td>\n     <td style=\"text-align:right;\"> 1 </td>\n     <td style=\"text-align:right;\"> 1 </td>\n     <td style=\"text-align:right;\"> 1 </td>\n    </tr>\n    <tr>\n     <td style=\"text-align:right;\"> 1 </td>\n     <td style=\"text-align:right;\"> 1 </td>\n     <td style=\"text-align:right;\"> 1 </td>\n     <td style=\"text-align:right;\"> 0 </td>\n     <td style=\"text-align:right;\"> 1 </td>\n    </tr>\n    <tr>\n     <td style=\"text-align:right;\"> 1 </td>\n     <td style=\"text-align:right;\"> 1 </td>\n     <td style=\"text-align:right;\"> 1 </td>\n     <td style=\"text-align:right;\"> 0 </td>\n     <td style=\"text-align:right;\"> 1 </td>\n    </tr>\n    <tr>\n     <td style=\"text-align:right;\"> 1 </td>\n     <td style=\"text-align:right;\"> 1 </td>\n     <td style=\"text-align:right;\"> 1 </td>\n     <td style=\"text-align:right;\"> 0 </td>\n     <td style=\"text-align:right;\"> 1 </td>\n    </tr>\n    <tr>\n     <td style=\"text-align:right;\"> 1 </td>\n     <td style=\"text-align:right;\"> 1 </td>\n     <td style=\"text-align:right;\"> 1 </td>\n     <td style=\"text-align:right;\"> 0 </td>\n     <td style=\"text-align:right;\"> 1 </td>\n    </tr>\n    <tr>\n     <td style=\"text-align:right;\"> 1 </td>\n     <td style=\"text-align:right;\"> 1 </td>\n     <td style=\"text-align:right;\"> 1 </td>\n     <td style=\"text-align:right;\"> 0 </td>\n     <td style=\"text-align:right;\"> 1 </td>\n    </tr>\n  </tbody>\n  </table>\n  \n  `````\n  :::\n  :::\n\n  \n  We now fit a linear model in which we predict `d` from the other predictors in the model matrix.\n  \n\n  ::: {.cell}\n  \n  ```{.r .cell-code}\n  mt <- lm(mm[,5] ~ mm[,1:4])\n  summary(mt)$r.squared\n  ```\n  \n  ::: {.cell-output .cell-output-stdout}\n  ```\n  [1] 0.784\n  ```\n  :::\n  :::\n\n  The R^2^ shows that the values of `d` are explained to 78.4 percent by the values of the other predictors in the model.\n  \n  Now, we can write a function [taken from @gries2021statistics] that converts this R^2^ value \n  \n\n  ::: {.cell}\n  \n  ```{.r .cell-code}\n  R2.to.VIF <- function(some.modelmatrix.r2) {\n  return(1/(1-some.modelmatrix.r2)) } \n  R2.to.VIF(0.784)\n  ```\n  \n  ::: {.cell-output .cell-output-stdout}\n  ```\n  [1] 4.62962962963\n  ```\n  :::\n  :::\n\n  \n  The function outputs the vif-value of `d`. This shows that the vif-value of `d` represents its predictability from the other predictors in the model matrix which represents the amount of shared variance between `d` and the other predictors in the model.\n  \n</details>\n\n</div>`\n\n***\n\n<br>\n\nWe now extract and check the VIFs of the model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvif(lr.glm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       AgeOld   GenderWomen \n1.00481494539 1.00481494539 \n```\n:::\n:::\n\n\nIn addition, predictors with 1/VIF values $<$ .1 must be removed (data points with values above .2 are considered problematic) [@menard1995applied] and the mean value of VIFs should be $~$ 1 [@bowerman1990linear].\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(vif(lr.glm))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1.00481494539\n```\n:::\n:::\n\n\n### Outlier detection{-}\n\nIn order to detect potential outliers, we will calculate diagnostic parameters and add these to our data set.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninfl <- influence.measures(lr.glm) # calculate influence statistics\nblrdata <- data.frame(blrdata, infl[[1]], infl[[2]]) # add influence statistics\n```\n:::\n\n\nIn a next step, we use these diagnostic parameters to check if there are data points which should be removed as they unduly affect the model fit.  \n\n\n### Sample Size{-}\n\nWe now check whether the sample size is sufficient for our analysis [@green1991many].\n\n* if you are interested in the overall model: 50 + 8k (k = number of predictors)\n\n* if you are interested in individual predictors: 104 + k\n\n* if you are interested in both: take the higher value!\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# function to evaluate sample size\nsmplesz <- function(x) {\n  ifelse((length(x$fitted) < (104 + ncol(summary(x)$coefficients)-1)) == TRUE,\n    return(\n      paste(\"Sample too small: please increase your sample by \",\n      104 + ncol(summary(x)$coefficients)-1 - length(x$fitted),\n      \" data points\", collapse = \"\")),\n    return(\"Sample size sufficient\")) }\n# apply unction to model\nsmplesz(lr.glm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Sample size sufficient\"\n```\n:::\n:::\n\n\nAccording to rule of thumb provided in @green1991many, the sample size is sufficient for our analysis.\n\n### Summarizing Results{-}\n\nAs a final step, we summarize our findings in tabulated form.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsjPlot::tab_model(lr.glm)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table style=\"border-collapse:collapse; border:none;\">\n<tr>\n<th style=\"border-top: double; text-align:center; font-style:normal; font-weight:bold; padding:0.2cm;  text-align:left; \">&nbsp;</th>\n<th colspan=\"3\" style=\"border-top: double; text-align:center; font-style:normal; font-weight:bold; padding:0.2cm; \">EH</th>\n</tr>\n<tr>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  text-align:left; \">Predictors</td>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  \">Odds Ratios</td>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  \">CI</td>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  \">p</td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; \">(Intercept)</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.79</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.76&nbsp;&ndash;&nbsp;0.83</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"><strong>&lt;0.001</strong></td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; \">Age [Old]</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.44</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.41&nbsp;&ndash;&nbsp;0.47</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"><strong>&lt;0.001</strong></td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; \">Gender [Women]</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.66</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.62&nbsp;&ndash;&nbsp;0.69</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"><strong>&lt;0.001</strong></td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; padding-top:0.1cm; padding-bottom:0.1cm; border-top:1px solid;\">Observations</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left; border-top:1px solid;\" colspan=\"3\">25821</td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; padding-top:0.1cm; padding-bottom:0.1cm;\">R<sup>2</sup> Tjur</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left;\" colspan=\"3\">0.032</td>\n</tr>\n\n</table>\n\n`````\n:::\n:::\n\n\n***\n\nA more detailed summary table can be retrieved as follows:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# load function\nsource(\"https://slcladal.github.io/rscripts/blrsummary.r\")\n# calculate accuracy \npredict.acc <- caret::confusionMatrix(blrdata$Prediction, blrdata$EH)\npredict.acc <- predict.acc[3]$overall[[1]]\n# create summary table\nblrsummarytb <- blrsummary(lr.glm, lr.lrm, predict.acc) \n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n```{=html}\n<div class=\"tabwid\"><style>.cl-bf46ee94{table-layout:auto;width:75%;}.cl-bf3fc02e{font-family:'DejaVu Sans';font-size:12pt;font-weight:bold;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-bf3fc042{font-family:'DejaVu Sans';font-size:12pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-bf3fd280{margin:0;text-align:center;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-bf402c9e{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-bf402ca8{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-bf402cb2{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-bf402cbc{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-bf402cbd{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-bf402cc6{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-bf402cd0{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-bf402cd1{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-bf402cda{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-bf402ce4{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-bf402ce5{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-bf402cee{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}</style><table class='cl-bf46ee94'>\n```\n<caption class=\"Table Caption\">\n\nResults of the binomial logistic regression analysis.\n\n</caption>\n```{=html}\n<thead><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-bf402ce5\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc02e\">Statistics</span></p></td><td class=\"cl-bf402ce4\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc02e\">Estimate</span></p></td><td class=\"cl-bf402ce4\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc02e\">VIF</span></p></td><td class=\"cl-bf402ce4\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc02e\">OddsRatio</span></p></td><td class=\"cl-bf402ce4\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc02e\">CI(2.5%)</span></p></td><td class=\"cl-bf402ce4\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc02e\">CI(97.5%)</span></p></td><td class=\"cl-bf402ce4\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc02e\">Std. Error</span></p></td><td class=\"cl-bf402ce4\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc02e\">z value</span></p></td><td class=\"cl-bf402ce4\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc02e\">Pr(&gt;|z|)</span></p></td><td class=\"cl-bf402cee\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc02e\">Significance</span></p></td></tr></thead><tbody><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-bf402ca8\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\">(Intercept)</span></p></td><td class=\"cl-bf402c9e\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\">-0.23</span></p></td><td class=\"cl-bf402c9e\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\"></span></p></td><td class=\"cl-bf402c9e\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\">0.79</span></p></td><td class=\"cl-bf402c9e\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\">0.76</span></p></td><td class=\"cl-bf402c9e\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\">0.83</span></p></td><td class=\"cl-bf402c9e\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\">0.02</span></p></td><td class=\"cl-bf402c9e\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\">-10.44</span></p></td><td class=\"cl-bf402c9e\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\">0</span></p></td><td class=\"cl-bf402cb2\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\">p &lt; .001***</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-bf402cbd\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\">AgeOld</span></p></td><td class=\"cl-bf402cbc\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\">-0.83</span></p></td><td class=\"cl-bf402cbc\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\">1</span></p></td><td class=\"cl-bf402cbc\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\">0.44</span></p></td><td class=\"cl-bf402cbc\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\">0.41</span></p></td><td class=\"cl-bf402cbc\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\">0.47</span></p></td><td class=\"cl-bf402cbc\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\">0.03</span></p></td><td class=\"cl-bf402cbc\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\">-24.78</span></p></td><td class=\"cl-bf402cbc\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\">0</span></p></td><td class=\"cl-bf402cc6\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\">p &lt; .001***</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-bf402ca8\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\">GenderWomen</span></p></td><td class=\"cl-bf402c9e\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\">-0.42</span></p></td><td class=\"cl-bf402c9e\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\">1</span></p></td><td class=\"cl-bf402c9e\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\">0.66</span></p></td><td class=\"cl-bf402c9e\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\">0.62</span></p></td><td class=\"cl-bf402c9e\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\">0.69</span></p></td><td class=\"cl-bf402c9e\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\">0.03</span></p></td><td class=\"cl-bf402c9e\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\">-15.42</span></p></td><td class=\"cl-bf402c9e\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\">0</span></p></td><td class=\"cl-bf402cb2\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\">p &lt; .001***</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-bf402cbd\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\">Model statistics</span></p></td><td class=\"cl-bf402cbc\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\"></span></p></td><td class=\"cl-bf402cbc\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\"></span></p></td><td class=\"cl-bf402cbc\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\"></span></p></td><td class=\"cl-bf402cbc\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\"></span></p></td><td class=\"cl-bf402cbc\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\"></span></p></td><td class=\"cl-bf402cbc\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\"></span></p></td><td class=\"cl-bf402cbc\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\"></span></p></td><td class=\"cl-bf402cbc\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\"></span></p></td><td class=\"cl-bf402cc6\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\">Value</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-bf402ca8\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\">Number of cases in model</span></p></td><td class=\"cl-bf402c9e\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\"></span></p></td><td class=\"cl-bf402c9e\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\"></span></p></td><td class=\"cl-bf402c9e\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\"></span></p></td><td class=\"cl-bf402c9e\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\"></span></p></td><td class=\"cl-bf402c9e\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\"></span></p></td><td class=\"cl-bf402c9e\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\"></span></p></td><td class=\"cl-bf402c9e\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\"></span></p></td><td class=\"cl-bf402c9e\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\"></span></p></td><td class=\"cl-bf402cb2\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\">25821</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-bf402cbd\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\">Observed misses</span></p></td><td class=\"cl-bf402cbc\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\"></span></p></td><td class=\"cl-bf402cbc\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\"></span></p></td><td class=\"cl-bf402cbc\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\"></span></p></td><td class=\"cl-bf402cbc\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\"></span></p></td><td class=\"cl-bf402cbc\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\"></span></p></td><td class=\"cl-bf402cbc\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\"></span></p></td><td class=\"cl-bf402cbc\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\"></span></p></td><td class=\"cl-bf402cbc\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\">0 :</span></p></td><td class=\"cl-bf402cc6\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\">17114</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-bf402ca8\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\">Observed successes</span></p></td><td class=\"cl-bf402c9e\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\"></span></p></td><td class=\"cl-bf402c9e\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\"></span></p></td><td class=\"cl-bf402c9e\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\"></span></p></td><td class=\"cl-bf402c9e\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\"></span></p></td><td class=\"cl-bf402c9e\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\"></span></p></td><td class=\"cl-bf402c9e\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\"></span></p></td><td class=\"cl-bf402c9e\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\"></span></p></td><td class=\"cl-bf402c9e\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\">1 :</span></p></td><td class=\"cl-bf402cb2\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\">8707</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-bf402cbd\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\">Null deviance</span></p></td><td class=\"cl-bf402cbc\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\"></span></p></td><td class=\"cl-bf402cbc\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\"></span></p></td><td class=\"cl-bf402cbc\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\"></span></p></td><td class=\"cl-bf402cbc\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\"></span></p></td><td class=\"cl-bf402cbc\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\"></span></p></td><td class=\"cl-bf402cbc\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\"></span></p></td><td class=\"cl-bf402cbc\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\"></span></p></td><td class=\"cl-bf402cbc\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\"></span></p></td><td class=\"cl-bf402cc6\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\">33007.75</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-bf402ca8\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\">Residual deviance</span></p></td><td class=\"cl-bf402c9e\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\"></span></p></td><td class=\"cl-bf402c9e\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\"></span></p></td><td class=\"cl-bf402c9e\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\"></span></p></td><td class=\"cl-bf402c9e\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\"></span></p></td><td class=\"cl-bf402c9e\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\"></span></p></td><td class=\"cl-bf402c9e\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\"></span></p></td><td class=\"cl-bf402c9e\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\"></span></p></td><td class=\"cl-bf402c9e\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\"></span></p></td><td class=\"cl-bf402cb2\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\">32139.54</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-bf402cbd\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\">R2 (Nagelkerke)</span></p></td><td class=\"cl-bf402cbc\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\"></span></p></td><td class=\"cl-bf402cbc\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\"></span></p></td><td class=\"cl-bf402cbc\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\"></span></p></td><td class=\"cl-bf402cbc\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\"></span></p></td><td class=\"cl-bf402cbc\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\"></span></p></td><td class=\"cl-bf402cbc\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\"></span></p></td><td class=\"cl-bf402cbc\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\"></span></p></td><td class=\"cl-bf402cbc\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\"></span></p></td><td class=\"cl-bf402cc6\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\">0.046</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-bf402ca8\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\">R2 (Hosmer &amp; Lemeshow)</span></p></td><td class=\"cl-bf402c9e\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\"></span></p></td><td class=\"cl-bf402c9e\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\"></span></p></td><td class=\"cl-bf402c9e\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\"></span></p></td><td class=\"cl-bf402c9e\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\"></span></p></td><td class=\"cl-bf402c9e\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\"></span></p></td><td class=\"cl-bf402c9e\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\"></span></p></td><td class=\"cl-bf402c9e\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\"></span></p></td><td class=\"cl-bf402c9e\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\"></span></p></td><td class=\"cl-bf402cb2\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\">0.026</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-bf402cbd\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\">R2 (Cox &amp; Snell)</span></p></td><td class=\"cl-bf402cbc\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\"></span></p></td><td class=\"cl-bf402cbc\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\"></span></p></td><td class=\"cl-bf402cbc\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\"></span></p></td><td class=\"cl-bf402cbc\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\"></span></p></td><td class=\"cl-bf402cbc\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\"></span></p></td><td class=\"cl-bf402cbc\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\"></span></p></td><td class=\"cl-bf402cbc\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\"></span></p></td><td class=\"cl-bf402cbc\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\"></span></p></td><td class=\"cl-bf402cc6\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\">0.033</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-bf402ca8\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\">C</span></p></td><td class=\"cl-bf402c9e\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\"></span></p></td><td class=\"cl-bf402c9e\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\"></span></p></td><td class=\"cl-bf402c9e\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\"></span></p></td><td class=\"cl-bf402c9e\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\"></span></p></td><td class=\"cl-bf402c9e\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\"></span></p></td><td class=\"cl-bf402c9e\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\"></span></p></td><td class=\"cl-bf402c9e\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\"></span></p></td><td class=\"cl-bf402c9e\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\"></span></p></td><td class=\"cl-bf402cb2\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\">0.602</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-bf402cbd\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\">Somers' Dxy</span></p></td><td class=\"cl-bf402cbc\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\"></span></p></td><td class=\"cl-bf402cbc\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\"></span></p></td><td class=\"cl-bf402cbc\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\"></span></p></td><td class=\"cl-bf402cbc\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\"></span></p></td><td class=\"cl-bf402cbc\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\"></span></p></td><td class=\"cl-bf402cbc\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\"></span></p></td><td class=\"cl-bf402cbc\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\"></span></p></td><td class=\"cl-bf402cbc\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\"></span></p></td><td class=\"cl-bf402cc6\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\">0.203</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-bf402ca8\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\">AIC</span></p></td><td class=\"cl-bf402c9e\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\"></span></p></td><td class=\"cl-bf402c9e\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\"></span></p></td><td class=\"cl-bf402c9e\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\"></span></p></td><td class=\"cl-bf402c9e\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\"></span></p></td><td class=\"cl-bf402c9e\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\"></span></p></td><td class=\"cl-bf402c9e\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\"></span></p></td><td class=\"cl-bf402c9e\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\"></span></p></td><td class=\"cl-bf402c9e\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\"></span></p></td><td class=\"cl-bf402cb2\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\">32145.54</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-bf402cbd\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\">Prediction accuracy</span></p></td><td class=\"cl-bf402cbc\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\"></span></p></td><td class=\"cl-bf402cbc\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\"></span></p></td><td class=\"cl-bf402cbc\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\"></span></p></td><td class=\"cl-bf402cbc\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\"></span></p></td><td class=\"cl-bf402cbc\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\"></span></p></td><td class=\"cl-bf402cbc\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\"></span></p></td><td class=\"cl-bf402cbc\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\"></span></p></td><td class=\"cl-bf402cbc\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\"></span></p></td><td class=\"cl-bf402cc6\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\">0.66%</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-bf402cd1\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\">Model Likelihood Ratio Test</span></p></td><td class=\"cl-bf402cd0\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\"></span></p></td><td class=\"cl-bf402cd0\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\"></span></p></td><td class=\"cl-bf402cd0\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\"></span></p></td><td class=\"cl-bf402cd0\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\"></span></p></td><td class=\"cl-bf402cd0\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\"></span></p></td><td class=\"cl-bf402cd0\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\">Model L.R.: 868.21</span></p></td><td class=\"cl-bf402cd0\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\">df: 2</span></p></td><td class=\"cl-bf402cd0\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\">p-value: 0</span></p></td><td class=\"cl-bf402cda\"><p class=\"cl-bf3fd280\"><span class=\"cl-bf3fc042\">sig: p &lt; .001***</span></p></td></tr></tbody></table></div>\n```\n:::\n:::\n\n\n**R^2^ (Hosmer & Lemeshow)**\n\nHosmer and Lemeshow's R^2^ \"is the proportional reduction in the absolute value of the log-likelihood  measure and as such it is a measure of how much the badness of fit improves  as a result of the inclusion of the predictor variables. It can vary between 0 (indicating that the predictors are useless at predicting the outcome variable) and 1 (indicating that the model predicts the outcome variable perfectly)\" [@field2012discovering 317].\n\n**R^2^ (Cox & Snell)**\n\n \"Cox and Snell's R^2^ (1989) is based on the deviance of the model (-2LL(newÂ»)\n and the deviance of the baseline model (-2LL(baseline), and the sample size,\n n [...]. However, this statistic never reaches its theoretical maximum of 1.\n\n**R2 (Nagelkerke)**\n\n Since R^2^ (Cox & Snell) never reaches its theoretical maximum of 1,\n Nagelkerke (1991) suggested Nagelkerke's R^2^ [@field2012discovering 317-318].\n\n**Somersâ€™ D~xy~**\n\n Somersâ€™ D~xy~ is a rank correlation between predicted probabilities and observed\n responses ranges between 0 (randomness) and 1 (perfect prediction). Somers' D~xy~ should have a value higher than .5 for the model to be meaningful [@baayen2008analyzing 204].\n\n**C**\n\n C is an index of concordance between the predicted probability and the\n observed response. When C takes the value 0.5, the predictions are random,\n when it is 1, prediction is perfect. A value above 0.8 indicates that the\n model may have some real predictive capacity [@baayen2008analyzing 204].\n\n**Akaike information criteria (AIC)**\n\nAkaike information criteria (AlC = -2LL + 2k) provide a value that reflects a ratio between the number of predictors in the model and the variance that is explained by these predictors. Changes in AIC can serve as a measure of whether the inclusion of a variable leads to a significant increase in the amount of variance that is explained by the model. \"You can think of this as the price you pay for something: you get a better value of R^2^, but you pay a higher price, and was that higher price worth it?  These information criteria help you to decide. The BIC is the same as the AIC but adjusts the penalty included in the AlC (i.e., 2k) by the number of cases: BlC = -2LL + 2k x log(n) in which n is the number of cases in the model\" [@field2012discovering 318].\n\nWe can use the `reports` package [@report] to summarize the analysis.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nreport::report(lr.glm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nWe fitted a logistic model (estimated using ML) to predict EH with Age and Gender (formula: EH ~ Age + Gender). The model's explanatory power is weak (Tjur's R2 = 0.03). The model's intercept, corresponding to Age = Young and Gender = Men, is at -0.23 (95% CI [-0.28, -0.19], p < .001). Within this model:\n\n  - The effect of Age [Old] is statistically significant and negative (beta = -0.83, 95% CI [-0.90, -0.77], p < .001; Std. beta = -0.83, 95% CI [-0.90, -0.77])\n  - The effect of Gender [Women] is statistically significant and negative (beta = -0.42, 95% CI [-0.47, -0.37], p < .001; Std. beta = -0.42, 95% CI [-0.47, -0.37])\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using \n```\n:::\n:::\n\n\nWe can use this output to write up a final report: \n\nWe fitted a logistic model (estimated using ML) to predict the use of the utterance-final discourse particle *eh* with Age and Gender (formula: EH ~ Age + Gender). The model's explanatory power is weak (Tjur's R^2^ = 0.03). The model's intercept, corresponding to Age = Young and Gender = Men, is at -0.23 (95% CI [-0.28, -0.19], p < .001). Within this model:\n\n* The effect of Age [Old] is statistically significant and negative (beta = -0.83, 95% CI [-0.90, -0.77], p < .001; Std. beta = -0.83, 95% CI [-0.90, -0.77])\n\n* The effect of Gender [Women] is statistically significant and negative (beta = -0.42, 95% CI [-0.47, -0.37], p < .001; Std. beta = -0.42, 95% CI [-0.47, -0.37])\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using \n\n## Ordinal Regression{-}\n\nOrdinal regression is very similar to multiple linear regression but takes an ordinal dependent variable [@agresti2010analysis]. For this reason, ordinal regression is one of the key methods in analysing Likert data. \n\nTo see how an ordinal regression is implemented in R, we load and inspect the Â´ordinaldataÂ´ data set. The data set consists of 400 observations of students that were either educated at this school (Internal = 1) or not (Internal = 0). Some of the students have been abroad (Exchange = 1) while other have not (Exchange = 0). In addition, the data set contains the students' final score of a language test (FinalScore) and the dependent variable which the recommendation of a committee for an additional, very prestigious program. The recommendation has three levels (*very likely*, *somewhat likely*, and *unlikely*) and reflects the committeesâ€™ assessment of whether the student is likely to succeed in the program.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# load data\nordata  <- base::readRDS(url(\"https://slcladal.github.io/data/ord.rda\", \"rb\")) %>%\n  dplyr::rename(Recommend = 1, \n              Internal = 2, \n              Exchange = 3, \n              FinalScore = 4) %>%\n  dplyr::mutate(FinalScore = round(FinalScore, 2))\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n```{=html}\n<div class=\"tabwid\"><style>.cl-c1a44f4c{table-layout:auto;width:75%;}.cl-c1a0a9aa{font-family:'DejaVu Sans';font-size:12pt;font-weight:bold;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-c1a0a9b4{font-family:'DejaVu Sans';font-size:12pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-c1a0b3f0{margin:0;text-align:center;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-c1a0b3fa{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-c1a0da88{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c1a0da89{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c1a0da92{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c1a0da93{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c1a0da9c{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c1a0da9d{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c1a0da9e{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c1a0daa6{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c1a0daa7{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c1a0dab0{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c1a0daba{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c1a0dabb{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}</style><table class='cl-c1a44f4c'>\n```\n<caption class=\"Table Caption\">\n\nFirst 15 rows of the ordata.\n\n</caption>\n```{=html}\n<thead><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-c1a0daba\"><p class=\"cl-c1a0b3f0\"><span class=\"cl-c1a0a9aa\">Recommend</span></p></td><td class=\"cl-c1a0dab0\"><p class=\"cl-c1a0b3fa\"><span class=\"cl-c1a0a9aa\">Internal</span></p></td><td class=\"cl-c1a0dab0\"><p class=\"cl-c1a0b3fa\"><span class=\"cl-c1a0a9aa\">Exchange</span></p></td><td class=\"cl-c1a0dabb\"><p class=\"cl-c1a0b3fa\"><span class=\"cl-c1a0a9aa\">FinalScore</span></p></td></tr></thead><tbody><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-c1a0da89\"><p class=\"cl-c1a0b3f0\"><span class=\"cl-c1a0a9b4\">very likely</span></p></td><td class=\"cl-c1a0da92\"><p class=\"cl-c1a0b3fa\"><span class=\"cl-c1a0a9b4\">0</span></p></td><td class=\"cl-c1a0da92\"><p class=\"cl-c1a0b3fa\"><span class=\"cl-c1a0a9b4\">0</span></p></td><td class=\"cl-c1a0da88\"><p class=\"cl-c1a0b3fa\"><span class=\"cl-c1a0a9b4\">3.26</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-c1a0da9c\"><p class=\"cl-c1a0b3f0\"><span class=\"cl-c1a0a9b4\">somewhat likely</span></p></td><td class=\"cl-c1a0da93\"><p class=\"cl-c1a0b3fa\"><span class=\"cl-c1a0a9b4\">1</span></p></td><td class=\"cl-c1a0da93\"><p class=\"cl-c1a0b3fa\"><span class=\"cl-c1a0a9b4\">0</span></p></td><td class=\"cl-c1a0da9d\"><p class=\"cl-c1a0b3fa\"><span class=\"cl-c1a0a9b4\">3.21</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-c1a0da89\"><p class=\"cl-c1a0b3f0\"><span class=\"cl-c1a0a9b4\">unlikely</span></p></td><td class=\"cl-c1a0da92\"><p class=\"cl-c1a0b3fa\"><span class=\"cl-c1a0a9b4\">1</span></p></td><td class=\"cl-c1a0da92\"><p class=\"cl-c1a0b3fa\"><span class=\"cl-c1a0a9b4\">1</span></p></td><td class=\"cl-c1a0da88\"><p class=\"cl-c1a0b3fa\"><span class=\"cl-c1a0a9b4\">3.94</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-c1a0da9c\"><p class=\"cl-c1a0b3f0\"><span class=\"cl-c1a0a9b4\">somewhat likely</span></p></td><td class=\"cl-c1a0da93\"><p class=\"cl-c1a0b3fa\"><span class=\"cl-c1a0a9b4\">0</span></p></td><td class=\"cl-c1a0da93\"><p class=\"cl-c1a0b3fa\"><span class=\"cl-c1a0a9b4\">0</span></p></td><td class=\"cl-c1a0da9d\"><p class=\"cl-c1a0b3fa\"><span class=\"cl-c1a0a9b4\">2.81</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-c1a0da89\"><p class=\"cl-c1a0b3f0\"><span class=\"cl-c1a0a9b4\">somewhat likely</span></p></td><td class=\"cl-c1a0da92\"><p class=\"cl-c1a0b3fa\"><span class=\"cl-c1a0a9b4\">0</span></p></td><td class=\"cl-c1a0da92\"><p class=\"cl-c1a0b3fa\"><span class=\"cl-c1a0a9b4\">0</span></p></td><td class=\"cl-c1a0da88\"><p class=\"cl-c1a0b3fa\"><span class=\"cl-c1a0a9b4\">2.53</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-c1a0da9c\"><p class=\"cl-c1a0b3f0\"><span class=\"cl-c1a0a9b4\">unlikely</span></p></td><td class=\"cl-c1a0da93\"><p class=\"cl-c1a0b3fa\"><span class=\"cl-c1a0a9b4\">0</span></p></td><td class=\"cl-c1a0da93\"><p class=\"cl-c1a0b3fa\"><span class=\"cl-c1a0a9b4\">1</span></p></td><td class=\"cl-c1a0da9d\"><p class=\"cl-c1a0b3fa\"><span class=\"cl-c1a0a9b4\">2.59</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-c1a0da89\"><p class=\"cl-c1a0b3f0\"><span class=\"cl-c1a0a9b4\">somewhat likely</span></p></td><td class=\"cl-c1a0da92\"><p class=\"cl-c1a0b3fa\"><span class=\"cl-c1a0a9b4\">0</span></p></td><td class=\"cl-c1a0da92\"><p class=\"cl-c1a0b3fa\"><span class=\"cl-c1a0a9b4\">0</span></p></td><td class=\"cl-c1a0da88\"><p class=\"cl-c1a0b3fa\"><span class=\"cl-c1a0a9b4\">2.56</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-c1a0da9c\"><p class=\"cl-c1a0b3f0\"><span class=\"cl-c1a0a9b4\">somewhat likely</span></p></td><td class=\"cl-c1a0da93\"><p class=\"cl-c1a0b3fa\"><span class=\"cl-c1a0a9b4\">0</span></p></td><td class=\"cl-c1a0da93\"><p class=\"cl-c1a0b3fa\"><span class=\"cl-c1a0a9b4\">0</span></p></td><td class=\"cl-c1a0da9d\"><p class=\"cl-c1a0b3fa\"><span class=\"cl-c1a0a9b4\">2.73</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-c1a0da89\"><p class=\"cl-c1a0b3f0\"><span class=\"cl-c1a0a9b4\">unlikely</span></p></td><td class=\"cl-c1a0da92\"><p class=\"cl-c1a0b3fa\"><span class=\"cl-c1a0a9b4\">0</span></p></td><td class=\"cl-c1a0da92\"><p class=\"cl-c1a0b3fa\"><span class=\"cl-c1a0a9b4\">0</span></p></td><td class=\"cl-c1a0da88\"><p class=\"cl-c1a0b3fa\"><span class=\"cl-c1a0a9b4\">3.00</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-c1a0da9c\"><p class=\"cl-c1a0b3f0\"><span class=\"cl-c1a0a9b4\">somewhat likely</span></p></td><td class=\"cl-c1a0da93\"><p class=\"cl-c1a0b3fa\"><span class=\"cl-c1a0a9b4\">1</span></p></td><td class=\"cl-c1a0da93\"><p class=\"cl-c1a0b3fa\"><span class=\"cl-c1a0a9b4\">0</span></p></td><td class=\"cl-c1a0da9d\"><p class=\"cl-c1a0b3fa\"><span class=\"cl-c1a0a9b4\">3.50</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-c1a0da89\"><p class=\"cl-c1a0b3f0\"><span class=\"cl-c1a0a9b4\">unlikely</span></p></td><td class=\"cl-c1a0da92\"><p class=\"cl-c1a0b3fa\"><span class=\"cl-c1a0a9b4\">1</span></p></td><td class=\"cl-c1a0da92\"><p class=\"cl-c1a0b3fa\"><span class=\"cl-c1a0a9b4\">1</span></p></td><td class=\"cl-c1a0da88\"><p class=\"cl-c1a0b3fa\"><span class=\"cl-c1a0a9b4\">3.65</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-c1a0da9c\"><p class=\"cl-c1a0b3f0\"><span class=\"cl-c1a0a9b4\">somewhat likely</span></p></td><td class=\"cl-c1a0da93\"><p class=\"cl-c1a0b3fa\"><span class=\"cl-c1a0a9b4\">0</span></p></td><td class=\"cl-c1a0da93\"><p class=\"cl-c1a0b3fa\"><span class=\"cl-c1a0a9b4\">0</span></p></td><td class=\"cl-c1a0da9d\"><p class=\"cl-c1a0b3fa\"><span class=\"cl-c1a0a9b4\">2.84</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-c1a0da89\"><p class=\"cl-c1a0b3f0\"><span class=\"cl-c1a0a9b4\">very likely</span></p></td><td class=\"cl-c1a0da92\"><p class=\"cl-c1a0b3fa\"><span class=\"cl-c1a0a9b4\">0</span></p></td><td class=\"cl-c1a0da92\"><p class=\"cl-c1a0b3fa\"><span class=\"cl-c1a0a9b4\">1</span></p></td><td class=\"cl-c1a0da88\"><p class=\"cl-c1a0b3fa\"><span class=\"cl-c1a0a9b4\">3.90</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-c1a0da9c\"><p class=\"cl-c1a0b3f0\"><span class=\"cl-c1a0a9b4\">somewhat likely</span></p></td><td class=\"cl-c1a0da93\"><p class=\"cl-c1a0b3fa\"><span class=\"cl-c1a0a9b4\">0</span></p></td><td class=\"cl-c1a0da93\"><p class=\"cl-c1a0b3fa\"><span class=\"cl-c1a0a9b4\">0</span></p></td><td class=\"cl-c1a0da9d\"><p class=\"cl-c1a0b3fa\"><span class=\"cl-c1a0a9b4\">2.68</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-c1a0daa7\"><p class=\"cl-c1a0b3f0\"><span class=\"cl-c1a0a9b4\">unlikely</span></p></td><td class=\"cl-c1a0da9e\"><p class=\"cl-c1a0b3fa\"><span class=\"cl-c1a0a9b4\">1</span></p></td><td class=\"cl-c1a0da9e\"><p class=\"cl-c1a0b3fa\"><span class=\"cl-c1a0a9b4\">0</span></p></td><td class=\"cl-c1a0daa6\"><p class=\"cl-c1a0b3fa\"><span class=\"cl-c1a0a9b4\">3.57</span></p></td></tr></tbody></table></div>\n```\n:::\n:::\n\n\nIn a first step, we need to relevel the ordinal variable to represent an ordinal factor (or a progression from \"unlikely\" over \"somewhat likely\" to \"very likely\". And we will also factorize Internal and Exchange to make it easier to interpret the output later on.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# relevel data\nordata <- ordata %>%\n  dplyr::mutate(Recommend = factor(Recommend, \n                           levels=c(\"unlikely\", \"somewhat likely\", \"very likely\"),\n                           labels=c(\"unlikely\",  \"somewhat likely\",  \"very likely\"))) %>%\n  dplyr::mutate(Exchange = ifelse(Exchange == 1, \"Exchange\", \"NoExchange\")) %>%\n  dplyr::mutate(Internal = ifelse(Internal == 1, \"Internal\", \"External\"))\n```\n:::\n\n\nNow that the dependent variable is releveled, we check the distribution of the variable levels by tabulating the data. To get a better understanding of the data we create frequency tables across variables rather than viewing the variables in isolation.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## three way cross tabs (xtabs) and flatten the table\nftable(xtabs(~ Exchange + Recommend + Internal, data = ordata))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                           Internal External Internal\nExchange   Recommend                                 \nExchange   unlikely                       25        6\n           somewhat likely                12        4\n           very likely                     7        3\nNoExchange unlikely                      175       14\n           somewhat likely                98       26\n           very likely                    20       10\n```\n:::\n:::\n\n\nWe also check the mean and standard deviation of the final score as final score is a numeric variable and cannot be tabulated (unless we convert it to a factor).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(ordata$FinalScore); sd(ordata$FinalScore)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n1.900000 2.720000 2.990000 2.998925 3.270000 4.000000 \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.397940933861\n```\n:::\n:::\n\n\nThe lowest score is 1.9 and the highest score is a 4.0 with a mean of approximately 3. Finally, we inspect the distributions graphically.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# visualize data\nggplot(ordata, aes(x = Recommend, y = FinalScore)) +\n  geom_boxplot(size = .75) +\n  geom_jitter(alpha = .5) +\n  facet_grid(Exchange ~ Internal, margins = TRUE) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1))\n```\n\n::: {.cell-output-display}\n![](regression_files/figure-html/orr6-1.png){width=672}\n:::\n:::\n\n\nWe see that we have only few students that have taken part in an exchange program and there are also only few internal students overall. With respect to recommendations, only few students are considered to very likely succeed in the program. We can now start with the modeling by using the `polr` function. To make things easier for us, we will only consider the main effects here as this tutorial only aims to how to implement an ordinal regression but not how it should be done in a proper study - then, the model fitting and diagnostic procedures would have to be performed accurately, of course. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# fit ordered logit model and store results 'm'\nm.or <- polr(Recommend ~ Internal + Exchange + FinalScore, data = ordata, Hess=TRUE)\n# summarize model\nsummary(m.or)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCall:\npolr(formula = Recommend ~ Internal + Exchange + FinalScore, \n    data = ordata, Hess = TRUE)\n\nCoefficients:\n                          Value  Std. Error     t value\nInternalInternal   1.0476639460 0.265789134 3.941710973\nExchangeNoExchange 0.0586810767 0.297858822 0.197009698\nFinalScore         0.6157435926 0.260631275 2.362508462\n\nIntercepts:\n                            Value       Std. Error  t value    \nunlikely|somewhat likely    2.261997623 0.882173604 2.564118460\nsomewhat likely|very likely 4.357441880 0.904467838 4.817685824\n\nResidual Deviance: 717.024871356 \nAIC: 727.024871356 \n```\n:::\n:::\n\n\nThe results show that having studied here at this school increases the chances of receiving a positive recommendation but that having been on an exchange has a negative but insignificant effect on the recommendation. The final score also correlates positively with a positive recommendation but not as much as having studied here.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## store table\n(ctable <- coef(summary(m.or)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                                      Value     Std. Error        t value\nInternalInternal            1.0476639460469 0.265789134041 3.941710972596\nExchangeNoExchange          0.0586810766831 0.297858821982 0.197009698396\nFinalScore                  0.6157435925546 0.260631274948 2.362508462104\nunlikely|somewhat likely    2.2619976233665 0.882173604268 2.564118459704\nsomewhat likely|very likely 4.3574418799106 0.904467837679 4.817685824069\n```\n:::\n:::\n\n\nAs the regression report does not provide p-values, we have to calculate them separately (after having calculated them, we add them to the coefficient table).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## calculate and store p values\np <- pnorm(abs(ctable[, \"t value\"]), lower.tail = FALSE) * 2\n## combined table\n(ctable <- cbind(ctable, \"p value\" = p))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                                      Value     Std. Error        t value\nInternalInternal            1.0476639460469 0.265789134041 3.941710972596\nExchangeNoExchange          0.0586810766831 0.297858821982 0.197009698396\nFinalScore                  0.6157435925546 0.260631274948 2.362508462104\nunlikely|somewhat likely    2.2619976233665 0.882173604268 2.564118459704\nsomewhat likely|very likely 4.3574418799106 0.904467837679 4.817685824069\n                                        p value\nInternalInternal            0.00008090242989074\nExchangeNoExchange          0.84381994829785212\nFinalScore                  0.01815172703306605\nunlikely|somewhat likely    0.01034382345525988\nsomewhat likely|very likely 0.00000145232812832\n```\n:::\n:::\n\nAs predicted, Exchange does not have a significant effect but FinalScore and Internal both correlate significantly with the likelihood of receiving a positive recommendation.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# extract profiled confidence intervals\nci <- confint(m.or)\n# calculate odds ratios and combine them with profiled CIs\nexp(cbind(OR = coef(m.or), ci))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                              OR          2.5 %        97.5 %\nInternalInternal   2.85098328212 1.695837799597 4.81711408266\nExchangeNoExchange 1.06043698872 0.595033205649 1.91977108408\nFinalScore         1.85103250193 1.113625249822 3.09849059342\n```\n:::\n:::\n\n\nThe odds ratios show that internal students are 2.85 times or 285 percent as likely as non-internal students to receive positive evaluations and that a 1-point increase in the test score lead to a 1.85 times or 185 percent increase in the chances of receiving a positive recommendation. The effect of an exchange is slightly negative but, as we have seen above, not significant.\n\n\n\n## Poisson Regression{-}\n\nThis section is based on [this tutorials](https://stats.idre.ucla.edu/r/dae/poisson-regression/) on how to perform a Poisson regression in R. \n\n\n<div class=\"warning\" style='padding:0.1em; background-color:#f2f2f2; color:#51247a'>\n<span>\n<p style='margin-top:1em; text-align:center'>\n<b>NOTE</b><br><br>Poisson regressions are used to analyze data where the dependent variable represents counts.</p>\n<p style='margin-left:1em;'>\n</p></span>\n</div>\n\n<br>\n\nThis applied particularly to counts that are based on observations of something that is measured in set intervals. For instances the number of pauses in two-minute-long conversations. Poisson regressions are particularly appealing when dealing with rare events, i.e. when something only occurs very infrequently. In such cases, normal linear regressions do not work because the instances that do occur are automatically considered outliers. Therefore, it is useful to check if the data conform to a Poisson distribution.\n\nHowever, the tricky thing about Poisson regressions is that the data has to conform to the Poisson distribution which is, according to my experience, rarely the case, unfortunately. The Gaussian Normal Distribution is very flexible because it is defined by two parameters, the mean (mu, i.e. $\\mu$) and the standard deviation (sigma, i.e. $\\sigma$). This allows the normal distribution to take very different shapes (for example, very high and slim (compressed) or very wide and flat). In contrast, the Poisson is defined by only one parameter (lambda, i.e. $\\lambda$) which mean that if we have a mean of 2, then the standard deviation is also 2 (actually we would have to say that the mean is $\\lambda$ and the standard deviation is also  $\\lambda$ or $\\lambda$ = $\\mu$ = $\\sigma$). This is much trickier for natural data as this means that the Poisson distribution is very rigid.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](regression_files/figure-html/pr0-1.png){width=672}\n:::\n:::\n\n\nAs we can see, as $\\lambda$ takes on higher values, the distribution becomes wider and flatter - a compressed distribution with a high mean can therefore not be Poisson-distributed. We will now start by loading the data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# load data\npoissondata  <- base::readRDS(url(\"https://slcladal.github.io/data/prd.rda\", \"rb\"))\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n```{=html}\n<div class=\"tabwid\"><style>.cl-c2b06146{table-layout:auto;width:75%;}.cl-c2ac240a{font-family:'DejaVu Sans';font-size:12pt;font-weight:bold;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-c2ac241e{font-family:'DejaVu Sans';font-size:12pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-c2ac313e{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-c2ac3148{margin:0;text-align:center;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-c2ac5dbc{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c2ac5dc6{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c2ac5dd0{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c2ac5dda{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c2ac5ddb{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c2ac5de4{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c2ac5de5{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c2ac5dee{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c2ac5df8{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c2ac5df9{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c2ac5e02{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c2ac5e0c{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c2ac5e0d{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c2ac5e0e{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c2ac5e16{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c2ac5e20{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}</style><table class='cl-c2b06146'>\n```\n<caption class=\"Table Caption\">\n\nFirst 15 rows of the poissondata.\n\n</caption>\n```{=html}\n<thead><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-c2ac5e20\"><p class=\"cl-c2ac313e\"><span class=\"cl-c2ac240a\">Id</span></p></td><td class=\"cl-c2ac5e0e\"><p class=\"cl-c2ac313e\"><span class=\"cl-c2ac240a\">Pauses</span></p></td><td class=\"cl-c2ac5e16\"><p class=\"cl-c2ac3148\"><span class=\"cl-c2ac240a\">Language</span></p></td><td class=\"cl-c2ac5e0d\"><p class=\"cl-c2ac313e\"><span class=\"cl-c2ac240a\">Alcohol</span></p></td></tr></thead><tbody><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-c2ac5dd0\"><p class=\"cl-c2ac313e\"><span class=\"cl-c2ac241e\">45</span></p></td><td class=\"cl-c2ac5dda\"><p class=\"cl-c2ac313e\"><span class=\"cl-c2ac241e\">0</span></p></td><td class=\"cl-c2ac5dbc\"><p class=\"cl-c2ac3148\"><span class=\"cl-c2ac241e\">German</span></p></td><td class=\"cl-c2ac5dc6\"><p class=\"cl-c2ac313e\"><span class=\"cl-c2ac241e\">41</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-c2ac5de4\"><p class=\"cl-c2ac313e\"><span class=\"cl-c2ac241e\">108</span></p></td><td class=\"cl-c2ac5ddb\"><p class=\"cl-c2ac313e\"><span class=\"cl-c2ac241e\">0</span></p></td><td class=\"cl-c2ac5dee\"><p class=\"cl-c2ac3148\"><span class=\"cl-c2ac241e\">Russian</span></p></td><td class=\"cl-c2ac5de5\"><p class=\"cl-c2ac313e\"><span class=\"cl-c2ac241e\">41</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-c2ac5dd0\"><p class=\"cl-c2ac313e\"><span class=\"cl-c2ac241e\">15</span></p></td><td class=\"cl-c2ac5dda\"><p class=\"cl-c2ac313e\"><span class=\"cl-c2ac241e\">0</span></p></td><td class=\"cl-c2ac5dbc\"><p class=\"cl-c2ac3148\"><span class=\"cl-c2ac241e\">German</span></p></td><td class=\"cl-c2ac5dc6\"><p class=\"cl-c2ac313e\"><span class=\"cl-c2ac241e\">44</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-c2ac5de4\"><p class=\"cl-c2ac313e\"><span class=\"cl-c2ac241e\">67</span></p></td><td class=\"cl-c2ac5ddb\"><p class=\"cl-c2ac313e\"><span class=\"cl-c2ac241e\">0</span></p></td><td class=\"cl-c2ac5dee\"><p class=\"cl-c2ac3148\"><span class=\"cl-c2ac241e\">German</span></p></td><td class=\"cl-c2ac5de5\"><p class=\"cl-c2ac313e\"><span class=\"cl-c2ac241e\">42</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-c2ac5dd0\"><p class=\"cl-c2ac313e\"><span class=\"cl-c2ac241e\">153</span></p></td><td class=\"cl-c2ac5dda\"><p class=\"cl-c2ac313e\"><span class=\"cl-c2ac241e\">0</span></p></td><td class=\"cl-c2ac5dbc\"><p class=\"cl-c2ac3148\"><span class=\"cl-c2ac241e\">German</span></p></td><td class=\"cl-c2ac5dc6\"><p class=\"cl-c2ac313e\"><span class=\"cl-c2ac241e\">40</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-c2ac5de4\"><p class=\"cl-c2ac313e\"><span class=\"cl-c2ac241e\">51</span></p></td><td class=\"cl-c2ac5ddb\"><p class=\"cl-c2ac313e\"><span class=\"cl-c2ac241e\">0</span></p></td><td class=\"cl-c2ac5dee\"><p class=\"cl-c2ac3148\"><span class=\"cl-c2ac241e\">Russian</span></p></td><td class=\"cl-c2ac5de5\"><p class=\"cl-c2ac313e\"><span class=\"cl-c2ac241e\">42</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-c2ac5dd0\"><p class=\"cl-c2ac313e\"><span class=\"cl-c2ac241e\">164</span></p></td><td class=\"cl-c2ac5dda\"><p class=\"cl-c2ac313e\"><span class=\"cl-c2ac241e\">0</span></p></td><td class=\"cl-c2ac5dbc\"><p class=\"cl-c2ac3148\"><span class=\"cl-c2ac241e\">German</span></p></td><td class=\"cl-c2ac5dc6\"><p class=\"cl-c2ac313e\"><span class=\"cl-c2ac241e\">46</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-c2ac5de4\"><p class=\"cl-c2ac313e\"><span class=\"cl-c2ac241e\">133</span></p></td><td class=\"cl-c2ac5ddb\"><p class=\"cl-c2ac313e\"><span class=\"cl-c2ac241e\">0</span></p></td><td class=\"cl-c2ac5dee\"><p class=\"cl-c2ac3148\"><span class=\"cl-c2ac241e\">German</span></p></td><td class=\"cl-c2ac5de5\"><p class=\"cl-c2ac313e\"><span class=\"cl-c2ac241e\">40</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-c2ac5dd0\"><p class=\"cl-c2ac313e\"><span class=\"cl-c2ac241e\">2</span></p></td><td class=\"cl-c2ac5dda\"><p class=\"cl-c2ac313e\"><span class=\"cl-c2ac241e\">0</span></p></td><td class=\"cl-c2ac5dbc\"><p class=\"cl-c2ac3148\"><span class=\"cl-c2ac241e\">German</span></p></td><td class=\"cl-c2ac5dc6\"><p class=\"cl-c2ac313e\"><span class=\"cl-c2ac241e\">33</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-c2ac5de4\"><p class=\"cl-c2ac313e\"><span class=\"cl-c2ac241e\">53</span></p></td><td class=\"cl-c2ac5ddb\"><p class=\"cl-c2ac313e\"><span class=\"cl-c2ac241e\">0</span></p></td><td class=\"cl-c2ac5dee\"><p class=\"cl-c2ac3148\"><span class=\"cl-c2ac241e\">German</span></p></td><td class=\"cl-c2ac5de5\"><p class=\"cl-c2ac313e\"><span class=\"cl-c2ac241e\">46</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-c2ac5dd0\"><p class=\"cl-c2ac313e\"><span class=\"cl-c2ac241e\">1</span></p></td><td class=\"cl-c2ac5dda\"><p class=\"cl-c2ac313e\"><span class=\"cl-c2ac241e\">0</span></p></td><td class=\"cl-c2ac5dbc\"><p class=\"cl-c2ac3148\"><span class=\"cl-c2ac241e\">German</span></p></td><td class=\"cl-c2ac5dc6\"><p class=\"cl-c2ac313e\"><span class=\"cl-c2ac241e\">40</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-c2ac5de4\"><p class=\"cl-c2ac313e\"><span class=\"cl-c2ac241e\">128</span></p></td><td class=\"cl-c2ac5ddb\"><p class=\"cl-c2ac313e\"><span class=\"cl-c2ac241e\">0</span></p></td><td class=\"cl-c2ac5dee\"><p class=\"cl-c2ac3148\"><span class=\"cl-c2ac241e\">English</span></p></td><td class=\"cl-c2ac5de5\"><p class=\"cl-c2ac313e\"><span class=\"cl-c2ac241e\">38</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-c2ac5dd0\"><p class=\"cl-c2ac313e\"><span class=\"cl-c2ac241e\">16</span></p></td><td class=\"cl-c2ac5dda\"><p class=\"cl-c2ac313e\"><span class=\"cl-c2ac241e\">0</span></p></td><td class=\"cl-c2ac5dbc\"><p class=\"cl-c2ac3148\"><span class=\"cl-c2ac241e\">German</span></p></td><td class=\"cl-c2ac5dc6\"><p class=\"cl-c2ac313e\"><span class=\"cl-c2ac241e\">44</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-c2ac5de4\"><p class=\"cl-c2ac313e\"><span class=\"cl-c2ac241e\">106</span></p></td><td class=\"cl-c2ac5ddb\"><p class=\"cl-c2ac313e\"><span class=\"cl-c2ac241e\">0</span></p></td><td class=\"cl-c2ac5dee\"><p class=\"cl-c2ac3148\"><span class=\"cl-c2ac241e\">German</span></p></td><td class=\"cl-c2ac5de5\"><p class=\"cl-c2ac313e\"><span class=\"cl-c2ac241e\">37</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-c2ac5df9\"><p class=\"cl-c2ac313e\"><span class=\"cl-c2ac241e\">89</span></p></td><td class=\"cl-c2ac5e0c\"><p class=\"cl-c2ac313e\"><span class=\"cl-c2ac241e\">0</span></p></td><td class=\"cl-c2ac5df8\"><p class=\"cl-c2ac3148\"><span class=\"cl-c2ac241e\">German</span></p></td><td class=\"cl-c2ac5e02\"><p class=\"cl-c2ac313e\"><span class=\"cl-c2ac241e\">40</span></p></td></tr></tbody></table></div>\n```\n:::\n:::\n\n\nWe will clean the data by factorizing Id which is currently considered a numeric variable rather than a factor.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# process data\npoissondata <- poissondata %>%\n  mutate(Id = factor(Id, levels = 1:200, labels = 1:200))\n# inspect data\nstr(poissondata)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n'data.frame':\t200 obs. of  4 variables:\n $ Id      : Factor w/ 200 levels \"1\",\"2\",\"3\",\"4\",..: 45 108 15 67 153 51 164 133 2 53 ...\n $ Pauses  : int  0 0 0 0 0 0 0 0 0 0 ...\n $ Language: chr  \"German\" \"Russian\" \"German\" \"German\" ...\n $ Alcohol : int  41 41 44 42 40 42 46 40 33 46 ...\n```\n:::\n:::\n\n\nFirst, we check if the conditions for a Poisson regression are met.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# output the results\ngf = vcd::goodfit(poissondata$Pauses, \n                  type= \"poisson\", \n                  method= \"ML\")\n# inspect results\nsummary(gf)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\t Goodness-of-fit test for poisson distribution\n\n                           X^2 df            P(> X^2)\nLikelihood Ratio 33.0122916717  5 0.00000374234139957\n```\n:::\n:::\n\n\nIf the p-values is smaller than .05, then data is not Poisson distributed which means that it differs significantly from a Poisson distribution and is very likely over-dispersed. We will check the divergence from a Poisson distribution visually by plotting the observed counts against the expected counts if the data were Poisson distributed.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(gf,main=\"Count data vs Poisson distribution\")\n```\n\n::: {.cell-output-display}\n![](regression_files/figure-html/pr5-1.png){width=672}\n:::\n:::\n\n\nAlthough the goodfit function reported that the data differs significantly from the Poisson distribution, the fit is rather good. We can use an additional Levene's test to check if variance homogeneity is given. \n\n<div class=\"warning\" style='padding:0.1em; background-color:#f2f2f2; color:#51247a'>\n<span>\n<p style='margin-top:1em; text-align:center'>\n<b>NOTE</b><br><br>The use of Levene's test to check if the model is heteroscedastic is generally not recommended as it is too lax when dealing with few observations (because in such cases it does not have the power to identify heteroscedasticity) while it is too harsh when dealing with many observations (when heteroscedasticity typically is not a severe problem).</p>\n<p style='margin-left:1em;'>\n</p></span>\n</div>\n\n\n<br>\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# check homogeneity\nleveneTest(poissondata$Pauses, poissondata$Language, center = mean)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLevene's Test for Homogeneity of Variance (center = mean)\n       Df  F value        Pr(>F)    \ngroup   2 17.15274 0.00000013571 ***\n      197                           \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\nThe Levene's test indicates that variance homogeneity is also violated. Since both the approximation to a Poisson distribution and variance homogeneity are violated, we should switch either to a quasi-Poisson model or a negative binomial model. However, as we are only interested in how to implement a Poisson model here, we continue despite the fact that this could not be recommended if we were actually interested in accurate results based on a reliable model. \n\nIn a next step, we summarize Progression by inspecting the means and standard deviations of the individual variable levels.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# extract mean and standard deviation\nwith(poissondata, tapply(Pauses, Language, function(x) {\n  sprintf(\"M (SD) = %1.2f (%1.2f)\", mean(x), sd(x))\n}))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n               English                 German                Russian \n\"M (SD) = 1.00 (1.28)\" \"M (SD) = 0.24 (0.52)\" \"M (SD) = 0.20 (0.40)\" \n```\n:::\n:::\n\n\nNow, we visualize the data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# plot data\nggplot(poissondata, aes(Pauses, fill = Language)) +\n  geom_histogram(binwidth=.5, position=\"dodge\") +\n  scale_fill_manual(values=c(\"gray30\", \"gray50\", \"gray70\"))\n```\n\n::: {.cell-output-display}\n![](regression_files/figure-html/pr8-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# calculate Poisson regression\nm1.poisson <- glm(Pauses ~ Language + Alcohol, family=\"poisson\", data=poissondata)\n# inspect model\nsummary(m1.poisson)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nglm(formula = Pauses ~ Language + Alcohol, family = \"poisson\", \n    data = poissondata)\n\nDeviance Residuals: \n         Min            1Q        Median            3Q           Max  \n-2.204338080  -0.843641817  -0.510586515   0.255772098   2.679576560  \n\nCoefficients:\n                     Estimate    Std. Error  z value         Pr(>|z|)    \n(Intercept)     -4.1632652529  0.6628774832 -6.28060 0.00000000033728 ***\nLanguageGerman  -0.7140499158  0.3200148750 -2.23130         0.025661 *  \nLanguageRussian -1.0838591456  0.3582529824 -3.02540         0.002483 ** \nAlcohol          0.0701523975  0.0105992050  6.61865 0.00000000003625 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 287.6722345  on 199  degrees of freedom\nResidual deviance: 189.4496199  on 196  degrees of freedom\nAIC: 373.5045031\n\nNumber of Fisher Scoring iterations: 6\n```\n:::\n:::\n\n\nIn addition to the Estimates for the coefficients, we could also calculate the confidence intervals for the coefficients (LL stands for lower limit and UL for upper limit in the table below).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# calculate model\ncov.m1 <- sandwich::vcovHC(m1.poisson, type=\"HC0\")\n# extract standard error\nstd.err <- sqrt(diag(cov.m1))\n# extract robust estimates\nr.est <- cbind(Estimate= coef(m1.poisson), \n               \"Robust SE\" = std.err,\n               \"Pr(>|z|)\" = 2 * pnorm(abs(coef(m1.poisson)/std.err),\n                                      lower.tail=FALSE),\nLL = coef(m1.poisson) - 1.96 * std.err,\nUL = coef(m1.poisson) + 1.96 * std.err)\n# inspect data\nr.est\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                        Estimate       Robust SE                 Pr(>|z|)\n(Intercept)     -4.1632652529178 0.6480942940026 0.0000000001328637743948\nLanguageGerman  -0.7140499157783 0.2986422497410 0.0168031204011183932234\nLanguageRussian -1.0838591456208 0.3210481575684 0.0007354744824167306532\nAlcohol          0.0701523974937 0.0104351647012 0.0000000000178397516955\n                              LL               UL\n(Intercept)     -5.4335300691629 -2.8930004366727\nLanguageGerman  -1.2993887252708 -0.1287111062859\nLanguageRussian -1.7131135344549 -0.4546047567867\nAlcohol          0.0496994746793  0.0906053203082\n```\n:::\n:::\n\n\nWe can now calculate the p-value of the model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwith(m1.poisson, cbind(res.deviance = deviance, df = df.residual,\n  p = pchisq(deviance, df.residual, lower.tail=FALSE)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     res.deviance  df              p\n[1,] 189.44961991 196 0.618227445717\n```\n:::\n:::\n\n\nNow, we check, if removing Language leads to a significant decrease in model fit.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# remove Language from the model\nm2.poisson <- update(m1.poisson, . ~ . -Language)\n# check if dropping Language causes a significant decrease in model fit\nanova(m2.poisson, m1.poisson, test=\"Chisq\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Deviance Table\n\nModel 1: Pauses ~ Alcohol\nModel 2: Pauses ~ Language + Alcohol\n  Resid. Df  Resid. Dev Df   Deviance   Pr(>Chi)    \n1       198 204.0213018                             \n2       196 189.4496199  2 14.5716819 0.00068517 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\nWe now calculate robust coefficients using the `msm` package [@msm].\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# get estimates\ns <- msm::deltamethod(list(~ exp(x1), ~ exp(x2), ~ exp(x3), ~ exp(x4)), \n                                                coef(m1.poisson), cov.m1)\n# exponentiate old estimates dropping the p values\nrexp.est <- exp(r.est[, -3])\n# replace SEs with estimates for exponentiated coefficients\nrexp.est[, \"Robust SE\"] <- s\n# display results\nrexp.est\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                       Estimate       Robust SE               LL\n(Intercept)     0.0155566784084 0.0100821945101 0.00436765044896\nLanguageGerman  0.4896571063601 0.1462322998451 0.27269843575906\nLanguageRussian 0.3382875026084 0.1086065794408 0.18030353649622\nAlcohol         1.0726716412682 0.0111935052470 1.05095521026088\n                             UL\n(Intercept)     0.0554097096208\nLanguageGerman  0.8792279322820\nLanguageRussian 0.6346987787641\nAlcohol         1.0948368101199\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# extract predicted values\n(s1 <- data.frame(Alcohol = mean(poissondata$Alcohol),\n  Language = factor(1:3, levels = 1:3, labels = names(table(poissondata$Language)))))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  Alcohol Language\n1  52.645  English\n2  52.645   German\n3  52.645  Russian\n```\n:::\n\n```{.r .cell-code}\n# show results\npredict(m1.poisson, s1, type=\"response\", se.fit=TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$fit\n             1              2              3 \n0.624944591447 0.306008560283 0.211410945109 \n\n$se.fit\n              1               2               3 \n0.0862811728183 0.0883370633684 0.0705010813453 \n\n$residual.scale\n[1] 1\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n## calculate and store predicted values\npoissondata$Predicted <- predict(m1.poisson, type=\"response\")\n## order by program and then by math\npoissondata <- poissondata[with(poissondata, order(Language, Alcohol)), ]\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n## create the plot\nggplot(poissondata, aes(x = Alcohol, y = Predicted, colour = Language)) +\n  geom_point(aes(y = Pauses), alpha=.5, \n             position=position_jitter(h=.2)) +\n  geom_line(size = 1) +\n  labs(x = \"Alcohol (ml)\", y = \"Expected number of pauses\") +\n  scale_color_manual(values=c(\"gray30\", \"gray50\", \"gray70\"))\n```\n\n::: {.cell-output-display}\n![](regression_files/figure-html/pr16-1.png){width=672}\n:::\n:::\n\n\n\n## Robust Regression{-}\n\nRobust regression represent an alternative to simple linear models which can handle overly influential data points (outliers). Robust regressions allow us to retain outliers in the data rather than having to remove them from the data by adding weights [@rousseeuw2005robust]. Thus, robust regressions are used when there are outliers present in the data and we can thus not use traditional models but we have no good argument to remove these data points. \n\n\n\n<div class=\"warning\" style='padding:0.1em; background-color:#f2f2f2; color:#51247a'>\n<span>\n<p style='margin-top:1em; text-align:center'>\n<b>NOTE</b><br><br>Robust regressions allow us to handle overly influential data points (outliers) by using weights. Thus, robust regressions enable us to retain all data points.</p>\n<p style='margin-left:1em;'>\n</p></span>\n</div>\n\n\n<br>\n\n\nWe begin by loading a data set (the mlrdata set which have used for multiple linear regression).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# load data\nrobustdata  <- base::readRDS(url(\"https://slcladal.github.io/data/mld.rda\", \"rb\"))\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n```{=html}\n<div class=\"tabwid\"><style>.cl-c34b9918{table-layout:auto;width:75%;}.cl-c347ce0a{font-family:'DejaVu Sans';font-size:12pt;font-weight:bold;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-c347ce14{font-family:'DejaVu Sans';font-size:12pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-c347d800{margin:0;text-align:center;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-c347d80a{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-c347f876{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c347f880{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c347f881{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c347f894{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c347f89e{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c347f8a8{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c347f8a9{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c347f8b2{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c347f8b3{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c347f8b4{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c347f8bc{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c347f8bd{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}</style><table class='cl-c34b9918'>\n```\n<caption class=\"Table Caption\">\n\nFirst 15 rows of the robustdata.\n\n</caption>\n```{=html}\n<thead><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-c347f8bc\"><p class=\"cl-c347d800\"><span class=\"cl-c347ce0a\">status</span></p></td><td class=\"cl-c347f8b4\"><p class=\"cl-c347d800\"><span class=\"cl-c347ce0a\">attraction</span></p></td><td class=\"cl-c347f8bd\"><p class=\"cl-c347d80a\"><span class=\"cl-c347ce0a\">money</span></p></td></tr></thead><tbody><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-c347f881\"><p class=\"cl-c347d800\"><span class=\"cl-c347ce14\">Relationship</span></p></td><td class=\"cl-c347f876\"><p class=\"cl-c347d800\"><span class=\"cl-c347ce14\">NotInterested</span></p></td><td class=\"cl-c347f880\"><p class=\"cl-c347d80a\"><span class=\"cl-c347ce14\">86.33</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-c347f8a8\"><p class=\"cl-c347d800\"><span class=\"cl-c347ce14\">Relationship</span></p></td><td class=\"cl-c347f894\"><p class=\"cl-c347d800\"><span class=\"cl-c347ce14\">NotInterested</span></p></td><td class=\"cl-c347f89e\"><p class=\"cl-c347d80a\"><span class=\"cl-c347ce14\">45.58</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-c347f881\"><p class=\"cl-c347d800\"><span class=\"cl-c347ce14\">Relationship</span></p></td><td class=\"cl-c347f876\"><p class=\"cl-c347d800\"><span class=\"cl-c347ce14\">NotInterested</span></p></td><td class=\"cl-c347f880\"><p class=\"cl-c347d80a\"><span class=\"cl-c347ce14\">68.43</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-c347f8a8\"><p class=\"cl-c347d800\"><span class=\"cl-c347ce14\">Relationship</span></p></td><td class=\"cl-c347f894\"><p class=\"cl-c347d800\"><span class=\"cl-c347ce14\">NotInterested</span></p></td><td class=\"cl-c347f89e\"><p class=\"cl-c347d80a\"><span class=\"cl-c347ce14\">52.93</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-c347f881\"><p class=\"cl-c347d800\"><span class=\"cl-c347ce14\">Relationship</span></p></td><td class=\"cl-c347f876\"><p class=\"cl-c347d800\"><span class=\"cl-c347ce14\">NotInterested</span></p></td><td class=\"cl-c347f880\"><p class=\"cl-c347d80a\"><span class=\"cl-c347ce14\">61.86</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-c347f8a8\"><p class=\"cl-c347d800\"><span class=\"cl-c347ce14\">Relationship</span></p></td><td class=\"cl-c347f894\"><p class=\"cl-c347d800\"><span class=\"cl-c347ce14\">NotInterested</span></p></td><td class=\"cl-c347f89e\"><p class=\"cl-c347d80a\"><span class=\"cl-c347ce14\">48.47</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-c347f881\"><p class=\"cl-c347d800\"><span class=\"cl-c347ce14\">Relationship</span></p></td><td class=\"cl-c347f876\"><p class=\"cl-c347d800\"><span class=\"cl-c347ce14\">NotInterested</span></p></td><td class=\"cl-c347f880\"><p class=\"cl-c347d80a\"><span class=\"cl-c347ce14\">32.79</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-c347f8a8\"><p class=\"cl-c347d800\"><span class=\"cl-c347ce14\">Relationship</span></p></td><td class=\"cl-c347f894\"><p class=\"cl-c347d800\"><span class=\"cl-c347ce14\">NotInterested</span></p></td><td class=\"cl-c347f89e\"><p class=\"cl-c347d80a\"><span class=\"cl-c347ce14\">35.91</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-c347f881\"><p class=\"cl-c347d800\"><span class=\"cl-c347ce14\">Relationship</span></p></td><td class=\"cl-c347f876\"><p class=\"cl-c347d800\"><span class=\"cl-c347ce14\">NotInterested</span></p></td><td class=\"cl-c347f880\"><p class=\"cl-c347d80a\"><span class=\"cl-c347ce14\">30.98</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-c347f8a8\"><p class=\"cl-c347d800\"><span class=\"cl-c347ce14\">Relationship</span></p></td><td class=\"cl-c347f894\"><p class=\"cl-c347d800\"><span class=\"cl-c347ce14\">NotInterested</span></p></td><td class=\"cl-c347f89e\"><p class=\"cl-c347d80a\"><span class=\"cl-c347ce14\">44.82</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-c347f881\"><p class=\"cl-c347d800\"><span class=\"cl-c347ce14\">Relationship</span></p></td><td class=\"cl-c347f876\"><p class=\"cl-c347d800\"><span class=\"cl-c347ce14\">NotInterested</span></p></td><td class=\"cl-c347f880\"><p class=\"cl-c347d80a\"><span class=\"cl-c347ce14\">35.05</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-c347f8a8\"><p class=\"cl-c347d800\"><span class=\"cl-c347ce14\">Relationship</span></p></td><td class=\"cl-c347f894\"><p class=\"cl-c347d800\"><span class=\"cl-c347ce14\">NotInterested</span></p></td><td class=\"cl-c347f89e\"><p class=\"cl-c347d80a\"><span class=\"cl-c347ce14\">64.49</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-c347f881\"><p class=\"cl-c347d800\"><span class=\"cl-c347ce14\">Relationship</span></p></td><td class=\"cl-c347f876\"><p class=\"cl-c347d800\"><span class=\"cl-c347ce14\">NotInterested</span></p></td><td class=\"cl-c347f880\"><p class=\"cl-c347d80a\"><span class=\"cl-c347ce14\">54.50</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-c347f8a8\"><p class=\"cl-c347d800\"><span class=\"cl-c347ce14\">Relationship</span></p></td><td class=\"cl-c347f894\"><p class=\"cl-c347d800\"><span class=\"cl-c347ce14\">NotInterested</span></p></td><td class=\"cl-c347f89e\"><p class=\"cl-c347d80a\"><span class=\"cl-c347ce14\">61.48</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-c347f8b3\"><p class=\"cl-c347d800\"><span class=\"cl-c347ce14\">Relationship</span></p></td><td class=\"cl-c347f8b2\"><p class=\"cl-c347d800\"><span class=\"cl-c347ce14\">NotInterested</span></p></td><td class=\"cl-c347f8a9\"><p class=\"cl-c347d80a\"><span class=\"cl-c347ce14\">55.51</span></p></td></tr></tbody></table></div>\n```\n:::\n:::\n\n\nWe first fit an ordinary linear model (and although we know from the section on multiple regression that the interaction between status and attraction is significant, we will disregard this for now as this will help to explain the weighing procedure which is the focus of this section).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# create model\nslm <- lm(money ~ status+attraction, data = robustdata)\n# inspect model\nsummary(slm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = money ~ status + attraction, data = robustdata)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-60.87070 -15.78645  -2.61010  13.88770  59.93710 \n\nCoefficients:\n                            Estimate   Std. Error   t value\n(Intercept)             114.94950000   4.28993616  26.79515\nstatusSingle             26.10340000   4.95359160   5.26959\nattractionNotInterested -79.25220000   4.95359160 -15.99894\n                                      Pr(>|t|)    \n(Intercept)             < 0.000000000000000222 ***\nstatusSingle                     0.00000082576 ***\nattractionNotInterested < 0.000000000000000222 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 24.767958 on 97 degrees of freedom\nMultiple R-squared:  0.745229335,\tAdjusted R-squared:  0.739976331 \nF-statistic: 141.867286 on 2 and 97 DF,  p-value: < 0.0000000000000002220446\n```\n:::\n:::\n\n\nWe now check whether the model is well fitted using diagnostic plots.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# generate plots\nautoplot(slm) + \n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) \n```\n\n::: {.cell-output-display}\n![](regression_files/figure-html/rr4-1.png){width=672}\n:::\n:::\n\n\n\nThe diagnostic plots indicate that there are three outliers in the data (data points 52, 83 and possibly 64). Therefore, we need to evaluate if the outliers severely affect the fit of the model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrobustdata[c(52, 64, 83),]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   status    attraction  money\n52 Single NotInterested   0.93\n64 Single NotInterested  84.28\n83 Single    Interested 200.99\n```\n:::\n:::\n\n\nWe can now calculate Cook's distance and standardized residuals check if the values of the potentially problematic points have unacceptably high values (-2 < ok < 2).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nCooksDistance <- cooks.distance(slm)\nStandardizedResiduals <- stdres(slm)\na <- cbind(robustdata, CooksDistance, StandardizedResiduals)\na[CooksDistance > 4/100, ]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n         status    attraction  money   CooksDistance StandardizedResiduals\n1  Relationship NotInterested  86.33 0.0444158829857         2.07565427025\n52       Single NotInterested   0.93 0.0641937458854        -2.49535435377\n65       Single NotInterested  12.12 0.0427613629698        -2.03662765573\n67       Single NotInterested  13.28 0.0407877963315        -1.98907421786\n83       Single    Interested 200.99 0.0622397126545         2.45708203516\n84       Single    Interested 193.69 0.0480020776213         2.15782333134\n88       Single    Interested 193.78 0.0481663678409         2.16151282221\n```\n:::\n:::\n\n\nWe will calculate the absolute value and reorder the table so that it is easier to check the values.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nAbsoluteStandardizedResiduals <- abs(StandardizedResiduals)\na <- cbind(robustdata, CooksDistance, StandardizedResiduals, AbsoluteStandardizedResiduals)\nasorted <- a[order(-AbsoluteStandardizedResiduals), ]\nasorted[1:10, ]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n         status    attraction  money   CooksDistance StandardizedResiduals\n52       Single NotInterested   0.93 0.0641937458854        -2.49535435377\n83       Single    Interested 200.99 0.0622397126545         2.45708203516\n88       Single    Interested 193.78 0.0481663678409         2.16151282221\n84       Single    Interested 193.69 0.0480020776213         2.15782333134\n1  Relationship NotInterested  86.33 0.0444158829857         2.07565427025\n65       Single NotInterested  12.12 0.0427613629698        -2.03662765573\n67       Single NotInterested  13.28 0.0407877963315        -1.98907421786\n78       Single    Interested 188.76 0.0394313968241         1.95572122040\n21 Relationship NotInterested  81.90 0.0369837409025         1.89404933081\n24 Relationship NotInterested  81.56 0.0364414260634         1.88011125419\n   AbsoluteStandardizedResiduals\n52                 2.49535435377\n83                 2.45708203516\n88                 2.16151282221\n84                 2.15782333134\n1                  2.07565427025\n65                 2.03662765573\n67                 1.98907421786\n78                 1.95572122040\n21                 1.89404933081\n24                 1.88011125419\n```\n:::\n:::\n\n\nAs Cook's distance and the standardized residuals do have unacceptable values, we re-calculate the linear model as a robust regression and inspect the results\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# create robust regression model\nrmodel <- robustbase::lmrob(money ~ status + attraction, data = robustdata)\n# inspect model\nsummary(rmodel)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nrobustbase::lmrob(formula = money ~ status + attraction, data = robustdata)\n \\--> method = \"MM\"\nResiduals:\n         Min           1Q       Median           3Q          Max \n-61.14269796 -15.20405781  -1.48712081  14.43502508  62.42342804 \n\nCoefficients:\n                            Estimate   Std. Error   t value\n(Intercept)             113.18405781   3.89777692  29.03811\nstatusSingle             25.38251415   5.08841106   4.98830\nattractionNotInterested -76.49387400   5.06626449 -15.09867\n                                      Pr(>|t|)    \n(Intercept)             < 0.000000000000000222 ***\nstatusSingle                      0.0000026725 ***\nattractionNotInterested < 0.000000000000000222 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nRobust residual standard error: 22.3497532 \nMultiple R-squared:  0.740716949,\tAdjusted R-squared:  0.735370907 \nConvergence in 11 IRWLS iterations\n\nRobustness weights: \n 10 weights are ~= 1. The remaining 90 ones are summarized as\n       Min.     1st Qu.      Median        Mean     3rd Qu.        Max. \n0.415507215 0.856134403 0.947485769 0.889078657 0.986192099 0.998890516 \nAlgorithmic parameters: \n           tuning.chi                    bb            tuning.psi \n1.5476399999999999046 0.5000000000000000000 4.6850610000000001421 \n           refine.tol               rel.tol             scale.tol \n0.0000001000000000000 0.0000001000000000000 0.0000000001000000000 \n            solve.tol           eps.outlier                 eps.x \n0.0000001000000000000 0.0010000000000000000 0.0000000000018189894 \n    warn.limit.reject     warn.limit.meanrw \n0.5000000000000000000 0.5000000000000000000 \n     nResample         max.it       best.r.s       k.fast.s          k.max \n           500             50              2              1            200 \n   maxit.scale      trace.lev            mts     compute.rd fast.s.large.n \n           200              0           1000              0           2000 \n                  psi           subsampling                   cov \n           \"bisquare\"         \"nonsingular\"         \".vcov.avar1\" \ncompute.outlier.stats \n                 \"SM\" \nseed : int(0) \n```\n:::\n:::\n\n\n\nThe output shows that both status and attraction are significant but, as we have seen above, the effect that really matters is the interaction between status and attraction. \n\nWe will briefly check the weights to understand the process of weighing better. The idea of weighing is to downgrade data points that are too influential while not punishing data points that have a good fit and are thus less influential. This means that the problematic data points should have lower weights than other data points (the maximum is 1 - so points can only be made \"lighter\"). \n\n\n::: {.cell}\n\n```{.r .cell-code}\nhweights <- data.frame(status = robustdata$status, resid = rmodel$resid, weight = rmodel$rweights)\nhweights2 <- hweights[order(rmodel$rweights), ]\nhweights2[1:15, ]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n         status          resid         weight\n83       Single  62.4234280426 0.415507214894\n52       Single -61.1426979566 0.434323578289\n88       Single  55.2134280426 0.521220526760\n84       Single  55.1234280426 0.522529106622\n78       Single  50.1934280426 0.593234343069\n65       Single -49.9526979566 0.596626306571\n1  Relationship  49.6398161925 0.601024865582\n67       Single -48.7926979566 0.612874578464\n21 Relationship  45.2098161925 0.661914499988\n24 Relationship  44.8698161925 0.666467581490\n39 Relationship -43.8940578083 0.679427975993\n79       Single  40.8234280426 0.719104361372\n58       Single -40.5226979566 0.722893449575\n89       Single  39.9734280426 0.729766992775\n95       Single  39.8234280426 0.731633375015\n```\n:::\n:::\n\n\nThe values of the weights support our assumption that those data points that were deemed too influential are made *lighter* as they now only have weights of 0.415507214894 and 0.434323578289 respectively. This was, however, not the focus of this sections as this section merely served to introduce the concept of weights and how they can be used in the context of a robust linear regression.\n\n# Mixed-Effects Regression\n\nIn contrast to fixed-effects regression models, mixed-effects models assume a hierarchical data structure in which data points are grouped or nested in higher order categories (e.g. students within classes). Mixed-effects models are rapidly increasing in use in data analysis because they allow us to incorporate hierarchical or nested data structures. Mixed-effects models are, of course, an extension of fixed-effects regression models and also multivariate and come in different types. \n\nIn the following, we will go over the most relevant and frequently used types of mixed-effect regression models, mixed-effects linear regression models and mixed-effects binomial logistic regression models. \n\nThe major difference between these types of models is that they take different types of dependent variables. While linear models take numeric dependent variables, logistic models take nominal variables.\n\n\n## Linear Mixed-Effects Regression{-}\n\nThe following focuses on an extension of ordinary multiple linear regressions: mixed-effects regression linear regression. Mixed-effects models have the following advantages over simpler statistical tests: \n\n* Mixed-effects models are multivariate, i.e. they test the effect of several predictors simultaneously while controlling for the effect of all other predictors. \n\n* Mixed models allow to statistically incorporate within-speaker variability and are thus fit to model hierarchical or nested data structures. This applies if several observations are produced by an individual speaker, for instance.\n\n* Mixed-models provide a wealth of diagnostic statistics which enables us to control e.g. (multi-)collinearity, i.e. correlations between predictors, and to test whether conditions or requirements are violated (e.g. homogeneity of variance, etc.). \n\nMajor disadvantages of mixed-effects regression modeling are that they are prone to producing high $\\beta$-errors [see @johnson2009getting] and that they require rather large data sets. \n\n### Introduction{-}\n\nSo far, the regression models that we have used only had fixed-effects. Having only fixed-effects means that all data points are treated as if they are completely independent and thus on the same hierarchical level. However, it is very common that the data is nested in the sense that data points are not independent because they are, for instance produced by the same speaker or are grouped by some other characteristic. In such cases, the data is considered hierarchical and statistical models should incorporate such structural features of the data they work upon. Fortunately, modeling hierarchical or nested data structures is very easy thanks to the `lme4` package [@lme4].\n\n\nWith respect to regression modeling, hierarchical structures are incorporated by what is called *random effects*. When models only have a fixed-effects structure, then they make use of only a single intercept and/or slope (as in the left panel in the figure below), while mixed effects models have intercepts for each level of a random effect. If the random effect structure represents speakers then this would mean that a mixed-model would have a separate intercept and/or slope for each speaker (in addition to the overall intercept that is shown as an orange line in the figure below). \n\n\n::: {.cell}\n::: {.cell-output-display}\n![](regression_files/figure-html/lmm1-1.png){width=672}\n:::\n:::\n\n\nThe idea behind regression analysis is expressed formally in the equation below where$f_{(x)}$ is the y-value we want to predict, $\\alpha$ is the intercept (the point where the regression line crosses the y-axis at x = 0), $\\beta$ is the coefficient (the slope of the regression line), and x is the value of a predictor (e.g. 180cm - if we would like to predict the weight of a person based on their height). The $\\epsilon$ is an error term that reflects the difference between the predicted value and the (actually) observed value ($\\epsilon$ is thus a residual that is important as regressions assume that residuals are, e.g., normally distributed). \n\n\\begin{equation}\nf_{(x)} = \\alpha + \\beta x + \\epsilon\n\\end{equation}\n\nIn other words, to estimate how much some weights who is 180cm tall, we would multiply the coefficient (slope of the line) with 180 ($x$) and add the value of the intercept (point where line crosses the y-axis  at x = 0). \n\nThe equation below represents a formal representation of a mixed-effects regression with varying intercepts [see @winter2019statistics, 235].\n\n\\begin{equation}\nf_{(x)} = \\alpha_{i} + \\beta x + \\epsilon\n\\end{equation}\n\nIn this random intercept model, each level of a random variable has a different intercept. To predict the value of a data point, we would thus take the appropriate intercept value (the model intercept + the intercept of the random effect) and add the product of the predictor coefficient and the value of x. \n\nFinally, the equation below represents a formal representation of a mixed-effects regression with varying intercepts and varying slopes [see @winter2019statistics, 235].\n\n\\begin{equation}\nf_{(x)} = \\alpha_{i} + \\beta_{i}x + \\epsilon\n\\end{equation}\n\nIn this last model, each level of a random variable has a different intercept and a different slope. To predict the value of a data point, we would thus take the appropriate intercept value (the model intercept + the intercept of the random effect) and add the coefficient of that random effect level multiplied by the value of x. \n\n### Random Effects{-}\n\n*Random Effects* can be visualized using two parameters: the intercept (the point where the regression line crosses the y-axis at x = 0) and the slope (the acclivity of the regression line). In contrast to fixed-effects models, that have only 1 intercept and one slope (left panel in the figure above), mixed-effects models can therefore have various *random intercepts* (center panel) or various *random slopes*, or both, various *random intercepts* and various *random slopes* (right panel). \n\nWhat features do distinguish random and fixed effects? \n\n1) Random effects represent a higher level variable under which data points are grouped. This implies that random effects must be categorical (or nominal but *they aÂ´cannot be continuous*!) [see @winter2019statistics, p. 236].\n\n2) Random effects represent a sample of an infinite number of possible levels. For instance, speakers, trials, items, subjects, or words represent a potentially infinite pool of elements from which many different samples can be drawn. Thus, random effects represent a random sample sample. Fixed effects, on the other hand, typically do not represent a random sample but a fixed set of variable levels (e.g. Age groups, or parts-of-speech).\n\n3) Random effects typically represent many different levels while fixed effects typically have only a few. @zuur2013beginner propose that a variable may be used as a fixed effect if it has less than 5 levels while it should be treated as a random effect if it has more than 10 levels. Variables with 5 to 10 levels can be used as both. However, this is a rule of thumb and ignores the theoretical reasons (random sample and nestedness) for considering something as a random effect and it also is at odds with the way that repeated measures are models (namely as mixed effects) although they typically only have very few levels.  \n\n4) Fixed effects represent an effect that if we draw many samples, the effect would be consistent across samples [@winter2019statistics] while random effects should vary for each new sample that is drawn.\n\nIn the following, we will only focus on models with random intercepts because this is the more common method and because including both random intercepts and random slopes requires larger data sets (but have a better fit because intercepts are not forced to be parallel and the lines therefore have a better fit). You should, however, always think about what random effects structure is appropriate for your model - a very recommendable explanation of how to chose which random effects structure is best (and about what the determining factors for this decision are) is give in @winter2019statistics[241-244]. Also, consider the center and the right plots above to understand what is meant by *random intercepts* and *random slopes*.\n\nAfter adding random intercepts, predictors (or fixed effects) are added to the model (just like with multiple regression). So mixed-effects are called mixed-effects because they contain both random and fixed effects.\n\nIn terms of general procedure, random effects are added first, and only after we have ascertained that including random effects is warranted, we test whether including fixed-effects is warranted [@field2012discovering]. We test whether including random effects is warranted by comparing a model, that bases its estimates of the depended variable solely on the base intercept (the mean), with a model, that bases its estimates of the dependent variable solely on the intercepts of the random effect. If the random-effect model explains significantly more variance than the simple model without random effect structure, then we continue with the mixed-effects model. In other words, including random effects is justified if they reduce residual deviance.\n\n### Example: Preposition Use across Time by Genre{-}\n\nTo explore how to implement a mixed-effects model in R we revisit the preposition data that contains relative frequencies of prepositions in English texts written between 1150 and 1913. As a first step, and to prepare our analysis, we load necessary R packages, specify options, and load as well as provide an overview of the data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# load data\nlmmdata  <- base::readRDS(url(\"https://slcladal.github.io/data/lmd.rda\", \"rb\")) %>%\n  # convert date into a numeric variable\n  dplyr::mutate(Date = as.numeric(Date))\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n```{=html}\n<div class=\"tabwid\"><style>.cl-c48a1804{table-layout:auto;width:75%;}.cl-c4856dea{font-family:'DejaVu Sans';font-size:12pt;font-weight:bold;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-c4856df4{font-family:'DejaVu Sans';font-size:12pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-c4857c22{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-c4857c2c{margin:0;text-align:center;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-c485af8a{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c485af94{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c485af9e{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c485afa8{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c485afa9{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c485afaa{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c485afb2{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c485afb3{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c485afb4{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c485afbc{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c485afc6{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c485afc7{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c485afd0{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c485afda{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c485afdb{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c485afe4{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}</style><table class='cl-c48a1804'>\n```\n<caption class=\"Table Caption\">\n\nFirst 15 rows of the lmmdata.\n\n</caption>\n```{=html}\n<thead><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-c485afd0\"><p class=\"cl-c4857c22\"><span class=\"cl-c4856dea\">Date</span></p></td><td class=\"cl-c485afda\"><p class=\"cl-c4857c2c\"><span class=\"cl-c4856dea\">Genre</span></p></td><td class=\"cl-c485afda\"><p class=\"cl-c4857c2c\"><span class=\"cl-c4856dea\">Text</span></p></td><td class=\"cl-c485afdb\"><p class=\"cl-c4857c22\"><span class=\"cl-c4856dea\">Prepositions</span></p></td><td class=\"cl-c485afe4\"><p class=\"cl-c4857c2c\"><span class=\"cl-c4856dea\">Region</span></p></td></tr></thead><tbody><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-c485afa8\"><p class=\"cl-c4857c22\"><span class=\"cl-c4856df4\">1,736</span></p></td><td class=\"cl-c485af8a\"><p class=\"cl-c4857c2c\"><span class=\"cl-c4856df4\">Science</span></p></td><td class=\"cl-c485af8a\"><p class=\"cl-c4857c2c\"><span class=\"cl-c4856df4\">albin</span></p></td><td class=\"cl-c485af94\"><p class=\"cl-c4857c22\"><span class=\"cl-c4856df4\">166.01</span></p></td><td class=\"cl-c485af9e\"><p class=\"cl-c4857c2c\"><span class=\"cl-c4856df4\">North</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-c485afb2\"><p class=\"cl-c4857c22\"><span class=\"cl-c4856df4\">1,711</span></p></td><td class=\"cl-c485afa9\"><p class=\"cl-c4857c2c\"><span class=\"cl-c4856df4\">Education</span></p></td><td class=\"cl-c485afa9\"><p class=\"cl-c4857c2c\"><span class=\"cl-c4856df4\">anon</span></p></td><td class=\"cl-c485afb3\"><p class=\"cl-c4857c22\"><span class=\"cl-c4856df4\">139.86</span></p></td><td class=\"cl-c485afaa\"><p class=\"cl-c4857c2c\"><span class=\"cl-c4856df4\">North</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-c485afa8\"><p class=\"cl-c4857c22\"><span class=\"cl-c4856df4\">1,808</span></p></td><td class=\"cl-c485af8a\"><p class=\"cl-c4857c2c\"><span class=\"cl-c4856df4\">PrivateLetter</span></p></td><td class=\"cl-c485af8a\"><p class=\"cl-c4857c2c\"><span class=\"cl-c4856df4\">austen</span></p></td><td class=\"cl-c485af94\"><p class=\"cl-c4857c22\"><span class=\"cl-c4856df4\">130.78</span></p></td><td class=\"cl-c485af9e\"><p class=\"cl-c4857c2c\"><span class=\"cl-c4856df4\">North</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-c485afb2\"><p class=\"cl-c4857c22\"><span class=\"cl-c4856df4\">1,878</span></p></td><td class=\"cl-c485afa9\"><p class=\"cl-c4857c2c\"><span class=\"cl-c4856df4\">Education</span></p></td><td class=\"cl-c485afa9\"><p class=\"cl-c4857c2c\"><span class=\"cl-c4856df4\">bain</span></p></td><td class=\"cl-c485afb3\"><p class=\"cl-c4857c22\"><span class=\"cl-c4856df4\">151.29</span></p></td><td class=\"cl-c485afaa\"><p class=\"cl-c4857c2c\"><span class=\"cl-c4856df4\">North</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-c485afa8\"><p class=\"cl-c4857c22\"><span class=\"cl-c4856df4\">1,743</span></p></td><td class=\"cl-c485af8a\"><p class=\"cl-c4857c2c\"><span class=\"cl-c4856df4\">Education</span></p></td><td class=\"cl-c485af8a\"><p class=\"cl-c4857c2c\"><span class=\"cl-c4856df4\">barclay</span></p></td><td class=\"cl-c485af94\"><p class=\"cl-c4857c22\"><span class=\"cl-c4856df4\">145.72</span></p></td><td class=\"cl-c485af9e\"><p class=\"cl-c4857c2c\"><span class=\"cl-c4856df4\">North</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-c485afb2\"><p class=\"cl-c4857c22\"><span class=\"cl-c4856df4\">1,908</span></p></td><td class=\"cl-c485afa9\"><p class=\"cl-c4857c2c\"><span class=\"cl-c4856df4\">Education</span></p></td><td class=\"cl-c485afa9\"><p class=\"cl-c4857c2c\"><span class=\"cl-c4856df4\">benson</span></p></td><td class=\"cl-c485afb3\"><p class=\"cl-c4857c22\"><span class=\"cl-c4856df4\">120.77</span></p></td><td class=\"cl-c485afaa\"><p class=\"cl-c4857c2c\"><span class=\"cl-c4856df4\">North</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-c485afa8\"><p class=\"cl-c4857c22\"><span class=\"cl-c4856df4\">1,906</span></p></td><td class=\"cl-c485af8a\"><p class=\"cl-c4857c2c\"><span class=\"cl-c4856df4\">Diary</span></p></td><td class=\"cl-c485af8a\"><p class=\"cl-c4857c2c\"><span class=\"cl-c4856df4\">benson</span></p></td><td class=\"cl-c485af94\"><p class=\"cl-c4857c22\"><span class=\"cl-c4856df4\">119.17</span></p></td><td class=\"cl-c485af9e\"><p class=\"cl-c4857c2c\"><span class=\"cl-c4856df4\">North</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-c485afb2\"><p class=\"cl-c4857c22\"><span class=\"cl-c4856df4\">1,897</span></p></td><td class=\"cl-c485afa9\"><p class=\"cl-c4857c2c\"><span class=\"cl-c4856df4\">Philosophy</span></p></td><td class=\"cl-c485afa9\"><p class=\"cl-c4857c2c\"><span class=\"cl-c4856df4\">boethja</span></p></td><td class=\"cl-c485afb3\"><p class=\"cl-c4857c22\"><span class=\"cl-c4856df4\">132.96</span></p></td><td class=\"cl-c485afaa\"><p class=\"cl-c4857c2c\"><span class=\"cl-c4856df4\">North</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-c485afa8\"><p class=\"cl-c4857c22\"><span class=\"cl-c4856df4\">1,785</span></p></td><td class=\"cl-c485af8a\"><p class=\"cl-c4857c2c\"><span class=\"cl-c4856df4\">Philosophy</span></p></td><td class=\"cl-c485af8a\"><p class=\"cl-c4857c2c\"><span class=\"cl-c4856df4\">boethri</span></p></td><td class=\"cl-c485af94\"><p class=\"cl-c4857c22\"><span class=\"cl-c4856df4\">130.49</span></p></td><td class=\"cl-c485af9e\"><p class=\"cl-c4857c2c\"><span class=\"cl-c4856df4\">North</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-c485afb2\"><p class=\"cl-c4857c22\"><span class=\"cl-c4856df4\">1,776</span></p></td><td class=\"cl-c485afa9\"><p class=\"cl-c4857c2c\"><span class=\"cl-c4856df4\">Diary</span></p></td><td class=\"cl-c485afa9\"><p class=\"cl-c4857c2c\"><span class=\"cl-c4856df4\">boswell</span></p></td><td class=\"cl-c485afb3\"><p class=\"cl-c4857c22\"><span class=\"cl-c4856df4\">135.94</span></p></td><td class=\"cl-c485afaa\"><p class=\"cl-c4857c2c\"><span class=\"cl-c4856df4\">North</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-c485afa8\"><p class=\"cl-c4857c22\"><span class=\"cl-c4856df4\">1,905</span></p></td><td class=\"cl-c485af8a\"><p class=\"cl-c4857c2c\"><span class=\"cl-c4856df4\">Travel</span></p></td><td class=\"cl-c485af8a\"><p class=\"cl-c4857c2c\"><span class=\"cl-c4856df4\">bradley</span></p></td><td class=\"cl-c485af94\"><p class=\"cl-c4857c22\"><span class=\"cl-c4856df4\">154.20</span></p></td><td class=\"cl-c485af9e\"><p class=\"cl-c4857c2c\"><span class=\"cl-c4856df4\">North</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-c485afb2\"><p class=\"cl-c4857c22\"><span class=\"cl-c4856df4\">1,711</span></p></td><td class=\"cl-c485afa9\"><p class=\"cl-c4857c2c\"><span class=\"cl-c4856df4\">Education</span></p></td><td class=\"cl-c485afa9\"><p class=\"cl-c4857c2c\"><span class=\"cl-c4856df4\">brightland</span></p></td><td class=\"cl-c485afb3\"><p class=\"cl-c4857c22\"><span class=\"cl-c4856df4\">149.14</span></p></td><td class=\"cl-c485afaa\"><p class=\"cl-c4857c2c\"><span class=\"cl-c4856df4\">North</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-c485afa8\"><p class=\"cl-c4857c22\"><span class=\"cl-c4856df4\">1,762</span></p></td><td class=\"cl-c485af8a\"><p class=\"cl-c4857c2c\"><span class=\"cl-c4856df4\">Sermon</span></p></td><td class=\"cl-c485af8a\"><p class=\"cl-c4857c2c\"><span class=\"cl-c4856df4\">burton</span></p></td><td class=\"cl-c485af94\"><p class=\"cl-c4857c22\"><span class=\"cl-c4856df4\">159.71</span></p></td><td class=\"cl-c485af9e\"><p class=\"cl-c4857c2c\"><span class=\"cl-c4856df4\">North</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-c485afb2\"><p class=\"cl-c4857c22\"><span class=\"cl-c4856df4\">1,726</span></p></td><td class=\"cl-c485afa9\"><p class=\"cl-c4857c2c\"><span class=\"cl-c4856df4\">Sermon</span></p></td><td class=\"cl-c485afa9\"><p class=\"cl-c4857c2c\"><span class=\"cl-c4856df4\">butler</span></p></td><td class=\"cl-c485afb3\"><p class=\"cl-c4857c22\"><span class=\"cl-c4856df4\">157.49</span></p></td><td class=\"cl-c485afaa\"><p class=\"cl-c4857c2c\"><span class=\"cl-c4856df4\">North</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-c485afb4\"><p class=\"cl-c4857c22\"><span class=\"cl-c4856df4\">1,835</span></p></td><td class=\"cl-c485afbc\"><p class=\"cl-c4857c2c\"><span class=\"cl-c4856df4\">PrivateLetter</span></p></td><td class=\"cl-c485afbc\"><p class=\"cl-c4857c2c\"><span class=\"cl-c4856df4\">carlyle</span></p></td><td class=\"cl-c485afc6\"><p class=\"cl-c4857c22\"><span class=\"cl-c4856df4\">124.16</span></p></td><td class=\"cl-c485afc7\"><p class=\"cl-c4857c2c\"><span class=\"cl-c4856df4\">North</span></p></td></tr></tbody></table></div>\n```\n:::\n:::\n\n\nThe data set contains the date when the text was written (*Date*), the genre of the text (*Genre*), the name of the text (*Text*), the relative frequency of prepositions in the text (*Prepositions*), and the region in which the text was written (*Region*). We now plot the data to get a first impression of its structure.\n\n\n::: {.cell}\n\n```{.r .cell-code}\np1 <- ggplot(lmmdata, aes(x = Date, y = Prepositions)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = F, color = \"red\", linetype = \"dashed\") +\n  theme_bw() +\n  labs(y = \"Frequency\\n(Prepositions)\")\np2 <- ggplot(lmmdata, aes(x = reorder(Genre, -Prepositions), y = Prepositions)) +\n  geom_boxplot() +\n  theme_bw() + \n  theme(axis.text.x = element_text(angle=90)) +\n  labs(x = \"Genre\", y = \"Frequency\\n(Prepositions)\")\np3 <- ggplot(lmmdata, aes(Prepositions)) +\n  geom_histogram() +\n  theme_bw() + \n  labs(y = \"Count\", x = \"Frequency (Prepositions)\")\ngrid.arrange(grobs = list(p1, p2, p3), widths = c(1, 1), layout_matrix = rbind(c(1, 1), c(2, 3)))\n```\n\n::: {.cell-output-display}\n![](regression_files/figure-html/lmm4-1.png){width=672}\n:::\n:::\n\n\n\n\n\nThe scatter plot in the upper panel indicates that the use of prepositions has moderately increased over time while the boxplots in the lower left panel show that the genres differ quite substantially with respect to their median frequencies of prepositions per text. Finally, the histogram in the lower right panel show that preposition use is distributed normally with a mean of 132.2 prepositions per text. \n\n\n::: {.cell}\n\n```{.r .cell-code}\np4 <- ggplot(lmmdata, aes(Date, Prepositions)) +\n  geom_point() +\n  labs(x = \"Year\", y = \"Prepositions per 1,000 words\") +\n  geom_smooth(method = \"lm\")  + \n  theme_bw()\np5 <- ggplot(lmmdata, aes(Region, Prepositions)) +\n  geom_boxplot() +\n  labs(x = \"Region\", y = \"Prepositions per 1,000 words\") +\n  geom_smooth(method = \"lm\")  + \n  theme_bw()\ngrid.arrange(p4, p5, nrow = 1)\n```\n\n::: {.cell-output-display}\n![](regression_files/figure-html/lmm5-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(lmmdata, aes(Date, Prepositions)) +\n  geom_point() +\n  facet_wrap(~ Genre, nrow = 4) +\n  geom_smooth(method = \"lm\") +\n  theme_bw() +\n  labs(x = \"Date of composition\", y = \"Prepositions per 1,000 words\") +\n  coord_cartesian(ylim = c(0, 220))\n```\n\n::: {.cell-output-display}\n![](regression_files/figure-html/lmm6-1.png){width=672}\n:::\n:::\n\n\nCentering or even scaling numeric variables is useful for later interpretation of regression models: if the date variable were not centered, the regression would show the effects of variables at year 0(!). If numeric variables are centered, other variables are variables are considered relative not to 0 but to the mean of that variable (in this case the mean of years in our data). Centering simply means that the mean of the numeric variable is subtracted from each value.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlmmdata$DateUnscaled <- lmmdata$Date\nlmmdata$Date <- scale(lmmdata$Date, scale = F)\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n```{=html}\n<div class=\"tabwid\"><style>.cl-c587cc24{table-layout:auto;width:75%;}.cl-c5831896{font-family:'DejaVu Sans';font-size:12pt;font-weight:bold;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-c58318a0{font-family:'DejaVu Sans';font-size:12pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-c583257a{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-c5832584{margin:0;text-align:center;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-c5835a36{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c5835a40{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c5835a4a{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c5835a4b{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c5835a54{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c5835a55{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c5835a5e{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c5835a68{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c5835a69{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c5835a72{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c5835a7c{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c5835a86{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c5835a87{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c5835a90{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c5835a9a{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c5835aa4{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}</style><table class='cl-c587cc24'>\n```\n<caption class=\"Table Caption\">\n\nFirst 15 rows of the lmmdata.\n\n</caption>\n```{=html}\n<thead><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-c5835a87\"><p class=\"cl-c583257a\"><span class=\"cl-c5831896\">Date</span></p></td><td class=\"cl-c5835a9a\"><p class=\"cl-c5832584\"><span class=\"cl-c5831896\">Genre</span></p></td><td class=\"cl-c5835a9a\"><p class=\"cl-c5832584\"><span class=\"cl-c5831896\">Text</span></p></td><td class=\"cl-c5835a90\"><p class=\"cl-c583257a\"><span class=\"cl-c5831896\">Prepositions</span></p></td><td class=\"cl-c5835a9a\"><p class=\"cl-c5832584\"><span class=\"cl-c5831896\">Region</span></p></td><td class=\"cl-c5835aa4\"><p class=\"cl-c583257a\"><span class=\"cl-c5831896\">DateUnscaled</span></p></td></tr></thead><tbody><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-c5835a40\"><p class=\"cl-c583257a\"><span class=\"cl-c58318a0\">109.8696461825</span></p></td><td class=\"cl-c5835a4a\"><p class=\"cl-c5832584\"><span class=\"cl-c58318a0\">Science</span></p></td><td class=\"cl-c5835a4a\"><p class=\"cl-c5832584\"><span class=\"cl-c58318a0\">albin</span></p></td><td class=\"cl-c5835a4b\"><p class=\"cl-c583257a\"><span class=\"cl-c58318a0\">166.01</span></p></td><td class=\"cl-c5835a4a\"><p class=\"cl-c5832584\"><span class=\"cl-c58318a0\">North</span></p></td><td class=\"cl-c5835a36\"><p class=\"cl-c583257a\"><span class=\"cl-c58318a0\">1,736</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-c5835a68\"><p class=\"cl-c583257a\"><span class=\"cl-c58318a0\">84.8696461825</span></p></td><td class=\"cl-c5835a5e\"><p class=\"cl-c5832584\"><span class=\"cl-c58318a0\">Education</span></p></td><td class=\"cl-c5835a5e\"><p class=\"cl-c5832584\"><span class=\"cl-c58318a0\">anon</span></p></td><td class=\"cl-c5835a54\"><p class=\"cl-c583257a\"><span class=\"cl-c58318a0\">139.86</span></p></td><td class=\"cl-c5835a5e\"><p class=\"cl-c5832584\"><span class=\"cl-c58318a0\">North</span></p></td><td class=\"cl-c5835a55\"><p class=\"cl-c583257a\"><span class=\"cl-c58318a0\">1,711</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-c5835a40\"><p class=\"cl-c583257a\"><span class=\"cl-c58318a0\">181.8696461825</span></p></td><td class=\"cl-c5835a4a\"><p class=\"cl-c5832584\"><span class=\"cl-c58318a0\">PrivateLetter</span></p></td><td class=\"cl-c5835a4a\"><p class=\"cl-c5832584\"><span class=\"cl-c58318a0\">austen</span></p></td><td class=\"cl-c5835a4b\"><p class=\"cl-c583257a\"><span class=\"cl-c58318a0\">130.78</span></p></td><td class=\"cl-c5835a4a\"><p class=\"cl-c5832584\"><span class=\"cl-c58318a0\">North</span></p></td><td class=\"cl-c5835a36\"><p class=\"cl-c583257a\"><span class=\"cl-c58318a0\">1,808</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-c5835a68\"><p class=\"cl-c583257a\"><span class=\"cl-c58318a0\">251.8696461825</span></p></td><td class=\"cl-c5835a5e\"><p class=\"cl-c5832584\"><span class=\"cl-c58318a0\">Education</span></p></td><td class=\"cl-c5835a5e\"><p class=\"cl-c5832584\"><span class=\"cl-c58318a0\">bain</span></p></td><td class=\"cl-c5835a54\"><p class=\"cl-c583257a\"><span class=\"cl-c58318a0\">151.29</span></p></td><td class=\"cl-c5835a5e\"><p class=\"cl-c5832584\"><span class=\"cl-c58318a0\">North</span></p></td><td class=\"cl-c5835a55\"><p class=\"cl-c583257a\"><span class=\"cl-c58318a0\">1,878</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-c5835a40\"><p class=\"cl-c583257a\"><span class=\"cl-c58318a0\">116.8696461825</span></p></td><td class=\"cl-c5835a4a\"><p class=\"cl-c5832584\"><span class=\"cl-c58318a0\">Education</span></p></td><td class=\"cl-c5835a4a\"><p class=\"cl-c5832584\"><span class=\"cl-c58318a0\">barclay</span></p></td><td class=\"cl-c5835a4b\"><p class=\"cl-c583257a\"><span class=\"cl-c58318a0\">145.72</span></p></td><td class=\"cl-c5835a4a\"><p class=\"cl-c5832584\"><span class=\"cl-c58318a0\">North</span></p></td><td class=\"cl-c5835a36\"><p class=\"cl-c583257a\"><span class=\"cl-c58318a0\">1,743</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-c5835a68\"><p class=\"cl-c583257a\"><span class=\"cl-c58318a0\">281.8696461825</span></p></td><td class=\"cl-c5835a5e\"><p class=\"cl-c5832584\"><span class=\"cl-c58318a0\">Education</span></p></td><td class=\"cl-c5835a5e\"><p class=\"cl-c5832584\"><span class=\"cl-c58318a0\">benson</span></p></td><td class=\"cl-c5835a54\"><p class=\"cl-c583257a\"><span class=\"cl-c58318a0\">120.77</span></p></td><td class=\"cl-c5835a5e\"><p class=\"cl-c5832584\"><span class=\"cl-c58318a0\">North</span></p></td><td class=\"cl-c5835a55\"><p class=\"cl-c583257a\"><span class=\"cl-c58318a0\">1,908</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-c5835a40\"><p class=\"cl-c583257a\"><span class=\"cl-c58318a0\">279.8696461825</span></p></td><td class=\"cl-c5835a4a\"><p class=\"cl-c5832584\"><span class=\"cl-c58318a0\">Diary</span></p></td><td class=\"cl-c5835a4a\"><p class=\"cl-c5832584\"><span class=\"cl-c58318a0\">benson</span></p></td><td class=\"cl-c5835a4b\"><p class=\"cl-c583257a\"><span class=\"cl-c58318a0\">119.17</span></p></td><td class=\"cl-c5835a4a\"><p class=\"cl-c5832584\"><span class=\"cl-c58318a0\">North</span></p></td><td class=\"cl-c5835a36\"><p class=\"cl-c583257a\"><span class=\"cl-c58318a0\">1,906</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-c5835a68\"><p class=\"cl-c583257a\"><span class=\"cl-c58318a0\">270.8696461825</span></p></td><td class=\"cl-c5835a5e\"><p class=\"cl-c5832584\"><span class=\"cl-c58318a0\">Philosophy</span></p></td><td class=\"cl-c5835a5e\"><p class=\"cl-c5832584\"><span class=\"cl-c58318a0\">boethja</span></p></td><td class=\"cl-c5835a54\"><p class=\"cl-c583257a\"><span class=\"cl-c58318a0\">132.96</span></p></td><td class=\"cl-c5835a5e\"><p class=\"cl-c5832584\"><span class=\"cl-c58318a0\">North</span></p></td><td class=\"cl-c5835a55\"><p class=\"cl-c583257a\"><span class=\"cl-c58318a0\">1,897</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-c5835a40\"><p class=\"cl-c583257a\"><span class=\"cl-c58318a0\">158.8696461825</span></p></td><td class=\"cl-c5835a4a\"><p class=\"cl-c5832584\"><span class=\"cl-c58318a0\">Philosophy</span></p></td><td class=\"cl-c5835a4a\"><p class=\"cl-c5832584\"><span class=\"cl-c58318a0\">boethri</span></p></td><td class=\"cl-c5835a4b\"><p class=\"cl-c583257a\"><span class=\"cl-c58318a0\">130.49</span></p></td><td class=\"cl-c5835a4a\"><p class=\"cl-c5832584\"><span class=\"cl-c58318a0\">North</span></p></td><td class=\"cl-c5835a36\"><p class=\"cl-c583257a\"><span class=\"cl-c58318a0\">1,785</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-c5835a68\"><p class=\"cl-c583257a\"><span class=\"cl-c58318a0\">149.8696461825</span></p></td><td class=\"cl-c5835a5e\"><p class=\"cl-c5832584\"><span class=\"cl-c58318a0\">Diary</span></p></td><td class=\"cl-c5835a5e\"><p class=\"cl-c5832584\"><span class=\"cl-c58318a0\">boswell</span></p></td><td class=\"cl-c5835a54\"><p class=\"cl-c583257a\"><span class=\"cl-c58318a0\">135.94</span></p></td><td class=\"cl-c5835a5e\"><p class=\"cl-c5832584\"><span class=\"cl-c58318a0\">North</span></p></td><td class=\"cl-c5835a55\"><p class=\"cl-c583257a\"><span class=\"cl-c58318a0\">1,776</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-c5835a40\"><p class=\"cl-c583257a\"><span class=\"cl-c58318a0\">278.8696461825</span></p></td><td class=\"cl-c5835a4a\"><p class=\"cl-c5832584\"><span class=\"cl-c58318a0\">Travel</span></p></td><td class=\"cl-c5835a4a\"><p class=\"cl-c5832584\"><span class=\"cl-c58318a0\">bradley</span></p></td><td class=\"cl-c5835a4b\"><p class=\"cl-c583257a\"><span class=\"cl-c58318a0\">154.20</span></p></td><td class=\"cl-c5835a4a\"><p class=\"cl-c5832584\"><span class=\"cl-c58318a0\">North</span></p></td><td class=\"cl-c5835a36\"><p class=\"cl-c583257a\"><span class=\"cl-c58318a0\">1,905</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-c5835a68\"><p class=\"cl-c583257a\"><span class=\"cl-c58318a0\">84.8696461825</span></p></td><td class=\"cl-c5835a5e\"><p class=\"cl-c5832584\"><span class=\"cl-c58318a0\">Education</span></p></td><td class=\"cl-c5835a5e\"><p class=\"cl-c5832584\"><span class=\"cl-c58318a0\">brightland</span></p></td><td class=\"cl-c5835a54\"><p class=\"cl-c583257a\"><span class=\"cl-c58318a0\">149.14</span></p></td><td class=\"cl-c5835a5e\"><p class=\"cl-c5832584\"><span class=\"cl-c58318a0\">North</span></p></td><td class=\"cl-c5835a55\"><p class=\"cl-c583257a\"><span class=\"cl-c58318a0\">1,711</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-c5835a40\"><p class=\"cl-c583257a\"><span class=\"cl-c58318a0\">135.8696461825</span></p></td><td class=\"cl-c5835a4a\"><p class=\"cl-c5832584\"><span class=\"cl-c58318a0\">Sermon</span></p></td><td class=\"cl-c5835a4a\"><p class=\"cl-c5832584\"><span class=\"cl-c58318a0\">burton</span></p></td><td class=\"cl-c5835a4b\"><p class=\"cl-c583257a\"><span class=\"cl-c58318a0\">159.71</span></p></td><td class=\"cl-c5835a4a\"><p class=\"cl-c5832584\"><span class=\"cl-c58318a0\">North</span></p></td><td class=\"cl-c5835a36\"><p class=\"cl-c583257a\"><span class=\"cl-c58318a0\">1,762</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-c5835a68\"><p class=\"cl-c583257a\"><span class=\"cl-c58318a0\">99.8696461825</span></p></td><td class=\"cl-c5835a5e\"><p class=\"cl-c5832584\"><span class=\"cl-c58318a0\">Sermon</span></p></td><td class=\"cl-c5835a5e\"><p class=\"cl-c5832584\"><span class=\"cl-c58318a0\">butler</span></p></td><td class=\"cl-c5835a54\"><p class=\"cl-c583257a\"><span class=\"cl-c58318a0\">157.49</span></p></td><td class=\"cl-c5835a5e\"><p class=\"cl-c5832584\"><span class=\"cl-c58318a0\">North</span></p></td><td class=\"cl-c5835a55\"><p class=\"cl-c583257a\"><span class=\"cl-c58318a0\">1,726</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-c5835a86\"><p class=\"cl-c583257a\"><span class=\"cl-c58318a0\">208.8696461825</span></p></td><td class=\"cl-c5835a7c\"><p class=\"cl-c5832584\"><span class=\"cl-c58318a0\">PrivateLetter</span></p></td><td class=\"cl-c5835a7c\"><p class=\"cl-c5832584\"><span class=\"cl-c58318a0\">carlyle</span></p></td><td class=\"cl-c5835a69\"><p class=\"cl-c583257a\"><span class=\"cl-c58318a0\">124.16</span></p></td><td class=\"cl-c5835a7c\"><p class=\"cl-c5832584\"><span class=\"cl-c58318a0\">North</span></p></td><td class=\"cl-c5835a72\"><p class=\"cl-c583257a\"><span class=\"cl-c58318a0\">1,835</span></p></td></tr></tbody></table></div>\n```\n:::\n:::\n\n\nWe now set up a fixed-effects model with the `glm` function and a mixed-effects model using the `glmer` function from the `lme4` package [@lme4] with Genre as a random effect.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# generate models\nm0.glm <- glm(Prepositions ~ 1, family = gaussian, data = lmmdata)\nm0.lmer = lmer(Prepositions ~ 1 + (1|Genre), REML = T, data = lmmdata)\n```\n:::\n\n\nNow that we have created the base-line models, we will test whether including a random effect structure is mathematically justified. It is important to note here that we are not going to test if including a random effect structure is theoretically motivated but simply if it causes a decrease in variance.\n\n\n### Testing Random Effects{-}\n\nAs a first step in the modeling process, we now need to determine whether or not including a random effect structure is justified. We do so by comparing the AIC of the base-line model without random intercepts to the AIC of the model with random intercepts. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nAIC(logLik(m0.glm))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 4718.19031114\n```\n:::\n\n```{.r .cell-code}\nAIC(logLik(m0.lmer))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 4497.77554693\n```\n:::\n:::\n\n\nThe inclusion of a random effect structure with random intercepts is justified as the AIC of the model with random intercepts is substantially lower than the AIC of the model without random intercepts. \n\nWhile I do not how how to *test* if including a random effect is justified, there are often situations, which require to test exactly which random effect structure is best. When doing this, it is important to use *restricted\nmaximum likelihood* (`REML = TRUE` or `method = REML`) rather than maximum likelihood [see @pinheiro2000mixedmodels; @winter2019statistics, 226].\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# generate models with 2 different random effect structures\nma.lmer = lmer(Prepositions ~ Date + (1|Genre), REML = T, data = lmmdata)\nmb.lmer = lmer(Prepositions ~ Date + (1 + Date | Genre), REML = T, data = lmmdata)\n# compare models\nanova(ma.lmer, mb.lmer, test = \"Chisq\", refit = F)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nData: lmmdata\nModels:\nma.lmer: Prepositions ~ Date + (1 | Genre)\nmb.lmer: Prepositions ~ Date + (1 + Date | Genre)\n        npar         AIC         BIC       logLik    deviance    Chisq Df\nma.lmer    4 4499.148092 4516.292084 -2245.574046 4491.148092            \nmb.lmer    6 4486.699509 4512.415498 -2237.349755 4474.699509 16.44858  2\n        Pr(>Chisq)    \nma.lmer               \nmb.lmer 0.00026806 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\nThe model comparison shows that the model with the more complex random effect structure has a significantly better fit to the data compared with the model with the simpler random effect structure. However, we will continue with the model with the simpler structure because this is just an example. \n\n\n\n<div class=\"warning\" style='padding:0.1em; background-color:#f2f2f2; color:#51247a'>\n<span>\n<p style='margin-top:1em; text-align:center'>\n<b>NOTE</b><br><br>In a real analysis, we would switch to a model with random intercepts and random slopes for Genre because it has a significantly better fit to the data.</p>\n<p style='margin-left:1em;'>\n</p></span>\n</div>\n\n<br>\n\n\n### Model Fitting{-}\n\nAfter having determined that including a random effect structure is justified, we can continue by fitting the model and including diagnostics as we go. Including diagnostics in the model fitting process can save time and prevent relying on models which only turn out to be unstable if we would perform the diagnostics after the fact.\n\nWe begin fitting our model by adding *Date* as a fixed effect and compare this model to our mixed-effects base-line model to see if *Date* improved the model fit by explaining variance and if *Date* significantly correlates with our dependent variable (this means that the difference between the models is the effect (size) of *Date*!)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm1.lmer <- lmer(Prepositions ~ (1|Genre) + Date, REML = T, data = lmmdata)\nanova(m1.lmer, m0.lmer, test = \"Chi\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nrefitting model(s) with ML (instead of REML)\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nData: lmmdata\nModels:\nm0.lmer: Prepositions ~ 1 + (1 | Genre)\nm1.lmer: Prepositions ~ (1 | Genre) + Date\n        npar         AIC         BIC       logLik    deviance  Chisq Df\nm0.lmer    3 4501.947337 4514.805331 -2247.973668 4495.947337          \nm1.lmer    4 4495.017736 4512.161728 -2243.508868 4487.017736 8.9296  1\n        Pr(>Chisq)   \nm0.lmer              \nm1.lmer  0.0028059 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stderr}\n```\nrefitting model(s) with ML (instead of REML)\n```\n:::\n:::\n\n\n\nThe model with *Date* is the better model (significant p-value and lower AIC). The significant p-value shows that *Date* correlates significantly with *Prepositions* ($\\chi$^2^(1): 8.929600937903, p  = 0.00281) . The $\\chi$^2^ value here is labeled *Chisq* and the degrees of freedom are calculated by subtracting the smaller number of DFs from the larger number of DFs.\n\nWe now test whether *Region* should also be part of the final minimal adequate model. The easiest way to add predictors is by using the `update` function (it saves time and typing).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# generate model\nm2.lmer <- update(m1.lmer, .~.+ Region)\n# test vifs\ncar::vif(m2.lmer)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n         Date        Region \n1.20287667936 1.20287667936 \n```\n:::\n\n```{.r .cell-code}\n# compare models                \nanova(m2.lmer, m1.lmer, test = \"Chi\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nrefitting model(s) with ML (instead of REML)\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nData: lmmdata\nModels:\nm1.lmer: Prepositions ~ (1 | Genre) + Date\nm2.lmer: Prepositions ~ (1 | Genre) + Date + Region\n        npar         AIC         BIC       logLik    deviance   Chisq Df\nm1.lmer    4 4495.017736 4512.161728 -2243.508868 4487.017736           \nm2.lmer    5 4494.624343 4516.054333 -2242.312171 4484.624343 2.39339  1\n        Pr(>Chisq)\nm1.lmer           \nm2.lmer    0.12185\n```\n:::\n:::\n\n\nThree things tell us that *Region* should not be included: \n\n1. the AIC does not decrease, \n\n2. the BIC increases(!), and \n\n3. the p-value is higher than .05. \n\nThis means, that we will continue fitting the model without having *Region* included. Well... not quite - just as a note on including variables: while *Region* is not significant as a main effect, it must still be included in a model if it were part of a significant interaction. To test if this is indeed the case, we fit another model with the interaction between *Date* and *Region* as predictor.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# generate model\nm3.lmer <- update(m1.lmer, .~.+ Region*Date)\n# extract vifs\ncar::vif(m3.lmer)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n         Date        Region   Date:Region \n1.96923042276 1.20324697637 1.78000887978 \n```\n:::\n\n```{.r .cell-code}\n# compare models                \nanova(m3.lmer, m1.lmer, test = \"Chi\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nrefitting model(s) with ML (instead of REML)\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nData: lmmdata\nModels:\nm1.lmer: Prepositions ~ (1 | Genre) + Date\nm3.lmer: Prepositions ~ (1 | Genre) + Date + Region + Date:Region\n        npar         AIC         BIC       logLik    deviance   Chisq Df\nm1.lmer    4 4495.017736 4512.161728 -2243.508868 4487.017736           \nm3.lmer    6 4496.124872 4521.840861 -2242.062436 4484.124872 2.89286  2\n        Pr(>Chisq)\nm1.lmer           \nm3.lmer    0.23541\n```\n:::\n:::\n\n\nAgain, the high p-value and the increase in AIC and BIC show that we have found our minimal adequate model with only contains *Date* as a main effect. In a next step, we can inspect the final minimal adequate model, i.e. the most parsimonious (the model that explains a maximum of variance with a minimum of predictors).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# inspect results\nsummary(m1.lmer)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear mixed model fit by REML ['lmerMod']\nFormula: Prepositions ~ (1 | Genre) + Date\n   Data: lmmdata\n\nREML criterion at convergence: 4491.1\n\nScaled residuals: \n         Min           1Q       Median           3Q          Max \n-3.734915441 -0.657038004  0.005865025  0.661298615  3.596659863 \n\nRandom effects:\n Groups   Name        Variance   Std.Dev.  \n Genre    (Intercept) 159.021120 12.6103576\n Residual             228.764179 15.1249522\nNumber of obs: 537, groups:  Genre, 16\n\nFixed effects:\n                   Estimate      Std. Error  t value\n(Intercept) 133.88516211469   3.24749296248 41.22724\nDate          0.01894493515   0.00632363682  2.99589\n\nCorrelation of Fixed Effects:\n     (Intr)\nDate 0.005 \n```\n:::\n:::\n\n\n### Model Diagnostics{-}\n\nWe can now evaluate the goodness of fit of the model and check if mathematical requirements and assumptions have been violated. In a first step, we generate diagnostic plots that focus on the random effect structure.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(m1.lmer, Genre ~ resid(.), abline = 0 ) # generate diagnostic plots\n```\n\n::: {.cell-output-display}\n![](regression_files/figure-html/lmm14-1.png){width=672}\n:::\n:::\n\n\nThe plot shows that there are some outliers (points outside the boxes) and that the variability within letters is greater than in other genres we therefore examine the genres in isolation standardized residuals versus fitted values [@pinheiro2000mixedmodels 175].\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(m1.lmer, resid(., type = \"pearson\") ~ fitted(.) | Genre, id = 0.05, \n     adj = -0.3, pch = 20, col = \"gray40\")\n```\n\n::: {.cell-output-display}\n![](regression_files/figure-html/lmm15-1.png){width=672}\n:::\n:::\n\n\nThe plot shows the standardized residuals (or Pearson's residuals) versus fitted values and suggests that there are outliers in the data (the names elements in the plots). To check if these outliers are a cause for concern, we will now use a Levene's test to check if the variance is distributed homogeneously (homoscedasticity) or whether the assumption of variance homogeneity is violated (due to the outliers).\n\n<div class=\"warning\" style='padding:0.1em; background-color:#f2f2f2; color:#51247a'>\n<span>\n<p style='margin-top:1em; text-align:center'>\n<b>NOTE</b><br><br>The use of Levene's test to check if the model is heteroscedastic is generally not recommended as it is too lax when dealing with few observations (because in such cases it does not have the power to identify heteroscedasticity) while it is too harsh when dealing with many observations (when heteroscedasticity typically is not a severe problem).</p>\n<p style='margin-left:1em;'>\n</p></span>\n</div>\n\n\n<br>\n\nWe use Levene's test here merely to check if it substantiates the impressions we got from the visual inspection. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# check homogeneity\nleveneTest(lmmdata$Prepositions, lmmdata$Genre, center = mean)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in leveneTest.default(lmmdata$Prepositions, lmmdata$Genre, center =\nmean): lmmdata$Genre coerced to factor.\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nLevene's Test for Homogeneity of Variance (center = mean)\n       Df F value   Pr(>F)  \ngroup  15 1.74289 0.039906 *\n      521                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\nThe Levene's test shows that the variance is distributed unevenly across genres which means that we do not simply continue but should either remove problematic data points (outliers) or use a weighing method. \n\nIn this case, we create a new model which uses weights to compensate for heterogeneity of variance and thus the influence of outliers - which is an alternative to removing the data points and rerunning the analysis [@pinheiro2000mixedmodels 177]. However, to do so, we need to use a different function (the `lme` function) which means that we have to create two models: the *old* minimal adequate model and the *new* minimal adequate model with added weights. After we have created these models, we will compare them to see if including weights has improved the fit.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# generate models\nm4.lme = lme(Prepositions ~ Date, random = ~1|Genre, data = lmmdata, method = \"ML\")\nm5.lme <- update(m4.lme, weights = varIdent(form = ~ 1 | Genre))\n# compare models\nanova(m5.lme, m4.lme)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       Model df           AIC           BIC         logLik   Test       L.Ratio\nm5.lme     1 19 4485.84955689 4567.28352069 -2223.92477845                     \nm4.lme     2  4 4495.01773596 4512.16172834 -2243.50886798 1 vs 2 39.1681790667\n       p-value\nm5.lme        \nm4.lme  0.0006\n```\n:::\n:::\n\n\nThe weight model (m5.lme) that uses weights to account for unequal variance is performing significantly better than the model without weights (m4.lme) and we therefore switch to the weight model and inspect its parameters.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# inspect results\nsummary(m5.lme)        \n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear mixed-effects model fit by maximum likelihood\n  Data: lmmdata \n            AIC           BIC         logLik\n  4485.84955689 4567.28352069 -2223.92477845\n\nRandom effects:\n Formula: ~1 | Genre\n          (Intercept)      Residual\nStdDev: 12.2631808124 14.3420265787\n\nVariance function:\n Structure: Different standard deviations per stratum\n Formula: ~1 | Genre \n Parameter estimates:\n          Bible       Biography           Diary       Education         Fiction \n 1.000000000000  0.340719922131  0.869529591941  0.788861415446  0.911718858006 \n       Handbook         History             Law      Philosophy   PrivateLetter \n 1.096586432734  0.978750069335  0.784977716011  0.736988751895  1.190652740883 \n   PublicLetter        Religion         Science          Sermon          Travel \n 1.218929184394  0.974646323962  0.848611735175  0.970873365315  1.086235097365 \nTrialProceeding \n 1.260207123026 \nFixed effects:  Prepositions ~ Date \n                     Value      Std.Error  DF       t-value p-value\n(Intercept) 133.9640139919 3.144348306618 520 42.6046992663  0.0000\nDate          0.0217418124 0.005454714153 520  3.9858756695  0.0001\n Correlation: \n     (Intr)\nDate 0.004 \n\nStandardized Within-Group Residuals:\n             Min               Q1              Med               Q3 \n-3.3190657058362 -0.6797224358638  0.0146860053179  0.6987149873619 \n             Max \n 3.1039114834876 \n\nNumber of Observations: 537\nNumber of Groups: 16 \n```\n:::\n:::\n\n\nWe can also use an ANOVA display which is more to the point. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nanova(m5.lme)          \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            numDF denDF        F-value p-value\n(Intercept)     1   520 1813.907203146  <.0001\nDate            1   520   15.887204853  0.0001\n```\n:::\n:::\n\n\nAs we did before, we now check, whether the final minimal model (with weights) outperforms an intercept-only base-line model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# generate base-line model\nm0.lme = lme(Prepositions ~ 1, random = ~1|Genre, data = lmmdata, method = \"ML\", weights = varIdent(form = ~ 1 | Genre))\nanova(m0.lme, m5.lme)  # test if date is significant\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       Model df           AIC           BIC         logLik   Test       L.Ratio\nm0.lme     1 18 4496.28563020 4573.43359590 -2230.14281510                     \nm5.lme     2 19 4485.84955689 4567.28352069 -2223.92477845 1 vs 2 12.4360733078\n       p-value\nm0.lme        \nm5.lme  0.0004\n```\n:::\n:::\n\n\nOur final minimal adequate model with weights performs significantly better than an intercept only base-line model. Before doing the final diagnostics, we well inspect the estimates for the random effect structure to check if there are values which require further inspection (e.g. because they are drastically different from all other values).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# extract estimates and sd for fixed and random effects\nintervals(m5.lme, which=\"fixed\")      \n```\n\n::: {.cell-output .cell-output-stdout}\n```\nApproximate 95% confidence intervals\n\n Fixed effects:\n                        lower              est.             upper\n(Intercept) 127.7983408437279 133.9640139919199 140.1296871401120\nDate          0.0110458012653   0.0217418124276   0.0324378235899\n```\n:::\n:::\n\n\nThe random effect estimates do not show any outliers or drastically increased or decreased values which means that the random effect structure is fine.\n\n### Effect Sizes{-}\n\nWe will now extract effect sizes (in the example: the effect size of *Date*) and calculate normalized effect size measures (this effect size measure works for all fixed effects). When you have factorial design, you can take the square root of the squared t-value divided by the t-value squared plus the degrees of freedom to calculate the effect size: \n\n\\begin{equation}\n\nr = \\sqrt{ \\frac{ t^2}{(t^2 + df) } } = \\sqrt{ \\frac{ 3.99^2}{(3.99^2 + 520) } } = 0.172\n\n\\end{equation}\n\n<br>\n\n<div class=\"warning\" style='padding:0.1em; background-color:#f2f2f2; color:#51247a'>\n<span>\n<p style='margin-top:1em; text-align:center'>\n<b>NOTE</b><br><br>Two words of warning though: <br>br>1. In our case, the effect we are interested in is not factorial but continuous which means that we should not use this effect size measure. We only show this here as an example for how you can calculate the effect size measure r.<br><br>2. Only apply this function to main effects that are not involved in interactions as they are meaningless because the amount of variance explained by main effects involved in interactions is unclear [@field2012discovering 641].</p>\n<p style='margin-left:1em;'>\n</p></span>\n</div>\n\n\n<br>\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsjPlot::tab_model(m5.lme)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table style=\"border-collapse:collapse; border:none;\">\n<tr>\n<th style=\"border-top: double; text-align:center; font-style:normal; font-weight:bold; padding:0.2cm;  text-align:left; \">&nbsp;</th>\n<th colspan=\"3\" style=\"border-top: double; text-align:center; font-style:normal; font-weight:bold; padding:0.2cm; \">Prepositions</th>\n</tr>\n<tr>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  text-align:left; \">Predictors</td>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  \">Estimates</td>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  \">CI</td>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  \">p</td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; \">(Intercept)</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">133.96</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">127.80&nbsp;&ndash;&nbsp;140.13</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"><strong>&lt;0.001</strong></td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; \">Date</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.02</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.01&nbsp;&ndash;&nbsp;0.03</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"><strong>&lt;0.001</strong></td>\n</tr>\n<tr>\n<td colspan=\"4\" style=\"font-weight:bold; text-align:left; padding-top:.8em;\">Random Effects</td>\n</tr>\n\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; padding-top:0.1cm; padding-bottom:0.1cm;\">&sigma;<sup>2</sup></td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left;\" colspan=\"3\">205.69</td>\n</tr>\n\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; padding-top:0.1cm; padding-bottom:0.1cm;\">&tau;<sub>00</sub> <sub>Genre</sub></td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left;\" colspan=\"3\">150.39</td>\n\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; padding-top:0.1cm; padding-bottom:0.1cm;\">N <sub>Genre</sub></td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left;\" colspan=\"3\">16</td>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; padding-top:0.1cm; padding-bottom:0.1cm; border-top:1px solid;\">Observations</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left; border-top:1px solid;\" colspan=\"3\">537</td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; padding-top:0.1cm; padding-bottom:0.1cm;\">Marginal R<sup>2</sup> / Conditional R<sup>2</sup></td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left;\" colspan=\"3\">0.030 / NA</td>\n</tr>\n\n</table>\n\n`````\n:::\n:::\n\n\n***\n\n<br>\n\n<div class=\"warning\" style='padding:0.1em; background-color:#f2f2f2; color:#51247a'>\n<span>\n<p style='margin-top:1em; text-align:center'>\n<b>NOTE</b><br><br>The R^2^ values of the summary table are incorrect (as indicated by the missing conditional R^2^ value). The more appropriate conditional and marginal coefficient of determination for generalized mixed-effect models can be extracted using the `r.squaredGLMM` function from the `MuMIn` package [@MuMIn].</p>\n<p style='margin-left:1em;'>\n</p></span>\n</div>\n\n\n<br>\n\nThe *marginal R^2^* (marginal coefficient of determination) represents the variance explained by the fixed effects while the *conditional R^2^* is interpreted as a variance explained by the entire model, including both fixed and random effects [@barton2020mumin].\n\nThe respective call for the model is:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# extract R2s\nr.squaredGLMM(m1.lmer)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                 R2m            R2c\n[1,] 0.0121971160211 0.417270545308\n```\n:::\n:::\n\n\nThe effects can be visualized using the `plot_model` function from the `sjPlot` package [@sjPlot].\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsjPlot::plot_model(m5.lme, type = \"pred\", terms = c(\"Date\")) +\n  # show uncentered date rather than centered date\n  scale_x_continuous(name = \"Date\", \n                     breaks = seq(-500, 300, 100), \n                     labels = seq(1150, 1950, 100))\n```\n\n::: {.cell-output-display}\n![](regression_files/figure-html/lmm21d-1.png){width=672}\n:::\n:::\n\n\nWhile we have already shown that the effect of *Date* is significant, it is small which means that the number of prepositions per text does not correlate very strongly with time. This suggests that other factors that are not included in the model also impact the frequency of prepositions (and probably more meaningfully, too).\n\nBefore turning to the diagnostics, we will use the fitted (or predicted) and the observed values with a regression line for the predicted values. This will not only show how good the model fit the data but also the direction and magnitude of the effect.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# extract predicted values\nlmmdata$Predicted <- predict(m5.lme, lmmdata)\n# plot predicted values\nggplot(lmmdata, aes(DateUnscaled, Predicted)) +\n  facet_wrap(~Genre) +\n  geom_point(aes(x = DateUnscaled, y = Prepositions), color = \"gray80\", size = .5) +\n  geom_smooth(aes(y = Predicted), color = \"gray20\", linetype = \"solid\", \n              se = T, method = \"lm\") +\n  guides(color=guide_legend(override.aes=list(fill=NA))) +  \n  theme_set(theme_bw(base_size = 10)) +\n  theme(legend.position=\"top\", legend.title = element_blank(),\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank()) + \n  xlab(\"Date of composition\")\n```\n\n::: {.cell-output-display}\n![](regression_files/figure-html/lmm21b-1.png){width=672}\n:::\n:::\n\n\n### Model Diagnostics{-}\n\nWe now create diagnostic plots. What we wish to see in the diagnostic plots is a cloud of dots in the middle of the window without any structure. What we do not want to see is a funnel-shaped cloud because this indicates an increase of the errors/residuals with an increase of the predictor(s) (because this would indicate heteroscedasticity) [@pinheiro2000mixedmodels 182].\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# start plotting\npar(mfrow = c(2, 2))           # display plots in 2 rows and 2 columns\nplot(m5.lme, pch = 20, col = \"black\", lty = \"dotted\"); par(mfrow = c(1, 1))\n```\n\n::: {.cell-output-display}\n![](regression_files/figure-html/lmm22-1.png){width=672}\n:::\n:::\n\n\nWhat a wonderful unstructured cloud - the lack of structure tells us that the model is \"healthy\" and does not suffer from heteroscedasticity. We will now create more diagnostic plots to find potential problems [@pinheiro2000mixedmodels 21].\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# fitted values by Genre\nplot(m5.lme, form = resid(., type = \"p\") ~ fitted(.) | Genre, abline = 0, \n     cex = .5, pch = 20, col = \"black\")\n```\n\n::: {.cell-output-display}\n![](regression_files/figure-html/lmm23-1.png){width=672}\n:::\n:::\n\n\nIn contrast to the unweight model, no data points are named which indicates that the outliers do no longer have unwarranted influence on the model. Now, we check the residuals of fitted values against observed values [@pinheiro2000mixedmodels 179]. What we would like to see is a straight, upwards going line.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# residuals of fitted values against observed\nqqnorm(m5.lme, pch = 20, col = \"black\")\n```\n\n::: {.cell-output-display}\n![](regression_files/figure-html/lmm24-1.png){width=672}\n:::\n:::\n\n\nA beautiful, straight line! The qqplot does not indicate any problems. It is, unfortunately, rather common that the dots deviate from the straight line at the very bottom or the very top which means that the model is good at estimating values around the middle of the dependent variable but rather bad at estimating lower or higher values. Next, we check the residuals by \"Genre\" [@pinheiro2000mixedmodels 179].\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# residuals by genre\nqqnorm(m5.lme, ~resid(.) | Genre, pch = 20, col = \"black\" )\n```\n\n::: {.cell-output-display}\n![](regression_files/figure-html/lmm25-1.png){width=672}\n:::\n:::\n\n\nBeautiful straight lines - perfection! Now, we inspect the observed responses versus the within-group fitted values [@pinheiro2000mixedmodels 178].\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# observed responses versus the within-group fitted values\nplot(m5.lme, Prepositions ~ fitted(.), id = 0.05, adj = -0.3, \n     xlim = c(80, 220), cex = .8, pch = 20, col = \"blue\")\n```\n\n::: {.cell-output-display}\n![](regression_files/figure-html/lmm26-1.png){width=672}\n:::\n:::\n\n\nAlthough some data points are named, the plot does not show any structure, like a funnel, which would have been problematic. \n\n### Reporting Results {-}\n\nBefore we do the write-up, we have a look at the model summary as this will provide us with at least some of the parameters that we want to report. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(m5.lme)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear mixed-effects model fit by maximum likelihood\n  Data: lmmdata \n            AIC           BIC         logLik\n  4485.84955689 4567.28352069 -2223.92477845\n\nRandom effects:\n Formula: ~1 | Genre\n          (Intercept)      Residual\nStdDev: 12.2631808124 14.3420265787\n\nVariance function:\n Structure: Different standard deviations per stratum\n Formula: ~1 | Genre \n Parameter estimates:\n          Bible       Biography           Diary       Education         Fiction \n 1.000000000000  0.340719922131  0.869529591941  0.788861415446  0.911718858006 \n       Handbook         History             Law      Philosophy   PrivateLetter \n 1.096586432734  0.978750069335  0.784977716011  0.736988751895  1.190652740883 \n   PublicLetter        Religion         Science          Sermon          Travel \n 1.218929184394  0.974646323962  0.848611735175  0.970873365315  1.086235097365 \nTrialProceeding \n 1.260207123026 \nFixed effects:  Prepositions ~ Date \n                     Value      Std.Error  DF       t-value p-value\n(Intercept) 133.9640139919 3.144348306618 520 42.6046992663  0.0000\nDate          0.0217418124 0.005454714153 520  3.9858756695  0.0001\n Correlation: \n     (Intr)\nDate 0.004 \n\nStandardized Within-Group Residuals:\n             Min               Q1              Med               Q3 \n-3.3190657058362 -0.6797224358638  0.0146860053179  0.6987149873619 \n             Max \n 3.1039114834876 \n\nNumber of Observations: 537\nNumber of Groups: 16 \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsjPlot::tab_model(m5.lme)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table style=\"border-collapse:collapse; border:none;\">\n<tr>\n<th style=\"border-top: double; text-align:center; font-style:normal; font-weight:bold; padding:0.2cm;  text-align:left; \">&nbsp;</th>\n<th colspan=\"3\" style=\"border-top: double; text-align:center; font-style:normal; font-weight:bold; padding:0.2cm; \">Prepositions</th>\n</tr>\n<tr>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  text-align:left; \">Predictors</td>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  \">Estimates</td>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  \">CI</td>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  \">p</td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; \">(Intercept)</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">133.96</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">127.80&nbsp;&ndash;&nbsp;140.13</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"><strong>&lt;0.001</strong></td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; \">Date</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.02</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.01&nbsp;&ndash;&nbsp;0.03</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"><strong>&lt;0.001</strong></td>\n</tr>\n<tr>\n<td colspan=\"4\" style=\"font-weight:bold; text-align:left; padding-top:.8em;\">Random Effects</td>\n</tr>\n\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; padding-top:0.1cm; padding-bottom:0.1cm;\">&sigma;<sup>2</sup></td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left;\" colspan=\"3\">205.69</td>\n</tr>\n\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; padding-top:0.1cm; padding-bottom:0.1cm;\">&tau;<sub>00</sub> <sub>Genre</sub></td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left;\" colspan=\"3\">150.39</td>\n\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; padding-top:0.1cm; padding-bottom:0.1cm;\">N <sub>Genre</sub></td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left;\" colspan=\"3\">16</td>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; padding-top:0.1cm; padding-bottom:0.1cm; border-top:1px solid;\">Observations</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left; border-top:1px solid;\" colspan=\"3\">537</td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; padding-top:0.1cm; padding-bottom:0.1cm;\">Marginal R<sup>2</sup> / Conditional R<sup>2</sup></td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left;\" colspan=\"3\">0.030 / NA</td>\n</tr>\n\n</table>\n\n`````\n:::\n:::\n\n\n***\n\n<br>\n\n<div class=\"warning\" style='padding:0.1em; background-color:#f2f2f2; color:#51247a'>\n<span>\n<p style='margin-top:1em; text-align:center'>\n<b>NOTE</b><br><br>The R^2^ values of the summary table are incorrect (as indicated by the missing conditional R^2^ value). The more appropriate conditional and marginal coefficient of determination for generalized mixed-effect models can be extracted using the `r.squaredGLMM` function from the `MuMIn` package [@MuMIn].</p>\n<p style='margin-left:1em;'>\n</p></span>\n</div>\n\n<br>\n\n\nThe respective call for the model is:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nr.squaredGLMM(m5.lme)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                 R2m            R2c\n[1,] 0.0174025274607 0.432390148426\n```\n:::\n:::\n\n\n\nWe can use the `reports` package [@report] to summarize the analysis.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nreport::report(m5.lme)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRandom effect variances not available. Returned R2 does not account for random effects.\nRandom effect variances not available. Returned R2 does not account for random effects.\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nWe fitted a linear mixed model (estimated using ML and nlminb optimizer) to predict Prepositions with Date (formula: Prepositions ~ Date). The model included Genre as random effect (formula: ~1 | Genre). The model's explanatory power related to the fixed effects alone (marginal R2) is 0.03. The model's intercept, corresponding to Date = 0, is at 133.96 (95% CI [127.80, 140.13], t(520) = 42.60, p < .001). Within this model:\n\n  - The effect of Date is statistically significant and positive (beta = 0.02, 95% CI [0.01, 0.03], t(520) = 3.99, p < .001; Std. beta = 0.13, 95% CI [0.07, 0.19])\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using \n```\n:::\n:::\n\n\nWe can use this output to write up a final report: \n\nA mixed-effect linear regression model which contained the genre of texts as random effect was fit to the data in a step-wise-step up procedure. Due to the presence of outliers in the data, weights were included into the model which led to a significantly improved model fit compared to an un-weight model ($\\chi$^2^(2): 39.17, p: 0.0006). The final minimal adequate model performed significantly better than an intercept-only base-line model ($\\chi$^2^(1): 12.44, p =.0004) and showed that the frequency of prepositions increases significantly but only marginally with the date of composition (Estimate: 0.02, CI: 0.01-0.03, p < .001, marginal R^2^ =  0.0174, conditional R^2^ =  0.4324). Neither the region where the text was composed nor a higher order interaction between genre and region significantly correlated with the use of prepositions in the data. \n\n### Remarks on Prediction{-}\n\nWhile the number of intercepts, the model reports, and the way how mixed- and fixed-effects arrive at predictions differ, their predictions are extremely similar and almost identical (at least when dealing with a simple random effect structure). Consider the following example where we create analogous fixed and mixed effect models and plot their predicted frequencies of prepositions per genre across the un-centered date of composition. The predictions of the mixed-effects model are plotted as a solid red line, while the predictions of the fixed-effects model are plotted as dashed blue lines.  \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# create lm model\nm5.lmeunweight <- lm(Prepositions ~ DateUnscaled + Genre, data = lmmdata)\nlmmdata$lmePredictions <- fitted(m5.lmeunweight, lmmdata)\nm5.lm <- lm(Prepositions ~ DateUnscaled + Genre, data = lmmdata)\nlmmdata$lmPredictions <- fitted(m5.lm, lmmdata)\n# plot predictions\nggplot(lmmdata, aes(x = DateUnscaled, y = lmePredictions, group = Genre)) +\n  geom_line(aes(y = lmmdata$lmePredictions), linetype = \"solid\", color = \"red\") +\n  geom_line(aes(y = lmmdata$lmPredictions), linetype = \"dashed\", color = \"blue\") +\n  facet_wrap(~ Genre, nrow = 4) +\n  theme_bw() +\n  labs(x = \"Date of composition\") +\n  labs(y = \"Prepositions per 1,000 words\") +\n  coord_cartesian(ylim = c(0, 220))\n```\n\n::: {.cell-output-display}\n![](regression_files/figure-html/lmm29-1.png){width=672}\n:::\n:::\n\n\nThe predictions overlap almost perfectly which means that the predictions of both are almost identical - irrespective of whether genre is part of the mixed or the fixed effects structure. \n\n## Mixed-Effects Binomial Logistic Regression{-}\n\nWe now turn to an extension of binomial logistic regression: mixed-effects binomial logistic regression. As is the case with linear mixed-effects models logistic mixed effects models have the following advantages over simpler statistical tests: \n\n* Mixed-effects models are multivariate, i.e. they test the effect of several predictors simultaneously while controlling for the effect of all other predictors. \n\n* Mixed models allow to statistically incorporate within-speaker variability and are thus fit to model hierarchical or nested data structures. This applies if several observations are produced by an individual speaker, for instance.\n\n* Mixed-models provide a wealth of diagnostic statistics which enables us to control e.g. multicollinearity, i.e. correlations between predictors, and to test whether conditions or requirements are violated (e.g. homogeneity of variance, etc.). \n\nMajor disadvantages of regression modeling are that they are prone to producing high $\\beta$-errors [see @johnson2009getting] and that they require rather large data sets. \n\n### Introduction {-}\n\nAs is the case with linear mixed-effects models, binomial logistic mixed-effect models are multivariate analyses that treat data points as hierarchical or grouped in some way. In other words, they take into account that the data is nested in the sense that data points are produced by the same speaker or are grouped by some other characteristics. In mixed-models, hierarchical structures are modelled as *random effects*. If the random effect structure represents speakers then this means that a mixed-model would have a separate intercept and/or slope for each speaker. \n\n*Random Effects* in linear models can be visualized using two parameters: the intercept (the point where the regression line crosses the y-axis at x = 0) and the slope (the acclivity of the regression line). In contrast to linear mixed-effects models, random effects differ in the position and the slope of the logistic function that is applied to the likelihood of the dependent variable.  *random intercepts* (center left panel \\ref{fig:mem02}) or various *random slopes* (center right panel \\ref{fig:mem02}), or both, various *random intercepts* and various *random slopes* (right panel \\ref{fig:mem02}). In the following, we will only focus on models with random intercepts because this is the by far more common method and because including both random intercepts and random slopes requires huge amounts of data. Consider the Figure below to understand what is meant by \"random intercepts\".\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](regression_files/figure-html/blmm1-1.png){width=672}\n:::\n:::\n\n\n\nThe upper left panel merely shows the logistic curve representing the predictions of a fixed-effects logistic regression with a single intercept and slope. The upper right panel shows the logistic curves representing the predictions of a of a mixed-effects logistic regression with random intercepts for each level of a grouping variable. The lower left panel shows the logistic curves representing the predictions of a mixed-effects logistic regression with one intercept but random slopes for each level of a grouping variable. The lower right panel shows the logistic curves representing the predictions of a mixed-effects logistic regression with random intercepts and random slopes for each level of a grouping variable.\n\nAfter adding random intercepts, predictors (or fixed effects) are added to the model (just like with multiple regression). So mixed-effects are called mixed-effects because they contain both random and fixed effects.\n\nIn terms of general procedure, random effects are added first, and only after we have ascertained that including random effects is warranted, we test whether including fixed-effects is warranted [@field2012discovering]. We test whether including random effects is warranted by comparing a model, that bases its estimates of the dependent variable solely on the base intercept, with a model that bases its estimates of the dependent variable solely on the intercepts of the random effect. If the mixed-effects model explains significantly more variance than the fixed-effects model without random effect structure, then we continue with the mixed-effects model. In other words, including random effects is justified.\n\n### Example: Discourse LIKE in Irish English{-}\n\nIn this example we will investigate which factors correlate with the use of *final discourse like* (e.g. \"*The weather is shite, like!*\") in Irish English. The data set represents speech units in a corpus that were coded for the speaker who uttered a given speech unit, the gender (Gender: Men versus Women) and age of that speaker (Age: Old versus Young), whether the interlocutors were of the same or a different gender (ConversationType: SameGender  versus MixedGender), and whether another *final discourse like* had been used up to three speech units before (Priming: NoPrime versus Prime), whether or not the speech unit contained an *final discourse like* (SUFLike: 1 = yes, 0 = no. To begin with, we load the data and inspect the structure of the data set,\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# load data\nmblrdata  <- base::readRDS(url(\"https://slcladal.github.io/data/mbd.rda\", \"rb\"))\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n```{=html}\n<div class=\"tabwid\"><style>.cl-ca291116{table-layout:auto;width:75%;}.cl-ca24b6ac{font-family:'DejaVu Sans';font-size:12pt;font-weight:bold;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-ca24b6b6{font-family:'DejaVu Sans';font-size:12pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-ca24c656{margin:0;text-align:center;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-ca24c660{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-ca251fa2{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ca251fb6{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ca251fc0{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ca251fca{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ca251fcb{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ca251fde{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ca251fdf{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ca251fe8{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ca251ff2{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ca252006{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ca252010{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ca252011{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}</style><table class='cl-ca291116'>\n```\n<caption class=\"Table Caption\">\n\nFirst 15 rows of the mblrdata.\n\n</caption>\n```{=html}\n<thead><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-ca252010\"><p class=\"cl-ca24c656\"><span class=\"cl-ca24b6ac\">ID</span></p></td><td class=\"cl-ca252006\"><p class=\"cl-ca24c656\"><span class=\"cl-ca24b6ac\">Gender</span></p></td><td class=\"cl-ca252006\"><p class=\"cl-ca24c656\"><span class=\"cl-ca24b6ac\">Age</span></p></td><td class=\"cl-ca252006\"><p class=\"cl-ca24c656\"><span class=\"cl-ca24b6ac\">ConversationType</span></p></td><td class=\"cl-ca252006\"><p class=\"cl-ca24c656\"><span class=\"cl-ca24b6ac\">Priming</span></p></td><td class=\"cl-ca252011\"><p class=\"cl-ca24c660\"><span class=\"cl-ca24b6ac\">SUFlike</span></p></td></tr></thead><tbody><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-ca251fb6\"><p class=\"cl-ca24c656\"><span class=\"cl-ca24b6b6\">S1A-061$C</span></p></td><td class=\"cl-ca251fc0\"><p class=\"cl-ca24c656\"><span class=\"cl-ca24b6b6\">Women</span></p></td><td class=\"cl-ca251fc0\"><p class=\"cl-ca24c656\"><span class=\"cl-ca24b6b6\">Young</span></p></td><td class=\"cl-ca251fc0\"><p class=\"cl-ca24c656\"><span class=\"cl-ca24b6b6\">MixedGender</span></p></td><td class=\"cl-ca251fc0\"><p class=\"cl-ca24c656\"><span class=\"cl-ca24b6b6\">NoPrime</span></p></td><td class=\"cl-ca251fa2\"><p class=\"cl-ca24c660\"><span class=\"cl-ca24b6b6\">0</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-ca251fca\"><p class=\"cl-ca24c656\"><span class=\"cl-ca24b6b6\">S1A-023$B</span></p></td><td class=\"cl-ca251fde\"><p class=\"cl-ca24c656\"><span class=\"cl-ca24b6b6\">Women</span></p></td><td class=\"cl-ca251fde\"><p class=\"cl-ca24c656\"><span class=\"cl-ca24b6b6\">Young</span></p></td><td class=\"cl-ca251fde\"><p class=\"cl-ca24c656\"><span class=\"cl-ca24b6b6\">MixedGender</span></p></td><td class=\"cl-ca251fde\"><p class=\"cl-ca24c656\"><span class=\"cl-ca24b6b6\">NoPrime</span></p></td><td class=\"cl-ca251fcb\"><p class=\"cl-ca24c660\"><span class=\"cl-ca24b6b6\">0</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-ca251fb6\"><p class=\"cl-ca24c656\"><span class=\"cl-ca24b6b6\">S1A-054$A</span></p></td><td class=\"cl-ca251fc0\"><p class=\"cl-ca24c656\"><span class=\"cl-ca24b6b6\">Women</span></p></td><td class=\"cl-ca251fc0\"><p class=\"cl-ca24c656\"><span class=\"cl-ca24b6b6\">Young</span></p></td><td class=\"cl-ca251fc0\"><p class=\"cl-ca24c656\"><span class=\"cl-ca24b6b6\">SameGender</span></p></td><td class=\"cl-ca251fc0\"><p class=\"cl-ca24c656\"><span class=\"cl-ca24b6b6\">NoPrime</span></p></td><td class=\"cl-ca251fa2\"><p class=\"cl-ca24c660\"><span class=\"cl-ca24b6b6\">0</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-ca251fca\"><p class=\"cl-ca24c656\"><span class=\"cl-ca24b6b6\">S1A-090$B</span></p></td><td class=\"cl-ca251fde\"><p class=\"cl-ca24c656\"><span class=\"cl-ca24b6b6\">Women</span></p></td><td class=\"cl-ca251fde\"><p class=\"cl-ca24c656\"><span class=\"cl-ca24b6b6\">Young</span></p></td><td class=\"cl-ca251fde\"><p class=\"cl-ca24c656\"><span class=\"cl-ca24b6b6\">MixedGender</span></p></td><td class=\"cl-ca251fde\"><p class=\"cl-ca24c656\"><span class=\"cl-ca24b6b6\">NoPrime</span></p></td><td class=\"cl-ca251fcb\"><p class=\"cl-ca24c660\"><span class=\"cl-ca24b6b6\">0</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-ca251fb6\"><p class=\"cl-ca24c656\"><span class=\"cl-ca24b6b6\">S1A-009$B</span></p></td><td class=\"cl-ca251fc0\"><p class=\"cl-ca24c656\"><span class=\"cl-ca24b6b6\">Women</span></p></td><td class=\"cl-ca251fc0\"><p class=\"cl-ca24c656\"><span class=\"cl-ca24b6b6\">Old</span></p></td><td class=\"cl-ca251fc0\"><p class=\"cl-ca24c656\"><span class=\"cl-ca24b6b6\">SameGender</span></p></td><td class=\"cl-ca251fc0\"><p class=\"cl-ca24c656\"><span class=\"cl-ca24b6b6\">Prime</span></p></td><td class=\"cl-ca251fa2\"><p class=\"cl-ca24c660\"><span class=\"cl-ca24b6b6\">0</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-ca251fca\"><p class=\"cl-ca24c656\"><span class=\"cl-ca24b6b6\">S1A-085$E</span></p></td><td class=\"cl-ca251fde\"><p class=\"cl-ca24c656\"><span class=\"cl-ca24b6b6\">Men</span></p></td><td class=\"cl-ca251fde\"><p class=\"cl-ca24c656\"><span class=\"cl-ca24b6b6\">Young</span></p></td><td class=\"cl-ca251fde\"><p class=\"cl-ca24c656\"><span class=\"cl-ca24b6b6\">MixedGender</span></p></td><td class=\"cl-ca251fde\"><p class=\"cl-ca24c656\"><span class=\"cl-ca24b6b6\">Prime</span></p></td><td class=\"cl-ca251fcb\"><p class=\"cl-ca24c660\"><span class=\"cl-ca24b6b6\">1</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-ca251fb6\"><p class=\"cl-ca24c656\"><span class=\"cl-ca24b6b6\">S1A-003$C</span></p></td><td class=\"cl-ca251fc0\"><p class=\"cl-ca24c656\"><span class=\"cl-ca24b6b6\">Women</span></p></td><td class=\"cl-ca251fc0\"><p class=\"cl-ca24c656\"><span class=\"cl-ca24b6b6\">Young</span></p></td><td class=\"cl-ca251fc0\"><p class=\"cl-ca24c656\"><span class=\"cl-ca24b6b6\">MixedGender</span></p></td><td class=\"cl-ca251fc0\"><p class=\"cl-ca24c656\"><span class=\"cl-ca24b6b6\">NoPrime</span></p></td><td class=\"cl-ca251fa2\"><p class=\"cl-ca24c660\"><span class=\"cl-ca24b6b6\">1</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-ca251fca\"><p class=\"cl-ca24c656\"><span class=\"cl-ca24b6b6\">S1A-084$C</span></p></td><td class=\"cl-ca251fde\"><p class=\"cl-ca24c656\"><span class=\"cl-ca24b6b6\">Women</span></p></td><td class=\"cl-ca251fde\"><p class=\"cl-ca24c656\"><span class=\"cl-ca24b6b6\">Young</span></p></td><td class=\"cl-ca251fde\"><p class=\"cl-ca24c656\"><span class=\"cl-ca24b6b6\">SameGender</span></p></td><td class=\"cl-ca251fde\"><p class=\"cl-ca24c656\"><span class=\"cl-ca24b6b6\">NoPrime</span></p></td><td class=\"cl-ca251fcb\"><p class=\"cl-ca24c660\"><span class=\"cl-ca24b6b6\">0</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-ca251fb6\"><p class=\"cl-ca24c656\"><span class=\"cl-ca24b6b6\">S1A-076$A</span></p></td><td class=\"cl-ca251fc0\"><p class=\"cl-ca24c656\"><span class=\"cl-ca24b6b6\">Women</span></p></td><td class=\"cl-ca251fc0\"><p class=\"cl-ca24c656\"><span class=\"cl-ca24b6b6\">Young</span></p></td><td class=\"cl-ca251fc0\"><p class=\"cl-ca24c656\"><span class=\"cl-ca24b6b6\">SameGender</span></p></td><td class=\"cl-ca251fc0\"><p class=\"cl-ca24c656\"><span class=\"cl-ca24b6b6\">NoPrime</span></p></td><td class=\"cl-ca251fa2\"><p class=\"cl-ca24c660\"><span class=\"cl-ca24b6b6\">0</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-ca251fca\"><p class=\"cl-ca24c656\"><span class=\"cl-ca24b6b6\">S1A-083$D</span></p></td><td class=\"cl-ca251fde\"><p class=\"cl-ca24c656\"><span class=\"cl-ca24b6b6\">Men</span></p></td><td class=\"cl-ca251fde\"><p class=\"cl-ca24c656\"><span class=\"cl-ca24b6b6\">Old</span></p></td><td class=\"cl-ca251fde\"><p class=\"cl-ca24c656\"><span class=\"cl-ca24b6b6\">MixedGender</span></p></td><td class=\"cl-ca251fde\"><p class=\"cl-ca24c656\"><span class=\"cl-ca24b6b6\">NoPrime</span></p></td><td class=\"cl-ca251fcb\"><p class=\"cl-ca24c660\"><span class=\"cl-ca24b6b6\">1</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-ca251fb6\"><p class=\"cl-ca24c656\"><span class=\"cl-ca24b6b6\">S1A-068$A</span></p></td><td class=\"cl-ca251fc0\"><p class=\"cl-ca24c656\"><span class=\"cl-ca24b6b6\">Women</span></p></td><td class=\"cl-ca251fc0\"><p class=\"cl-ca24c656\"><span class=\"cl-ca24b6b6\">Young</span></p></td><td class=\"cl-ca251fc0\"><p class=\"cl-ca24c656\"><span class=\"cl-ca24b6b6\">SameGender</span></p></td><td class=\"cl-ca251fc0\"><p class=\"cl-ca24c656\"><span class=\"cl-ca24b6b6\">NoPrime</span></p></td><td class=\"cl-ca251fa2\"><p class=\"cl-ca24c660\"><span class=\"cl-ca24b6b6\">0</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-ca251fca\"><p class=\"cl-ca24c656\"><span class=\"cl-ca24b6b6\">S1A-066$B</span></p></td><td class=\"cl-ca251fde\"><p class=\"cl-ca24c656\"><span class=\"cl-ca24b6b6\">Women</span></p></td><td class=\"cl-ca251fde\"><p class=\"cl-ca24c656\"><span class=\"cl-ca24b6b6\">Young</span></p></td><td class=\"cl-ca251fde\"><p class=\"cl-ca24c656\"><span class=\"cl-ca24b6b6\">SameGender</span></p></td><td class=\"cl-ca251fde\"><p class=\"cl-ca24c656\"><span class=\"cl-ca24b6b6\">NoPrime</span></p></td><td class=\"cl-ca251fcb\"><p class=\"cl-ca24c660\"><span class=\"cl-ca24b6b6\">0</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-ca251fb6\"><p class=\"cl-ca24c656\"><span class=\"cl-ca24b6b6\">S1A-061$A</span></p></td><td class=\"cl-ca251fc0\"><p class=\"cl-ca24c656\"><span class=\"cl-ca24b6b6\">Men</span></p></td><td class=\"cl-ca251fc0\"><p class=\"cl-ca24c656\"><span class=\"cl-ca24b6b6\">Old</span></p></td><td class=\"cl-ca251fc0\"><p class=\"cl-ca24c656\"><span class=\"cl-ca24b6b6\">MixedGender</span></p></td><td class=\"cl-ca251fc0\"><p class=\"cl-ca24c656\"><span class=\"cl-ca24b6b6\">NoPrime</span></p></td><td class=\"cl-ca251fa2\"><p class=\"cl-ca24c660\"><span class=\"cl-ca24b6b6\">1</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-ca251fca\"><p class=\"cl-ca24c656\"><span class=\"cl-ca24b6b6\">S1A-049$A</span></p></td><td class=\"cl-ca251fde\"><p class=\"cl-ca24c656\"><span class=\"cl-ca24b6b6\">Women</span></p></td><td class=\"cl-ca251fde\"><p class=\"cl-ca24c656\"><span class=\"cl-ca24b6b6\">Young</span></p></td><td class=\"cl-ca251fde\"><p class=\"cl-ca24c656\"><span class=\"cl-ca24b6b6\">SameGender</span></p></td><td class=\"cl-ca251fde\"><p class=\"cl-ca24c656\"><span class=\"cl-ca24b6b6\">NoPrime</span></p></td><td class=\"cl-ca251fcb\"><p class=\"cl-ca24c660\"><span class=\"cl-ca24b6b6\">0</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-ca251fdf\"><p class=\"cl-ca24c656\"><span class=\"cl-ca24b6b6\">S1A-022$B</span></p></td><td class=\"cl-ca251fe8\"><p class=\"cl-ca24c656\"><span class=\"cl-ca24b6b6\">Women</span></p></td><td class=\"cl-ca251fe8\"><p class=\"cl-ca24c656\"><span class=\"cl-ca24b6b6\">Young</span></p></td><td class=\"cl-ca251fe8\"><p class=\"cl-ca24c656\"><span class=\"cl-ca24b6b6\">MixedGender</span></p></td><td class=\"cl-ca251fe8\"><p class=\"cl-ca24c656\"><span class=\"cl-ca24b6b6\">NoPrime</span></p></td><td class=\"cl-ca251ff2\"><p class=\"cl-ca24c660\"><span class=\"cl-ca24b6b6\">0</span></p></td></tr></tbody></table></div>\n```\n:::\n:::\n\n\nAs all variables except for the dependent variable (`SUFlike`) are character strings, we\nfactorize the independent variables.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# def. variables to be factorized\nvrs <- c(\"ID\", \"Age\", \"Gender\", \"ConversationType\", \"Priming\")\n# def. vector with variables\nfctr <- which(colnames(mblrdata) %in% vrs)     \n# factorize variables\nmblrdata[,fctr] <- lapply(mblrdata[,fctr], factor)\n# relevel Age (Young = Reference)\nmblrdata$Age <- relevel(mblrdata$Age, \"Young\")\n# order data by ID\nmblrdata <- mblrdata %>%\n  dplyr::arrange(ID)\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n```{=html}\n<div class=\"tabwid\"><style>.cl-ca39b52a{table-layout:auto;width:75%;}.cl-ca356920{font-family:'DejaVu Sans';font-size:12pt;font-weight:bold;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-ca35692a{font-family:'DejaVu Sans';font-size:12pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-ca357410{margin:0;text-align:center;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-ca35741a{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-ca35a3b8{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ca35a3c2{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ca35a3cc{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ca35a3cd{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ca35a3d6{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ca35a3d7{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ca35a3e0{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ca35a3e1{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ca35a3ea{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ca35a3eb{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ca35a3fe{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ca35a3ff{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}</style><table class='cl-ca39b52a'>\n```\n<caption class=\"Table Caption\">\n\nFirst 15 rows of the mblrdata arranged by ID.\n\n</caption>\n```{=html}\n<thead><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-ca35a3fe\"><p class=\"cl-ca357410\"><span class=\"cl-ca356920\">ID</span></p></td><td class=\"cl-ca35a3eb\"><p class=\"cl-ca357410\"><span class=\"cl-ca356920\">Gender</span></p></td><td class=\"cl-ca35a3eb\"><p class=\"cl-ca357410\"><span class=\"cl-ca356920\">Age</span></p></td><td class=\"cl-ca35a3eb\"><p class=\"cl-ca357410\"><span class=\"cl-ca356920\">ConversationType</span></p></td><td class=\"cl-ca35a3eb\"><p class=\"cl-ca357410\"><span class=\"cl-ca356920\">Priming</span></p></td><td class=\"cl-ca35a3ff\"><p class=\"cl-ca35741a\"><span class=\"cl-ca356920\">SUFlike</span></p></td></tr></thead><tbody><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-ca35a3c2\"><p class=\"cl-ca357410\"><span class=\"cl-ca35692a\">S1A-001$A</span></p></td><td class=\"cl-ca35a3cc\"><p class=\"cl-ca357410\"><span class=\"cl-ca35692a\">Men</span></p></td><td class=\"cl-ca35a3cc\"><p class=\"cl-ca357410\"><span class=\"cl-ca35692a\">Old</span></p></td><td class=\"cl-ca35a3cc\"><p class=\"cl-ca357410\"><span class=\"cl-ca35692a\">SameGender</span></p></td><td class=\"cl-ca35a3cc\"><p class=\"cl-ca357410\"><span class=\"cl-ca35692a\">NoPrime</span></p></td><td class=\"cl-ca35a3b8\"><p class=\"cl-ca35741a\"><span class=\"cl-ca35692a\">0</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-ca35a3cd\"><p class=\"cl-ca357410\"><span class=\"cl-ca35692a\">S1A-001$A</span></p></td><td class=\"cl-ca35a3d7\"><p class=\"cl-ca357410\"><span class=\"cl-ca35692a\">Men</span></p></td><td class=\"cl-ca35a3d7\"><p class=\"cl-ca357410\"><span class=\"cl-ca35692a\">Old</span></p></td><td class=\"cl-ca35a3d7\"><p class=\"cl-ca357410\"><span class=\"cl-ca35692a\">SameGender</span></p></td><td class=\"cl-ca35a3d7\"><p class=\"cl-ca357410\"><span class=\"cl-ca35692a\">NoPrime</span></p></td><td class=\"cl-ca35a3d6\"><p class=\"cl-ca35741a\"><span class=\"cl-ca35692a\">0</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-ca35a3c2\"><p class=\"cl-ca357410\"><span class=\"cl-ca35692a\">S1A-001$A</span></p></td><td class=\"cl-ca35a3cc\"><p class=\"cl-ca357410\"><span class=\"cl-ca35692a\">Men</span></p></td><td class=\"cl-ca35a3cc\"><p class=\"cl-ca357410\"><span class=\"cl-ca35692a\">Old</span></p></td><td class=\"cl-ca35a3cc\"><p class=\"cl-ca357410\"><span class=\"cl-ca35692a\">SameGender</span></p></td><td class=\"cl-ca35a3cc\"><p class=\"cl-ca357410\"><span class=\"cl-ca35692a\">NoPrime</span></p></td><td class=\"cl-ca35a3b8\"><p class=\"cl-ca35741a\"><span class=\"cl-ca35692a\">0</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-ca35a3cd\"><p class=\"cl-ca357410\"><span class=\"cl-ca35692a\">S1A-001$A</span></p></td><td class=\"cl-ca35a3d7\"><p class=\"cl-ca357410\"><span class=\"cl-ca35692a\">Men</span></p></td><td class=\"cl-ca35a3d7\"><p class=\"cl-ca357410\"><span class=\"cl-ca35692a\">Old</span></p></td><td class=\"cl-ca35a3d7\"><p class=\"cl-ca357410\"><span class=\"cl-ca35692a\">SameGender</span></p></td><td class=\"cl-ca35a3d7\"><p class=\"cl-ca357410\"><span class=\"cl-ca35692a\">NoPrime</span></p></td><td class=\"cl-ca35a3d6\"><p class=\"cl-ca35741a\"><span class=\"cl-ca35692a\">0</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-ca35a3c2\"><p class=\"cl-ca357410\"><span class=\"cl-ca35692a\">S1A-001$A</span></p></td><td class=\"cl-ca35a3cc\"><p class=\"cl-ca357410\"><span class=\"cl-ca35692a\">Men</span></p></td><td class=\"cl-ca35a3cc\"><p class=\"cl-ca357410\"><span class=\"cl-ca35692a\">Old</span></p></td><td class=\"cl-ca35a3cc\"><p class=\"cl-ca357410\"><span class=\"cl-ca35692a\">SameGender</span></p></td><td class=\"cl-ca35a3cc\"><p class=\"cl-ca357410\"><span class=\"cl-ca35692a\">NoPrime</span></p></td><td class=\"cl-ca35a3b8\"><p class=\"cl-ca35741a\"><span class=\"cl-ca35692a\">0</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-ca35a3cd\"><p class=\"cl-ca357410\"><span class=\"cl-ca35692a\">S1A-001$A</span></p></td><td class=\"cl-ca35a3d7\"><p class=\"cl-ca357410\"><span class=\"cl-ca35692a\">Men</span></p></td><td class=\"cl-ca35a3d7\"><p class=\"cl-ca357410\"><span class=\"cl-ca35692a\">Old</span></p></td><td class=\"cl-ca35a3d7\"><p class=\"cl-ca357410\"><span class=\"cl-ca35692a\">SameGender</span></p></td><td class=\"cl-ca35a3d7\"><p class=\"cl-ca357410\"><span class=\"cl-ca35692a\">NoPrime</span></p></td><td class=\"cl-ca35a3d6\"><p class=\"cl-ca35741a\"><span class=\"cl-ca35692a\">0</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-ca35a3c2\"><p class=\"cl-ca357410\"><span class=\"cl-ca35692a\">S1A-001$A</span></p></td><td class=\"cl-ca35a3cc\"><p class=\"cl-ca357410\"><span class=\"cl-ca35692a\">Men</span></p></td><td class=\"cl-ca35a3cc\"><p class=\"cl-ca357410\"><span class=\"cl-ca35692a\">Old</span></p></td><td class=\"cl-ca35a3cc\"><p class=\"cl-ca357410\"><span class=\"cl-ca35692a\">SameGender</span></p></td><td class=\"cl-ca35a3cc\"><p class=\"cl-ca357410\"><span class=\"cl-ca35692a\">NoPrime</span></p></td><td class=\"cl-ca35a3b8\"><p class=\"cl-ca35741a\"><span class=\"cl-ca35692a\">0</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-ca35a3cd\"><p class=\"cl-ca357410\"><span class=\"cl-ca35692a\">S1A-001$A</span></p></td><td class=\"cl-ca35a3d7\"><p class=\"cl-ca357410\"><span class=\"cl-ca35692a\">Men</span></p></td><td class=\"cl-ca35a3d7\"><p class=\"cl-ca357410\"><span class=\"cl-ca35692a\">Old</span></p></td><td class=\"cl-ca35a3d7\"><p class=\"cl-ca357410\"><span class=\"cl-ca35692a\">SameGender</span></p></td><td class=\"cl-ca35a3d7\"><p class=\"cl-ca357410\"><span class=\"cl-ca35692a\">NoPrime</span></p></td><td class=\"cl-ca35a3d6\"><p class=\"cl-ca35741a\"><span class=\"cl-ca35692a\">0</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-ca35a3c2\"><p class=\"cl-ca357410\"><span class=\"cl-ca35692a\">S1A-001$B</span></p></td><td class=\"cl-ca35a3cc\"><p class=\"cl-ca357410\"><span class=\"cl-ca35692a\">Women</span></p></td><td class=\"cl-ca35a3cc\"><p class=\"cl-ca357410\"><span class=\"cl-ca35692a\">Old</span></p></td><td class=\"cl-ca35a3cc\"><p class=\"cl-ca357410\"><span class=\"cl-ca35692a\">MixedGender</span></p></td><td class=\"cl-ca35a3cc\"><p class=\"cl-ca357410\"><span class=\"cl-ca35692a\">NoPrime</span></p></td><td class=\"cl-ca35a3b8\"><p class=\"cl-ca35741a\"><span class=\"cl-ca35692a\">0</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-ca35a3cd\"><p class=\"cl-ca357410\"><span class=\"cl-ca35692a\">S1A-001$B</span></p></td><td class=\"cl-ca35a3d7\"><p class=\"cl-ca357410\"><span class=\"cl-ca35692a\">Women</span></p></td><td class=\"cl-ca35a3d7\"><p class=\"cl-ca357410\"><span class=\"cl-ca35692a\">Old</span></p></td><td class=\"cl-ca35a3d7\"><p class=\"cl-ca357410\"><span class=\"cl-ca35692a\">MixedGender</span></p></td><td class=\"cl-ca35a3d7\"><p class=\"cl-ca357410\"><span class=\"cl-ca35692a\">NoPrime</span></p></td><td class=\"cl-ca35a3d6\"><p class=\"cl-ca35741a\"><span class=\"cl-ca35692a\">0</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-ca35a3c2\"><p class=\"cl-ca357410\"><span class=\"cl-ca35692a\">S1A-001$B</span></p></td><td class=\"cl-ca35a3cc\"><p class=\"cl-ca357410\"><span class=\"cl-ca35692a\">Women</span></p></td><td class=\"cl-ca35a3cc\"><p class=\"cl-ca357410\"><span class=\"cl-ca35692a\">Old</span></p></td><td class=\"cl-ca35a3cc\"><p class=\"cl-ca357410\"><span class=\"cl-ca35692a\">MixedGender</span></p></td><td class=\"cl-ca35a3cc\"><p class=\"cl-ca357410\"><span class=\"cl-ca35692a\">NoPrime</span></p></td><td class=\"cl-ca35a3b8\"><p class=\"cl-ca35741a\"><span class=\"cl-ca35692a\">0</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-ca35a3cd\"><p class=\"cl-ca357410\"><span class=\"cl-ca35692a\">S1A-001$B</span></p></td><td class=\"cl-ca35a3d7\"><p class=\"cl-ca357410\"><span class=\"cl-ca35692a\">Women</span></p></td><td class=\"cl-ca35a3d7\"><p class=\"cl-ca357410\"><span class=\"cl-ca35692a\">Old</span></p></td><td class=\"cl-ca35a3d7\"><p class=\"cl-ca357410\"><span class=\"cl-ca35692a\">MixedGender</span></p></td><td class=\"cl-ca35a3d7\"><p class=\"cl-ca357410\"><span class=\"cl-ca35692a\">Prime</span></p></td><td class=\"cl-ca35a3d6\"><p class=\"cl-ca35741a\"><span class=\"cl-ca35692a\">0</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-ca35a3c2\"><p class=\"cl-ca357410\"><span class=\"cl-ca35692a\">S1A-001$B</span></p></td><td class=\"cl-ca35a3cc\"><p class=\"cl-ca357410\"><span class=\"cl-ca35692a\">Women</span></p></td><td class=\"cl-ca35a3cc\"><p class=\"cl-ca357410\"><span class=\"cl-ca35692a\">Old</span></p></td><td class=\"cl-ca35a3cc\"><p class=\"cl-ca357410\"><span class=\"cl-ca35692a\">MixedGender</span></p></td><td class=\"cl-ca35a3cc\"><p class=\"cl-ca357410\"><span class=\"cl-ca35692a\">NoPrime</span></p></td><td class=\"cl-ca35a3b8\"><p class=\"cl-ca35741a\"><span class=\"cl-ca35692a\">1</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-ca35a3cd\"><p class=\"cl-ca357410\"><span class=\"cl-ca35692a\">S1A-001$B</span></p></td><td class=\"cl-ca35a3d7\"><p class=\"cl-ca357410\"><span class=\"cl-ca35692a\">Women</span></p></td><td class=\"cl-ca35a3d7\"><p class=\"cl-ca357410\"><span class=\"cl-ca35692a\">Old</span></p></td><td class=\"cl-ca35a3d7\"><p class=\"cl-ca357410\"><span class=\"cl-ca35692a\">MixedGender</span></p></td><td class=\"cl-ca35a3d7\"><p class=\"cl-ca357410\"><span class=\"cl-ca35692a\">NoPrime</span></p></td><td class=\"cl-ca35a3d6\"><p class=\"cl-ca35741a\"><span class=\"cl-ca35692a\">0</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-ca35a3e0\"><p class=\"cl-ca357410\"><span class=\"cl-ca35692a\">S1A-001$B</span></p></td><td class=\"cl-ca35a3e1\"><p class=\"cl-ca357410\"><span class=\"cl-ca35692a\">Women</span></p></td><td class=\"cl-ca35a3e1\"><p class=\"cl-ca357410\"><span class=\"cl-ca35692a\">Old</span></p></td><td class=\"cl-ca35a3e1\"><p class=\"cl-ca357410\"><span class=\"cl-ca35692a\">MixedGender</span></p></td><td class=\"cl-ca35a3e1\"><p class=\"cl-ca357410\"><span class=\"cl-ca35692a\">NoPrime</span></p></td><td class=\"cl-ca35a3ea\"><p class=\"cl-ca35741a\"><span class=\"cl-ca35692a\">0</span></p></td></tr></tbody></table></div>\n```\n:::\n:::\n\n\nBefore continuing, a few words about the minimum number of random effect levels and the minimum number of observations per random effect level are in order.\n\nWhile many data points per random variable level increases statistical power and thus to more robust estimates of the random effects [@austin2018multilevel], it has been shown that small numbers of observations per random effect variable level do not cause serious bias and it does not negatively affect the estimates of the fixed-effects coefficients  [@bell2008multilevel; @clarke2008can; @clarke2007addressing; @maas2005sufficient]. The minimum number of observations per random effect variable level is therefore 1.\n\nIn simulation study, [@bell2008multilevel] tested the impact of random variable levels with only a single observation ranging from 0 to 70 percent. As long as there was a relatively high number of random effect variable levels (500 or more), small numbers of observations had almost no impact on bias and Type 1 error control.\n\nWe now plot the data to inspect the relationships within the data set. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(mblrdata, aes(Gender, SUFlike, color = Priming)) +\n  facet_wrap(Age~ConversationType) +\n  stat_summary(fun = mean, geom = \"point\") +\n  stat_summary(fun.data = mean_cl_boot, geom = \"errorbar\", width = 0.2) +\n  theme_set(theme_bw(base_size = 10)) +\n  theme(legend.position = \"top\") +\n  labs(x = \"\", y = \"Observed Probabilty of discourse like\") +\n  scale_color_manual(values = c(\"gray20\", \"gray70\"))\n```\n\n::: {.cell-output-display}\n![](regression_files/figure-html/blmm8-1.png){width=672}\n:::\n:::\n\n\nThe upper left panel in the Figure above indicates that men use discourse *like* more frequently than women. The center right panel suggests that priming significantly increases the likelihood of discourse like being used. The center left panel suggests that speakers use discourse like more frequently in mixed-gender conversations.  However, the lower right panel indicates an interaction between gender and conversation type as women appear to use discourse like less frequently in same gender conversations while the conversation type does not seem to have an effect on men. After visualizing the data, we will now turn to the model building process.\n\n### Model Building{-}\n\nIn a first step, we set the options.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# set options\noptions(contrasts  =c(\"contr.treatment\", \"contr.poly\"))\nmblrdata.dist <- datadist(mblrdata)\noptions(datadist = \"mblrdata.dist\")\n```\n:::\n\n\nIn a next step, we generate fixed-effects minimal base-line models and a base-line mixed-model using the \"glmer\" function with a random intercept for ID (a lmer object of the final minimal adequate model will be created later).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# baseline model glm\nm0.glm = glm(SUFlike ~ 1, family = binomial, data = mblrdata) \n# base-line mixed-model\nm0.glmer = glmer(SUFlike ~ (1|ID), data = mblrdata, family = binomial) \n```\n:::\n\n\n### Testing the Random Effect{-}\n\nNow, we check if including the random effect is permitted by comparing the AICs from the glm to AIC from the glmer model. If the AIC of the glmer object is smaller than the AIC of the glm object, then this indicates that including random intercepts is justified.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\naic.glmer <- AIC(logLik(m0.glmer))\naic.glm <- AIC(logLik(m0.glm))\naic.glmer; aic.glm\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1828.49227107\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1838.17334856\n```\n:::\n:::\n\n\nThe AIC of the glmer object is smaller which shows that including the random intercepts is justified. To confirm whether the AIC reduction is sufficient for justifying the inclusion of a random-effect structure, we also test whether the mixed-effects minimal base-line model explains significantly more variance by applying a Model Likelihood Ratio Test to the fixed- and the mixed effects minimal base-line models.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# test random effects\nnull.id = -2 * logLik(m0.glm) + 2 * logLik(m0.glmer)\npchisq(as.numeric(null.id), df=1, lower.tail=F) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.000631389572445\n```\n:::\n\n```{.r .cell-code}\n# sig m0.glmer better than m0.glm\n```\n:::\n\n\nThe p-value of the Model Likelihood Ratio Test is lower than .05 which shows that the inclusion of the random-effects structure is warranted. We can now continue with the model fitting process.\n\n### Model Fitting{-}\n\nThe next step is to fit the model which means that we aim to find the \"best\" model, i.e. the minimal adequate model. In this case, we will use a manual step-wise step-up, forward elimination procedure.\nBefore we begin with the model fitting process we need to add Â´control = glmerControl(optimizer = \"bobyqa\")Â´ to avoid unnecessary failures to converge.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm0.glmer <- glmer(SUFlike ~ 1+ (1|ID), family = binomial, data = mblrdata, control=glmerControl(optimizer=\"bobyqa\"))\n```\n:::\n\n\nDuring each step of the fitting procedure, we test whether certain assumptions on which the model relies are violated. To avoid *incomplete information* (a combination of variables does not occur in the data), we tabulate the variables we intend to include and make sure that all possible combinations are present in the data. Including variables although not all combinations are present in the data would lead to unreliable models that report (vastly) inaccurate results. A special case of incomplete information is *complete separation* which occurs if one predictor perfectly explains an outcome (in that case the incomplete information would be caused by a level of the dependent variable). In addition, we make sure that the VIFs do not exceed a maximum of 3 for main effects [@zuur2010protocol] - @booth1994regression suggest that VIFs should ideally be lower than 3 for as higher values would indicate multicollinearity and thus that the model is unstable. The value of 3 should be taken with a pinch of salt because there is no clear consensus about what the maximum VIF for interactions should be or if it should be considered at all. The reason is that we would, of course, expect the VIFs to increase when we are dealing with interactions as the main effects that are part of the interaction are very likely to correlate with the interaction itself. However, if the VIFs are too high, then this will still cause the issues with the attribution of variance. The value of 3 was chosen based on recommendations in the standard literature on multicollinearity [@zuur2009mixedmodels; @neter1990vif]. Only once we have confirmed that the incomplete information, complete separation, and *multicollinearity* are not a major concern, we generate the more saturated model and test whether the inclusion of a predictor leads to a significant reduction in residual deviance. If the predictor explains a significant amount of variance, it is retained in the model while being disregarded in case it does not explain a sufficient quantity of variance.  \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# add Priming\nifelse(min(ftable(mblrdata$Priming, mblrdata$SUFlike)) == 0, \"incomplete information\", \"okay\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"okay\"\n```\n:::\n\n```{.r .cell-code}\nm1.glmer <- update(m0.glmer, .~.+Priming)\nanova(m1.glmer, m0.glmer, test = \"Chi\") \n```\n\n::: {.cell-output .cell-output-stdout}\n```\nData: mblrdata\nModels:\nm0.glmer: SUFlike ~ 1 + (1 | ID)\nm1.glmer: SUFlike ~ (1 | ID) + Priming\n         npar         AIC         BIC       logLik    deviance     Chisq Df\nm0.glmer    2 1828.492271 1839.694076 -912.2461355 1824.492271             \nm1.glmer    3 1702.773341 1719.576048 -848.3866704 1696.773341 127.71893  1\n                     Pr(>Chisq)    \nm0.glmer                           \nm1.glmer < 0.000000000000000222 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\nSince the tests do not show problems relating to incomplete information, because including *Priming* significantly improves the model fit (decrease in AIC and BIC values), and since it correlates significantly with our dependent variable, we include *Priming* into our model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# add Age\nifelse(min(ftable(mblrdata$Age, mblrdata$SUFlike)) == 0, \"incomplete information\", \"okay\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"okay\"\n```\n:::\n\n```{.r .cell-code}\nm2.glmer <- update(m1.glmer, .~.+ Age)\nifelse(max(car::vif(m2.glmer)) <= 3,  \"VIFs okay\", \"VIFs unacceptable\") \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"VIFs okay\"\n```\n:::\n\n```{.r .cell-code}\nanova(m2.glmer, m1.glmer, test = \"Chi\")   \n```\n\n::: {.cell-output .cell-output-stdout}\n```\nData: mblrdata\nModels:\nm1.glmer: SUFlike ~ (1 | ID) + Priming\nm2.glmer: SUFlike ~ (1 | ID) + Priming + Age\n         npar         AIC         BIC       logLik    deviance   Chisq Df\nm1.glmer    3 1702.773341 1719.576048 -848.3866704 1696.773341           \nm2.glmer    4 1704.210790 1726.614400 -848.1053950 1696.210790 0.56255  1\n         Pr(>Chisq)\nm1.glmer           \nm2.glmer    0.45323\n```\n:::\n\n```{.r .cell-code}\nAnova(m2.glmer, test = \"Chi\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Deviance Table (Type II Wald chisquare tests)\n\nResponse: SUFlike\n            Chisq Df           Pr(>Chisq)    \nPriming 129.51509  1 < 0.0000000000000002 ***\nAge       0.56943  1              0.45049    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\nThe ANOVAs show that *Age* is not significant and the first ANOVA also shows that the BIC has increased which indicates that *Age* does not decrease variance. In such cases, the variable should not be included. \n\nHowever, if the second ANOVA would report *Age* as being marginally significant, a case could be made for including it but it would be better to change the ordering in which predictors are added to the model. This is, however, just a theoretical issue here as *Age* is clearly not significant.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# add Gender\nifelse(min(ftable(mblrdata$Gender, mblrdata$SUFlike)) == 0, \"incomplete information\", \"okay\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"okay\"\n```\n:::\n\n```{.r .cell-code}\nm3.glmer <- update(m1.glmer, .~.+Gender)\nifelse(max(car::vif(m3.glmer)) <= 3,  \"VIFs okay\", \"VIFs unacceptable\") \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"VIFs okay\"\n```\n:::\n\n```{.r .cell-code}\nanova(m3.glmer, m1.glmer, test = \"Chi\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nData: mblrdata\nModels:\nm1.glmer: SUFlike ~ (1 | ID) + Priming\nm3.glmer: SUFlike ~ (1 | ID) + Priming + Gender\n         npar         AIC         BIC       logLik    deviance    Chisq Df\nm1.glmer    3 1702.773341 1719.576048 -848.3866704 1696.773341            \nm3.glmer    4 1679.397070 1701.800680 -835.6985349 1671.397070 25.37627  1\n            Pr(>Chisq)    \nm1.glmer                  \nm3.glmer 0.00000047168 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n\n```{.r .cell-code}\nAnova(m3.glmer, test = \"Chi\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Deviance Table (Type II Wald chisquare tests)\n\nResponse: SUFlike\n            Chisq Df             Pr(>Chisq)    \nPriming 124.40764  1 < 0.000000000000000222 ***\nGender   28.56705  1         0.000000090509 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\n*Gender* is significant and will therefore be included as a predictor (you can also observe that including Gender has substantially decreased both AIC and BIC).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# add ConversationType\nifelse(min(ftable(mblrdata$ConversationType, mblrdata$SUFlike)) == 0, \"incomplete information\", \"okay\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"okay\"\n```\n:::\n\n```{.r .cell-code}\nm4.glmer <- update(m3.glmer, .~.+ConversationType)\nifelse(max(car::vif(m4.glmer)) <= 3,  \"VIFs okay\", \"VIFs unacceptable\") \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"VIFs okay\"\n```\n:::\n\n```{.r .cell-code}\nanova(m4.glmer, m3.glmer, test = \"Chi\") \n```\n\n::: {.cell-output .cell-output-stdout}\n```\nData: mblrdata\nModels:\nm3.glmer: SUFlike ~ (1 | ID) + Priming + Gender\nm4.glmer: SUFlike ~ (1 | ID) + Priming + Gender + ConversationType\n         npar         AIC         BIC       logLik    deviance    Chisq Df\nm3.glmer    4 1679.397070 1701.800680 -835.6985349 1671.397070            \nm4.glmer    5 1668.583222 1696.587734 -829.2916108 1658.583222 12.81385  1\n         Pr(>Chisq)    \nm3.glmer               \nm4.glmer 0.00034406 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n\n```{.r .cell-code}\nAnova(m4.glmer, test = \"Chi\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Deviance Table (Type II Wald chisquare tests)\n\nResponse: SUFlike\n                     Chisq Df             Pr(>Chisq)    \nPriming          130.68714  1 < 0.000000000000000222 ***\nGender            13.44456  1             0.00024572 ***\nConversationType  12.99243  1             0.00031275 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\n*ConversationType* improves model fit (AIC and BIC decrease and it is reported as being significant) and will, therefore, be included in the model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# add Priming*Age\nifelse(min(ftable(mblrdata$Priming, mblrdata$Age, mblrdata$SUFlike)) == 0, \"incomplete information\", \"okay\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"okay\"\n```\n:::\n\n```{.r .cell-code}\nm5.glmer <- update(m4.glmer, .~.+Priming*Age)\nifelse(max(car::vif(m5.glmer)) <= 3,  \"VIFs okay\", \"WARNING: high VIFs!\") \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"VIFs okay\"\n```\n:::\n\n```{.r .cell-code}\nanova(m5.glmer, m4.glmer, test = \"Chi\") \n```\n\n::: {.cell-output .cell-output-stdout}\n```\nData: mblrdata\nModels:\nm4.glmer: SUFlike ~ (1 | ID) + Priming + Gender + ConversationType\nm5.glmer: SUFlike ~ (1 | ID) + Priming + Gender + ConversationType + Age + Priming:Age\n         npar         AIC         BIC       logLik    deviance   Chisq Df\nm4.glmer    5 1668.583222 1696.587734 -829.2916108 1658.583222           \nm5.glmer    7 1671.599008 1710.805326 -828.7995042 1657.599008 0.98421  2\n         Pr(>Chisq)\nm4.glmer           \nm5.glmer    0.61134\n```\n:::\n:::\n\n\nThe interaction between *Priming* and *Age* is not significant and we thus not be included.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# add Priming*Gender\nifelse(min(ftable(mblrdata$Priming, mblrdata$Gender, mblrdata$SUFlike)) == 0, \"incomplete information\", \"okay\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"okay\"\n```\n:::\n\n```{.r .cell-code}\nm6.glmer <- update(m4.glmer, .~.+Priming*Gender)\nifelse(max(car::vif(m6.glmer)) <= 3,  \"VIFs okay\", \"WARNING: high VIFs!\") \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"WARNING: high VIFs!\"\n```\n:::\n:::\n\n\nWe get the warning that the VIFs are high (>= 3) which means that the model suffers from (multi-)collinearity. We thus check the VIFs to determine how to proceed. If the VIFs are > 10, then we definitely cannot use the model as the multicollinearity is excessive. \n\n\n::: {.cell}\n\n```{.r .cell-code}\ncar::vif(m6.glmer)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n         Priming           Gender ConversationType   Priming:Gender \n   4.35644725184    1.43315974183    1.20288306932    4.58280387132 \n```\n:::\n:::\n\n\nThe VIFs are below 5 which is not good (VIFs of 5 mean \"that column in the model matrix is explainable from the others with an\nR^2^ of 0.8\" [@gries2021statistics]) but it is still arguably acceptable and we will thus check if including the interaction between *Priming* and *Gender* significantly improved model fit. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nanova(m6.glmer, m4.glmer, test = \"Chi\") \n```\n\n::: {.cell-output .cell-output-stdout}\n```\nData: mblrdata\nModels:\nm4.glmer: SUFlike ~ (1 | ID) + Priming + Gender + ConversationType\nm6.glmer: SUFlike ~ (1 | ID) + Priming + Gender + ConversationType + Priming:Gender\n         npar         AIC         BIC       logLik    deviance   Chisq Df\nm4.glmer    5 1668.583222 1696.587734 -829.2916108 1658.583222           \nm6.glmer    6 1663.164481 1696.769896 -825.5822404 1651.164481 7.41874  1\n         Pr(>Chisq)   \nm4.glmer              \nm6.glmer  0.0064548 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n\n```{.r .cell-code}\nAnova(m6.glmer, test = \"Chi\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Deviance Table (Type II Wald chisquare tests)\n\nResponse: SUFlike\n                     Chisq Df             Pr(>Chisq)    \nPriming          131.96244  1 < 0.000000000000000222 ***\nGender            13.57723  1             0.00022895 ***\nConversationType  10.70707  1             0.00106727 ** \nPriming:Gender     7.45362  1             0.00633089 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\nThe interaction between *Priming* and *Gender* improved model fit (AIC and BIC reduction) and significantly correlates with the use of speech-unit final *like*. It will therefore be included in the model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# add Priming*ConversationType\nifelse(min(ftable(mblrdata$Priming, mblrdata$ConversationType, mblrdata$SUFlike)) == 0, \"incomplete information\", \"okay\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"okay\"\n```\n:::\n\n```{.r .cell-code}\nm7.glmer <- update(m6.glmer, .~.+Priming*ConversationType)\nifelse(max(car::vif(m7.glmer)) <= 3,  \"VIFs okay\", \"WARNING: high VIFs!\") \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"WARNING: high VIFs!\"\n```\n:::\n:::\n\n\nWhen including the interaction between *Priming* and *ConversationType* we get a warning that there are high VIFs (multicollinearity) so we inspect the VIFs in more detail.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# check VIFs\ncar::vif(m7.glmer) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                 Priming                   Gender         ConversationType \n           5.10717661703            1.49290487846            1.49158369697 \n          Priming:Gender Priming:ConversationType \n           4.93025403307            3.41077852978 \n```\n:::\n:::\n\n\nThe VIF of *Priming* is above 5 so we would normally continue without checking if including the interaction between *Priming* and *ConversationType* leads to a significant improvement in model fit. However, given that this is just a practical example, we check if including this interaction significantly improves model fit.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nanova(m7.glmer, m6.glmer, test = \"Chi\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nData: mblrdata\nModels:\nm6.glmer: SUFlike ~ (1 | ID) + Priming + Gender + ConversationType + Priming:Gender\nm7.glmer: SUFlike ~ (1 | ID) + Priming + Gender + ConversationType + Priming:Gender + Priming:ConversationType\n         npar         AIC         BIC       logLik    deviance   Chisq Df\nm6.glmer    6 1663.164481 1696.769896 -825.5822404 1651.164481           \nm7.glmer    7 1663.134968 1702.341285 -824.5674840 1649.134968 2.02951  1\n         Pr(>Chisq)\nm6.glmer           \nm7.glmer    0.15427\n```\n:::\n:::\n\n\n\nThe interaction between *Priming* and *ConversationType* does not significantly correlate with the use of speech-unit final *like* and it does not explain much variance (AIC and BIC increase). It will be not be included in the model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# add Age*Gender\nifelse(min(ftable(mblrdata$Age, mblrdata$Gender, mblrdata$SUFlike)) == 0, \"incomplete information\", \"okay\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"okay\"\n```\n:::\n\n```{.r .cell-code}\nm8.glmer <- update(m6.glmer, .~.+Age*Gender)\nifelse(max(car::vif(m8.glmer)) <= 3,  \"VIFs okay\", \"WARNING: high VIFs!\") \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"WARNING: high VIFs!\"\n```\n:::\n:::\n\n\nWhen including the interaction between *Age* and *Gender* we get a warning that there are high VIFs (multicollinearity) so we inspect the VIFs in more detail.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# check VIFs\ncar::vif(m8.glmer)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n         Priming           Gender ConversationType              Age \n   4.46161514975    2.14158283266    1.20633411411    3.25166979907 \n  Priming:Gender       Gender:Age \n   4.66786414860    3.21354842006 \n```\n:::\n:::\n\n\nThe VIFs are all below 5 so we test if including the interaction between *Gender* and *Age* significantly improves model fit.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nanova(m8.glmer, m6.glmer, test = \"Chi\") \n```\n\n::: {.cell-output .cell-output-stdout}\n```\nData: mblrdata\nModels:\nm6.glmer: SUFlike ~ (1 | ID) + Priming + Gender + ConversationType + Priming:Gender\nm8.glmer: SUFlike ~ (1 | ID) + Priming + Gender + ConversationType + Age + Priming:Gender + Gender:Age\n         npar         AIC         BIC       logLik    deviance   Chisq Df\nm6.glmer    6 1663.164481 1696.769896 -825.5822404 1651.164481           \nm8.glmer    8 1665.634538 1710.441758 -824.8172690 1649.634538 1.52994  2\n         Pr(>Chisq)\nm6.glmer           \nm8.glmer    0.46535\n```\n:::\n:::\n\n\nThe interaction between *Age* and *Gender* is not significant and will thus continue without it.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# add Age*ConversationType\nifelse(min(ftable(mblrdata$Age, mblrdata$ConversationType, mblrdata$SUFlike)) == 0, \"incomplete information\", \"okay\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"okay\"\n```\n:::\n\n```{.r .cell-code}\nm9.glmer <- update(m6.glmer, .~.+Age*ConversationType)\nifelse(max(car::vif(m9.glmer)) <= 3,  \"VIFs okay\", \"WARNING: high VIFs!\") \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"WARNING: high VIFs!\"\n```\n:::\n:::\n\n\nWhen including the interaction between *Age* and *ConversationType* we get a warning that there are high VIFs (multicollinearity) so we inspect the VIFs in more detail.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# check VIFs\ncar::vif(m9.glmer)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n             Priming               Gender     ConversationType \n       4.37760628012        1.50657679188        1.51468866164 \n                 Age       Priming:Gender ConversationType:Age \n       1.96937607096        4.61285236598        2.05894940257 \n```\n:::\n:::\n\n\nThe VIFs are all below 5 so we test if including the interaction between *ConversationType* and *Age* significantly improves model fit.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nanova(m9.glmer, m6.glmer, test = \"Chi\") \n```\n\n::: {.cell-output .cell-output-stdout}\n```\nData: mblrdata\nModels:\nm6.glmer: SUFlike ~ (1 | ID) + Priming + Gender + ConversationType + Priming:Gender\nm9.glmer: SUFlike ~ (1 | ID) + Priming + Gender + ConversationType + Age + Priming:Gender + ConversationType:Age\n         npar         AIC         BIC       logLik    deviance   Chisq Df\nm6.glmer    6 1663.164481 1696.769896 -825.5822404 1651.164481           \nm9.glmer    8 1666.262166 1711.069386 -825.1310831 1650.262166 0.90231  2\n         Pr(>Chisq)\nm6.glmer           \nm9.glmer    0.63689\n```\n:::\n:::\n\n\nThe interaction between *Age* and *ConversationType* is insignificant and does not improve model fit (AIC and BIC reduction). It will therefore not be included in the model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# add Gender*ConversationType\nifelse(min(ftable(mblrdata$Gender, mblrdata$ConversationType, mblrdata$SUFlike)) == 0, \"incomplete information\", \"okay\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"okay\"\n```\n:::\n\n```{.r .cell-code}\nm10.glmer <- update(m6.glmer, .~.+Gender*ConversationType)\nifelse(max(car::vif(m10.glmer)) <= 3,  \"VIFs okay\", \"WARNING: high VIFs!\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"WARNING: high VIFs!\"\n```\n:::\n:::\n\n\nWhen including the interaction between *Gender* and *ConversationType* we get a warning that there are high VIFs (multicollinearity) so we inspect the VIFs in more detail.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# check VIFs\ncar::vif(m10.glmer) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                Priming                  Gender        ConversationType \n          4.96223841163           1.73315963551           7.76458422494 \n         Priming:Gender Gender:ConversationType \n          5.11897075069           9.24867418732 \n```\n:::\n:::\n\n\nThe highest VIF is almost 10 (9.248674187318) which is why the interaction between *Gender* and *ConversationType* will not be included in the model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# add Priming*Age*Gender\nifelse(min(ftable(mblrdata$Priming,mblrdata$Age, mblrdata$Gender, mblrdata$SUFlike)) == 0, \"incomplete information\", \"okay\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"okay\"\n```\n:::\n\n```{.r .cell-code}\nm11.glmer <- update(m6.glmer, .~.+Priming*Age*Gender)\nifelse(max(car::vif(m11.glmer)) <= 3,  \"VIFs okay\", \"WARNING: high VIFs!\") \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"WARNING: high VIFs!\"\n```\n:::\n:::\n\n\n\nWhen including the interaction between *Priming*, *Age*, and *Gender* we get a warning that there are high VIFs (multicollinearity) so we inspect the VIFs in more detail.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# check VIFs\ncar::vif(m11.glmer)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n           Priming             Gender   ConversationType                Age \n     6.57538416079      2.28504186327      1.22325183699      3.66037124792 \n    Priming:Gender        Priming:Age         Gender:Age Priming:Gender:Age \n     6.66183402912      5.85367812826      3.75879850896      5.98139175840 \n```\n:::\n:::\n\n\nThere are several VIFs with values greater than 5 and we will thus continue without including the interaction between *Priming*, *Age*, and *Gender* into the model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# add Priming*Age*ConversationType\nifelse(min(ftable(mblrdata$Priming,mblrdata$Age, mblrdata$ConversationType, mblrdata$SUFlike)) == 0, \"incomplete information\", \"okay\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"okay\"\n```\n:::\n\n```{.r .cell-code}\nm12.glmer <- update(m6.glmer, .~.+Priming*Age*ConversationType)\nifelse(max(car::vif(m12.glmer)) <= 3,  \"VIFs okay\", \"WARNING: high VIFs!\") \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"WARNING: high VIFs!\"\n```\n:::\n:::\n\n\nWhen including the interaction between *Priming*, *Age*, and *Gender* we get a warning that there are high VIFs (multicollinearity) so we inspect the VIFs in more detail.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# check VIFs\ncar::vif(m12.glmer)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                     Priming                       Gender \n               7.14771665816                1.61068940841 \n            ConversationType                          Age \n               1.83168707059                2.20184432953 \n              Priming:Gender                  Priming:Age \n               4.99337045657                3.33295681086 \n    Priming:ConversationType         ConversationType:Age \n               4.87448199535                2.38314262793 \nPriming:ConversationType:Age \n               2.99929551111 \n```\n:::\n:::\n\n\nThe VIF of Priming is very high (7.147716658159) which is why we will thus continue without including the interaction between *Priming*, *Age*, and *Gender* in the model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# add Priming*Gender*ConversationType\nifelse(min(ftable(mblrdata$Priming,mblrdata$Gender, mblrdata$ConversationType, mblrdata$SUFlike)) == 0, \"incomplete information\", \"okay\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"okay\"\n```\n:::\n\n```{.r .cell-code}\nm13.glmer <- update(m6.glmer, .~.+Priming*Gender*ConversationType)\nifelse(max(car::vif(m13.glmer)) <= 3,  \"VIFs okay\", \"WARNING: high VIFs!\") \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"WARNING: high VIFs!\"\n```\n:::\n:::\n\n\nThe VIFs are excessive with a maximum value is 23.863401882333 which shows an unacceptable degree of multicollinearity so that we abort and move on the next model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncar::vif(m13.glmer)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                        Priming                          Gender \n                  7.69529718176                   1.81201132511 \n               ConversationType                  Priming:Gender \n                 21.92712607379                  10.01452723810 \n       Priming:ConversationType         Gender:ConversationType \n                 21.62322307216                  23.86340188233 \nPriming:Gender:ConversationType \n                 22.67125500326 \n```\n:::\n:::\n\n\nThe VIFs are excessive with a maximum value is 23.863401882333 which shows an unacceptable degree of multicollinearity so that we abort and move on the next model.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# add Age*Gender*ConversationType\nifelse(min(ftable(mblrdata$Age,mblrdata$Gender, mblrdata$ConversationType, mblrdata$SUFlike)) == 0, \"incomplete information\", \"okay\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"okay\"\n```\n:::\n\n```{.r .cell-code}\nm14.glmer <- update(m6.glmer, .~.+Age*Gender*ConversationType)\nifelse(max(car::vif(m14.glmer)) <= 3,  \"VIFs okay\", \"WARNING: high VIFs!\") \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"WARNING: high VIFs!\"\n```\n:::\n:::\n\n\nWhen including the interaction between *Age*, *Gender*, *ConversationType*, we get a warning that there are high VIFs (multicollinearity) so we inspect the VIFs in more detail.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncar::vif(m14.glmer)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                    Priming                      Gender \n              5.33214037289               2.60055549758 \n           ConversationType                         Age \n             11.94207363385               3.80035469665 \n             Priming:Gender                  Gender:Age \n              5.45398766628               5.66107224954 \n       ConversationType:Age     Gender:ConversationType \n             16.98200292781              13.80649554087 \nGender:ConversationType:Age \n             19.07180232877 \n```\n:::\n:::\n\n\nAgain, the VIFs are excessive with a maximum value of 19.071802328769 which shows an unacceptable degree of multicollinearity so that we abort and move on the next model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# add Priming*Age*Gender*ConversationType\nifelse(min(ftable(mblrdata$Priming,mblrdata$Age,mblrdata$Gender, mblrdata$ConversationType, mblrdata$SUFlike)) == 0, \"incomplete information\", \"okay\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"incomplete information\"\n```\n:::\n:::\n\n\nThe model suffers from incomplete information! As this was the last possible model, we have found our final minimal adequate model in m6.glmer.\n\nIn a next step, we create an overview of model comparisons which serves as a summary for the model fitting process and provides AIC, BIC, and $\\chi$^2^ values.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsource(\"https://slcladal.github.io/rscripts/ModelFittingSummarySWSU.r\") \n# comparisons of glmer objects\nm1.m0 <- anova(m1.glmer, m0.glmer, test = \"Chi\") \nm2.m1 <- anova(m2.glmer, m1.glmer, test = \"Chi\")   \nm3.m1 <- anova(m3.glmer, m1.glmer, test = \"Chi\")\nm4.m3 <- anova(m4.glmer, m3.glmer, test = \"Chi\") \nm5.m4 <- anova(m5.glmer, m4.glmer, test = \"Chi\") \nm6.m4 <- anova(m6.glmer, m4.glmer, test = \"Chi\") \nm7.m6 <- anova(m7.glmer, m6.glmer, test = \"Chi\")\nm8.m6 <- anova(m8.glmer, m6.glmer, test = \"Chi\") \nm9.m6 <- anova(m9.glmer, m6.glmer, test = \"Chi\") \n# create a list of the model comparisons\nmdlcmp <- list(m1.m0, m2.m1, m3.m1, m4.m3, m5.m4, m6.m4, m7.m6, m8.m6, m9.m6)\n# summary table for model fitting\nmdlft <- mdl.fttng.swsu(mdlcmp)\nmdlft <- mdlft[,-2]\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n```{=html}\n<div class=\"tabwid\"><style>.cl-cfd6ecbe{table-layout:auto;width:75%;}.cl-cfd141f6{font-family:'DejaVu Sans';font-size:12pt;font-weight:bold;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-cfd14200{font-family:'DejaVu Sans';font-size:12pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-cfd15646{margin:0;text-align:center;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-cfd19bc4{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-cfd19bd8{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-cfd19be2{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-cfd19bec{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-cfd19bed{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-cfd19bf6{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-cfd19c00{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-cfd19c0a{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-cfd19c14{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-cfd19c1e{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-cfd19c1f{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-cfd19c28{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}</style><table class='cl-cfd6ecbe'>\n```\n<caption class=\"Table Caption\">\n\nFirst 15 rows of the model fitting summary table.\n\n</caption>\n```{=html}\n<thead><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-cfd19c28\"><p class=\"cl-cfd15646\"><span class=\"cl-cfd141f6\">Model</span></p></td><td class=\"cl-cfd19c1e\"><p class=\"cl-cfd15646\"><span class=\"cl-cfd141f6\">Term Added</span></p></td><td class=\"cl-cfd19c1e\"><p class=\"cl-cfd15646\"><span class=\"cl-cfd141f6\">Compared to...</span></p></td><td class=\"cl-cfd19c1e\"><p class=\"cl-cfd15646\"><span class=\"cl-cfd141f6\">DF</span></p></td><td class=\"cl-cfd19c1e\"><p class=\"cl-cfd15646\"><span class=\"cl-cfd141f6\">AIC</span></p></td><td class=\"cl-cfd19c1e\"><p class=\"cl-cfd15646\"><span class=\"cl-cfd141f6\">BIC</span></p></td><td class=\"cl-cfd19c1e\"><p class=\"cl-cfd15646\"><span class=\"cl-cfd141f6\">LogLikelihood</span></p></td><td class=\"cl-cfd19c1e\"><p class=\"cl-cfd15646\"><span class=\"cl-cfd141f6\">Residual Deviance</span></p></td><td class=\"cl-cfd19c1e\"><p class=\"cl-cfd15646\"><span class=\"cl-cfd141f6\">X2</span></p></td><td class=\"cl-cfd19c1e\"><p class=\"cl-cfd15646\"><span class=\"cl-cfd141f6\">X2DF</span></p></td><td class=\"cl-cfd19c1e\"><p class=\"cl-cfd15646\"><span class=\"cl-cfd141f6\">p-value</span></p></td><td class=\"cl-cfd19c1f\"><p class=\"cl-cfd15646\"><span class=\"cl-cfd141f6\">Significance</span></p></td></tr></thead><tbody><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-cfd19be2\"><p class=\"cl-cfd15646\"><span class=\"cl-cfd14200\">m1.glmer</span></p></td><td class=\"cl-cfd19bd8\"><p class=\"cl-cfd15646\"><span class=\"cl-cfd14200\">1+Priming</span></p></td><td class=\"cl-cfd19bd8\"><p class=\"cl-cfd15646\"><span class=\"cl-cfd14200\">m0.glmer</span></p></td><td class=\"cl-cfd19bd8\"><p class=\"cl-cfd15646\"><span class=\"cl-cfd14200\">3</span></p></td><td class=\"cl-cfd19bd8\"><p class=\"cl-cfd15646\"><span class=\"cl-cfd14200\">1702.77</span></p></td><td class=\"cl-cfd19bd8\"><p class=\"cl-cfd15646\"><span class=\"cl-cfd14200\">1719.58</span></p></td><td class=\"cl-cfd19bd8\"><p class=\"cl-cfd15646\"><span class=\"cl-cfd14200\">-848.39</span></p></td><td class=\"cl-cfd19bd8\"><p class=\"cl-cfd15646\"><span class=\"cl-cfd14200\">1696.77</span></p></td><td class=\"cl-cfd19bd8\"><p class=\"cl-cfd15646\"><span class=\"cl-cfd14200\">127.72</span></p></td><td class=\"cl-cfd19bd8\"><p class=\"cl-cfd15646\"><span class=\"cl-cfd14200\">1</span></p></td><td class=\"cl-cfd19bd8\"><p class=\"cl-cfd15646\"><span class=\"cl-cfd14200\">0</span></p></td><td class=\"cl-cfd19bc4\"><p class=\"cl-cfd15646\"><span class=\"cl-cfd14200\">p &lt; .001***</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-cfd19bed\"><p class=\"cl-cfd15646\"><span class=\"cl-cfd14200\">m2.glmer</span></p></td><td class=\"cl-cfd19bec\"><p class=\"cl-cfd15646\"><span class=\"cl-cfd14200\">Age</span></p></td><td class=\"cl-cfd19bec\"><p class=\"cl-cfd15646\"><span class=\"cl-cfd14200\">m1.glmer</span></p></td><td class=\"cl-cfd19bec\"><p class=\"cl-cfd15646\"><span class=\"cl-cfd14200\">4</span></p></td><td class=\"cl-cfd19bec\"><p class=\"cl-cfd15646\"><span class=\"cl-cfd14200\">1704.21</span></p></td><td class=\"cl-cfd19bec\"><p class=\"cl-cfd15646\"><span class=\"cl-cfd14200\">1726.61</span></p></td><td class=\"cl-cfd19bec\"><p class=\"cl-cfd15646\"><span class=\"cl-cfd14200\">-848.11</span></p></td><td class=\"cl-cfd19bec\"><p class=\"cl-cfd15646\"><span class=\"cl-cfd14200\">1696.21</span></p></td><td class=\"cl-cfd19bec\"><p class=\"cl-cfd15646\"><span class=\"cl-cfd14200\">0.56</span></p></td><td class=\"cl-cfd19bec\"><p class=\"cl-cfd15646\"><span class=\"cl-cfd14200\">1</span></p></td><td class=\"cl-cfd19bec\"><p class=\"cl-cfd15646\"><span class=\"cl-cfd14200\">0.45323</span></p></td><td class=\"cl-cfd19bf6\"><p class=\"cl-cfd15646\"><span class=\"cl-cfd14200\">n.s.</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-cfd19be2\"><p class=\"cl-cfd15646\"><span class=\"cl-cfd14200\">m3.glmer</span></p></td><td class=\"cl-cfd19bd8\"><p class=\"cl-cfd15646\"><span class=\"cl-cfd14200\">Gender</span></p></td><td class=\"cl-cfd19bd8\"><p class=\"cl-cfd15646\"><span class=\"cl-cfd14200\">m1.glmer</span></p></td><td class=\"cl-cfd19bd8\"><p class=\"cl-cfd15646\"><span class=\"cl-cfd14200\">4</span></p></td><td class=\"cl-cfd19bd8\"><p class=\"cl-cfd15646\"><span class=\"cl-cfd14200\">1679.4</span></p></td><td class=\"cl-cfd19bd8\"><p class=\"cl-cfd15646\"><span class=\"cl-cfd14200\">1701.8</span></p></td><td class=\"cl-cfd19bd8\"><p class=\"cl-cfd15646\"><span class=\"cl-cfd14200\">-835.7</span></p></td><td class=\"cl-cfd19bd8\"><p class=\"cl-cfd15646\"><span class=\"cl-cfd14200\">1671.4</span></p></td><td class=\"cl-cfd19bd8\"><p class=\"cl-cfd15646\"><span class=\"cl-cfd14200\">25.38</span></p></td><td class=\"cl-cfd19bd8\"><p class=\"cl-cfd15646\"><span class=\"cl-cfd14200\">1</span></p></td><td class=\"cl-cfd19bd8\"><p class=\"cl-cfd15646\"><span class=\"cl-cfd14200\">0</span></p></td><td class=\"cl-cfd19bc4\"><p class=\"cl-cfd15646\"><span class=\"cl-cfd14200\">p &lt; .001***</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-cfd19bed\"><p class=\"cl-cfd15646\"><span class=\"cl-cfd14200\">m4.glmer</span></p></td><td class=\"cl-cfd19bec\"><p class=\"cl-cfd15646\"><span class=\"cl-cfd14200\">ConversationType</span></p></td><td class=\"cl-cfd19bec\"><p class=\"cl-cfd15646\"><span class=\"cl-cfd14200\">m3.glmer</span></p></td><td class=\"cl-cfd19bec\"><p class=\"cl-cfd15646\"><span class=\"cl-cfd14200\">5</span></p></td><td class=\"cl-cfd19bec\"><p class=\"cl-cfd15646\"><span class=\"cl-cfd14200\">1668.58</span></p></td><td class=\"cl-cfd19bec\"><p class=\"cl-cfd15646\"><span class=\"cl-cfd14200\">1696.59</span></p></td><td class=\"cl-cfd19bec\"><p class=\"cl-cfd15646\"><span class=\"cl-cfd14200\">-829.29</span></p></td><td class=\"cl-cfd19bec\"><p class=\"cl-cfd15646\"><span class=\"cl-cfd14200\">1658.58</span></p></td><td class=\"cl-cfd19bec\"><p class=\"cl-cfd15646\"><span class=\"cl-cfd14200\">12.81</span></p></td><td class=\"cl-cfd19bec\"><p class=\"cl-cfd15646\"><span class=\"cl-cfd14200\">1</span></p></td><td class=\"cl-cfd19bec\"><p class=\"cl-cfd15646\"><span class=\"cl-cfd14200\">0.00034</span></p></td><td class=\"cl-cfd19bf6\"><p class=\"cl-cfd15646\"><span class=\"cl-cfd14200\">p &lt; .001***</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-cfd19be2\"><p class=\"cl-cfd15646\"><span class=\"cl-cfd14200\">m5.glmer</span></p></td><td class=\"cl-cfd19bd8\"><p class=\"cl-cfd15646\"><span class=\"cl-cfd14200\">Age+Priming:Age</span></p></td><td class=\"cl-cfd19bd8\"><p class=\"cl-cfd15646\"><span class=\"cl-cfd14200\">m4.glmer</span></p></td><td class=\"cl-cfd19bd8\"><p class=\"cl-cfd15646\"><span class=\"cl-cfd14200\">7</span></p></td><td class=\"cl-cfd19bd8\"><p class=\"cl-cfd15646\"><span class=\"cl-cfd14200\">1671.6</span></p></td><td class=\"cl-cfd19bd8\"><p class=\"cl-cfd15646\"><span class=\"cl-cfd14200\">1710.81</span></p></td><td class=\"cl-cfd19bd8\"><p class=\"cl-cfd15646\"><span class=\"cl-cfd14200\">-828.8</span></p></td><td class=\"cl-cfd19bd8\"><p class=\"cl-cfd15646\"><span class=\"cl-cfd14200\">1657.6</span></p></td><td class=\"cl-cfd19bd8\"><p class=\"cl-cfd15646\"><span class=\"cl-cfd14200\">0.98</span></p></td><td class=\"cl-cfd19bd8\"><p class=\"cl-cfd15646\"><span class=\"cl-cfd14200\">2</span></p></td><td class=\"cl-cfd19bd8\"><p class=\"cl-cfd15646\"><span class=\"cl-cfd14200\">0.61134</span></p></td><td class=\"cl-cfd19bc4\"><p class=\"cl-cfd15646\"><span class=\"cl-cfd14200\">n.s.</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-cfd19bed\"><p class=\"cl-cfd15646\"><span class=\"cl-cfd14200\">m6.glmer</span></p></td><td class=\"cl-cfd19bec\"><p class=\"cl-cfd15646\"><span class=\"cl-cfd14200\">Priming:Gender</span></p></td><td class=\"cl-cfd19bec\"><p class=\"cl-cfd15646\"><span class=\"cl-cfd14200\">m4.glmer</span></p></td><td class=\"cl-cfd19bec\"><p class=\"cl-cfd15646\"><span class=\"cl-cfd14200\">6</span></p></td><td class=\"cl-cfd19bec\"><p class=\"cl-cfd15646\"><span class=\"cl-cfd14200\">1663.16</span></p></td><td class=\"cl-cfd19bec\"><p class=\"cl-cfd15646\"><span class=\"cl-cfd14200\">1696.77</span></p></td><td class=\"cl-cfd19bec\"><p class=\"cl-cfd15646\"><span class=\"cl-cfd14200\">-825.58</span></p></td><td class=\"cl-cfd19bec\"><p class=\"cl-cfd15646\"><span class=\"cl-cfd14200\">1651.16</span></p></td><td class=\"cl-cfd19bec\"><p class=\"cl-cfd15646\"><span class=\"cl-cfd14200\">7.42</span></p></td><td class=\"cl-cfd19bec\"><p class=\"cl-cfd15646\"><span class=\"cl-cfd14200\">1</span></p></td><td class=\"cl-cfd19bec\"><p class=\"cl-cfd15646\"><span class=\"cl-cfd14200\">0.00645</span></p></td><td class=\"cl-cfd19bf6\"><p class=\"cl-cfd15646\"><span class=\"cl-cfd14200\">p &lt;  .01 **</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-cfd19be2\"><p class=\"cl-cfd15646\"><span class=\"cl-cfd14200\">m7.glmer</span></p></td><td class=\"cl-cfd19bd8\"><p class=\"cl-cfd15646\"><span class=\"cl-cfd14200\">Priming:ConversationType</span></p></td><td class=\"cl-cfd19bd8\"><p class=\"cl-cfd15646\"><span class=\"cl-cfd14200\">m6.glmer</span></p></td><td class=\"cl-cfd19bd8\"><p class=\"cl-cfd15646\"><span class=\"cl-cfd14200\">7</span></p></td><td class=\"cl-cfd19bd8\"><p class=\"cl-cfd15646\"><span class=\"cl-cfd14200\">1663.13</span></p></td><td class=\"cl-cfd19bd8\"><p class=\"cl-cfd15646\"><span class=\"cl-cfd14200\">1702.34</span></p></td><td class=\"cl-cfd19bd8\"><p class=\"cl-cfd15646\"><span class=\"cl-cfd14200\">-824.57</span></p></td><td class=\"cl-cfd19bd8\"><p class=\"cl-cfd15646\"><span class=\"cl-cfd14200\">1649.13</span></p></td><td class=\"cl-cfd19bd8\"><p class=\"cl-cfd15646\"><span class=\"cl-cfd14200\">2.03</span></p></td><td class=\"cl-cfd19bd8\"><p class=\"cl-cfd15646\"><span class=\"cl-cfd14200\">1</span></p></td><td class=\"cl-cfd19bd8\"><p class=\"cl-cfd15646\"><span class=\"cl-cfd14200\">0.15427</span></p></td><td class=\"cl-cfd19bc4\"><p class=\"cl-cfd15646\"><span class=\"cl-cfd14200\">n.s.</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-cfd19bed\"><p class=\"cl-cfd15646\"><span class=\"cl-cfd14200\">m8.glmer</span></p></td><td class=\"cl-cfd19bec\"><p class=\"cl-cfd15646\"><span class=\"cl-cfd14200\">Age+Gender:Age</span></p></td><td class=\"cl-cfd19bec\"><p class=\"cl-cfd15646\"><span class=\"cl-cfd14200\">m6.glmer</span></p></td><td class=\"cl-cfd19bec\"><p class=\"cl-cfd15646\"><span class=\"cl-cfd14200\">8</span></p></td><td class=\"cl-cfd19bec\"><p class=\"cl-cfd15646\"><span class=\"cl-cfd14200\">1665.63</span></p></td><td class=\"cl-cfd19bec\"><p class=\"cl-cfd15646\"><span class=\"cl-cfd14200\">1710.44</span></p></td><td class=\"cl-cfd19bec\"><p class=\"cl-cfd15646\"><span class=\"cl-cfd14200\">-824.82</span></p></td><td class=\"cl-cfd19bec\"><p class=\"cl-cfd15646\"><span class=\"cl-cfd14200\">1649.63</span></p></td><td class=\"cl-cfd19bec\"><p class=\"cl-cfd15646\"><span class=\"cl-cfd14200\">1.53</span></p></td><td class=\"cl-cfd19bec\"><p class=\"cl-cfd15646\"><span class=\"cl-cfd14200\">2</span></p></td><td class=\"cl-cfd19bec\"><p class=\"cl-cfd15646\"><span class=\"cl-cfd14200\">0.46535</span></p></td><td class=\"cl-cfd19bf6\"><p class=\"cl-cfd15646\"><span class=\"cl-cfd14200\">n.s.</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-cfd19c14\"><p class=\"cl-cfd15646\"><span class=\"cl-cfd14200\">m9.glmer</span></p></td><td class=\"cl-cfd19c00\"><p class=\"cl-cfd15646\"><span class=\"cl-cfd14200\">Age+ConversationType:Age</span></p></td><td class=\"cl-cfd19c00\"><p class=\"cl-cfd15646\"><span class=\"cl-cfd14200\">m6.glmer</span></p></td><td class=\"cl-cfd19c00\"><p class=\"cl-cfd15646\"><span class=\"cl-cfd14200\">8</span></p></td><td class=\"cl-cfd19c00\"><p class=\"cl-cfd15646\"><span class=\"cl-cfd14200\">1666.26</span></p></td><td class=\"cl-cfd19c00\"><p class=\"cl-cfd15646\"><span class=\"cl-cfd14200\">1711.07</span></p></td><td class=\"cl-cfd19c00\"><p class=\"cl-cfd15646\"><span class=\"cl-cfd14200\">-825.13</span></p></td><td class=\"cl-cfd19c00\"><p class=\"cl-cfd15646\"><span class=\"cl-cfd14200\">1650.26</span></p></td><td class=\"cl-cfd19c00\"><p class=\"cl-cfd15646\"><span class=\"cl-cfd14200\">0.9</span></p></td><td class=\"cl-cfd19c00\"><p class=\"cl-cfd15646\"><span class=\"cl-cfd14200\">2</span></p></td><td class=\"cl-cfd19c00\"><p class=\"cl-cfd15646\"><span class=\"cl-cfd14200\">0.63689</span></p></td><td class=\"cl-cfd19c0a\"><p class=\"cl-cfd15646\"><span class=\"cl-cfd14200\">n.s.</span></p></td></tr></tbody></table></div>\n```\n:::\n:::\n\n\nWe now rename our final minimal adequate model, test whether it performs significantly better than the minimal base-line model, and print the regression summary.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# rename final minimal adequate model\nmlr.glmer <- m6.glmer \n# final model better than base-line model\nsigfit <- anova(mlr.glmer, m0.glmer, test = \"Chi\") \n# inspect\nsigfit\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nData: mblrdata\nModels:\nm0.glmer: SUFlike ~ 1 + (1 | ID)\nmlr.glmer: SUFlike ~ (1 | ID) + Priming + Gender + ConversationType + Priming:Gender\n          npar         AIC         BIC       logLik    deviance     Chisq Df\nm0.glmer     2 1828.492271 1839.694076 -912.2461355 1824.492271             \nmlr.glmer    6 1663.164481 1696.769896 -825.5822404 1651.164481 173.32779  4\n                      Pr(>Chisq)    \nm0.glmer                            \nmlr.glmer < 0.000000000000000222 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# inspect final minimal adequate model\nprint(mlr.glmer, corr = F)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: \nSUFlike ~ (1 | ID) + Priming + Gender + ConversationType + Priming:Gender\n   Data: mblrdata\n      AIC       BIC    logLik  deviance  df.resid \n1663.1645 1696.7699 -825.5822 1651.1645      1994 \nRandom effects:\n Groups Name        Std.Dev.   \n ID     (Intercept) 0.292489586\nNumber of obs: 2000, groups:  ID, 208\nFixed Effects:\n               (Intercept)                PrimingPrime  \n              -0.920849062                 1.060138443  \n               GenderWomen  ConversationTypeSameGender  \n              -0.867735748                -0.492309646  \n  PrimingPrime:GenderWomen  \n               1.035741994  \n```\n:::\n:::\n\n\nTo extract the effect sizes of the significant fixed effects, we compare the model with that effect to a model without that effect. This can be problematic when checking the effect of main effects that are involved in significant interactions though [@field2012discovering 622].\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# effect of ConversationType\nef_conv <- anova(m4.glmer, m3.glmer, test = \"Chi\") \n# inspect\nef_conv\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nData: mblrdata\nModels:\nm3.glmer: SUFlike ~ (1 | ID) + Priming + Gender\nm4.glmer: SUFlike ~ (1 | ID) + Priming + Gender + ConversationType\n         npar         AIC         BIC       logLik    deviance    Chisq Df\nm3.glmer    4 1679.397070 1701.800680 -835.6985349 1671.397070            \nm4.glmer    5 1668.583222 1696.587734 -829.2916108 1658.583222 12.81385  1\n         Pr(>Chisq)    \nm3.glmer               \nm4.glmer 0.00034406 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# effect of Priming:Gender\nef_prigen <- anova(m6.glmer, m4.glmer, test = \"Chi\")\n# inspect\nef_prigen\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nData: mblrdata\nModels:\nm4.glmer: SUFlike ~ (1 | ID) + Priming + Gender + ConversationType\nm6.glmer: SUFlike ~ (1 | ID) + Priming + Gender + ConversationType + Priming:Gender\n         npar         AIC         BIC       logLik    deviance   Chisq Df\nm4.glmer    5 1668.583222 1696.587734 -829.2916108 1658.583222           \nm6.glmer    6 1663.164481 1696.769896 -825.5822404 1651.164481 7.41874  1\n         Pr(>Chisq)   \nm4.glmer              \nm6.glmer  0.0064548 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\n### Visualizing Effects{-}\n\nAs we will see the effects in the final summary, we visualize the effects here by showing the probability of discourse *like* based on the predicted values.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# extract predicted values\nmblrdata$Predicted <- predict(m6.glmer, mblrdata, type = \"response\")\n# plot\nggplot(mblrdata, aes(ConversationType, Predicted)) +\n  stat_summary(fun = mean, geom = \"point\") +\n  stat_summary(fun.data = mean_cl_boot, geom = \"errorbar\", width = 0.2) +\n  theme_set(theme_bw(base_size = 10)) +\n  theme(legend.position = \"top\") +\n    ylim(0, .75) +\n  labs(x = \"\", y = \"Predicted Probabilty of discourse like\") +\n  scale_color_manual(values = c(\"gray20\", \"gray70\"))\n```\n\n::: {.cell-output-display}\n![](regression_files/figure-html/blmm33-1.png){width=672}\n:::\n:::\n\n\nA proper visualization of the marginal effects can be extracted using the `sjPlot` package [@sjPlot].\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_model(m6.glmer, type = \"pred\", terms = c(\"Priming\", \"Gender\"))\n```\n\n::: {.cell-output-display}\n![](regression_files/figure-html/blmm33b-1.png){width=672}\n:::\n:::\n\n\nWe can see that discourse like is more likely to surface in primed contexts but that in contrast to women and men in same-gender conversations as well as women in mixed-gender conversations, priming appears to affect the use of discourse like by men in mixed-gender conversations only very little. \n\n### Extracting Model Fit Parameters{-}\n\nWe now  extract model fit parameters [@baayen2008analyzing 281].\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprobs = 1/(1+exp(-fitted(mlr.glmer)))\nprobs = binomial()$linkinv(fitted(mlr.glmer))\nsomers2(probs, as.numeric(mblrdata$SUFlike))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                C               Dxy                 n           Missing \n   0.758332104539    0.516664209078 2000.000000000000    0.000000000000 \n```\n:::\n:::\n\n\nThe two lines that start with `probs` are simply two different ways to do the same thing (you only need one of these).\n\nThe model fit parameters indicate a suboptimal fit. Both the C-value and Somers's D~xy~ show poor fit between predicted and observed occurrences of discourse *like*.  If the C-value is 0.5, the predictions are random, while the predictions are perfect if the C-value is 1. C-values above 0.8 indicate real predictive capacity [@baayen2008analyzing 204]. Somersâ€™ D~xy~ is a value that represents a rank correlation between predicted probabilities and observed responses. Somersâ€™ D~xy~ values range between 0, which indicates complete randomness, and 1, which indicates perfect prediction [@baayen2008analyzing 204]. The C.value of 0.758332104539 suggests that the model has some predictive and explanatory power, but not at an optimal level. We will now perform the model diagnostics.\n\n### Model Diagnostics{-}\n\nWe begin the model diagnostics by generating a diagnostic that plots the fitted or predicted values against the residuals.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(mlr.glmer, pch = 20, col = \"black\", lty = \"dotted\")\n```\n\n::: {.cell-output-display}\n![](regression_files/figure-html/blmm38-1.png){width=672}\n:::\n:::\n\n\nAs a final step, we summarize our findings in tabulated form.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# summarize final model\nsjPlot::tab_model(mlr.glmer)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table style=\"border-collapse:collapse; border:none;\">\n<tr>\n<th style=\"border-top: double; text-align:center; font-style:normal; font-weight:bold; padding:0.2cm;  text-align:left; \">&nbsp;</th>\n<th colspan=\"3\" style=\"border-top: double; text-align:center; font-style:normal; font-weight:bold; padding:0.2cm; \">SUFlike</th>\n</tr>\n<tr>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  text-align:left; \">Predictors</td>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  \">Odds Ratios</td>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  \">CI</td>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  \">p</td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; \">(Intercept)</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.40</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.29&nbsp;&ndash;&nbsp;0.54</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"><strong>&lt;0.001</strong></td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; \">Priming [Prime]</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">2.89</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">1.50&nbsp;&ndash;&nbsp;5.56</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"><strong>0.002</strong></td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; \">Gender [Women]</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.42</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.29&nbsp;&ndash;&nbsp;0.61</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"><strong>&lt;0.001</strong></td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; \">ConversationType<br>[SameGender]</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.61</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.46&nbsp;&ndash;&nbsp;0.82</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"><strong>0.001</strong></td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; \">Priming [Prime] * Gender<br>[Women]</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">2.82</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">1.34&nbsp;&ndash;&nbsp;5.93</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"><strong>0.006</strong></td>\n</tr>\n<tr>\n<td colspan=\"4\" style=\"font-weight:bold; text-align:left; padding-top:.8em;\">Random Effects</td>\n</tr>\n\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; padding-top:0.1cm; padding-bottom:0.1cm;\">&sigma;<sup>2</sup></td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left;\" colspan=\"3\">3.29</td>\n</tr>\n\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; padding-top:0.1cm; padding-bottom:0.1cm;\">&tau;<sub>00</sub> <sub>ID</sub></td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left;\" colspan=\"3\">0.09</td>\n\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; padding-top:0.1cm; padding-bottom:0.1cm;\">ICC</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left;\" colspan=\"3\">0.03</td>\n\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; padding-top:0.1cm; padding-bottom:0.1cm;\">N <sub>ID</sub></td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left;\" colspan=\"3\">208</td>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; padding-top:0.1cm; padding-bottom:0.1cm; border-top:1px solid;\">Observations</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left; border-top:1px solid;\" colspan=\"3\">2000</td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; padding-top:0.1cm; padding-bottom:0.1cm;\">Marginal R<sup>2</sup> / Conditional R<sup>2</sup></td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left;\" colspan=\"3\">0.138 / 0.160</td>\n</tr>\n\n</table>\n\n`````\n:::\n:::\n\n\n***\n\nWe can use the `reports` package [@report] to summarize the analysis.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nreport::report(mlr.glmer)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nWe fitted a logistic mixed model (estimated using ML and BOBYQA optimizer) to predict SUFlike with Priming, Gender and ConversationType (formula: SUFlike ~ Priming + Gender + ConversationType + Priming:Gender). The model included ID as random effect (formula: ~1 | ID). The model's total explanatory power is moderate (conditional R2 = 0.16) and the part related to the fixed effects alone (marginal R2) is of 0.14. The model's intercept, corresponding to Priming = NoPrime, Gender = Men and ConversationType = MixedGender, is at -0.92 (95% CI [-1.22, -0.62], p < .001). Within this model:\n\n  - The effect of Priming [Prime] is statistically significant and positive (beta = 1.06, 95% CI [0.41, 1.71], p = 0.002; Std. beta = 1.06, 95% CI [0.41, 1.71])\n  - The effect of Gender [Women] is statistically significant and negative (beta = -0.87, 95% CI [-1.25, -0.49], p < .001; Std. beta = -0.87, 95% CI [-1.25, -0.49])\n  - The effect of ConversationType [SameGender] is statistically significant and negative (beta = -0.49, 95% CI [-0.79, -0.20], p = 0.001; Std. beta = -0.49, 95% CI [-0.79, -0.20])\n  - The interaction effect of Gender [Women] on Priming [Prime] is statistically significant and positive (beta = 1.04, 95% CI [0.29, 1.78], p = 0.006; Std. beta = 1.04, 95% CI [0.29, 1.78])\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using the Wald approximation.\n```\n:::\n:::\n\n\nWe can use this output to write up a final report: \n\n\nA mixed-effect binomial logistic regression model with random intercepts for speakers was fit to the data in a step-wise-step up procedure. The final minimal adequate model performed significantly better than an intercept-only base line model ($\\chi$^2^(4): 173.327790313894, p  = 0) and a good but not optimal fit (C: 0.758332104539, Somers' D~xy~: 0.516664209078). The final minimal adequate model reported that speakers use more discourse *like* in mixed-gender conversations compared to same-gender conversations ($\\chi$^2^(1): `r 12.813848196363, p  = 0.00034) and that there is an interaction between priming and gender with men using more discourse *like* in un-primed contexts while this gender difference is not present in primed contexts where speakers more more likely to use discourse *like* regardless of gender ($\\chi$^2^(1): 7.418740944005, p  = 0.00645). \n\n## Mixed-Effects (Quasi-)Poisson and Negative-Binomial Regression{-}\n\nLike fixed-effects Poisson models, mixed-effects Poisson models take counts as dependent variables. The data for this analysis was collected on three separate evenings (Trial). The number of the filler *uhm* (UHM) was counted in two-minute conversations that were either in English, German, Russian, or Mandarin (Language). In addition, the number of shots that speakers drank before they talked was recorded (Shots).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# load data\ncountdata  <- base::readRDS(url(\"https://slcladal.github.io/data/cld.rda\", \"rb\"))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# inspect data\ncountdata %>%\n  as.data.frame() %>%\n  head(15) %>%\n  flextable() %>%\n  flextable::set_table_properties(width = .75, layout = \"autofit\") %>%\n  flextable::theme_zebra() %>%\n  flextable::fontsize(size = 12) %>%\n  flextable::fontsize(size = 12, part = \"header\") %>%\n  flextable::align_text_col(align = \"center\") %>%\n  flextable::set_caption(caption = \"First 15 rows of the countdata data.\")  %>%\n  flextable::border_outer()\n```\n\n::: {.cell-output-display}\n```{=html}\n<div class=\"tabwid\"><style>.cl-d1f64ce2{table-layout:auto;width:75%;}.cl-d1f2e610{font-family:'DejaVu Sans';font-size:12pt;font-weight:bold;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-d1f2e61a{font-family:'DejaVu Sans';font-size:12pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-d1f2ee76{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-d1f2ee80{margin:0;text-align:center;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-d1f31496{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d1f314a0{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d1f314a1{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d1f314aa{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d1f314b4{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d1f314be{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d1f314c8{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d1f314c9{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d1f314ca{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d1f314d2{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d1f314d3{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d1f314dc{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d1f314dd{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d1f314de{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d1f314e6{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d1f314e7{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}</style><table class='cl-d1f64ce2'>\n```\n<caption class=\"Table Caption\">\n\nFirst 15 rows of the countdata data.\n\n</caption>\n```{=html}\n<thead><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-d1f314e7\"><p class=\"cl-d1f2ee76\"><span class=\"cl-d1f2e610\">ID</span></p></td><td class=\"cl-d1f314e6\"><p class=\"cl-d1f2ee76\"><span class=\"cl-d1f2e610\">Trial</span></p></td><td class=\"cl-d1f314dd\"><p class=\"cl-d1f2ee80\"><span class=\"cl-d1f2e610\">Language</span></p></td><td class=\"cl-d1f314dd\"><p class=\"cl-d1f2ee80\"><span class=\"cl-d1f2e610\">Gender</span></p></td><td class=\"cl-d1f314e6\"><p class=\"cl-d1f2ee76\"><span class=\"cl-d1f2e610\">UHM</span></p></td><td class=\"cl-d1f314de\"><p class=\"cl-d1f2ee76\"><span class=\"cl-d1f2e610\">Shots</span></p></td></tr></thead><tbody><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-d1f314a0\"><p class=\"cl-d1f2ee76\"><span class=\"cl-d1f2e61a\">1</span></p></td><td class=\"cl-d1f314aa\"><p class=\"cl-d1f2ee76\"><span class=\"cl-d1f2e61a\">3</span></p></td><td class=\"cl-d1f314a1\"><p class=\"cl-d1f2ee80\"><span class=\"cl-d1f2e61a\">Russian</span></p></td><td class=\"cl-d1f314a1\"><p class=\"cl-d1f2ee80\"><span class=\"cl-d1f2e61a\">Man</span></p></td><td class=\"cl-d1f314aa\"><p class=\"cl-d1f2ee76\"><span class=\"cl-d1f2e61a\">0</span></p></td><td class=\"cl-d1f31496\"><p class=\"cl-d1f2ee76\"><span class=\"cl-d1f2e61a\">0</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-d1f314be\"><p class=\"cl-d1f2ee76\"><span class=\"cl-d1f2e61a\">2</span></p></td><td class=\"cl-d1f314c8\"><p class=\"cl-d1f2ee76\"><span class=\"cl-d1f2e61a\">3</span></p></td><td class=\"cl-d1f314c9\"><p class=\"cl-d1f2ee80\"><span class=\"cl-d1f2e61a\">Russian</span></p></td><td class=\"cl-d1f314c9\"><p class=\"cl-d1f2ee80\"><span class=\"cl-d1f2e61a\">Man</span></p></td><td class=\"cl-d1f314c8\"><p class=\"cl-d1f2ee76\"><span class=\"cl-d1f2e61a\">0</span></p></td><td class=\"cl-d1f314b4\"><p class=\"cl-d1f2ee76\"><span class=\"cl-d1f2e61a\">0</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-d1f314a0\"><p class=\"cl-d1f2ee76\"><span class=\"cl-d1f2e61a\">3</span></p></td><td class=\"cl-d1f314aa\"><p class=\"cl-d1f2ee76\"><span class=\"cl-d1f2e61a\">3</span></p></td><td class=\"cl-d1f314a1\"><p class=\"cl-d1f2ee80\"><span class=\"cl-d1f2e61a\">German</span></p></td><td class=\"cl-d1f314a1\"><p class=\"cl-d1f2ee80\"><span class=\"cl-d1f2e61a\">Man</span></p></td><td class=\"cl-d1f314aa\"><p class=\"cl-d1f2ee76\"><span class=\"cl-d1f2e61a\">0</span></p></td><td class=\"cl-d1f31496\"><p class=\"cl-d1f2ee76\"><span class=\"cl-d1f2e61a\">5</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-d1f314be\"><p class=\"cl-d1f2ee76\"><span class=\"cl-d1f2e61a\">4</span></p></td><td class=\"cl-d1f314c8\"><p class=\"cl-d1f2ee76\"><span class=\"cl-d1f2e61a\">1</span></p></td><td class=\"cl-d1f314c9\"><p class=\"cl-d1f2ee80\"><span class=\"cl-d1f2e61a\">German</span></p></td><td class=\"cl-d1f314c9\"><p class=\"cl-d1f2ee80\"><span class=\"cl-d1f2e61a\">Man</span></p></td><td class=\"cl-d1f314c8\"><p class=\"cl-d1f2ee76\"><span class=\"cl-d1f2e61a\">0</span></p></td><td class=\"cl-d1f314b4\"><p class=\"cl-d1f2ee76\"><span class=\"cl-d1f2e61a\">3</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-d1f314a0\"><p class=\"cl-d1f2ee76\"><span class=\"cl-d1f2e61a\">5</span></p></td><td class=\"cl-d1f314aa\"><p class=\"cl-d1f2ee76\"><span class=\"cl-d1f2e61a\">1</span></p></td><td class=\"cl-d1f314a1\"><p class=\"cl-d1f2ee80\"><span class=\"cl-d1f2e61a\">German</span></p></td><td class=\"cl-d1f314a1\"><p class=\"cl-d1f2ee80\"><span class=\"cl-d1f2e61a\">Woman</span></p></td><td class=\"cl-d1f314aa\"><p class=\"cl-d1f2ee76\"><span class=\"cl-d1f2e61a\">2</span></p></td><td class=\"cl-d1f31496\"><p class=\"cl-d1f2ee76\"><span class=\"cl-d1f2e61a\">6</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-d1f314be\"><p class=\"cl-d1f2ee76\"><span class=\"cl-d1f2e61a\">6</span></p></td><td class=\"cl-d1f314c8\"><p class=\"cl-d1f2ee76\"><span class=\"cl-d1f2e61a\">3</span></p></td><td class=\"cl-d1f314c9\"><p class=\"cl-d1f2ee80\"><span class=\"cl-d1f2e61a\">German</span></p></td><td class=\"cl-d1f314c9\"><p class=\"cl-d1f2ee80\"><span class=\"cl-d1f2e61a\">Man</span></p></td><td class=\"cl-d1f314c8\"><p class=\"cl-d1f2ee76\"><span class=\"cl-d1f2e61a\">1</span></p></td><td class=\"cl-d1f314b4\"><p class=\"cl-d1f2ee76\"><span class=\"cl-d1f2e61a\">5</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-d1f314a0\"><p class=\"cl-d1f2ee76\"><span class=\"cl-d1f2e61a\">7</span></p></td><td class=\"cl-d1f314aa\"><p class=\"cl-d1f2ee76\"><span class=\"cl-d1f2e61a\">1</span></p></td><td class=\"cl-d1f314a1\"><p class=\"cl-d1f2ee80\"><span class=\"cl-d1f2e61a\">Mandarin</span></p></td><td class=\"cl-d1f314a1\"><p class=\"cl-d1f2ee80\"><span class=\"cl-d1f2e61a\">Man</span></p></td><td class=\"cl-d1f314aa\"><p class=\"cl-d1f2ee76\"><span class=\"cl-d1f2e61a\">1</span></p></td><td class=\"cl-d1f31496\"><p class=\"cl-d1f2ee76\"><span class=\"cl-d1f2e61a\">1</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-d1f314be\"><p class=\"cl-d1f2ee76\"><span class=\"cl-d1f2e61a\">8</span></p></td><td class=\"cl-d1f314c8\"><p class=\"cl-d1f2ee76\"><span class=\"cl-d1f2e61a\">3</span></p></td><td class=\"cl-d1f314c9\"><p class=\"cl-d1f2ee80\"><span class=\"cl-d1f2e61a\">German</span></p></td><td class=\"cl-d1f314c9\"><p class=\"cl-d1f2ee80\"><span class=\"cl-d1f2e61a\">Woman</span></p></td><td class=\"cl-d1f314c8\"><p class=\"cl-d1f2ee76\"><span class=\"cl-d1f2e61a\">0</span></p></td><td class=\"cl-d1f314b4\"><p class=\"cl-d1f2ee76\"><span class=\"cl-d1f2e61a\">4</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-d1f314a0\"><p class=\"cl-d1f2ee76\"><span class=\"cl-d1f2e61a\">9</span></p></td><td class=\"cl-d1f314aa\"><p class=\"cl-d1f2ee76\"><span class=\"cl-d1f2e61a\">3</span></p></td><td class=\"cl-d1f314a1\"><p class=\"cl-d1f2ee80\"><span class=\"cl-d1f2e61a\">Russian</span></p></td><td class=\"cl-d1f314a1\"><p class=\"cl-d1f2ee80\"><span class=\"cl-d1f2e61a\">Woman</span></p></td><td class=\"cl-d1f314aa\"><p class=\"cl-d1f2ee76\"><span class=\"cl-d1f2e61a\">0</span></p></td><td class=\"cl-d1f31496\"><p class=\"cl-d1f2ee76\"><span class=\"cl-d1f2e61a\">0</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-d1f314be\"><p class=\"cl-d1f2ee76\"><span class=\"cl-d1f2e61a\">10</span></p></td><td class=\"cl-d1f314c8\"><p class=\"cl-d1f2ee76\"><span class=\"cl-d1f2e61a\">2</span></p></td><td class=\"cl-d1f314c9\"><p class=\"cl-d1f2ee80\"><span class=\"cl-d1f2e61a\">German</span></p></td><td class=\"cl-d1f314c9\"><p class=\"cl-d1f2ee80\"><span class=\"cl-d1f2e61a\">Man</span></p></td><td class=\"cl-d1f314c8\"><p class=\"cl-d1f2ee76\"><span class=\"cl-d1f2e61a\">0</span></p></td><td class=\"cl-d1f314b4\"><p class=\"cl-d1f2ee76\"><span class=\"cl-d1f2e61a\">2</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-d1f314a0\"><p class=\"cl-d1f2ee76\"><span class=\"cl-d1f2e61a\">11</span></p></td><td class=\"cl-d1f314aa\"><p class=\"cl-d1f2ee76\"><span class=\"cl-d1f2e61a\">3</span></p></td><td class=\"cl-d1f314a1\"><p class=\"cl-d1f2ee80\"><span class=\"cl-d1f2e61a\">Russian</span></p></td><td class=\"cl-d1f314a1\"><p class=\"cl-d1f2ee80\"><span class=\"cl-d1f2e61a\">Man</span></p></td><td class=\"cl-d1f314aa\"><p class=\"cl-d1f2ee76\"><span class=\"cl-d1f2e61a\">0</span></p></td><td class=\"cl-d1f31496\"><p class=\"cl-d1f2ee76\"><span class=\"cl-d1f2e61a\">1</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-d1f314be\"><p class=\"cl-d1f2ee76\"><span class=\"cl-d1f2e61a\">12</span></p></td><td class=\"cl-d1f314c8\"><p class=\"cl-d1f2ee76\"><span class=\"cl-d1f2e61a\">2</span></p></td><td class=\"cl-d1f314c9\"><p class=\"cl-d1f2ee80\"><span class=\"cl-d1f2e61a\">German</span></p></td><td class=\"cl-d1f314c9\"><p class=\"cl-d1f2ee80\"><span class=\"cl-d1f2e61a\">Man</span></p></td><td class=\"cl-d1f314c8\"><p class=\"cl-d1f2ee76\"><span class=\"cl-d1f2e61a\">0</span></p></td><td class=\"cl-d1f314b4\"><p class=\"cl-d1f2ee76\"><span class=\"cl-d1f2e61a\">1</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-d1f314a0\"><p class=\"cl-d1f2ee76\"><span class=\"cl-d1f2e61a\">13</span></p></td><td class=\"cl-d1f314aa\"><p class=\"cl-d1f2ee76\"><span class=\"cl-d1f2e61a\">3</span></p></td><td class=\"cl-d1f314a1\"><p class=\"cl-d1f2ee80\"><span class=\"cl-d1f2e61a\">Russian</span></p></td><td class=\"cl-d1f314a1\"><p class=\"cl-d1f2ee80\"><span class=\"cl-d1f2e61a\">Woman</span></p></td><td class=\"cl-d1f314aa\"><p class=\"cl-d1f2ee76\"><span class=\"cl-d1f2e61a\">0</span></p></td><td class=\"cl-d1f31496\"><p class=\"cl-d1f2ee76\"><span class=\"cl-d1f2e61a\">1</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-d1f314be\"><p class=\"cl-d1f2ee76\"><span class=\"cl-d1f2e61a\">14</span></p></td><td class=\"cl-d1f314c8\"><p class=\"cl-d1f2ee76\"><span class=\"cl-d1f2e61a\">2</span></p></td><td class=\"cl-d1f314c9\"><p class=\"cl-d1f2ee80\"><span class=\"cl-d1f2e61a\">Russian</span></p></td><td class=\"cl-d1f314c9\"><p class=\"cl-d1f2ee80\"><span class=\"cl-d1f2e61a\">Woman</span></p></td><td class=\"cl-d1f314c8\"><p class=\"cl-d1f2ee76\"><span class=\"cl-d1f2e61a\">4</span></p></td><td class=\"cl-d1f314b4\"><p class=\"cl-d1f2ee76\"><span class=\"cl-d1f2e61a\">4</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-d1f314d2\"><p class=\"cl-d1f2ee76\"><span class=\"cl-d1f2e61a\">15</span></p></td><td class=\"cl-d1f314d3\"><p class=\"cl-d1f2ee76\"><span class=\"cl-d1f2e61a\">2</span></p></td><td class=\"cl-d1f314dc\"><p class=\"cl-d1f2ee80\"><span class=\"cl-d1f2e61a\">English</span></p></td><td class=\"cl-d1f314dc\"><p class=\"cl-d1f2ee80\"><span class=\"cl-d1f2e61a\">Man</span></p></td><td class=\"cl-d1f314d3\"><p class=\"cl-d1f2ee76\"><span class=\"cl-d1f2e61a\">0</span></p></td><td class=\"cl-d1f314ca\"><p class=\"cl-d1f2ee76\"><span class=\"cl-d1f2e61a\">4</span></p></td></tr></tbody></table></div>\n```\n:::\n:::\n\n\n\n\n\nSince the data contains character variables, we need to factorize the data before we can analyse it further and we also remove the ID column.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# factorize variables\ncountdata <- countdata %>%\n  dplyr::select(-ID) %>%\n  dplyr::mutate_if(is.character, factor)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# inspect data\ncountdata %>%\n  as.data.frame() %>%\n  head(15) %>%\n  flextable() %>%\n  flextable::set_table_properties(width = .75, layout = \"autofit\") %>%\n  flextable::theme_zebra() %>%\n  flextable::fontsize(size = 12) %>%\n  flextable::fontsize(size = 12, part = \"header\") %>%\n  flextable::align_text_col(align = \"center\") %>%\n  flextable::set_caption(caption = \"First 15 rows of the countdata data.\")  %>%\n  flextable::border_outer()\n```\n\n::: {.cell-output-display}\n```{=html}\n<div class=\"tabwid\"><style>.cl-d205e86e{table-layout:auto;width:75%;}.cl-d201f470{font-family:'DejaVu Sans';font-size:12pt;font-weight:bold;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-d201f471{font-family:'DejaVu Sans';font-size:12pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-d201fed4{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-d201fede{margin:0;text-align:center;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-d20228dc{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d20228e6{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d20228e7{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d20228fa{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d20228fb{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d2022904{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d202290e{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d202290f{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d2022918{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d2022919{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d2022922{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d2022923{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d202292c{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d202292d{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d202292e{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d2022936{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}</style><table class='cl-d205e86e'>\n```\n<caption class=\"Table Caption\">\n\nFirst 15 rows of the countdata data.\n\n</caption>\n```{=html}\n<thead><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-d2022936\"><p class=\"cl-d201fed4\"><span class=\"cl-d201f470\">Trial</span></p></td><td class=\"cl-d202292d\"><p class=\"cl-d201fede\"><span class=\"cl-d201f470\">Language</span></p></td><td class=\"cl-d202292d\"><p class=\"cl-d201fede\"><span class=\"cl-d201f470\">Gender</span></p></td><td class=\"cl-d202292c\"><p class=\"cl-d201fed4\"><span class=\"cl-d201f470\">UHM</span></p></td><td class=\"cl-d202292e\"><p class=\"cl-d201fed4\"><span class=\"cl-d201f470\">Shots</span></p></td></tr></thead><tbody><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-d20228fa\"><p class=\"cl-d201fed4\"><span class=\"cl-d201f471\">3</span></p></td><td class=\"cl-d20228e7\"><p class=\"cl-d201fede\"><span class=\"cl-d201f471\">Russian</span></p></td><td class=\"cl-d20228e7\"><p class=\"cl-d201fede\"><span class=\"cl-d201f471\">Man</span></p></td><td class=\"cl-d20228e6\"><p class=\"cl-d201fed4\"><span class=\"cl-d201f471\">0</span></p></td><td class=\"cl-d20228dc\"><p class=\"cl-d201fed4\"><span class=\"cl-d201f471\">0</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-d202290f\"><p class=\"cl-d201fed4\"><span class=\"cl-d201f471\">3</span></p></td><td class=\"cl-d2022904\"><p class=\"cl-d201fede\"><span class=\"cl-d201f471\">Russian</span></p></td><td class=\"cl-d2022904\"><p class=\"cl-d201fede\"><span class=\"cl-d201f471\">Man</span></p></td><td class=\"cl-d202290e\"><p class=\"cl-d201fed4\"><span class=\"cl-d201f471\">0</span></p></td><td class=\"cl-d20228fb\"><p class=\"cl-d201fed4\"><span class=\"cl-d201f471\">0</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-d20228fa\"><p class=\"cl-d201fed4\"><span class=\"cl-d201f471\">3</span></p></td><td class=\"cl-d20228e7\"><p class=\"cl-d201fede\"><span class=\"cl-d201f471\">German</span></p></td><td class=\"cl-d20228e7\"><p class=\"cl-d201fede\"><span class=\"cl-d201f471\">Man</span></p></td><td class=\"cl-d20228e6\"><p class=\"cl-d201fed4\"><span class=\"cl-d201f471\">0</span></p></td><td class=\"cl-d20228dc\"><p class=\"cl-d201fed4\"><span class=\"cl-d201f471\">5</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-d202290f\"><p class=\"cl-d201fed4\"><span class=\"cl-d201f471\">1</span></p></td><td class=\"cl-d2022904\"><p class=\"cl-d201fede\"><span class=\"cl-d201f471\">German</span></p></td><td class=\"cl-d2022904\"><p class=\"cl-d201fede\"><span class=\"cl-d201f471\">Man</span></p></td><td class=\"cl-d202290e\"><p class=\"cl-d201fed4\"><span class=\"cl-d201f471\">0</span></p></td><td class=\"cl-d20228fb\"><p class=\"cl-d201fed4\"><span class=\"cl-d201f471\">3</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-d20228fa\"><p class=\"cl-d201fed4\"><span class=\"cl-d201f471\">1</span></p></td><td class=\"cl-d20228e7\"><p class=\"cl-d201fede\"><span class=\"cl-d201f471\">German</span></p></td><td class=\"cl-d20228e7\"><p class=\"cl-d201fede\"><span class=\"cl-d201f471\">Woman</span></p></td><td class=\"cl-d20228e6\"><p class=\"cl-d201fed4\"><span class=\"cl-d201f471\">2</span></p></td><td class=\"cl-d20228dc\"><p class=\"cl-d201fed4\"><span class=\"cl-d201f471\">6</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-d202290f\"><p class=\"cl-d201fed4\"><span class=\"cl-d201f471\">3</span></p></td><td class=\"cl-d2022904\"><p class=\"cl-d201fede\"><span class=\"cl-d201f471\">German</span></p></td><td class=\"cl-d2022904\"><p class=\"cl-d201fede\"><span class=\"cl-d201f471\">Man</span></p></td><td class=\"cl-d202290e\"><p class=\"cl-d201fed4\"><span class=\"cl-d201f471\">1</span></p></td><td class=\"cl-d20228fb\"><p class=\"cl-d201fed4\"><span class=\"cl-d201f471\">5</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-d20228fa\"><p class=\"cl-d201fed4\"><span class=\"cl-d201f471\">1</span></p></td><td class=\"cl-d20228e7\"><p class=\"cl-d201fede\"><span class=\"cl-d201f471\">Mandarin</span></p></td><td class=\"cl-d20228e7\"><p class=\"cl-d201fede\"><span class=\"cl-d201f471\">Man</span></p></td><td class=\"cl-d20228e6\"><p class=\"cl-d201fed4\"><span class=\"cl-d201f471\">1</span></p></td><td class=\"cl-d20228dc\"><p class=\"cl-d201fed4\"><span class=\"cl-d201f471\">1</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-d202290f\"><p class=\"cl-d201fed4\"><span class=\"cl-d201f471\">3</span></p></td><td class=\"cl-d2022904\"><p class=\"cl-d201fede\"><span class=\"cl-d201f471\">German</span></p></td><td class=\"cl-d2022904\"><p class=\"cl-d201fede\"><span class=\"cl-d201f471\">Woman</span></p></td><td class=\"cl-d202290e\"><p class=\"cl-d201fed4\"><span class=\"cl-d201f471\">0</span></p></td><td class=\"cl-d20228fb\"><p class=\"cl-d201fed4\"><span class=\"cl-d201f471\">4</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-d20228fa\"><p class=\"cl-d201fed4\"><span class=\"cl-d201f471\">3</span></p></td><td class=\"cl-d20228e7\"><p class=\"cl-d201fede\"><span class=\"cl-d201f471\">Russian</span></p></td><td class=\"cl-d20228e7\"><p class=\"cl-d201fede\"><span class=\"cl-d201f471\">Woman</span></p></td><td class=\"cl-d20228e6\"><p class=\"cl-d201fed4\"><span class=\"cl-d201f471\">0</span></p></td><td class=\"cl-d20228dc\"><p class=\"cl-d201fed4\"><span class=\"cl-d201f471\">0</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-d202290f\"><p class=\"cl-d201fed4\"><span class=\"cl-d201f471\">2</span></p></td><td class=\"cl-d2022904\"><p class=\"cl-d201fede\"><span class=\"cl-d201f471\">German</span></p></td><td class=\"cl-d2022904\"><p class=\"cl-d201fede\"><span class=\"cl-d201f471\">Man</span></p></td><td class=\"cl-d202290e\"><p class=\"cl-d201fed4\"><span class=\"cl-d201f471\">0</span></p></td><td class=\"cl-d20228fb\"><p class=\"cl-d201fed4\"><span class=\"cl-d201f471\">2</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-d20228fa\"><p class=\"cl-d201fed4\"><span class=\"cl-d201f471\">3</span></p></td><td class=\"cl-d20228e7\"><p class=\"cl-d201fede\"><span class=\"cl-d201f471\">Russian</span></p></td><td class=\"cl-d20228e7\"><p class=\"cl-d201fede\"><span class=\"cl-d201f471\">Man</span></p></td><td class=\"cl-d20228e6\"><p class=\"cl-d201fed4\"><span class=\"cl-d201f471\">0</span></p></td><td class=\"cl-d20228dc\"><p class=\"cl-d201fed4\"><span class=\"cl-d201f471\">1</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-d202290f\"><p class=\"cl-d201fed4\"><span class=\"cl-d201f471\">2</span></p></td><td class=\"cl-d2022904\"><p class=\"cl-d201fede\"><span class=\"cl-d201f471\">German</span></p></td><td class=\"cl-d2022904\"><p class=\"cl-d201fede\"><span class=\"cl-d201f471\">Man</span></p></td><td class=\"cl-d202290e\"><p class=\"cl-d201fed4\"><span class=\"cl-d201f471\">0</span></p></td><td class=\"cl-d20228fb\"><p class=\"cl-d201fed4\"><span class=\"cl-d201f471\">1</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-d20228fa\"><p class=\"cl-d201fed4\"><span class=\"cl-d201f471\">3</span></p></td><td class=\"cl-d20228e7\"><p class=\"cl-d201fede\"><span class=\"cl-d201f471\">Russian</span></p></td><td class=\"cl-d20228e7\"><p class=\"cl-d201fede\"><span class=\"cl-d201f471\">Woman</span></p></td><td class=\"cl-d20228e6\"><p class=\"cl-d201fed4\"><span class=\"cl-d201f471\">0</span></p></td><td class=\"cl-d20228dc\"><p class=\"cl-d201fed4\"><span class=\"cl-d201f471\">1</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-d202290f\"><p class=\"cl-d201fed4\"><span class=\"cl-d201f471\">2</span></p></td><td class=\"cl-d2022904\"><p class=\"cl-d201fede\"><span class=\"cl-d201f471\">Russian</span></p></td><td class=\"cl-d2022904\"><p class=\"cl-d201fede\"><span class=\"cl-d201f471\">Woman</span></p></td><td class=\"cl-d202290e\"><p class=\"cl-d201fed4\"><span class=\"cl-d201f471\">4</span></p></td><td class=\"cl-d20228fb\"><p class=\"cl-d201fed4\"><span class=\"cl-d201f471\">4</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-d2022923\"><p class=\"cl-d201fed4\"><span class=\"cl-d201f471\">2</span></p></td><td class=\"cl-d2022922\"><p class=\"cl-d201fede\"><span class=\"cl-d201f471\">English</span></p></td><td class=\"cl-d2022922\"><p class=\"cl-d201fede\"><span class=\"cl-d201f471\">Man</span></p></td><td class=\"cl-d2022919\"><p class=\"cl-d201fed4\"><span class=\"cl-d201f471\">0</span></p></td><td class=\"cl-d2022918\"><p class=\"cl-d201fed4\"><span class=\"cl-d201f471\">4</span></p></td></tr></tbody></table></div>\n```\n:::\n:::\n\n\n\nAfter the data is factorized, we can visualize the data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncountdata %>%\n  # prepare data\n  dplyr::select(Language, Shots) %>%\n  dplyr::group_by(Language) %>%\n  dplyr::mutate(Mean = round(mean(Shots), 1)) %>%\n  dplyr::mutate(SD = round(sd(Shots), 1)) %>%\n  # start plot\n  ggplot(aes(Language, Shots, color = Language, fill = Language)) +\n  geom_violin(trim=FALSE, color = \"gray20\")+ \n  geom_boxplot(width=0.1, fill=\"white\", color = \"gray20\") +\n  geom_text(aes(y=-4,label=paste(\"mean: \", Mean, sep = \"\")), size = 3, color = \"black\") +\n  geom_text(aes(y=-5,label=paste(\"SD: \", SD, sep = \"\")), size = 3, color = \"black\") +\n  scale_fill_manual(values=rep(\"grey90\",4)) + \n  theme_set(theme_bw(base_size = 10)) +\n  theme(legend.position=\"none\", legend.title = element_blank(),\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank()) + \n  ylim(-5, 15) +\n  labs(x = \"Language\", y = \"Shots\")\n```\n\n::: {.cell-output-display}\n![](regression_files/figure-html/pmm3-1.png){width=672}\n:::\n:::\n\n\nThe violin plots show that the English speakers drank more shots than speakers of other languages with Mandarin speakers drinking the fewest shots.\n\nIn the present case, we will a Boruta variable selection procedure to streamline the model fitting process. Thus, before fitting the model, we will test which variables have any kind of relationship with the dependent variable and therefore deserve to be evaluated in the regression modeling. As this is just an example, we will only consider variables which are deemed important and disregard both unimportant and tentative variables. We start the Boruta analysis by setting a seed and running an initial Boruta analysis.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# perform variable selection\nset.seed(20191220)\nboruta <- Boruta(UHM ~.,data=countdata)\nprint(boruta)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nBoruta performed 99 iterations in 7.04685592651 secs.\n 1 attributes confirmed important: Shots;\n 1 attributes confirmed unimportant: Gender;\n 2 tentative attributes left: Language, Trial;\n```\n:::\n:::\n\n\nAs only Shots is confirmed as important, we will only check for the effect of Shots and include Language as a random effect in the regression modeling. Including Language as a random effect is probably not justified statistically (given that the Boruta analysis showed that it only has a tentative effect) but for theoretical reasons as the speakers are nested into Languages. Before we start with the modeling, however, we proceed by checking if the data does indeed approximate a Poisson distribution. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# output the results\ngf = goodfit(countdata$UHM,type= \"poisson\", method= \"ML\")\nplot(gf, main=\"Count data vs Poisson distribution\")\n```\n\n::: {.cell-output-display}\n![](regression_files/figure-html/pmm5-1.png){width=672}\n:::\n:::\n\n\nThe data does not perfectly match a distribution that would be expected if the data approximated a Poisson distribution. We will use a goodness-of-fit test to check if the data does indeed diverge significantly from being Poisson distributed. If the p-values of the goodness-of-fit test is smaller than .05, then the distribution of the data differs significantly from a Poisson distribution and, given the visualization is likely over-dispersed. \n\nIn case of overdispersion, we may have to use a quasi-Poisson or, even better, a negative binomial model but we will, for now continue with the Poisson model and perform diagnostics later to check if we have to switch to a more robust method. \nOne effect of overdispersion is that the standard errors of a model are biased and quasi-Poisson models scale the standard errors to compensate bias. However, @zuur2013beginner suggest to use negative-binomial model instead. This is so because the scaling of the standard errors performed by quasi-Poisson models only affects the significance of coefficients (the p-values) but it does not affect the coefficients which, however, may be affected themselves by overdispersion. Thus, the coefficients of Poisson as well as quasi-Poisson models (which are identical) may be unreliable when dealing with overdispersion. Negative binomial models, in contrast, include an additional dispersion or heterogeneity parameter which accommodates overdispersion better than merely scaling the standard errors [see @zuur2013beginner 21].\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(gf)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\t Goodness-of-fit test for poisson distribution\n\n                           X^2 df                                     P(> X^2)\nLikelihood Ratio 153.422771085  5 0.000000000000000000000000000000249336691328\n```\n:::\n:::\n\n\nThe p-value is indeed smaller than .05 which means that we should indeed use a negative-binomial model rather than a Poisson model. We will ignore this, for now, and proceed to fit a Poisson mixed-effects model and check what happens if a Poisson model is fit to over-dispersed data.\n\n### Mixed-Effects Poisson Regression{-}\n\nIn a first step, we create mixed-effect intercept-only baseline models and then test if including \"Shots\" significantly improves model fit and, thus, has a significant impact on the number of *uhms*.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# base-line mixed-model\nm0.glmer = glmer(UHM ~ 1 + (1 | Language), data = countdata, family = poisson,\n                 control=glmerControl(optimizer=\"bobyqa\"))\n# add Shots\nm1.glmer <- update(m0.glmer, .~.+ Shots)\nAnova(m1.glmer, test = \"Chi\")           \n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Deviance Table (Type II Wald chisquare tests)\n\nResponse: UHM\n          Chisq Df             Pr(>Chisq)    \nShots 321.24968  1 < 0.000000000000000222 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\nThe ANOVA confirms that Shots have a significant impact on the number of instances of *uhm*. \nHowever, we get the warning that the fitted mixed model is (almost / near) singular. In such cases, the model should not be reported. As this is only an example, we will continue by having a look at the model summary.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(m1.glmer)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: poisson  ( log )\nFormula: UHM ~ (1 | Language) + Shots\n   Data: countdata\nControl: glmerControl(optimizer = \"bobyqa\")\n\n     AIC      BIC   logLik deviance df.resid \n  1041.8   1054.5   -517.9   1035.8      497 \n\nScaled residuals: \n         Min           1Q       Median           3Q          Max \n-1.509633852 -0.666423093 -0.592950422  0.586114082  4.338639382 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n Language (Intercept) 0        0       \nNumber of obs: 500, groups:  Language, 4\n\nFixed effects:\n                 Estimate    Std. Error   z value               Pr(>|z|)    \n(Intercept) -1.2789168850  0.0893313713 -14.31655 < 0.000000000000000222 ***\nShots        0.2336279071  0.0130347699  17.92344 < 0.000000000000000222 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n      (Intr)\nShots -0.806\noptimizer (bobyqa) convergence code: 0 (OK)\nboundary (singular) fit: see help('isSingular')\n```\n:::\n:::\n\n\nThe model summary confirms that the number of shots does have a significantly positive effect on the number of occurrences of *uhm*. Furthermore, the scaled residuals are distributed very unevenly which suggests overdispersion. Including Language as a random effect is not justified given that they have 0 variance and a standard deviation of 0 (which means that Language does not account for or explain any additional variance). \n\nWe now check if the model suffers from overdispersion following @zuur2013beginner[138].\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# extract pearson residuals\nPearsonResiduals <- resid(m1.glmer, type = \"pearson\")\n# extract number of cases in model\nCases <- nrow(countdata)\n# extract number of predictors (plus intercept)\nNumberOfPredictors <- length(fixef(m1.glmer)) +1\n# calculate overdispersion\nOverdispersion <- sum(PearsonResiduals^2) / (Cases-NumberOfPredictors)\n# inspect overdispersion\nOverdispersion\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1.16901460967\n```\n:::\n:::\n\n\nThe data is slightly over-dispersed. It would also be advisable to plot the Cook's distance (which should not show data points with values > 1). If there are data points with high Cook's D values, we could exclude them which would, very likely reduce the overdispersion [see @zuur2013beginner 22]. We ignore this, for now, and use diagnostic plots to check if the plots indicate problems.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndiag_data <- data.frame(PearsonResiduals, fitted(m1.glmer)) %>%\n  dplyr::rename(Pearson = 1,\n                Fitted = 2)\np9 <- ggplot(diag_data, aes(x = Fitted, y = Pearson)) +\n  geom_point() +\n  geom_hline(yintercept = 0, linetype = \"dotted\")\np10 <- ggplot(countdata, aes(x = Shots, y = diag_data$Pearson)) +\n  geom_point()  +\n  geom_hline(yintercept = 0, linetype = \"dotted\") +\n  labs(y = \"Pearson\")\np11 <- ggplot(countdata, aes(x = Language, y = diag_data$Pearson)) +\n  geom_boxplot() +\n  labs(y = \"Pearson\") + \n  theme(axis.text.x = element_text(angle=90))\ngrid.arrange(p9, p10, p11, nrow = 1)\n```\n\n::: {.cell-output-display}\n![](regression_files/figure-html/pmm10-1.png){width=672}\n:::\n:::\n\n\nThe diagnostic plots show problems as the dots in the first two plots are not random but show a pattern in the lower left corner. In addition, the variance of English (left boxplot) is notable larger than the variance of Russian (right boxplot). As a final step, we plot the predicted vales of the model to check if the predictions make sense.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_model(m1.glmer, type = \"pred\", terms = c(\"Shots\"))\n```\n\n::: {.cell-output-display}\n![](regression_files/figure-html/pmm11b-1.png){width=672}\n:::\n:::\n\n\nThe model predicts that the instances of *uhm* increase with the number of shots. Note that the increase is not homogeneous as the y-axis labels indicate! We now compare the predicted number of *uhm* with the actually observed instances of *uhm* to check if the results of the model make sense.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncountdata %>%\n  mutate(Predicted = predict(m1.glmer, type = \"response\")) %>%\n  dplyr::rename(Observed = UHM) %>%\n  tidyr::gather(Type, Frequency, c(Observed, Predicted)) %>%\n  dplyr::mutate(Shots = factor(Shots),\n                Type = factor(Type)) %>%\n  dplyr::group_by(Shots, Type) %>%\n  dplyr::summarize(Frequency = mean(Frequency)) %>%\n  ggplot(aes(Shots, Frequency, group = Type, color = Type, linetype = Type)) +\n  geom_line(size = 1.2) +\n  scale_color_manual(values = c(\"orange\", \"lightblue\")) \n```\n\n::: {.cell-output-display}\n![](regression_files/figure-html/pmm11f-1.png){width=672}\n:::\n:::\n\n\nThe comparison between the observed and the predicted uses of *uhm* becomes somewhat volatile and shows fluctuations after eight shots. We will now summarize the results as if the violations (overdispersion measure > 1 and excessive multicollinearity (singular fit)) had NOT occurred(!) - again: this is only because we are practicing here - this would be absolutely unacceptable in a proper write-up of an analysis!\n\nThe summary of the model can be extracted using the tab_model function from the `sjPlot` package [@sjPlot].\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsjPlot::tab_model(m1.glmer)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table style=\"border-collapse:collapse; border:none;\">\n<tr>\n<th style=\"border-top: double; text-align:center; font-style:normal; font-weight:bold; padding:0.2cm;  text-align:left; \">&nbsp;</th>\n<th colspan=\"3\" style=\"border-top: double; text-align:center; font-style:normal; font-weight:bold; padding:0.2cm; \">UHM</th>\n</tr>\n<tr>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  text-align:left; \">Predictors</td>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  \">Incidence Rate Ratios</td>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  \">CI</td>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  \">p</td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; \">(Intercept)</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.28</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.23&nbsp;&ndash;&nbsp;0.33</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"><strong>&lt;0.001</strong></td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; \">Shots</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">1.26</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">1.23&nbsp;&ndash;&nbsp;1.30</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"><strong>&lt;0.001</strong></td>\n</tr>\n<tr>\n<td colspan=\"4\" style=\"font-weight:bold; text-align:left; padding-top:.8em;\">Random Effects</td>\n</tr>\n\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; padding-top:0.1cm; padding-bottom:0.1cm;\">&sigma;<sup>2</sup></td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left;\" colspan=\"3\">0.89</td>\n</tr>\n\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; padding-top:0.1cm; padding-bottom:0.1cm;\">&tau;<sub>00</sub> <sub>Language</sub></td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left;\" colspan=\"3\">0.00</td>\n\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; padding-top:0.1cm; padding-bottom:0.1cm;\">N <sub>Language</sub></td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left;\" colspan=\"3\">4</td>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; padding-top:0.1cm; padding-bottom:0.1cm; border-top:1px solid;\">Observations</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left; border-top:1px solid;\" colspan=\"3\">500</td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; padding-top:0.1cm; padding-bottom:0.1cm;\">Marginal R<sup>2</sup> / Conditional R<sup>2</sup></td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left;\" colspan=\"3\">0.288 / NA</td>\n</tr>\n\n</table>\n\n`````\n:::\n:::\n\n\n***\n\n<br>\n\n<div class=\"warning\" style='padding:0.1em; background-color:#f2f2f2; color:#51247a'>\n<span>\n<p style='margin-top:1em; text-align:center'>\n<b>NOTE</b><br><br>The R^2^ values of the summary table are incorrect (as indicated by the missing conditional R^2^ value). The more appropriate conditional and marginal coefficient of determination for generalized mixed-effect models can be extracted using the `r.squaredGLMM` function from the `MuMIn` package [@MuMIn].</p>\n<p style='margin-left:1em;'>\n</p></span>\n</div>\n\n\n<br>\n\n\n::: {.cell}\n\n```{.r .cell-code}\nr.squaredGLMM(m1.glmer)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                     R2m            R2c\ndelta     0.208910674745 0.208910674745\nlognormal 0.294971460548 0.294971460548\ntrigamma  0.120419614835 0.120419614835\n```\n:::\n:::\n\n\n\nAlso note that our model suffers from a serious problem (near singular fit). If this were not just an example, you should not(!) report this model!\n\n### Mixed-Effects Quasi-Possion Regression{-}\n\nThe Quasi-Poisson Regression is a generalization of the Poisson regression and is used when modeling an overdispersed count variable. Poisson models are based on the Poisson distribution which is defined as a distribution where the variance is equal to the mean (which is very restrictive and not often the case). Quasi-Poisson models scale the standard errors which has a positive effect when dealing with overdispersed data. \n\nTherefore, when the variance is greater than the mean, a Quasi-Poisson model, which assumes that the variance is a linear function of the mean, is more appropriate as it handles over-dispersed data better than normal Poisson-models. \n\nWe begin the model fitting process by creating a mixed- and a fixed-effects intercept-only base-line model. Unfortunately, there is not yet a procedure in place for quasi-Poisson models to test if the inclusion of random effects is justified. However, here the Boruta also provides valuable information: Language was only considered tentative but not important which suggests that it will not explain variance which means that including Language as a random effect may not be justified. This would require further inspection. Because we are only dealing with an example here, we ignore this fact (which you should not do in proper analyses) and continue right away with adding shots. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# base-line mixed-model\nm0.qp = glmmPQL(UHM ~ 1, random = ~ 1 | Language, data = countdata, \n                   family = quasipoisson(link='log'))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\niteration 1\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\niteration 2\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\niteration 3\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\niteration 4\n```\n:::\n\n```{.r .cell-code}\n# add Shots\nm1.qp <- update(m0.qp, .~.+ Shots)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\niteration 1\n```\n:::\n\n```{.r .cell-code}\nAnova(m1.qp, test = \"Chi\")           # SIG! (p<0.0000000000000002 ***)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Deviance Table (Type II tests)\n\nResponse: zz\n         Chisq Df             Pr(>Chisq)    \nShots 276.4523  1 < 0.000000000000000222 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\nThe ANOVA confirms that Shots have a significant impact on the number of instances of *uhm*. We will now have a look at the model summary.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(m1.qp)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear mixed-effects model fit by maximum likelihood\n  Data: countdata \n  AIC BIC logLik\n   NA  NA     NA\n\nRandom effects:\n Formula: ~1 | Language\n               (Intercept)      Residual\nStdDev: 0.0000407595004855 1.07801257915\n\nVariance function:\n Structure: fixed weights\n Formula: ~invwt \nFixed effects:  UHM ~ Shots \n                      Value       Std.Error  DF        t-value p-value\n(Intercept) -1.278929721022 0.0964886326643 495 -13.2547190867       0\nShots        0.233630231741 0.0140795660799 495  16.5935676153       0\n Correlation: \n      (Intr)\nShots -0.806\n\nStandardized Within-Group Residuals:\n            Min              Q1             Med              Q3             Max \n-1.400417640404 -0.618228066949 -0.550071160973  0.543731905024  4.024828853746 \n\nNumber of Observations: 500\nNumber of Groups: 4 \n```\n:::\n:::\n\n\nThe model summary does not provide much information such as,e.g. AIC or BIC values. The coefficient for *Shots* is highly significant (p <.001) and the data is notably over-dispersed (the Standardized Within-Group Residuals deviate substantially from a normal distribution with higher values having a thick tail). Also, in contrast to the Poisson model, Language does explain at least a minimal share of the variance now as the mean and standard deviation are no longer 0. Note also, that the coefficients are identical to the Poisson coefficients but the standard errors and p-values differ (the model provides t- rather than z-values).\n\nIn a next step, we will calculate the odds ratios of the coefficient (as we only have one). We will use the coefficients from the fixed-effects model as the coefficients for mixed- and fixed-effects models are identical (the random effect structure only affects the standard error and p-values but not the coefficients; you can check by uncommenting the summary command).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm1.glm = glm(UHM ~ Shots, data = countdata, family = quasipoisson(link='log'))\nexp(coef(m1.glm))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   (Intercept)          Shots \n0.278338612235 1.263174385573 \n```\n:::\n:::\n\n\nThe standardized or $\\beta$-coefficient tells us that the likelihood of *uhm* increases by 1.26 (or 26.32 percent) with each additional shot.\n\nBefore inspecting the relationship between Shots and *uhm*, we will check if the overdispersion was reduced.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# extract pearson residuals\nPearsonResiduals <- resid(m1.qp, type = \"pearson\")\n# extract number of cases in model\nCases <- nrow(countdata)\n# extract number of predictors (plus intercept)\nNumberOfPredictors <- length(fixef(m1.qp)) +1\n# calculate overdispersion\nOverdispersion <- sum(PearsonResiduals^2) / (Cases-NumberOfPredictors)\n# inspect overdispersion\nOverdispersion\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1.00603621722\n```\n:::\n:::\n\n\nThe overdispersion has indeed decreased and is not so close to 1 that overdispersion is no longer an issue. \n\nWe continue to diagnose the model by plotting the Pearson's residuals against fitted values. This diagnostic plot should not show a funnel-like structure or patterning as we observed in the case of the Poisson model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# diagnostic plot\nplot(m1.qp, pch = 20, col = \"black\", lty= \"dotted\", ylab = \"Pearson's residuals\")\n```\n\n::: {.cell-output-display}\n![](regression_files/figure-html/qpmm5-1.png){width=672}\n:::\n:::\n\n\nIndeed, the plot exhibits a (slight) funnel shape (but not drastically so) and thus indicates heteroscedasticity. However, the patterning that we observed with the Poisson model has disappeared. We continue by plotting the random effect adjustments.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# generate diagnostic plots\nplot(m1.qp, Language ~ resid(.), abline = 0, fill = \"gray70\") \n```\n\n::: {.cell-output-display}\n![](regression_files/figure-html/qpmm10-1.png){width=672}\n:::\n:::\n\n\nThe adjustments by \"Language\" are marginal (which was somewhat expected given that Language was only deemed tentative), which shows that there is very little variation between the languages and that we have no statistical reason to include Language as a random effect. \n\nIn a final step, we plot the fixed-effect of *Shots* using the `predictorEffects` function from the `effects` package [@effects].\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_model(m1.qp, type = \"pred\", terms = c(\"Shots\"))\n```\n\n::: {.cell-output-display}\n![](regression_files/figure-html/qpmm11b-1.png){width=672}\n:::\n:::\n\n\nThe effects plot shows that the number of *uhms* increases exponentially with the number of shots a speaker has had. We now compare the predicted number of *uhm* with the actually observed instances of *uhm* to check if the results of the model make sense.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncountdata %>%\n  mutate(Predicted = predict(m1.qp, type = \"response\")) %>%\n  dplyr::rename(Observed = UHM) %>%\n  tidyr::gather(Type, Frequency, c(Observed, Predicted)) %>%\n  dplyr::mutate(Shots = factor(Shots),\n                Type = factor(Type)) %>%\n  dplyr::group_by(Shots, Type) %>%\n  dplyr::summarize(Frequency = mean(Frequency)) %>%\n  ggplot(aes(Shots, Frequency, group = Type, color = Type, linetype = Type)) +\n  geom_line(size = 1.2) +\n  scale_color_manual(values = c(\"orange\", \"lightblue\")) \n```\n\n::: {.cell-output-display}\n![](regression_files/figure-html/qpmm11f-1.png){width=672}\n:::\n:::\n\n\nGiven that the overdispersion measure of this Quasi-Poisson model is close to 1, that the model did not suffer from excessive multicollinearity (singular fit), and because this model shows improvements compared to the Poisson model with respect to the model diagnostics (some adjustments by Language and less patterning in the diagnostic plots), we would choose this quasi-Poisson model over the Poisson model. \n\nFinally, we extract the summary table of this model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsjPlot::tab_model(m1.qp)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table style=\"border-collapse:collapse; border:none;\">\n<tr>\n<th style=\"border-top: double; text-align:center; font-style:normal; font-weight:bold; padding:0.2cm;  text-align:left; \">&nbsp;</th>\n<th colspan=\"3\" style=\"border-top: double; text-align:center; font-style:normal; font-weight:bold; padding:0.2cm; \">UHM</th>\n</tr>\n<tr>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  text-align:left; \">Predictors</td>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  \">Incidence Rate Ratios</td>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  \">CI</td>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  \">p</td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; \">(Intercept)</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.28</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.23&nbsp;&ndash;&nbsp;0.34</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"><strong>&lt;0.001</strong></td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; \">Shots</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">1.26</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">1.23&nbsp;&ndash;&nbsp;1.30</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"><strong>&lt;0.001</strong></td>\n</tr>\n\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; padding-top:0.1cm; padding-bottom:0.1cm;\">N <sub>Language</sub></td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left;\" colspan=\"3\">4</td>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; padding-top:0.1cm; padding-bottom:0.1cm; border-top:1px solid;\">Observations</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left; border-top:1px solid;\" colspan=\"3\">500</td>\n</tr>\n\n</table>\n\n`````\n:::\n:::\n\n\n***\n\n<br>\n\n<div class=\"warning\" style='padding:0.1em; background-color:#f2f2f2; color:#51247a'>\n<span>\n<p style='margin-top:1em; text-align:center'>\n<b>NOTE</b><br><br>The R^2^ values of the summary table are incorrect (as indicated by the missing conditional R^2^ value). The more appropriate conditional and marginal coefficient of determination for generalized mixed-effect models can be extracted using the `r.squaredGLMM` function from the `MuMIn` package [@MuMIn].</p>\n<p style='margin-left:1em;'>\n</p></span>\n</div>\n\n\n<br>\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nr.squaredGLMM(m1.qp)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                      R2m             R2c\ndelta     0.1856941878845 0.1856941887428\nlognormal 0.2752763979322 0.2752763992045\ntrigamma  0.0977040663474 0.0977040667989\n```\n:::\n:::\n\n\n### Mixed-Effects Negative Binomial Regression{-}\n\nNegative binomial regression models are a generalization of Poisson regression which loosens the restrictive assumption that the variance is equal to the mean made by the Poisson model. This is a major advantage as the most common issue that one faces with Poisson regressions is that the data deviate too substantially from the assumed Poisson distribution. \n\nTo implement a Negative-Binomial Mixed-Effects Regression, we first create a mixed-effects intercept-only baseline model and then test if including `Shots` significantly improves model fit and, thus, has a significant impact on the number of *uhms*.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# base-line mixed-model\nm0.nb = glmer.nb(UHM ~ 1 + (1 | Language), data = countdata)\n# add Shots\nm1.nb <- update(m0.nb, .~.+ Shots)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nboundary (singular) fit: see help('isSingular')\n```\n:::\n\n```{.r .cell-code}\nanova(m1.nb, m0.nb)           \n```\n\n::: {.cell-output .cell-output-stdout}\n```\nData: countdata\nModels:\nm0.nb: UHM ~ 1 + (1 | Language)\nm1.nb: UHM ~ (1 | Language) + Shots\n      npar         AIC         BIC       logLik    deviance     Chisq Df\nm0.nb    3 1159.000894 1171.644718 -576.5004470 1153.000894             \nm1.nb    4 1051.593288 1068.451721 -521.7966442 1043.593288 109.40761  1\n                  Pr(>Chisq)    \nm0.nb                           \nm1.nb < 0.000000000000000222 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\nThe negative-binomial model also reports a significant impact of shots on the number of *uhms*. \nIn a next step, we calculate the overdispersion.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# extract pearson residuals\nPearsonResiduals <- resid(m1.nb, type = \"pearson\")\n# extract number of betas + predictors + sigma\nNumberOfPredictors <- 2+1+1\n# extract number of cases in model\nCases <- nrow(countdata)\n# calculate overdispersion parameter\nOverdispersion <- sum(PearsonResiduals^2) / (Cases / NumberOfPredictors)# show overdispersion parameter\nOverdispersion\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2.46949245808\n```\n:::\n:::\n\n\nThe overdispersion has increased which is rather suboptimal. In this case, we would report the  Quasi-Poisson Regression rather than the Negative Binomial Model (which is rather rare as Negative Binomial Models typically perform better than (Quasi-)Poisson models. However, this tutorial focuses merely on how to implement a Negative Binomial Mixed-Effects Regression and we thus continue with generating diagnostic plots to check for problems.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndiag_data <- data.frame(PearsonResiduals, fitted(m1.nb)) %>%\n  dplyr::rename(Pearson = 1,\n                Fitted = 2)\np9 <- ggplot(diag_data, aes(x = Fitted, y = Pearson)) +\n  geom_point() +\n  geom_hline(yintercept = 0, linetype = \"dotted\")\np10 <- ggplot(countdata, aes(x = Shots, y = diag_data$Pearson)) +\n  geom_point()  +\n  geom_hline(yintercept = 0, linetype = \"dotted\") +\n  labs(y = \"Pearson\")\np11 <- ggplot(countdata, aes(x = Language, y = diag_data$Pearson)) +\n  geom_boxplot() +\n  labs(y = \"Pearson\") + \n  theme(axis.text.x = element_text(angle=90))\ngrid.arrange(p9, p10, p11, nrow = 1)\n```\n\n::: {.cell-output-display}\n![](regression_files/figure-html/nbmm4-1.png){width=672}\n:::\n:::\n\n\nThe diagnostics show patterning similar to the one we saw with the Poisson model which suggest that the negative binomial model is also not an optimal model for our data. We continue by plotting the predicted values and, subsequently, summarize the analysis. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_model(m1.nb, type = \"pred\", terms = c(\"Shots\"))\n```\n\n::: {.cell-output-display}\n![](regression_files/figure-html/nbmm6b-1.png){width=672}\n:::\n:::\n\n\nThe effect plot shows that the predicted number of shots increases exponentially with each shot. We now compare the predicted number of *uhm* with the actually observed instances of *uhm* to check if the results of the model make sense.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncountdata %>%\n  mutate(Predicted = predict(m1.nb, type = \"response\")) %>%\n  dplyr::rename(Observed = UHM) %>%\n  tidyr::gather(Type, Frequency, c(Observed, Predicted)) %>%\n  dplyr::mutate(Shots = factor(Shots),\n                Type = factor(Type)) %>%\n  dplyr::group_by(Shots, Type) %>%\n  dplyr::summarize(Frequency = mean(Frequency)) %>%\n  ggplot(aes(Shots, Frequency, group = Type, color = Type, linetype = Type)) +\n  geom_line(size = 1.2) +\n  scale_color_manual(values = c(\"orange\", \"lightblue\")) \n```\n\n::: {.cell-output-display}\n![](regression_files/figure-html/nbmm6f-1.png){width=672}\n:::\n:::\n\n\n\nThe comparison between the observed and the predicted uses of *uhm* becomes somewhat volatile and shows fluctuations after eight shots. We will now summarize the results as if the violations had NOT occurred(!) - again: this is only because we are practicing here - this would be absolutely unacceptable in a proper write-up of an analysis!\n\n\nA mixed-effect negative binomial regression model which contained the language in which the conversation took place as random effect was fit to the data. Prior to the regression modeling, a Boruta analysis was applied to determine whether any of the predictors had a meaningful relationship with the dependent variable (instances of *uhm*). Since the Boruta analysis indicated that only the number of shots a speaker had was important, only \"Shots\" was tested during model fitting. The final minimal adequate model showed that the number of *uhm* as fillers increases significantly, and near-linearly with the number of shots speakers had ($\\chi$^2^(1):83.0, p <.0001, $\\beta$: 0.2782). An inspection of the random effect structure conveyed that there was almost no variability between languages and language did not contribute meaningfully to the model fit.\n\n## Mixed-Effects Multinomial Regression{-}\n\nIn this section, we will focus on how to implement a mixed-effects multinomial regression model using the `mblogit` function from the `mclogit` package [see @mclogit]. As we have already gone though model fitting and model validation procedures above, we will strictly see how to implement this type of model here - we will not go through all the other steps that a proper regression analysis would require.\n\nWe begin the analysis by loading the example data set. The data represents observations gathered during an experiment where speakers had to report what they saw. The responses are categorized into four groups: \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# description data\npict  <- base::readRDS(url(\"https://slcladal.github.io/data/pict.rda\", \"rb\"))\n# inspect\nhead(pict)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  Id Participant       Group Item     Response Gender Age\n1  1        G001 German_Mono    1  NumeralNoun   Male  18\n2  2        G002 German_Mono    3  NumeralNoun   Male  18\n3  3        G003 German_Mono    4  NumeralNoun   Male  18\n4  4        G004 German_Mono    6 QuantAdjNoun   Male  18\n5  5        G005 German_Mono    8  NumeralNoun   Male  18\n6  6        G006 German_Mono    9 QuantAdjNoun   Male  18\n```\n:::\n:::\n\n\nIn a first step, we generate a baseline model that we call `m0`. This model only contains the random effect structure and the intercept as the sole predictor.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm0.mn <- mblogit(formula = Response ~ 1, \n              random = ~ 1 | Participant, \n              data = pict)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nIteration 1 - deviance = 3038.65479447 - criterion = 0.788153856566\nIteration 2 - deviance = 1039.57449206 - criterion = 0.176903386816\nIteration 3 - deviance = 547.058967782 - criterion = 0.0855470770333\nIteration 4 - deviance = 1256.54718254 - criterion = 0.0813989708889\nIteration 5 - deviance = 63.4217806776 - criterion = 0.0986885267217\nIteration 6 - deviance = -394.76643481 - criterion = 0.0776120932314\nIteration 7 - deviance = 5.62985445935 - criterion = 0.104610835582\nIteration 8 - deviance = -210.444206442 - criterion = 0.0603396486691\nIteration 9 - deviance = 596.528255883 - criterion = 0.135066522725\nIteration 10 - deviance = -124.162950554 - criterion = 0.0415097050311\nIteration 11 - deviance = 377.975373084 - criterion = 0.0328094246095\nIteration 12 - deviance = 140.095311688 - criterion = 0.133864208674\nIteration 13 - deviance = 1013.05642125 - criterion = 0.231316954023\nIteration 14 - deviance = -3.30112378888 - criterion = 0.161806646675\nIteration 15 - deviance = 937.452340502 - criterion = 0.0971884881609\nIteration 16 - deviance = -170.314936427 - criterion = 0.110950531568\nIteration 17 - deviance = 240.541276454 - criterion = 0.0895026719048\nIteration 18 - deviance = -1028.78181767 - criterion = 0.139902469189\nIteration 19 - deviance = -482.901482304 - criterion = 0.0704521880486\nIteration 20 - deviance = -217.021371436 - criterion = 0.0539352925313\nIteration 21 - deviance = 501.706207685 - criterion = 0.16311970482\nIteration 22 - deviance = -156.634283459 - criterion = 0.0469752281655\nIteration 23 - deviance = 284.371554236 - criterion = 0.0384622152383\nIteration 24 - deviance = 477.190984196 - criterion = 0.275130330768\nIteration 25 - deviance = 1091.09306626 - criterion = 0.137081279068\n```\n:::\n:::\n\n\nIn this case, the algorithm did not converge properly - if this were a real analysis, we could not simply continue but would have to inspect possible causes for this. However, as this is just a showcase, we will ignore this and move on. Next, we add the fixed effects (Gender and Group).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm1.mn <- mblogit(formula = Response ~ Gender + Group, \n              random = ~ 1 | Item, \n              data = pict)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nIteration 1 - deviance = 1667.89123406 - criterion = 0.801491607033\nIteration 2 - deviance = 1576.66992662 - criterion = 0.0818733799283\nIteration 3 - deviance = 1550.62063217 - criterion = 0.0307203102233\nIteration 4 - deviance = 1541.31639399 - criterion = 0.00976465320773\nIteration 5 - deviance = 1537.50858746 - criterion = 0.00251480377207\nIteration 6 - deviance = 1536.10265122 - criterion = 0.000306996036811\nIteration 7 - deviance = 1535.72249302 - criterion = 0.0000470947760996\nIteration 8 - deviance = 1535.59319629 - criterion = 0.00000732037942916\nIteration 9 - deviance = 1535.54664057 - criterion = 0.00000119558493383\nIteration 10 - deviance = 1535.52876695 - criterion = 0.000000199398597452\nIteration 11 - deviance = 1535.52165999 - criterion = 0.0000000334243542711\nIteration 12 - deviance = 1535.518789 - criterion = 0.00000000560129429554\nconverged\n```\n:::\n:::\n\n\n\nNow, we can compare the models to see if including the fixed-effects into the model has significantly improved the model fit.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nanova(m0.mn, m1.mn)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Deviance Table\n\nModel 1: Response ~ 1\nModel 2: Response ~ Gender + Group\n  Resid. Df  Resid. Dev Df     Deviance\n1      3261 1091.093066                \n2      3249 1535.518789 12 -444.4257227\n```\n:::\n:::\n\n\nAs the second model is significantly better, we are justified to believe that our fixed effects have explanatory power. We can now use the `getSummary.mmblogit` function to get a summary of the model with the fixed effects. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# inspect\nmclogit::getSummary.mmblogit(m1.mn)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$coef\n, , NumeralNoun/BareNoun\n\n                                  est             se             stat\n(Intercept)           1.0874092952417 0.893295577758   1.217300658726\nGenderMale            0.0692427468008 0.197359653418   0.350845502623\nGroupGerman_Mono     -3.3053490899596 0.306602481360 -10.780568621958\nGroupL2_Advanced     -0.4607564819019 0.309284978199  -1.489747366926\nGroupL2_Intermediate -1.1782785696607 0.322487307276  -3.653720760718\n                                                            p             lwr\n(Intercept)          0.22348984377707278858871120519324904308 -0.663417864712\nGenderMale           0.72570425820753281520580912911100313067 -0.317575065899\nGroupGerman_Mono     0.00000000000000000000000000425248830713 -3.906278910995\nGroupL2_Advanced     0.13629067498365624033773713108530500904 -1.066943900131\nGroupL2_Intermediate 0.00025846735930856122023455134772973452 -1.810342077392\n                                 upr\n(Intercept)           2.838236455195\nGenderMale            0.456060559501\nGroupGerman_Mono     -2.704419268924\nGroupL2_Advanced      0.145430936328\nGroupL2_Intermediate -0.546215061929\n\n, , QuantAdjNoun/BareNoun\n\n                                 est             se             stat\n(Intercept)           0.571176946931 0.692608541009   0.824674997653\nGenderMale            0.298987635166 0.248639839890   1.202492872013\nGroupGerman_Mono     -3.875407740288 0.378808841683 -10.230510257033\nGroupL2_Advanced     -2.198916269400 0.349918227201  -6.284086104881\nGroupL2_Intermediate -3.025120759319 0.400642905543  -7.550665985766\n                                                         p             lwr\n(Intercept)          0.40955613002243967946114366895926651 -0.786310848832\nGenderMale           0.22917262613964711759351189357403200 -0.188337496140\nGroupGerman_Mono     0.00000000000000000000000144754650331 -4.617859427011\nGroupL2_Advanced     0.00000000032978789549465373397243521 -2.884743392248\nGroupL2_Intermediate 0.00000000000004330381141474033699687 -3.810366424844\n                                 upr\n(Intercept)           1.928664742695\nGenderMale            0.786312766471\nGroupGerman_Mono     -3.132956053565\nGroupL2_Advanced     -1.513089146551\nGroupL2_Intermediate -2.239875093794\n\n, , QuantNoun/BareNoun\n\n                                 est             se            stat\n(Intercept)          -2.464598002828 0.998541739748 -2.468197276811\nGenderMale           -0.377291526526 0.434033290744 -0.869268635775\nGroupGerman_Mono     -2.485588291752 0.818754737248 -3.035815463011\nGroupL2_Advanced      0.781434518047 0.689498783889  1.133337050486\nGroupL2_Intermediate -0.177522551612 0.744857214668 -0.238330982255\n                                    p            lwr             upr\n(Intercept)          0.01357954576529 -4.42170384979 -0.507492155862\nGenderMale           0.38470021375801 -1.22798114448  0.473398091424\nGroupGerman_Mono     0.00239886133501 -4.09031808893 -0.880858494574\nGroupL2_Advanced     0.25707273828459 -0.56995826576  2.132827301853\nGroupL2_Intermediate 0.81162439319654 -1.63741586599  1.282370762763\n\n\n$Item\n, , 1\n\n                                              est            se stat  p lwr upr\nNumeralNoun/BareNoun: VCov(~1,~1)   7.04843777880 61.5809194674   NA NA  NA  NA\nQuantAdjNoun/BareNoun: VCov(~1,~1) -1.45406596713 84.3719670062   NA NA  NA  NA\nQuantNoun/BareNoun: VCov(~1,~1)     1.91291563134 87.9836147377   NA NA  NA  NA\n\n, , 2\n\n                                              est            se stat  p lwr upr\nNumeralNoun/BareNoun: VCov(~1,~1)  -1.45406596713 84.3719670062   NA NA  NA  NA\nQuantAdjNoun/BareNoun: VCov(~1,~1)  3.83022239605 48.3623057477   NA NA  NA  NA\nQuantNoun/BareNoun: VCov(~1,~1)     3.17487495693 17.9994914254   NA NA  NA  NA\n\n, , 3\n\n                                             est            se stat  p lwr upr\nNumeralNoun/BareNoun: VCov(~1,~1)  1.91291563134 87.9836147377   NA NA  NA  NA\nQuantAdjNoun/BareNoun: VCov(~1,~1) 3.17487495693 17.9994914254   NA NA  NA  NA\nQuantNoun/BareNoun: VCov(~1,~1)    5.35288426315 41.9596189735   NA NA  NA  NA\n\n\n$Groups\nGroups by Item \n            10 \n\n$sumstat\n               LR                df          deviance          McFadden \n1486.602918242263   21.000000000000 1535.518788999098    0.491907031633 \n        Cox.Snell        Nagelkerke               AIC               BIC \n   0.744326972782    0.793948770968 1577.518788999098 1682.391381478785 \n                N \n1090.000000000000 \n\n$call\nmblogit(formula = Response ~ Gender + Group, data = pict, random = ~1 | \n    Item)\n\n$contrasts\n$contrasts$Gender\n[1] \"contr.treatment\"\n\n$contrasts$Group\n[1] \"contr.treatment\"\n\n\n$xlevels\n$xlevels$Gender\n[1] \"Female\" \"Male\"  \n\n$xlevels$Group\n[1] \"English_Mono\"    \"German_Mono\"     \"L2_Advanced\"     \"L2_Intermediate\"\n```\n:::\n:::\n\n\nThe NAs (not available information) is a result of the model having a bad fit to the data and, optimally, we would need to inspect why the model has a bad fit. Again, we ignore this and move on. Next, we check the VIFs to see if the model does not violate multicollinearity assumptions.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncar::vif(m1.mn) # maybe use cut-off of 5 (maybe 10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                GVIF Df GVIF^(1/(2*Df))\nGender 3.12579996640  1   1.76799320316\nGroup  5.37692515348  3   1.32359666797\n```\n:::\n:::\n\n\nThe VIFs are a bit high - especially the GVIF for Group would be a cause for concern if this was not just a demo analysis! However, as we only want to implement a multinomial mixed-effects model here and not provide a proper, clean analysis, we will ignore this issue here. \n\nIn a next step, we visualize effects to get a better understanding of how the predictors that are part of the fixed-effect structure of the mode affect the outcome (the response variable).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsjPlot::plot_model(m1.mn)\n```\n\n::: {.cell-output-display}\n![](regression_files/figure-html/mult08-1.png){width=672}\n:::\n:::\n\n\nFinally, we can extract an alternative summary table produced by the `tab_model` function from the `sjPlot` package [see @sjPlot].\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsjPlot::tab_model(m1.mn)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table style=\"border-collapse:collapse; border:none;\">\n<tr>\n<th style=\"border-top: double; text-align:center; font-style:normal; font-weight:bold; padding:0.2cm;  text-align:left; \">&nbsp;</th>\n<th colspan=\"3\" style=\"border-top: double; text-align:center; font-style:normal; font-weight:bold; padding:0.2cm; \">Response</th>\n</tr>\n<tr>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  text-align:left; \">Predictors</td>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  \">Estimates</td>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  \">CI</td>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  \">p</td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; \">NumeralNoun~(Intercept)</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">1.09</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">&#45;0.66&nbsp;&ndash;&nbsp;2.84</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.224</td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; \">QuantAdjNoun~(Intercept)</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.57</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">&#45;0.79&nbsp;&ndash;&nbsp;1.93</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.410</td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; \">QuantNoun~(Intercept)</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">&#45;2.46</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">&#45;4.42&nbsp;&ndash;&nbsp;-0.51</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"><strong>0.014</strong></td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; \">NumeralNoun~GenderMale</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.07</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">&#45;0.32&nbsp;&ndash;&nbsp;0.46</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.726</td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; \">QuantAdjNoun~GenderMale</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.30</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">&#45;0.19&nbsp;&ndash;&nbsp;0.79</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.229</td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; \">QuantNoun~GenderMale</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">&#45;0.38</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">&#45;1.23&nbsp;&ndash;&nbsp;0.47</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.385</td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; \">NumeralNoun~GroupGerman<br>Mono</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">&#45;3.31</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">&#45;3.91&nbsp;&ndash;&nbsp;-2.70</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"><strong>&lt;0.001</strong></td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; \">QuantAdjNoun~GroupGerman<br>Mono</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">&#45;3.88</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">&#45;4.62&nbsp;&ndash;&nbsp;-3.13</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"><strong>&lt;0.001</strong></td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; \">QuantNoun~GroupGerman<br>Mono</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">&#45;2.49</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">&#45;4.09&nbsp;&ndash;&nbsp;-0.88</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"><strong>0.002</strong></td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; \">NumeralNoun~GroupL2<br>Advanced</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">&#45;0.46</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">&#45;1.07&nbsp;&ndash;&nbsp;0.15</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.136</td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; \">QuantAdjNoun~GroupL2<br>Advanced</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">&#45;2.20</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">&#45;2.88&nbsp;&ndash;&nbsp;-1.51</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"><strong>&lt;0.001</strong></td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; \">QuantNoun~GroupL2<br>Advanced</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.78</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">&#45;0.57&nbsp;&ndash;&nbsp;2.13</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.257</td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; \">NumeralNoun~GroupL2<br>Intermediate</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">&#45;1.18</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">&#45;1.81&nbsp;&ndash;&nbsp;-0.55</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"><strong>&lt;0.001</strong></td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; \">QuantAdjNoun~GroupL2<br>Intermediate</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">&#45;3.03</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">&#45;3.81&nbsp;&ndash;&nbsp;-2.24</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"><strong>&lt;0.001</strong></td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; \">QuantNoun~GroupL2<br>Intermediate</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">&#45;0.18</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">&#45;1.64&nbsp;&ndash;&nbsp;1.28</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.812</td>\n</tr>\n\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; padding-top:0.1cm; padding-bottom:0.1cm;\">N <sub>Item</sub></td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left;\" colspan=\"3\">10</td>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; padding-top:0.1cm; padding-bottom:0.1cm; border-top:1px solid;\">Observations</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left; border-top:1px solid;\" colspan=\"3\">1090</td>\n</tr>\n\n</table>\n\n`````\n:::\n:::\n\n\n***\n\nThis is the final step in implementing a a mixed-effects multinomial regression model using the `mblogit` function from the `mclogit` package [see @mclogit]. We are aware that the analysis shown here is supervifial(!) - but please keep in mind that we just wanted to showcase the implementation here rather than providing a properly and carefully done analysis.\n\n\n## Mixed-Effects Ordinal Regression{-}\n\nIn this section, we will strictly focus on how to implement a mixed-effects ordinal regression model using the `clmm` function from the `ordinal` package [see @ordinal]. This type of regression model is extremely useful when dealing with Likert data or other types of questionnaire and survey data where the responses have some kind of hierarchical structure (i.e. responses are not truly independent because they come from different points in time or from different regions).\nload data\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# rating experiment data\nratex  <- base::readRDS(url(\"https://slcladal.github.io/data/ratex.rda\", \"rb\"))\n# inspect data\nhead(ratex)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  Rater Child Group       Accent AccentNumeric       Family\n1    R1  C001 Child StrongAccent             2 DomBilingual\n2    R2  C001 Child StrongAccent             2 DomBilingual\n3    R3  C001 Child StrongAccent             2 DomBilingual\n4    R4  C001 Child StrongAccent             2 DomBilingual\n5    R5  C001 Child StrongAccent             2 DomBilingual\n6    R6  C001 Child StrongAccent             2 DomBilingual\n```\n:::\n:::\n\n\nWe now tabulate the data to get a better understanding of the data structure.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nratex %>%\n  dplyr::group_by(Family, Accent) %>%\n  dplyr::summarise(Frequency = n()) %>%\n  tidyr::spread(Accent, Frequency)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`summarise()` has grouped output by 'Family'. You can override using the\n`.groups` argument.\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 Ã— 4\n# Groups:   Family [3]\n  Family         NoAccent StrongAccent WeakAccent\n  <fct>             <int>        <int>      <int>\n1 DomBilingual         80          145        174\n2 EqualBilingual       20           22         63\n3 Monolingual         209            1         41\n```\n:::\n:::\n\n\nNext, we visualize the data to inspect its properties.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nratex %>%\n  ggplot(aes(Family, AccentNumeric, color = Group)) + \n  stat_summary(fun = mean, geom = \"point\") +          \n  stat_summary(fun.data = mean_cl_boot, geom = \"errorbar\", width = 0.2) +\n  theme_bw() +\n  theme(legend.position = \"top\") +\n  scale_color_manual(values = c(\"gray20\", \"gray70\"))\n```\n\n::: {.cell-output-display}\n![](regression_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\n\nAn alternative plot shows other properties of the data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nratex %>%\n  dplyr::group_by(Family, Rater, Group) %>%\n  dplyr::summarise(AccentMean = mean(AccentNumeric)) %>%\n  ggplot(aes(Family, AccentMean, fill = Group)) + \n  geom_boxplot() +\n  theme_bw() +\n  theme(legend.position = \"top\") +\n  scale_fill_manual(values = c(\"gray50\", \"gray85\"))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`summarise()` has grouped output by 'Family', 'Rater'. You can override using\nthe `.groups` argument.\n```\n:::\n\n::: {.cell-output-display}\n![](regression_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\n\nWe now start the modeling by generating a model with Family as the sole predictor.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# fit baseline model\nm1.or <- clmm(Accent ~ (1|Rater) + Family, link=\"logit\", data = ratex)\n# test for incomplete information\nifelse(min(ftable(ratex$Accent, ratex$Family)) == 0, \"incomplete information\", \"okay\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"okay\"\n```\n:::\n\n```{.r .cell-code}\n# extract aic\naic.glmer <- AIC(logLik(m1.or))\n# inspect aic\naic.glmer\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1380.25888675\n```\n:::\n\n```{.r .cell-code}\n# summarize model\nsummary(m1.or)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCumulative Link Mixed Model fitted with the Laplace approximation\n\nformula: Accent ~ (1 | Rater) + Family\ndata:    ratex\n\n link  threshold nobs logLik  AIC     niter    max.grad cond.H \n logit flexible  755  -685.13 1380.26 371(435) 3.04e-07 2.9e+05\n\nRandom effects:\n Groups Name        Variance            Std.Dev.       \n Rater  (Intercept) 0.00000000353434196 0.0000594503318\nNumber of groups:  Rater 21 \n\nCoefficients:\n                         Estimate   Std. Error   z value             Pr(>|z|)\nFamilyEqualBilingual  0.477744668  0.214314867   2.22917             0.025802\nFamilyMonolingual    -2.550224082  0.198852092 -12.82473 < 0.0000000000000002\n                        \nFamilyEqualBilingual *  \nFamilyMonolingual    ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThreshold coefficients:\n                             Estimate    Std. Error   z value\nNoAccent|StrongAccent   -1.0798112480  0.1058010986 -10.20605\nStrongAccent|WeakAccent  0.1060945025  0.0951357711   1.11519\n```\n:::\n:::\n\n\nWe can now perform Post-Hoc tests to see which comparisons are significant.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlsmeans(m1.or, pairwise~Family, adjust=\"tukey\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$lsmeans\n Family         lsmean     SE  df asymp.LCL asymp.UCL\n DomBilingual    0.487 0.0914 Inf     0.308     0.666\n EqualBilingual  0.965 0.1954 Inf     0.582     1.347\n Monolingual    -2.063 0.1740 Inf    -2.404    -1.722\n\nConfidence level used: 0.95 \n\n$contrasts\n contrast                      estimate    SE  df z.ratio p.value\n DomBilingual - EqualBilingual   -0.478 0.214 Inf  -2.229  0.0664\n DomBilingual - Monolingual       2.550 0.199 Inf  12.825  <.0001\n EqualBilingual - Monolingual     3.028 0.265 Inf  11.438  <.0001\n\nP value adjustment: tukey method for comparing a family of 3 estimates \n```\n:::\n:::\n\n\nFinally, we can summarize the model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsjPlot::tab_model(m1.or)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table style=\"border-collapse:collapse; border:none;\">\n<tr>\n<th style=\"border-top: double; text-align:center; font-style:normal; font-weight:bold; padding:0.2cm;  text-align:left; \">&nbsp;</th>\n<th colspan=\"3\" style=\"border-top: double; text-align:center; font-style:normal; font-weight:bold; padding:0.2cm; \">Accent</th>\n</tr>\n<tr>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  text-align:left; \">Predictors</td>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  \">Odds Ratios</td>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  \">CI</td>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  \">p</td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; \">NoAccent|StrongAccent</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.34</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.28&nbsp;&ndash;&nbsp;0.42</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"><strong>&lt;0.001</strong></td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; \">StrongAccent|WeakAccent</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">1.11</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.92&nbsp;&ndash;&nbsp;1.34</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.265</td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; \">Family [EqualBilingual]</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">1.61</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">1.06&nbsp;&ndash;&nbsp;2.45</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"><strong>0.026</strong></td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; \">Family [Monolingual]</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.08</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.05&nbsp;&ndash;&nbsp;0.12</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"><strong>&lt;0.001</strong></td>\n</tr>\n<tr>\n<td colspan=\"4\" style=\"font-weight:bold; text-align:left; padding-top:.8em;\">Random Effects</td>\n</tr>\n\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; padding-top:0.1cm; padding-bottom:0.1cm;\">&sigma;<sup>2</sup></td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left;\" colspan=\"3\">3.29</td>\n</tr>\n\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; padding-top:0.1cm; padding-bottom:0.1cm;\">&tau;<sub>00</sub> <sub>Rater</sub></td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left;\" colspan=\"3\">0.00</td>\n\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; padding-top:0.1cm; padding-bottom:0.1cm;\">N <sub>Rater</sub></td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left;\" colspan=\"3\">21</td>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; padding-top:0.1cm; padding-bottom:0.1cm; border-top:1px solid;\">Observations</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left; border-top:1px solid;\" colspan=\"3\">755</td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; padding-top:0.1cm; padding-bottom:0.1cm;\">Marginal R<sup>2</sup> / Conditional R<sup>2</sup></td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left;\" colspan=\"3\">0.325 / NA</td>\n</tr>\n\n</table>\n\n`````\n:::\n:::\n\n\n***\n\nAnd we can visualize the effects.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_model(m1.or, type = \"pred\", terms = c(\"Family\"))\n```\n\n::: {.cell-output-display}\n![](regression_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\nThat's it for this tutorial. We hope that you have enjoyed this tutorial and learned how to perform regression analysis including model fitting and model diagnostics as well as reporting regression results.\n\n\n# Citation & Session Info {-}\n\nSchweinberger, Martin. 2022. *Fixed- and Mixed-Effects Regression Models in R*. Brisbane: The University of Queensland. url: https://slcladal.github.io/regression.html (Version 2022.08.31).\n\n```\n@manual{schweinberger2022regression,\n  author = {Schweinberger, Martin},\n  title = {Fixed- and Mixed-Effects Regression Models in R},\n  note = {https://slcladal.github.io/regression.html},\n  year = {2022},\n  organization = \"The University of Queensland, Australia. School of Languages and Cultures},\n  address = {Brisbane},\n  edition = {2022.08.31}\n}\n```\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsessionInfo()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nR version 4.2.1 (2022-06-23)\nPlatform: x86_64-pc-linux-gnu (64-bit)\nRunning under: Ubuntu 22.04.1 LTS\n\nMatrix products: default\nBLAS:   /usr/lib/x86_64-linux-gnu/blas/libblas.so.3.10.0\nLAPACK: /usr/lib/x86_64-linux-gnu/lapack/liblapack.so.3.10.0\n\nlocale:\n [1] LC_CTYPE=en_AU.UTF-8       LC_NUMERIC=C              \n [3] LC_TIME=en_AU.UTF-8        LC_COLLATE=en_AU.UTF-8    \n [5] LC_MONETARY=en_AU.UTF-8    LC_MESSAGES=en_AU.UTF-8   \n [7] LC_PAPER=en_AU.UTF-8       LC_NAME=C                 \n [9] LC_ADDRESS=C               LC_TELEPHONE=C            \n[11] LC_MEASUREMENT=en_AU.UTF-8 LC_IDENTIFICATION=C       \n\nattached base packages:\n[1] grid      stats     graphics  grDevices datasets  utils     methods  \n[8] base     \n\nother attached packages:\n [1] vcd_1.4-10         tibble_3.1.7       stringr_1.4.0      sjPlot_2.8.10     \n [5] robustbase_0.95-0  rms_6.3-0          SparseM_1.81       ordinal_2019.12-10\n [9] nlme_3.1-158       MuMIn_1.46.0       mclogit_0.9.4.2    MASS_7.3-58.1     \n[13] lme4_1.1-30        Matrix_1.4-1       knitr_1.39         Hmisc_4.7-1       \n[17] Formula_1.2-4      survival_3.4-0     lattice_0.20-45    ggfortify_0.4.14  \n[21] emmeans_1.7.5      effects_4.2-1      car_3.1-0          carData_3.0-5     \n[25] Boruta_7.0.0       vip_0.3.2          ggpubr_0.4.0       ggplot2_3.3.6     \n[29] flextable_0.7.3    dplyr_1.0.9       \n\nloaded via a namespace (and not attached):\n  [1] utf8_1.2.2           tidyselect_1.1.2     htmlwidgets_1.5.4   \n  [4] ranger_0.14.1        pROC_1.18.0          munsell_0.5.0       \n  [7] codetools_0.2-18     effectsize_0.7.0     interp_1.1-2        \n [10] future_1.26.1        withr_2.5.0          colorspace_2.0-3    \n [13] highr_0.9            uuid_1.1-0           rstudioapi_0.13     \n [16] stats4_4.2.1         ggsignif_0.6.3       officer_0.4.3       \n [19] listenv_0.8.0        labeling_0.4.2       report_0.5.1        \n [22] repr_1.1.4           farver_2.1.1         datawizard_0.4.1    \n [25] coda_0.19-4          parallelly_1.32.0    vctrs_0.4.1         \n [28] generics_0.1.3       TH.data_1.1-1        ipred_0.9-13        \n [31] xfun_0.31            R6_2.5.1             memisc_0.99.30.7    \n [34] assertthat_0.2.1     scales_1.2.0         multcomp_1.4-19     \n [37] nnet_7.3-17          gtable_0.3.0         globals_0.15.1      \n [40] klippy_0.0.0.9500    sandwich_3.0-2       timeDate_3043.102   \n [43] rlang_1.0.4          MatrixModels_0.5-0   systemfonts_1.0.4   \n [46] splines_4.2.1        rstatix_0.7.0        ModelMetrics_1.2.2.2\n [49] broom_1.0.0          checkmate_2.1.0      yaml_2.3.5          \n [52] reshape2_1.4.4       abind_1.4-5          modelr_0.1.8        \n [55] backports_1.4.1      caret_6.0-93         tools_4.2.1         \n [58] lava_1.6.10          ellipsis_0.3.2       kableExtra_1.3.4    \n [61] RColorBrewer_1.1-3   proxy_0.4-27         Rcpp_1.0.8.3        \n [64] plyr_1.8.7           base64enc_0.1-3      purrr_0.3.4         \n [67] rpart_4.1.16         deldir_1.0-6         cowplot_1.1.1       \n [70] zoo_1.8-10           cluster_2.1.3        survey_4.1-1        \n [73] magrittr_2.0.3       data.table_1.14.2    lmtest_0.9-40       \n [76] mvtnorm_1.1-3        sjmisc_2.8.9         evaluate_0.15       \n [79] xtable_1.8-4         sjstats_0.18.1       jpeg_0.1-9          \n [82] gridExtra_2.3        ggeffects_1.1.2      compiler_4.2.1      \n [85] crayon_1.5.1         minqa_1.2.4          htmltools_0.5.2     \n [88] mgcv_1.8-40          tidyr_1.2.0          expm_0.999-6        \n [91] lubridate_1.8.0      DBI_1.1.3            sjlabelled_1.2.0    \n [94] boot_1.3-28          cli_3.3.0            mitools_2.4         \n [97] parallel_4.2.1       insight_0.18.0       gower_1.0.0         \n[100] pkgconfig_2.0.3      numDeriv_2016.8-1.1  foreign_0.8-82      \n[103] recipes_1.0.1        xml2_1.3.3           foreach_1.5.2       \n[106] svglite_2.1.0        hardhat_1.2.0        webshot_0.5.3       \n[109] estimability_1.4     prodlim_2019.11.13   rvest_1.0.2         \n[112] digest_0.6.29        parameters_0.18.1    msm_1.6.9           \n[115] rmarkdown_2.14       htmlTable_2.4.1      gdtools_0.2.4       \n[118] quantreg_5.93        nloptr_2.0.3         lifecycle_1.0.1     \n[121] jsonlite_1.8.0       viridisLite_0.4.0    fansi_1.0.3         \n[124] pillar_1.7.0         fastmap_1.1.0        httr_1.4.3          \n[127] DEoptimR_1.0-11      glue_1.6.2           bayestestR_0.12.1   \n[130] zip_2.2.0            png_0.1-7            iterators_1.0.14    \n[133] class_7.3-20         stringi_1.7.8        performance_0.9.1   \n[136] polspline_1.1.20     latticeExtra_0.6-30  renv_0.15.4         \n[139] ucminf_1.1-4         e1071_1.7-11         future.apply_1.9.0  \n```\n:::\n:::\n\n\n\n***\n\n[Back to top](#introduction)\n\n[Back to HOME](https://slcladal.github.io/index.html)\n\n***\n\n\n# References{-}\n\n\n\n",
    "supporting": [
      "regression_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"site_libs/tabwid-1.0.0/tabwid.css\" rel=\"stylesheet\" />\n<link href=\"site_libs/tabwid-1.0.0/scrool.css\" rel=\"stylesheet\" />\n<script src=\"site_libs/clipboard-1.7.1/clipboard.min.js\"></script>\n<link href=\"site_libs/primer-tooltips-1.4.0/build.css\" rel=\"stylesheet\" />\n<link href=\"site_libs/klippy-0.0.0.9500/css/klippy.min.css\" rel=\"stylesheet\" />\n<script src=\"site_libs/klippy-0.0.0.9500/js/klippy.min.js\"></script>\n<script src=\"site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}