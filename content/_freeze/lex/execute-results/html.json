{
  "hash": "6478f29abf57e05d69b783419c24f6f7",
  "result": {
    "markdown": "---\ntitle: \"Lexicography with R\"\nauthor: \"Martin Schweinberger\"\ndate: \"2022-08-31\"\n---\n\n::: {.cell}\n::: {.cell-output-display}\n![](https://slcladal.github.io/images/uq1.jpg){width=100%}\n:::\n:::\n\n\n# Introduction{-}\n\nThis tutorial introduces lexicography with R and shows how to use R to create dictionaries and find synonyms through determining semantic similarity in R. While the initial example focuses on English, subsequent sections show how easily this approach can be generalized to languages other than English (e.g. German, French, Spanish, Italian, or Dutch). The entire R-markdown document for the sections below can be downloaded [here](https://slcladal.github.io/content/lex.Rmd).\n\nTraditionally, dictionaries are listing of words that are commonly arranged alphabetically, which may include information on definitions, usage, etymologies, pronunciations, translation, etc. [see @agnes2002webster; @steiner1985dictionaries]. If such dictionaries, that are typically published as books contain translations of words in other languages, they are referred to as lexicons. Therefore, lexicographical references show the inter-relationships among lexical data, i.e. words.\n\nSimilarly, in computational linguistics, dictionaries represent a specific format of data where elements are linked to or paired with other elements in a  systematic way. *Computational lexicology* refers to a branch of computational linguistics, which is concerned with the use of computers in the study of lexicons. Hence, computational lexicology has been defined as the use of computers in the study of machine-readable dictionaries  [see e.g. @amsler1981structure]. Computational lexicology is distinguished from *computational lexicography*, which can be defined as the use of computers in the construction of dictionaries which is the focus of this tutorial. It should be noted, thought, that computational lexicology and computational lexicography are often used synonymously. \n\n## Preparation and session set up{-}\n\nThis tutorial is based on R. If you have not installed R or are new to it, you will find an introduction to and more information how to use R [here](https://slcladal.github.io/Intror.html). For this tutorials, we need to install certain *packages* from an R *library* so that the scripts shown below are executed without errors. Before turning to the code below, please install the packages by running the code below this paragraph. If you have already installed the packages mentioned below, then you can skip ahead and ignore this section. To install the necessary packages, simply run the following code - it may take some time (between 1 and 5 minutes to install all of the packages so you do not need to worry if it takes some time).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# set options\noptions(stringsAsFactors = F)         # no automatic data transformation\noptions(\"scipen\" = 100, \"digits\" = 4) # suppress math annotation\n# install packages\ninstall.packages(\"dplyr\")\ninstall.packages(\"stringr\")\ninstall.packages(\"udpipe\")\ninstall.packages(\"tidytext\")\ninstall.packages(\"coop\")\ninstall.packages(\"cluster\")\ninstall.packages(\"flextable\")\ninstall.packages(\"textdata\")\n# install klippy for copy-to-clipboard button in code chunks\ninstall.packages(\"remotes\")\nremotes::install_github(\"rlesur/klippy\")\n```\n:::\n\n\nIn a next step, we load the packages.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# load packages\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(udpipe)\nlibrary(tidytext)\nlibrary(coop)\nlibrary(cluster)\nlibrary(flextable)\n# activate klippy for copy-to-clipboard button\nklippy::klippy()\n```\n\n::: {.cell-output-display}\n```{=html}\n<script>\n  addClassKlippyTo(\"pre.r, pre.markdown\");\n  addKlippy('left', 'top', 'auto', '1', 'Copy code', 'Copied!');\n</script>\n```\n:::\n:::\n\n\n\nOnce you have installed R and RStudio and once you have initiated the session by executing the code shown above, you are good to go.\n\n# Creating dictionaries\n\nIn a first step, we load a text. In this case, we load George Orwell's *Nineteen Eighty-Four*.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntext <- readLines(\"https://slcladal.github.io/data/orwell.txt\") %>%\n  paste0(collapse = \" \")\n# show the first 500 characters of the text\nsubstr(text, start=1, stop=500)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"1984 George Orwell Part 1, Chapter 1 It was a bright cold day in April, and the clocks were striking thirteen. Winston Smith, his chin nuzzled into his breast in an effort to escape the vile wind, slipped quickly through the glass doors of Victory Mansions, though not quickly enough to prevent a swirl of gritty dust from entering along with him. The hallway smelt of boiled cabbage and old rag mats. At one end of it a coloured poster, too large for indoor display, had been tacked to the wall. It \"\n```\n:::\n:::\n\n\nNext, we download a `udpipe` language model. In this case, we download a `udpipe` language model for English, but you can download `udpipe`  language models for more than 60 languages.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# download language model\nm_eng   <- udpipe::udpipe_download_model(language = \"english-ewt\")\n```\n:::\n\n\nIn my case, I have stored this model in a folder called `udpipemodels` and you can load it (if you have also save the model in a folder called `udpipemodels` within your Rproj folder as shown below.\n)\n\n::: {.cell}\n\n```{.r .cell-code}\n# load language model from your computer after you have downloaded it once\nm_eng <- udpipe_load_model(file = here::here(\"udpipemodels\", \"english-ewt-ud-2.5-191206.udpipe\"))\n```\n:::\n\n\nIn a next step, we implement the part-of-speech tagger.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# tokenise, tag, dependency parsing\ntext_ann <- udpipe::udpipe_annotate(m_eng, x = text) %>%\n  # convert into a data frame\n  as.data.frame() %>%\n  # remove columns we do not need\n  dplyr::select(-sentence, -paragraph_id, -sentence_id, -feats, \n                -head_token_id, -dep_rel, -deps, -misc)\n# inspect\nhead(text_ann, 10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   doc_id token_id   token   lemma  upos xpos\n1    doc1        1    1984    1984 PROPN  NNP\n2    doc1        2  George  George PROPN  NNP\n3    doc1        3  Orwell  Orwell PROPN  NNP\n4    doc1        4    Part    part PROPN  NNP\n5    doc1        5       1       1   NUM   CD\n6    doc1        6       ,       , PUNCT    ,\n7    doc1        7 Chapter chapter PROPN  NNP\n8    doc1        8       1       1   NUM   CD\n9    doc1        1      It      it  PRON  PRP\n10   doc1        2     was      be   AUX  VBD\n```\n:::\n:::\n\n\n\nWe can now use the resulting table to generate a first, basic dictionary that holds information about the word form (*token*), the part-of speech tag (*upos*), the lemmatized word type (*lemma*), and the frequency with which the word form is used as that part-of speech.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# generate dictionary\ntext_dict_raw <- text_ann %>%\n  # remove non-words\n  dplyr::filter(!stringr::str_detect(token, \"\\\\W\")) %>%\n  # filter out numbers\n  dplyr::filter(!stringr::str_detect(token, \"[0-9]\")) %>%\n  # group data\n  dplyr::group_by(token, lemma, upos) %>%\n  # summarize data\n  dplyr::summarise(frequency = dplyr::n()) %>%\n  # arrange by frequency\n  dplyr::arrange(-frequency)\n# inspect\nhead(text_dict_raw, 10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 10 × 4\n# Groups:   token, lemma [10]\n   token lemma upos  frequency\n   <chr> <chr> <chr>     <int>\n 1 the   the   DET        5249\n 2 of    of    ADP        2908\n 3 a     a     DET        2277\n 4 and   and   CCONJ      2064\n 5 was   be    AUX        1795\n 6 in    in    ADP        1446\n 7 to    to    PART       1336\n 8 it    it    PRON       1295\n 9 he    he    PRON       1270\n10 had   have  AUX        1018\n```\n:::\n:::\n\n\nThe above display is ordered by frequency but it is, of course more common, to arrange dictionaries alphabetically. To do this, we can simply use the `àrrange` function from the `dplyr` package as shown below.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# generate dictionary\ntext_dict <- text_dict_raw %>%\n  # arrange alphabetically\n  dplyr::arrange(token)\n# inspect\nhead(text_dict, 10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 10 × 4\n# Groups:   token, lemma [7]\n   token     lemma     upos  frequency\n   <chr>     <chr>     <chr>     <int>\n 1 a         a         DET        2277\n 2 A         a         DET         107\n 3 A         a         NOUN          1\n 4 Aaronson  Aaronson  PROPN         8\n 5 aback     aback     ADV           1\n 6 aback     aback     NOUN          1\n 7 abandon   abandon   VERB          2\n 8 abandon   abandon   ADP           1\n 9 abandoned abandon   VERB          4\n10 abasement abasement NOUN          1\n```\n:::\n:::\n\n\nWe have now generated a basic dictionary of English but, as you can see above, there are still some errors as the part-of-speech tagging was not perfect. As such, you will still need to check and edit the results manually but you have already a rather clean dictionary based on George Orwell's *Nineteen Eighty-Four* to work with. \n\n\n## Correcting and Extending Dictionaries{-}\n\nFortunately, it is very easy in R to correct entries, i.e., changing lemmas or part-of-speech tags, and to extend entries, i.e., adding additional layers of information such as urls or examples. \n\nWe will begin to extend our dictionary by adding an additional column (called `annotation`) in which we will add information.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# generate dictionary\ntext_dict_ext <- text_dict %>%\n  # removing an entry\n  dplyr::filter(!(lemma == \"a\" & upos == \"NOUN\")) %>%\n  # editing entries\n  dplyr::mutate(upos = ifelse(lemma == \"aback\" & upos == \"NOUN\", \"PREP\", upos)) %>%\n  # adding comments \n  dplyr::mutate(comment = dplyr::case_when(lemma == \"a\" ~ \"also an before vowels\",\n                                           lemma == \"Aaronson\" ~ \"Name of someone.\", \n                                           T ~ \"\"))\n# inspect\nhead(text_dict_ext, 10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 10 × 5\n# Groups:   token, lemma [8]\n   token     lemma     upos  frequency comment                \n   <chr>     <chr>     <chr>     <int> <chr>                  \n 1 a         a         DET        2277 \"also an before vowels\"\n 2 A         a         DET         107 \"also an before vowels\"\n 3 Aaronson  Aaronson  PROPN         8 \"Name of someone.\"     \n 4 aback     aback     ADV           1 \"\"                     \n 5 aback     aback     PREP          1 \"\"                     \n 6 abandon   abandon   VERB          2 \"\"                     \n 7 abandon   abandon   ADP           1 \"\"                     \n 8 abandoned abandon   VERB          4 \"\"                     \n 9 abasement abasement NOUN          1 \"\"                     \n10 abashed   abashed   VERB          1 \"\"                     \n```\n:::\n:::\n\n\nTo make it a bit more interesting but also keep this tutorial simple and straight-forward, we will add information about the polarity and emotionally of the words in our dictionary. We can do this by performing a sentiment analysis on the lemmas using the `tidytext` package.\n\nThe `tidytext` package contains three sentiment dictionaries (`nrc`, `bing`, and `afinn`). For the present purpose, we use the `ncr`dictionary which represents the Word-Emotion Association Lexicon [@mohammad2013crowdsourcing]. The Word-Emotion Association Lexicon which comprises 10,170 terms, and in which lexical elements are assigned scores based on ratings gathered through the crowd-sourced Amazon Mechanical Turk service. For the Word-Emotion Association Lexicon raters were asked whether a given word was associated with one of eight emotions. The resulting associations between terms and emotions are based on 38,726 ratings from 2,216 raters who answered a sequence of questions for each word which were then fed into the emotion association rating [cf. @mohammad2013crowdsourcing]. Each term was rated 5 times. For 85 percent of words, at least 4 raters provided identical ratings. For instance, the word *cry* or *tragedy* are more readily associated with SADNESS while words such as *happy* or *beautiful* are indicative of JOY and words like *fit* or *burst* may indicate ANGER. This means that the sentiment analysis here allows us to investigate the expression of certain core emotions rather than merely classifying statements along the lines of a crude positive-negative distinction.\n\nTo be able to use the Word-Emotion Association Lexicon we need to add another column to our data frame called `word` which simply contains the lemmatized word. The reason is that the lexicon expects this column and only works if it finds a word column in the data. The code below shows how to add the emotion and polarity entries to our dictionary.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# generate dictionary\ntext_dict_snt <- text_dict_ext %>%\n  dplyr::mutate(word = lemma) %>%\n  dplyr::left_join(get_sentiments(\"nrc\")) %>%\n  dplyr::group_by(token, lemma, upos, comment) %>%\n  dplyr::summarise(sentiment = paste0(sentiment, collapse = \", \"))\n# inspect\nhead(text_dict_snt, 10) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 10 × 5\n# Groups:   token, lemma, upos [10]\n   token     lemma     upos  comment                 sentiment              \n   <chr>     <chr>     <chr> <chr>                   <chr>                  \n 1 a         a         DET   \"also an before vowels\" NA                     \n 2 A         a         DET   \"also an before vowels\" NA                     \n 3 Aaronson  Aaronson  PROPN \"Name of someone.\"      NA                     \n 4 aback     aback     ADV   \"\"                      NA                     \n 5 aback     aback     PREP  \"\"                      NA                     \n 6 abandon   abandon   ADP   \"\"                      fear, negative, sadness\n 7 abandon   abandon   VERB  \"\"                      fear, negative, sadness\n 8 abandoned abandon   VERB  \"\"                      fear, negative, sadness\n 9 abasement abasement NOUN  \"\"                      NA                     \n10 abashed   abashed   VERB  \"\"                      NA                     \n```\n:::\n:::\n\n\nThe resulting extended dictionary now contains not only the token, the lemma, and the pos-tag but also the sentiment from the Word-Emotion Association Lexicon.\n\n## Generating dictionaries for other languages{-}\n\nAs mentioned above, the procedure for generating dictionaries can easily be applied to languages other than English. If you want to follow exactly the procedure described above, then the language set of the TreeTagger is the limiting factors as its R implementation only supports English, German, French, Italian, Spanish, and Dutch. fa part-of-speech tagged text in another language is already available to you, and you do not require the TreeTagger for the part-of-speech tagging, then you can skip the code chunk that is related to the tagging and you can modify the procedure described above to virtually any language.\n\nWe will now briefly create a German dictionary based on a subsection of the fairy tales collected by the brothers Grimm to show how the above procedure can be applied to a language other than English. In a  first step, we load a German text into R. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngrimm <- readLines(\"https://slcladal.github.io/data/GrimmsFairytales.txt\",\n                   encoding = \"latin1\") %>%\n  paste0(collapse = \" \")\n# show the first 500 characters of the text\nsubstr(grimm, start=1, stop=200)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Der Froschkönig oder der eiserne Heinrich  Ein Märchen der Brüder Grimm Brüder Grimm  In den alten Zeiten, wo das Wünschen noch geholfen hat, lebte ein König, dessen Töchter waren alle schön; aber die\"\n```\n:::\n:::\n\n\n\nNext, we download a `udpipe` language model. In this case, we download a `udpipe` language model for German, but you can download `udpipe`  language models for more than 60 languages.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# download language model\nudpipe::udpipe_download_model(language = \"german-hdt\")\n```\n:::\n\n\nIn my case, I have stored this model in a folder called `udpipemodels` and you can load it (if you have also save the model in a folder called `udpipemodels` within your Rproj folder as shown below).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# load language model from your computer after you have downloaded it once\nm_ger <- udpipe_load_model(file = here::here(\"udpipemodels\",\n                                             #\"german-hdt-ud-2.5-191206.udpipe\"))\n                                             \"german-gsd-ud-2.5-191206.udpipe\"))\n```\n:::\n\n\nIn a next step, we generating the dictionary based on the brothers' Grimm fairy tales. We go through the same steps as for the English dictionary and collapse all the steps into a single code block. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# tokenise, tag, dependency parsing\ngrimm_ann <- udpipe::udpipe_annotate(m_ger, x = grimm) %>%\n  # convert into a data frame\n  as.data.frame() %>%\n  # remove non-words\n  dplyr::filter(!stringr::str_detect(token, \"\\\\W\")) %>%\n  # filter out numbers\n  dplyr::filter(!stringr::str_detect(token, \"[0-9]\")) %>%\n  dplyr::group_by(token, lemma, upos) %>%\n  dplyr::summarise(frequency = dplyr::n()) %>%\n  dplyr::arrange(lemma)\n# inspect\nhead(grimm_ann, 10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 10 × 4\n# Groups:   token, lemma [8]\n   token    lemma    upos  frequency\n   <chr>    <chr>    <chr>     <int>\n 1 A        A        NOUN          1\n 2 ab       ab       ADP          12\n 3 abends   abend    ADV           2\n 4 Abend    Abend    NOUN          3\n 5 abends   abends   ADV           1\n 6 aber     aber     ADJ           1\n 7 aber     aber     ADV          56\n 8 aber     aber     CCONJ        32\n 9 Aber     aber     CCONJ        16\n10 abfallen abfallen VERB          1\n```\n:::\n:::\n\n\n\n\n\n\nAs with the English dictionary, we have created a customized German dictionary based of a subsample of the brothers' Grimm fairy tales holding the word form(*token*), the part-of-speech tag (*tag*), the lemmatized word type (*lemma*), the general word class (*wclass*), ad the frequency with which a word form occurs as a part-of-speech in the data (*frequency*).\n\n# Finding synonyms: creating a thesaurus\n\nAnother task that is quite common in lexicography is to determine if words share some form of relationship such as whether they are synonyms or antonyms. In computational linguistics, this is commonly determined based on the collocational profiles of words. These collocational profiles are also called *word vectors* or *word embeddings* and approaches which determine semantic similarity based on collocational profiles or word embeddings are called distributional approaches (or distributional semantics). The basic assumption of distributional approaches is that words that occur in the same context and therefore have similar collocational profiles are also semantically similar. In fact, various packages, such as `qdap` or , `wordnet` already provide synonyms for terms (all of which are based on similar collocational profiles) but we would like to determine if words are similar without knowing it in advance. \n\nIn this example, we want to determine if two degree adverbs (such as *very*, *really*, *so*, *completely*, *totally*, *amazingly*, etc.) are synonymous and can therefore be exchanged without changing the meaning of the sentence (or, at least, not changing it dramatically). This is relevant in lexicography as such terms can then be linked to each other and inform readers that these words are interchangeable. \n\nAs a first step, we load the data which contains three columns: \n\n* one column holding the degree adverbs which is called *pint* \n\n* one column called *adjs* holding the adjectives that the degree adverbs have modified\n\n* one column called *remove* which contains the word *keep* and which we will remove as it is not relevant for this tutorial\n\nWhen loading the data, we \n\n* remove the *remove* column \n\n* rename the *pint* column as *degree_adverb* \n\n* rename the *adjs* column as *adjectives*\n\n* filter out all instances where the degree adverb column has the value `0` (which means that the adjective was not modified)\n\n* remove instances where *well* functions as a degree adverb (because it behaves rather differently from other degree adverbs)\n \n\n::: {.cell}\n\n```{.r .cell-code}\n# load data\ndegree_adverbs <- base::readRDS(url(\"https://slcladal.github.io/data/dad.rda\", \"rb\")) %>%\n  dplyr::select(-remove) %>%\n  dplyr::rename(degree_adverb = pint,\n                adjective = adjs) %>%\n  dplyr::filter(degree_adverb != \"0\",\n                degree_adverb != \"well\")\n# inspect\nhead(degree_adverbs, 10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   degree_adverb adjective\n1           real       bad\n2         really      nice\n3           very      good\n4         really     early\n5         really       bad\n6         really       bad\n7             so      long\n8         really wonderful\n9         pretty      good\n10        really      easy\n```\n:::\n:::\n\n\n\n\nIn a next step, we create a matrix from this data frame which maps how often a given amplifier co-occurred with a given adjective. In text mining, this format is called a text-document matrix or tdm (which is a transposed [document-term matrix](https://en.wikipedia.org/wiki/Document-term_matrix) of dtm).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# tabulate data (create term-document matrix)\ntdm <- ftable(degree_adverbs$adjective, degree_adverbs$degree_adverb)\n# extract amplifiers and adjectives \namplifiers <- as.vector(unlist(attr(tdm, \"col.vars\")[1]))\nadjectives <- as.vector(unlist(attr(tdm, \"row.vars\")[1]))\n# attach row and column names to tdm\nrownames(tdm) <- adjectives\ncolnames(tdm) <- amplifiers\n# inspect data\ntdm[1:5, 1:5]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          completely extremely pretty real really\nable               0         1      0    0      0\nactual             0         0      0    1      0\namazing            0         0      0    0      4\navailable          0         0      0    0      1\nbad                0         0      1    2      3\n```\n:::\n:::\n\n\nIn a next step, we extract the expected values of the co-occurrences if the amplifiers were distributed homogeneously and calculate the *Pointwise Mutual Information* (PMI) score and use that to then calculate the *Positive Pointwise Mutual Information* (PPMI) scores. According to @levshina2015linguistics 327 - referring to @bullinaria2007extracting - PPMI perform better than PMI as negative values are replaced with zeros. In a next step, we calculate the cosine similarity which will for the bases for the subsequent clustering.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# compute expected values\ntdm.exp <- chisq.test(tdm)$expected\n# calculate PMI and PPMI\nPMI <- log2(tdm/tdm.exp)\nPPMI <- ifelse(PMI < 0, 0, PMI)\n# calculate cosine similarity\ncosinesimilarity <- cosine(PPMI)\n# inspect cosine values\ncosinesimilarity[1:5, 1:5]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n           completely   extremely      pretty       real      really\ncompletely 1.00000000 0.204188725 0.000000000 0.05304354 0.126668434\nextremely  0.20418873 1.000000000 0.007319316 0.00000000 0.004235346\npretty     0.00000000 0.007319316 1.000000000 0.09441299 0.062323271\nreal       0.05304354 0.000000000 0.094412995 1.00000000 0.131957473\nreally     0.12666843 0.004235346 0.062323271 0.13195747 1.000000000\n```\n:::\n:::\n\n\nAs we have now obtained a similarity measure, we can go ahead and perform a cluster analysis on these similarity values. However, as we have to extract the maximum values in the similarity matrix that is not 1 as we will use this to create a distance matrix. While we could also have simply subtracted the cosine similarity values from 1 to convert the similarity matrix into a distance matrix, we follow the procedure proposed by @levshina2015linguistics.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# find max value that is not 1\ncosinesimilarity.test <- apply(cosinesimilarity, 1, function(x){\n  x <- ifelse(x == 1, 0, x) } )\nmaxval <- max(cosinesimilarity.test)\n# create distance matrix\namplifier.dist <- 1 - (cosinesimilarity/maxval)\nclustd <- as.dist(amplifier.dist)\n```\n:::\n\n\nIn a next step, we visualize the results of the semantic vector space model as a dendrogram.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# create cluster object\ncd <- hclust(clustd, method=\"ward.D\")    \n# plot cluster object\nplot(cd, main = \"\", sub = \"\", yaxt = \"n\", ylab = \"\", xlab = \"\", cex = .8)\n```\n\n::: {.cell-output-display}\n![](lex_files/figure-html/vsm8-1.png){width=672}\n:::\n:::\n\n\nThe clustering solution shows that, as expected, *completely*, *extremely*, and *totally* - while similar to each other and thus interchangeable with each other - form a separate cluster from all other amplifiers. In addition, *real* and *really* form a cluster together. The clustering of *very*, *pretty*, *so*, *really*, and *real* suggest that these amplifiers are more or less interchangeable with each other but not with *totally*, *completely*, and *extremely*. \n\nTo extract synonyms automatically, we can use the cosine similarity matrix that a´we generated before. This is what we need to do:\n\n* generate a column called word\n* replace the perfect similarity values of the diagonal with 0\n* look up the lowest value, i.e. the word that has the lowest distance to a given word\n* create a vector which holds those words (the synonym candidates).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsyntb <- cosinesimilarity %>%\n  as.data.frame() %>%\n  dplyr::mutate(word = colnames(cosinesimilarity)) %>%\n  dplyr::mutate_each(funs(replace(., . == 1, 0))) %>%\n  dplyr::mutate(synonym = colnames(.)[apply(.,1,which.max)]) %>%\n  dplyr::select(word, synonym)\nsyntb\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                 word    synonym\ncompletely completely  extremely\nextremely   extremely completely\npretty         pretty       real\nreal             real     really\nreally         really       real\nso                 so       real\ntotally       totally completely\nvery             very         so\n```\n:::\n:::\n\n\n***\n\n<div class=\"warning\" style='padding:0.1em; background-color:#51247a; color:#f2f2f2'>\n<span>\n<p style='margin-top:1em; text-align:center'>\n<b>NOTE</b><br><br>Remember that this is only a tutorial! A proper study would have to take the syntactic context into account because, while we can say *This really great tutorial helped me a lot*. we probably would not say *This so great tutorial helped me a lot*. This is because so syntactically more restricted and is strongly disfavored in attributive contexts. Therefore, the syntactic context would have to be considered in a more thorough study.</p>\n<p style='margin-left:1em;'>\n</p></span>\n</div>\n\n<div class=\"question\">` \n\n\n</div>`\n\n***\n\nThere are many more useful methods for identifying semantic similarity. A very useful method (which we have implemented here but only superficially is Semantic Vector Space Modeling. If you want to know more about this, this [tutorial by Gede Primahadi Wijaya Rajeg, Karlina Denistia, and Simon Musgrave](https://gederajeg.github.io/vector_space_model_indonesian/) [@rajeg2020semvec] is highly recommended and will give a better understanding of SVM but this should suffice to get you started.\n\n# Creating bilingual dictionaries\n\nDictionaries commonly contain information about elements. Bilingual or translation dictionaries represent a sub-category of dictionaries that provide a specific type of information about a given word: the translation of that word in another language. In principle, generating translation dictionaries is relatively easy and straight forward. However, not only is the devil hidden in the details but the generation of data-driven translation dictionaries also require a substantial data set consisting of sentences and their translation. This is often quite tricky as well aligned translations are unfortunately, and unexpectedly, rather hard to come by. \n\nDespite these issues, if you have access to clean and well aligned, parallel multilingual data, then you simply need to check which correlation between the word in language A and language B is the highest and you have a likely candidate for its translation. The same procedure can be extended to generate multilingual dictionaries. Problems arise due to grammatical differences between languages, idiomatic expressions, homonymy and polysemy as well as due to word class issues. The latter, word class issues, can be solved by part-of-speech tagging and then only considering words that belong to the same (or realistically similar) parts-of speech. The other issues can also be solved but require substantial amounts of (annotated) data.   \n\nTo explore how to generate a multilingual lexicon, we load a sample of English sentences and their German translations.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# load translations\ntranslations <- readLines(\"https://slcladal.github.io/data/translation.txt\",\n                          encoding = \"UTF-8\", skipNul = T)\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n```{=html}\n<div class=\"tabwid\"><style>.cl-7ba43d54{table-layout:auto;width:50%;}.cl-7b9e0952{font-family:'DejaVu Sans';font-size:12pt;font-weight:bold;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-7b9e0966{font-family:'DejaVu Sans';font-size:12pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-7b9e2f90{margin:0;text-align:center;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-7b9e7e64{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-7b9e7e78{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-7b9e7e8c{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-7b9e7e96{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}</style><table class='cl-7ba43d54'>\n```\n<caption class=\"Table Caption\">\n\nFirst 15 rows of the translations data.\n\n</caption>\n```{=html}\n<thead><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-7b9e7e96\"><p class=\"cl-7b9e2f90\"><span class=\"cl-7b9e0952\">.</span></p></td></tr></thead><tbody><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-7b9e7e64\"><p class=\"cl-7b9e2f90\"><span class=\"cl-7b9e0966\">Guten Tag! — Good day! </span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-7b9e7e78\"><p class=\"cl-7b9e2f90\"><span class=\"cl-7b9e0966\">Guten Morgen! — Good morning!</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-7b9e7e64\"><p class=\"cl-7b9e2f90\"><span class=\"cl-7b9e0966\">Guten Abend! — Good evening!</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-7b9e7e78\"><p class=\"cl-7b9e2f90\"><span class=\"cl-7b9e0966\">Hallo! — Hello!</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-7b9e7e64\"><p class=\"cl-7b9e2f90\"><span class=\"cl-7b9e0966\">Wo kommst du her? — Where are you from?</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-7b9e7e78\"><p class=\"cl-7b9e2f90\"><span class=\"cl-7b9e0966\">Woher kommen Sie? — Where are you from?</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-7b9e7e64\"><p class=\"cl-7b9e2f90\"><span class=\"cl-7b9e0966\">Ich bin aus Hamburg.  — I am from Hamburg.</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-7b9e7e78\"><p class=\"cl-7b9e2f90\"><span class=\"cl-7b9e0966\">Ich komme aus Hamburg. — I come from Hamburg.</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-7b9e7e64\"><p class=\"cl-7b9e2f90\"><span class=\"cl-7b9e0966\">Ich bin Deutscher. — I am German.</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-7b9e7e78\"><p class=\"cl-7b9e2f90\"><span class=\"cl-7b9e0966\">Schön Sie zu treffen. — Pleasure to meet you!</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-7b9e7e64\"><p class=\"cl-7b9e2f90\"><span class=\"cl-7b9e0966\">Wie lange lebst du schon in Brisbane? — How long have you been living in Brisbane?</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-7b9e7e78\"><p class=\"cl-7b9e2f90\"><span class=\"cl-7b9e0966\">Leben Sie schon lange hier? — Have you been living here for long?</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-7b9e7e64\"><p class=\"cl-7b9e2f90\"><span class=\"cl-7b9e0966\">Welcher Bus geht nach Brisbane? — Which bus goes to Brisbane?</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-7b9e7e78\"><p class=\"cl-7b9e2f90\"><span class=\"cl-7b9e0966\">Von welchem Gleis aus fährt der Zug? — Which platform is the train leaving from?</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-7b9e7e8c\"><p class=\"cl-7b9e2f90\"><span class=\"cl-7b9e0966\">Ist dies der Bus nach Toowong? — Is this the bus going to Toowong?</span></p></td></tr></tbody></table></div>\n```\n:::\n:::\n\n\n\nIn a next step, we generate separate tables which hold the German and English sentences. However, the sentences and their translations are identified by an identification number (*id*) so that we keep the information about which sentence is linked to which translation.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# german sentences\ngerman <- str_remove_all(translations, \" — .*\") %>%\n  str_remove_all(., \"[:punct:]\")\n# english sentences\nenglish <- str_remove_all(translations, \".* — \") %>%\n  str_remove_all(., \"[:punct:]\")\n# sentence id\nsentence <- 1:length(german)\n# combine into table\ngermantb <- data.frame(sentence, german)\n# sentence id\nsentence <- 1:length(english)\n# combine into table\nenglishtb <- data.frame(sentence, english)\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n```{=html}\n<div class=\"tabwid\"><style>.cl-7bb60e94{table-layout:auto;width:50%;}.cl-7bafa068{font-family:'DejaVu Sans';font-size:12pt;font-weight:bold;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-7bafa072{font-family:'DejaVu Sans';font-size:12pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-7bafadd8{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-7bafade2{margin:0;text-align:center;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-7bafd740{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-7bafd741{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-7bafd754{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-7bafd75e{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-7bafd75f{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-7bafd768{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-7bafd772{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-7bafd773{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}</style><table class='cl-7bb60e94'>\n```\n<caption class=\"Table Caption\">\n\nFirst 15 rows of the germantb data.\n\n</caption>\n```{=html}\n<thead><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-7bafd773\"><p class=\"cl-7bafadd8\"><span class=\"cl-7bafa068\">sentence</span></p></td><td class=\"cl-7bafd772\"><p class=\"cl-7bafade2\"><span class=\"cl-7bafa068\">german</span></p></td></tr></thead><tbody><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-7bafd741\"><p class=\"cl-7bafadd8\"><span class=\"cl-7bafa072\">1</span></p></td><td class=\"cl-7bafd740\"><p class=\"cl-7bafade2\"><span class=\"cl-7bafa072\">Guten Tag</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-7bafd75e\"><p class=\"cl-7bafadd8\"><span class=\"cl-7bafa072\">2</span></p></td><td class=\"cl-7bafd754\"><p class=\"cl-7bafade2\"><span class=\"cl-7bafa072\">Guten Morgen</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-7bafd741\"><p class=\"cl-7bafadd8\"><span class=\"cl-7bafa072\">3</span></p></td><td class=\"cl-7bafd740\"><p class=\"cl-7bafade2\"><span class=\"cl-7bafa072\">Guten Abend</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-7bafd75e\"><p class=\"cl-7bafadd8\"><span class=\"cl-7bafa072\">4</span></p></td><td class=\"cl-7bafd754\"><p class=\"cl-7bafade2\"><span class=\"cl-7bafa072\">Hallo</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-7bafd741\"><p class=\"cl-7bafadd8\"><span class=\"cl-7bafa072\">5</span></p></td><td class=\"cl-7bafd740\"><p class=\"cl-7bafade2\"><span class=\"cl-7bafa072\">Wo kommst du her</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-7bafd75e\"><p class=\"cl-7bafadd8\"><span class=\"cl-7bafa072\">6</span></p></td><td class=\"cl-7bafd754\"><p class=\"cl-7bafade2\"><span class=\"cl-7bafa072\">Woher kommen Sie</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-7bafd741\"><p class=\"cl-7bafadd8\"><span class=\"cl-7bafa072\">7</span></p></td><td class=\"cl-7bafd740\"><p class=\"cl-7bafade2\"><span class=\"cl-7bafa072\">Ich bin aus Hamburg </span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-7bafd75e\"><p class=\"cl-7bafadd8\"><span class=\"cl-7bafa072\">8</span></p></td><td class=\"cl-7bafd754\"><p class=\"cl-7bafade2\"><span class=\"cl-7bafa072\">Ich komme aus Hamburg</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-7bafd741\"><p class=\"cl-7bafadd8\"><span class=\"cl-7bafa072\">9</span></p></td><td class=\"cl-7bafd740\"><p class=\"cl-7bafade2\"><span class=\"cl-7bafa072\">Ich bin Deutscher</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-7bafd75e\"><p class=\"cl-7bafadd8\"><span class=\"cl-7bafa072\">10</span></p></td><td class=\"cl-7bafd754\"><p class=\"cl-7bafade2\"><span class=\"cl-7bafa072\">Schön Sie zu treffen</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-7bafd741\"><p class=\"cl-7bafadd8\"><span class=\"cl-7bafa072\">11</span></p></td><td class=\"cl-7bafd740\"><p class=\"cl-7bafade2\"><span class=\"cl-7bafa072\">Wie lange lebst du schon in Brisbane</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-7bafd75e\"><p class=\"cl-7bafadd8\"><span class=\"cl-7bafa072\">12</span></p></td><td class=\"cl-7bafd754\"><p class=\"cl-7bafade2\"><span class=\"cl-7bafa072\">Leben Sie schon lange hier</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-7bafd741\"><p class=\"cl-7bafadd8\"><span class=\"cl-7bafa072\">13</span></p></td><td class=\"cl-7bafd740\"><p class=\"cl-7bafade2\"><span class=\"cl-7bafa072\">Welcher Bus geht nach Brisbane</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-7bafd75e\"><p class=\"cl-7bafadd8\"><span class=\"cl-7bafa072\">14</span></p></td><td class=\"cl-7bafd754\"><p class=\"cl-7bafade2\"><span class=\"cl-7bafa072\">Von welchem Gleis aus fährt der Zug</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-7bafd768\"><p class=\"cl-7bafadd8\"><span class=\"cl-7bafa072\">15</span></p></td><td class=\"cl-7bafd75f\"><p class=\"cl-7bafade2\"><span class=\"cl-7bafa072\">Ist dies der Bus nach Toowong</span></p></td></tr></tbody></table></div>\n```\n:::\n:::\n\n\nWe now unnest the tokens (split the sentences into words) and subsequently add the translations which we again unnest. The resulting table consists of two columns holding German and English words. The relevant point here is that each German word is linked with each English word that occurs in the translated sentence.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(plyr)\n# tokenize by sentence: german\ntranstb <- germantb %>%\n  unnest_tokens(word, german) %>%\n  # add english data\n  plyr::join(., englishtb, by = \"sentence\") %>%\n  unnest_tokens(trans, english) %>%\n  dplyr::rename(german = word,\n                english = trans) %>%\n  dplyr::select(german, english) %>%\n  dplyr::mutate(german = factor(german),\n                english = factor(english))\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n```{=html}\n<div class=\"tabwid\"><style>.cl-7bc40c9c{table-layout:auto;width:25%;}.cl-7bc071fe{font-family:'DejaVu Sans';font-size:12pt;font-weight:bold;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-7bc071ff{font-family:'DejaVu Sans';font-size:12pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-7bc07c12{margin:0;text-align:center;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-7bc09fa8{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-7bc09fbc{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-7bc09fbd{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-7bc09fc6{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-7bc09fd0{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-7bc09fda{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-7bc09fdb{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-7bc09fe4{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}</style><table class='cl-7bc40c9c'>\n```\n<caption class=\"Table Caption\">\n\nFirst 15 rows of the transtb data.\n\n</caption>\n```{=html}\n<thead><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-7bc09fe4\"><p class=\"cl-7bc07c12\"><span class=\"cl-7bc071fe\">german</span></p></td><td class=\"cl-7bc09fdb\"><p class=\"cl-7bc07c12\"><span class=\"cl-7bc071fe\">english</span></p></td></tr></thead><tbody><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-7bc09fbc\"><p class=\"cl-7bc07c12\"><span class=\"cl-7bc071ff\">guten</span></p></td><td class=\"cl-7bc09fa8\"><p class=\"cl-7bc07c12\"><span class=\"cl-7bc071ff\">good</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-7bc09fc6\"><p class=\"cl-7bc07c12\"><span class=\"cl-7bc071ff\">guten</span></p></td><td class=\"cl-7bc09fbd\"><p class=\"cl-7bc07c12\"><span class=\"cl-7bc071ff\">day</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-7bc09fbc\"><p class=\"cl-7bc07c12\"><span class=\"cl-7bc071ff\">tag</span></p></td><td class=\"cl-7bc09fa8\"><p class=\"cl-7bc07c12\"><span class=\"cl-7bc071ff\">good</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-7bc09fc6\"><p class=\"cl-7bc07c12\"><span class=\"cl-7bc071ff\">tag</span></p></td><td class=\"cl-7bc09fbd\"><p class=\"cl-7bc07c12\"><span class=\"cl-7bc071ff\">day</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-7bc09fbc\"><p class=\"cl-7bc07c12\"><span class=\"cl-7bc071ff\">guten</span></p></td><td class=\"cl-7bc09fa8\"><p class=\"cl-7bc07c12\"><span class=\"cl-7bc071ff\">good</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-7bc09fc6\"><p class=\"cl-7bc07c12\"><span class=\"cl-7bc071ff\">guten</span></p></td><td class=\"cl-7bc09fbd\"><p class=\"cl-7bc07c12\"><span class=\"cl-7bc071ff\">morning</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-7bc09fbc\"><p class=\"cl-7bc07c12\"><span class=\"cl-7bc071ff\">morgen</span></p></td><td class=\"cl-7bc09fa8\"><p class=\"cl-7bc07c12\"><span class=\"cl-7bc071ff\">good</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-7bc09fc6\"><p class=\"cl-7bc07c12\"><span class=\"cl-7bc071ff\">morgen</span></p></td><td class=\"cl-7bc09fbd\"><p class=\"cl-7bc07c12\"><span class=\"cl-7bc071ff\">morning</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-7bc09fbc\"><p class=\"cl-7bc07c12\"><span class=\"cl-7bc071ff\">guten</span></p></td><td class=\"cl-7bc09fa8\"><p class=\"cl-7bc07c12\"><span class=\"cl-7bc071ff\">good</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-7bc09fc6\"><p class=\"cl-7bc07c12\"><span class=\"cl-7bc071ff\">guten</span></p></td><td class=\"cl-7bc09fbd\"><p class=\"cl-7bc07c12\"><span class=\"cl-7bc071ff\">evening</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-7bc09fbc\"><p class=\"cl-7bc07c12\"><span class=\"cl-7bc071ff\">abend</span></p></td><td class=\"cl-7bc09fa8\"><p class=\"cl-7bc07c12\"><span class=\"cl-7bc071ff\">good</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-7bc09fc6\"><p class=\"cl-7bc07c12\"><span class=\"cl-7bc071ff\">abend</span></p></td><td class=\"cl-7bc09fbd\"><p class=\"cl-7bc07c12\"><span class=\"cl-7bc071ff\">evening</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-7bc09fbc\"><p class=\"cl-7bc07c12\"><span class=\"cl-7bc071ff\">hallo</span></p></td><td class=\"cl-7bc09fa8\"><p class=\"cl-7bc07c12\"><span class=\"cl-7bc071ff\">hello</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-7bc09fc6\"><p class=\"cl-7bc07c12\"><span class=\"cl-7bc071ff\">wo</span></p></td><td class=\"cl-7bc09fbd\"><p class=\"cl-7bc07c12\"><span class=\"cl-7bc071ff\">where</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-7bc09fda\"><p class=\"cl-7bc07c12\"><span class=\"cl-7bc071ff\">wo</span></p></td><td class=\"cl-7bc09fd0\"><p class=\"cl-7bc07c12\"><span class=\"cl-7bc071ff\">are</span></p></td></tr></tbody></table></div>\n```\n:::\n:::\n\n\nBased on this table, we can now generate a term-document matrix which shows how frequently each word co-occurred in the translation of any of the sentences. For instance, the German word *alles* occurred one time in a translation of a sentence which contained the English word *all*.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# tabulate data (create term-document matrix)\ntdm <- ftable(transtb$german, transtb$english)\n# extract amplifiers and adjectives \ngerman <- as.vector(unlist(attr(tdm, \"col.vars\")[1]))\nenglish <- as.vector(unlist(attr(tdm, \"row.vars\")[1]))\n# attach row and column names to tdm\nrownames(tdm) <- english\ncolnames(tdm) <- german\n# inspect data\ntdm[1:10, 1:10]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n         a accident all am ambulance an and any anything are\nab       0        0   0  0         0  0   0   0        0   0\nabend    0        0   0  0         0  0   0   0        0   0\nallem    0        0   0  0         0  0   0   0        0   0\nalles    0        0   1  0         0  0   0   0        0   0\nam       0        0   0  0         0  0   0   0        0   0\nan       0        0   0  0         0  0   0   0        0   0\nanderen  1        0   0  0         0  0   0   0        0   0\napotheke 1        0   0  1         0  0   0   0        0   0\narzt     1        0   0  0         0  0   0   0        0   0\nauch     3        0   0  0         0  0   0   0        1   0\n```\n:::\n:::\n\n\nNow, we reformat this co-occurrence matrix so that we have the frequency information that is necessary for setting up 2x2 contingency tables which we will use to calculate the co-occurrence strength between each word and its potential translation. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncoocdf <- as.data.frame(as.matrix(tdm))\ncooctb <- coocdf %>%\n  dplyr::mutate(German = rownames(coocdf)) %>%\n  tidyr::gather(English, TermCoocFreq,\n                colnames(coocdf)[1]:colnames(coocdf)[ncol(coocdf)]) %>%\n  dplyr::mutate(German = factor(German),\n                English = factor(English)) %>%\n  dplyr::mutate(AllFreq = sum(TermCoocFreq)) %>%\n  dplyr::group_by(German) %>%\n  dplyr::mutate(TermFreq = sum(TermCoocFreq)) %>%\n  dplyr::ungroup(German) %>%\n  dplyr::group_by(English) %>%\n  dplyr::mutate(CoocFreq = sum(TermCoocFreq)) %>%\n  dplyr::arrange(German) %>%\n  dplyr::mutate(a = TermCoocFreq,\n                b = TermFreq - a,\n                c = CoocFreq - a, \n                d = AllFreq - (a + b + c)) %>%\n  dplyr::mutate(NRows = nrow(coocdf))%>%\n  dplyr::filter(TermCoocFreq > 0)\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n```{=html}\n<div class=\"tabwid\"><style>.cl-7bf1e626{table-layout:auto;width:75%;}.cl-7bed715e{font-family:'DejaVu Sans';font-size:12pt;font-weight:bold;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-7bed7168{font-family:'DejaVu Sans';font-size:12pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-7bed7df2{margin:0;text-align:center;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-7bed7df3{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-7bedb592{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-7bedb59c{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-7bedb59d{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-7bedb5a6{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-7bedb5a7{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-7bedb5a8{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-7bedb5a9{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-7bedb5b0{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-7bedb5b1{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-7bedb5b2{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-7bedb5b3{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-7bedb5ba{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-7bedb5bb{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-7bedb5bc{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-7bedb5c4{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-7bedb5c5{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}</style><table class='cl-7bf1e626'>\n```\n<caption class=\"Table Caption\">\n\nFirst 15 rows of the cooctb data.\n\n</caption>\n```{=html}\n<thead><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-7bedb5c4\"><p class=\"cl-7bed7df2\"><span class=\"cl-7bed715e\">German</span></p></td><td class=\"cl-7bedb5c5\"><p class=\"cl-7bed7df2\"><span class=\"cl-7bed715e\">English</span></p></td><td class=\"cl-7bedb5bb\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed715e\">TermCoocFreq</span></p></td><td class=\"cl-7bedb5bb\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed715e\">AllFreq</span></p></td><td class=\"cl-7bedb5bb\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed715e\">TermFreq</span></p></td><td class=\"cl-7bedb5bb\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed715e\">CoocFreq</span></p></td><td class=\"cl-7bedb5bb\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed715e\">a</span></p></td><td class=\"cl-7bedb5bb\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed715e\">b</span></p></td><td class=\"cl-7bedb5bb\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed715e\">c</span></p></td><td class=\"cl-7bedb5bb\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed715e\">d</span></p></td><td class=\"cl-7bedb5bc\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed715e\">NRows</span></p></td></tr></thead><tbody><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-7bedb592\"><p class=\"cl-7bed7df2\"><span class=\"cl-7bed7168\">ab</span></p></td><td class=\"cl-7bedb59d\"><p class=\"cl-7bed7df2\"><span class=\"cl-7bed7168\">departing</span></p></td><td class=\"cl-7bedb59c\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">1</span></p></td><td class=\"cl-7bedb59c\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">3,504</span></p></td><td class=\"cl-7bedb59c\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">5</span></p></td><td class=\"cl-7bedb59c\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">5</span></p></td><td class=\"cl-7bedb59c\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">1</span></p></td><td class=\"cl-7bedb59c\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">4</span></p></td><td class=\"cl-7bedb59c\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">4</span></p></td><td class=\"cl-7bedb59c\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">3,495</span></p></td><td class=\"cl-7bedb5a6\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">215</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-7bedb5b0\"><p class=\"cl-7bed7df2\"><span class=\"cl-7bed7168\">ab</span></p></td><td class=\"cl-7bedb5a9\"><p class=\"cl-7bed7df2\"><span class=\"cl-7bed7168\">is</span></p></td><td class=\"cl-7bedb5a7\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">1</span></p></td><td class=\"cl-7bedb5a7\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">3,504</span></p></td><td class=\"cl-7bedb5a7\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">5</span></p></td><td class=\"cl-7bedb5a7\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">116</span></p></td><td class=\"cl-7bedb5a7\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">1</span></p></td><td class=\"cl-7bedb5a7\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">4</span></p></td><td class=\"cl-7bedb5a7\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">115</span></p></td><td class=\"cl-7bedb5a7\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">3,384</span></p></td><td class=\"cl-7bedb5a8\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">215</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-7bedb592\"><p class=\"cl-7bed7df2\"><span class=\"cl-7bed7168\">ab</span></p></td><td class=\"cl-7bedb59d\"><p class=\"cl-7bed7df2\"><span class=\"cl-7bed7168\">the</span></p></td><td class=\"cl-7bedb59c\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">1</span></p></td><td class=\"cl-7bedb59c\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">3,504</span></p></td><td class=\"cl-7bedb59c\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">5</span></p></td><td class=\"cl-7bedb59c\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">125</span></p></td><td class=\"cl-7bedb59c\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">1</span></p></td><td class=\"cl-7bedb59c\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">4</span></p></td><td class=\"cl-7bedb59c\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">124</span></p></td><td class=\"cl-7bedb59c\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">3,375</span></p></td><td class=\"cl-7bedb5a6\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">215</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-7bedb5b0\"><p class=\"cl-7bed7df2\"><span class=\"cl-7bed7168\">ab</span></p></td><td class=\"cl-7bedb5a9\"><p class=\"cl-7bed7df2\"><span class=\"cl-7bed7168\">train</span></p></td><td class=\"cl-7bedb5a7\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">1</span></p></td><td class=\"cl-7bedb5a7\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">3,504</span></p></td><td class=\"cl-7bedb5a7\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">5</span></p></td><td class=\"cl-7bedb5a7\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">16</span></p></td><td class=\"cl-7bedb5a7\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">1</span></p></td><td class=\"cl-7bedb5a7\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">4</span></p></td><td class=\"cl-7bedb5a7\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">15</span></p></td><td class=\"cl-7bedb5a7\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">3,484</span></p></td><td class=\"cl-7bedb5a8\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">215</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-7bedb592\"><p class=\"cl-7bed7df2\"><span class=\"cl-7bed7168\">ab</span></p></td><td class=\"cl-7bedb59d\"><p class=\"cl-7bed7df2\"><span class=\"cl-7bed7168\">when</span></p></td><td class=\"cl-7bedb59c\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">1</span></p></td><td class=\"cl-7bedb59c\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">3,504</span></p></td><td class=\"cl-7bedb59c\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">5</span></p></td><td class=\"cl-7bedb59c\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">27</span></p></td><td class=\"cl-7bedb59c\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">1</span></p></td><td class=\"cl-7bedb59c\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">4</span></p></td><td class=\"cl-7bedb59c\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">26</span></p></td><td class=\"cl-7bedb59c\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">3,473</span></p></td><td class=\"cl-7bedb5a6\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">215</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-7bedb5b0\"><p class=\"cl-7bed7df2\"><span class=\"cl-7bed7168\">abend</span></p></td><td class=\"cl-7bedb5a9\"><p class=\"cl-7bed7df2\"><span class=\"cl-7bed7168\">evening</span></p></td><td class=\"cl-7bedb5a7\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">1</span></p></td><td class=\"cl-7bedb5a7\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">3,504</span></p></td><td class=\"cl-7bedb5a7\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">2</span></p></td><td class=\"cl-7bedb5a7\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">2</span></p></td><td class=\"cl-7bedb5a7\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">1</span></p></td><td class=\"cl-7bedb5a7\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">1</span></p></td><td class=\"cl-7bedb5a7\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">1</span></p></td><td class=\"cl-7bedb5a7\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">3,501</span></p></td><td class=\"cl-7bedb5a8\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">215</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-7bedb592\"><p class=\"cl-7bed7df2\"><span class=\"cl-7bed7168\">abend</span></p></td><td class=\"cl-7bedb59d\"><p class=\"cl-7bed7df2\"><span class=\"cl-7bed7168\">good</span></p></td><td class=\"cl-7bedb59c\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">1</span></p></td><td class=\"cl-7bedb59c\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">3,504</span></p></td><td class=\"cl-7bedb59c\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">2</span></p></td><td class=\"cl-7bedb59c\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">16</span></p></td><td class=\"cl-7bedb59c\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">1</span></p></td><td class=\"cl-7bedb59c\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">1</span></p></td><td class=\"cl-7bedb59c\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">15</span></p></td><td class=\"cl-7bedb59c\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">3,487</span></p></td><td class=\"cl-7bedb5a6\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">215</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-7bedb5b0\"><p class=\"cl-7bed7df2\"><span class=\"cl-7bed7168\">allem</span></p></td><td class=\"cl-7bedb5a9\"><p class=\"cl-7bed7df2\"><span class=\"cl-7bed7168\">döner</span></p></td><td class=\"cl-7bedb5a7\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">1</span></p></td><td class=\"cl-7bedb5a7\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">3,504</span></p></td><td class=\"cl-7bedb5a7\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">5</span></p></td><td class=\"cl-7bedb5a7\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">10</span></p></td><td class=\"cl-7bedb5a7\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">1</span></p></td><td class=\"cl-7bedb5a7\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">4</span></p></td><td class=\"cl-7bedb5a7\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">9</span></p></td><td class=\"cl-7bedb5a7\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">3,490</span></p></td><td class=\"cl-7bedb5a8\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">215</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-7bedb592\"><p class=\"cl-7bed7df2\"><span class=\"cl-7bed7168\">allem</span></p></td><td class=\"cl-7bedb59d\"><p class=\"cl-7bed7df2\"><span class=\"cl-7bed7168\">everything</span></p></td><td class=\"cl-7bedb59c\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">1</span></p></td><td class=\"cl-7bedb59c\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">3,504</span></p></td><td class=\"cl-7bedb59c\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">5</span></p></td><td class=\"cl-7bedb59c\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">5</span></p></td><td class=\"cl-7bedb59c\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">1</span></p></td><td class=\"cl-7bedb59c\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">4</span></p></td><td class=\"cl-7bedb59c\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">4</span></p></td><td class=\"cl-7bedb59c\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">3,495</span></p></td><td class=\"cl-7bedb5a6\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">215</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-7bedb5b0\"><p class=\"cl-7bed7df2\"><span class=\"cl-7bed7168\">allem</span></p></td><td class=\"cl-7bedb5a9\"><p class=\"cl-7bed7df2\"><span class=\"cl-7bed7168\">one</span></p></td><td class=\"cl-7bedb5a7\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">1</span></p></td><td class=\"cl-7bedb5a7\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">3,504</span></p></td><td class=\"cl-7bedb5a7\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">5</span></p></td><td class=\"cl-7bedb5a7\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">30</span></p></td><td class=\"cl-7bedb5a7\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">1</span></p></td><td class=\"cl-7bedb5a7\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">4</span></p></td><td class=\"cl-7bedb5a7\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">29</span></p></td><td class=\"cl-7bedb5a7\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">3,470</span></p></td><td class=\"cl-7bedb5a8\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">215</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-7bedb592\"><p class=\"cl-7bed7df2\"><span class=\"cl-7bed7168\">allem</span></p></td><td class=\"cl-7bedb59d\"><p class=\"cl-7bed7df2\"><span class=\"cl-7bed7168\">please</span></p></td><td class=\"cl-7bedb59c\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">1</span></p></td><td class=\"cl-7bedb59c\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">3,504</span></p></td><td class=\"cl-7bedb59c\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">5</span></p></td><td class=\"cl-7bedb59c\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">111</span></p></td><td class=\"cl-7bedb59c\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">1</span></p></td><td class=\"cl-7bedb59c\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">4</span></p></td><td class=\"cl-7bedb59c\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">110</span></p></td><td class=\"cl-7bedb59c\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">3,389</span></p></td><td class=\"cl-7bedb5a6\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">215</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-7bedb5b0\"><p class=\"cl-7bed7df2\"><span class=\"cl-7bed7168\">allem</span></p></td><td class=\"cl-7bedb5a9\"><p class=\"cl-7bed7df2\"><span class=\"cl-7bed7168\">with</span></p></td><td class=\"cl-7bedb5a7\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">1</span></p></td><td class=\"cl-7bedb5a7\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">3,504</span></p></td><td class=\"cl-7bedb5a7\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">5</span></p></td><td class=\"cl-7bedb5a7\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">22</span></p></td><td class=\"cl-7bedb5a7\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">1</span></p></td><td class=\"cl-7bedb5a7\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">4</span></p></td><td class=\"cl-7bedb5a7\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">21</span></p></td><td class=\"cl-7bedb5a7\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">3,478</span></p></td><td class=\"cl-7bedb5a8\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">215</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-7bedb592\"><p class=\"cl-7bed7df2\"><span class=\"cl-7bed7168\">alles</span></p></td><td class=\"cl-7bedb59d\"><p class=\"cl-7bed7df2\"><span class=\"cl-7bed7168\">all</span></p></td><td class=\"cl-7bedb59c\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">1</span></p></td><td class=\"cl-7bedb59c\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">3,504</span></p></td><td class=\"cl-7bedb59c\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">6</span></p></td><td class=\"cl-7bedb59c\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">5</span></p></td><td class=\"cl-7bedb59c\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">1</span></p></td><td class=\"cl-7bedb59c\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">5</span></p></td><td class=\"cl-7bedb59c\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">4</span></p></td><td class=\"cl-7bedb59c\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">3,494</span></p></td><td class=\"cl-7bedb5a6\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">215</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-7bedb5b0\"><p class=\"cl-7bed7df2\"><span class=\"cl-7bed7168\">alles</span></p></td><td class=\"cl-7bedb5a9\"><p class=\"cl-7bed7df2\"><span class=\"cl-7bed7168\">for</span></p></td><td class=\"cl-7bedb5a7\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">1</span></p></td><td class=\"cl-7bedb5a7\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">3,504</span></p></td><td class=\"cl-7bedb5a7\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">6</span></p></td><td class=\"cl-7bedb5a7\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">93</span></p></td><td class=\"cl-7bedb5a7\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">1</span></p></td><td class=\"cl-7bedb5a7\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">5</span></p></td><td class=\"cl-7bedb5a7\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">92</span></p></td><td class=\"cl-7bedb5a7\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">3,406</span></p></td><td class=\"cl-7bedb5a8\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">215</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-7bedb5ba\"><p class=\"cl-7bed7df2\"><span class=\"cl-7bed7168\">alles</span></p></td><td class=\"cl-7bedb5b1\"><p class=\"cl-7bed7df2\"><span class=\"cl-7bed7168\">no</span></p></td><td class=\"cl-7bedb5b2\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">1</span></p></td><td class=\"cl-7bedb5b2\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">3,504</span></p></td><td class=\"cl-7bedb5b2\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">6</span></p></td><td class=\"cl-7bedb5b2\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">7</span></p></td><td class=\"cl-7bedb5b2\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">1</span></p></td><td class=\"cl-7bedb5b2\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">5</span></p></td><td class=\"cl-7bedb5b2\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">6</span></p></td><td class=\"cl-7bedb5b2\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">3,492</span></p></td><td class=\"cl-7bedb5b3\"><p class=\"cl-7bed7df3\"><span class=\"cl-7bed7168\">215</span></p></td></tr></tbody></table></div>\n```\n:::\n:::\n\n\nIn a final step, we extract those potential translations that correlate most strongly with each given term. The results then form a list of words and their most likely translation. \n\n\n::: {.cell}\n\n```{.r .cell-code}\ntranslationtb <- cooctb  %>%\n  dplyr::rowwise() %>%\n  dplyr::mutate(p = round(as.vector(unlist(fisher.test(matrix(c(a, b, c, d), ncol = 2, byrow = T))[1])), 5), \n                x2 = round(as.vector(unlist(chisq.test(matrix(c(a, b, c, d), ncol = 2, byrow = T))[1])), 3)) %>%\n  dplyr::mutate(phi = round(sqrt((x2/(a + b + c + d))), 3),\n                expected = as.vector(unlist(chisq.test(matrix(c(a, b, c, d), ncol = 2, byrow = T))$expected[1]))) %>%\n  dplyr::filter(TermCoocFreq > expected) %>%\n  dplyr::arrange(-phi) %>%\n  dplyr::select(-AllFreq, -a, -b, -c, -d, -NRows, -expected)\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n```{=html}\n<div class=\"tabwid\"><style>.cl-909dd9b8{table-layout:auto;width:75%;}.cl-9098cb76{font-family:'DejaVu Sans';font-size:12pt;font-weight:bold;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-9098cb80{font-family:'DejaVu Sans';font-size:12pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-9098d97c{margin:0;text-align:center;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-9098d97d{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-90991978{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-90991982{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9099198c{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9099198d{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-90991996{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-909919a0{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-909919a1{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-909919aa{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-909919b4{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-909919b5{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-909919be{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-909919bf{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-909919c8{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-909919c9{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-909919ca{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-909919d2{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}</style><table class='cl-909dd9b8'>\n```\n<caption class=\"Table Caption\">\n\nFirst 15 rows of the translationtb data.\n\n</caption>\n```{=html}\n<thead><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-909919d2\"><p class=\"cl-9098d97c\"><span class=\"cl-9098cb76\">German</span></p></td><td class=\"cl-909919c9\"><p class=\"cl-9098d97c\"><span class=\"cl-9098cb76\">English</span></p></td><td class=\"cl-909919c8\"><p class=\"cl-9098d97d\"><span class=\"cl-9098cb76\">TermCoocFreq</span></p></td><td class=\"cl-909919c8\"><p class=\"cl-9098d97d\"><span class=\"cl-9098cb76\">TermFreq</span></p></td><td class=\"cl-909919c8\"><p class=\"cl-9098d97d\"><span class=\"cl-9098cb76\">CoocFreq</span></p></td><td class=\"cl-909919c8\"><p class=\"cl-9098d97d\"><span class=\"cl-9098cb76\">p</span></p></td><td class=\"cl-909919c8\"><p class=\"cl-9098d97d\"><span class=\"cl-9098cb76\">x2</span></p></td><td class=\"cl-909919ca\"><p class=\"cl-9098d97d\"><span class=\"cl-9098cb76\">phi</span></p></td></tr></thead><tbody><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-90991978\"><p class=\"cl-9098d97c\"><span class=\"cl-9098cb80\">hallo</span></p></td><td class=\"cl-9099198d\"><p class=\"cl-9098d97c\"><span class=\"cl-9098cb80\">hello</span></p></td><td class=\"cl-90991982\"><p class=\"cl-9098d97d\"><span class=\"cl-9098cb80\">1</span></p></td><td class=\"cl-90991982\"><p class=\"cl-9098d97d\"><span class=\"cl-9098cb80\">1</span></p></td><td class=\"cl-90991982\"><p class=\"cl-9098d97d\"><span class=\"cl-9098cb80\">1</span></p></td><td class=\"cl-90991982\"><p class=\"cl-9098d97d\"><span class=\"cl-9098cb80\">0.00029</span></p></td><td class=\"cl-90991982\"><p class=\"cl-9098d97d\"><span class=\"cl-9098cb80\">875.500</span></p></td><td class=\"cl-9099198c\"><p class=\"cl-9098d97d\"><span class=\"cl-9098cb80\">0.500</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-909919aa\"><p class=\"cl-9098d97c\"><span class=\"cl-9098cb80\">abend</span></p></td><td class=\"cl-909919a0\"><p class=\"cl-9098d97c\"><span class=\"cl-9098cb80\">evening</span></p></td><td class=\"cl-90991996\"><p class=\"cl-9098d97d\"><span class=\"cl-9098cb80\">1</span></p></td><td class=\"cl-90991996\"><p class=\"cl-9098d97d\"><span class=\"cl-9098cb80\">2</span></p></td><td class=\"cl-90991996\"><p class=\"cl-9098d97d\"><span class=\"cl-9098cb80\">2</span></p></td><td class=\"cl-90991996\"><p class=\"cl-9098d97d\"><span class=\"cl-9098cb80\">0.00114</span></p></td><td class=\"cl-90991996\"><p class=\"cl-9098d97d\"><span class=\"cl-9098cb80\">218.250</span></p></td><td class=\"cl-909919a1\"><p class=\"cl-9098d97d\"><span class=\"cl-9098cb80\">0.250</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-90991978\"><p class=\"cl-9098d97c\"><span class=\"cl-9098cb80\">ja</span></p></td><td class=\"cl-9099198d\"><p class=\"cl-9098d97c\"><span class=\"cl-9098cb80\">yes</span></p></td><td class=\"cl-90991982\"><p class=\"cl-9098d97d\"><span class=\"cl-9098cb80\">1</span></p></td><td class=\"cl-90991982\"><p class=\"cl-9098d97d\"><span class=\"cl-9098cb80\">2</span></p></td><td class=\"cl-90991982\"><p class=\"cl-9098d97d\"><span class=\"cl-9098cb80\">2</span></p></td><td class=\"cl-90991982\"><p class=\"cl-9098d97d\"><span class=\"cl-9098cb80\">0.00114</span></p></td><td class=\"cl-90991982\"><p class=\"cl-9098d97d\"><span class=\"cl-9098cb80\">218.250</span></p></td><td class=\"cl-9099198c\"><p class=\"cl-9098d97d\"><span class=\"cl-9098cb80\">0.250</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-909919aa\"><p class=\"cl-9098d97c\"><span class=\"cl-9098cb80\">morgen</span></p></td><td class=\"cl-909919a0\"><p class=\"cl-9098d97c\"><span class=\"cl-9098cb80\">morning</span></p></td><td class=\"cl-90991996\"><p class=\"cl-9098d97d\"><span class=\"cl-9098cb80\">1</span></p></td><td class=\"cl-90991996\"><p class=\"cl-9098d97d\"><span class=\"cl-9098cb80\">2</span></p></td><td class=\"cl-90991996\"><p class=\"cl-9098d97d\"><span class=\"cl-9098cb80\">2</span></p></td><td class=\"cl-90991996\"><p class=\"cl-9098d97d\"><span class=\"cl-9098cb80\">0.00114</span></p></td><td class=\"cl-90991996\"><p class=\"cl-9098d97d\"><span class=\"cl-9098cb80\">218.250</span></p></td><td class=\"cl-909919a1\"><p class=\"cl-9098d97d\"><span class=\"cl-9098cb80\">0.250</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-90991978\"><p class=\"cl-9098d97c\"><span class=\"cl-9098cb80\">tag</span></p></td><td class=\"cl-9099198d\"><p class=\"cl-9098d97c\"><span class=\"cl-9098cb80\">day</span></p></td><td class=\"cl-90991982\"><p class=\"cl-9098d97d\"><span class=\"cl-9098cb80\">1</span></p></td><td class=\"cl-90991982\"><p class=\"cl-9098d97d\"><span class=\"cl-9098cb80\">2</span></p></td><td class=\"cl-90991982\"><p class=\"cl-9098d97d\"><span class=\"cl-9098cb80\">2</span></p></td><td class=\"cl-90991982\"><p class=\"cl-9098d97d\"><span class=\"cl-9098cb80\">0.00114</span></p></td><td class=\"cl-90991982\"><p class=\"cl-9098d97d\"><span class=\"cl-9098cb80\">218.250</span></p></td><td class=\"cl-9099198c\"><p class=\"cl-9098d97d\"><span class=\"cl-9098cb80\">0.250</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-909919aa\"><p class=\"cl-9098d97c\"><span class=\"cl-9098cb80\">guten</span></p></td><td class=\"cl-909919a0\"><p class=\"cl-9098d97c\"><span class=\"cl-9098cb80\">good</span></p></td><td class=\"cl-90991996\"><p class=\"cl-9098d97d\"><span class=\"cl-9098cb80\">4</span></p></td><td class=\"cl-90991996\"><p class=\"cl-9098d97d\"><span class=\"cl-9098cb80\">13</span></p></td><td class=\"cl-90991996\"><p class=\"cl-9098d97d\"><span class=\"cl-9098cb80\">16</span></p></td><td class=\"cl-90991996\"><p class=\"cl-9098d97d\"><span class=\"cl-9098cb80\">0.00000</span></p></td><td class=\"cl-90991996\"><p class=\"cl-9098d97d\"><span class=\"cl-9098cb80\">201.086</span></p></td><td class=\"cl-909919a1\"><p class=\"cl-9098d97d\"><span class=\"cl-9098cb80\">0.240</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-90991978\"><p class=\"cl-9098d97c\"><span class=\"cl-9098cb80\">brauche</span></p></td><td class=\"cl-9099198d\"><p class=\"cl-9098d97c\"><span class=\"cl-9098cb80\">need</span></p></td><td class=\"cl-90991982\"><p class=\"cl-9098d97d\"><span class=\"cl-9098cb80\">5</span></p></td><td class=\"cl-90991982\"><p class=\"cl-9098d97d\"><span class=\"cl-9098cb80\">20</span></p></td><td class=\"cl-90991982\"><p class=\"cl-9098d97d\"><span class=\"cl-9098cb80\">27</span></p></td><td class=\"cl-90991982\"><p class=\"cl-9098d97d\"><span class=\"cl-9098cb80\">0.00000</span></p></td><td class=\"cl-90991982\"><p class=\"cl-9098d97d\"><span class=\"cl-9098cb80\">124.215</span></p></td><td class=\"cl-9099198c\"><p class=\"cl-9098d97d\"><span class=\"cl-9098cb80\">0.188</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-909919aa\"><p class=\"cl-9098d97c\"><span class=\"cl-9098cb80\">nein</span></p></td><td class=\"cl-909919a0\"><p class=\"cl-9098d97c\"><span class=\"cl-9098cb80\">no</span></p></td><td class=\"cl-90991996\"><p class=\"cl-9098d97d\"><span class=\"cl-9098cb80\">2</span></p></td><td class=\"cl-90991996\"><p class=\"cl-9098d97d\"><span class=\"cl-9098cb80\">9</span></p></td><td class=\"cl-90991996\"><p class=\"cl-9098d97d\"><span class=\"cl-9098cb80\">7</span></p></td><td class=\"cl-90991996\"><p class=\"cl-9098d97d\"><span class=\"cl-9098cb80\">0.00012</span></p></td><td class=\"cl-90991996\"><p class=\"cl-9098d97d\"><span class=\"cl-9098cb80\">122.721</span></p></td><td class=\"cl-909919a1\"><p class=\"cl-9098d97d\"><span class=\"cl-9098cb80\">0.187</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-90991978\"><p class=\"cl-9098d97c\"><span class=\"cl-9098cb80\">bier</span></p></td><td class=\"cl-9099198d\"><p class=\"cl-9098d97c\"><span class=\"cl-9098cb80\">beer</span></p></td><td class=\"cl-90991982\"><p class=\"cl-9098d97d\"><span class=\"cl-9098cb80\">2</span></p></td><td class=\"cl-90991982\"><p class=\"cl-9098d97d\"><span class=\"cl-9098cb80\">8</span></p></td><td class=\"cl-90991982\"><p class=\"cl-9098d97d\"><span class=\"cl-9098cb80\">8</span></p></td><td class=\"cl-90991982\"><p class=\"cl-9098d97d\"><span class=\"cl-9098cb80\">0.00013</span></p></td><td class=\"cl-90991982\"><p class=\"cl-9098d97d\"><span class=\"cl-9098cb80\">120.757</span></p></td><td class=\"cl-9099198c\"><p class=\"cl-9098d97d\"><span class=\"cl-9098cb80\">0.186</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-909919aa\"><p class=\"cl-9098d97c\"><span class=\"cl-9098cb80\">hamburg</span></p></td><td class=\"cl-909919a0\"><p class=\"cl-9098d97c\"><span class=\"cl-9098cb80\">hamburg</span></p></td><td class=\"cl-90991996\"><p class=\"cl-9098d97d\"><span class=\"cl-9098cb80\">2</span></p></td><td class=\"cl-90991996\"><p class=\"cl-9098d97d\"><span class=\"cl-9098cb80\">8</span></p></td><td class=\"cl-90991996\"><p class=\"cl-9098d97d\"><span class=\"cl-9098cb80\">8</span></p></td><td class=\"cl-90991996\"><p class=\"cl-9098d97d\"><span class=\"cl-9098cb80\">0.00013</span></p></td><td class=\"cl-90991996\"><p class=\"cl-9098d97d\"><span class=\"cl-9098cb80\">120.757</span></p></td><td class=\"cl-909919a1\"><p class=\"cl-9098d97d\"><span class=\"cl-9098cb80\">0.186</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-90991978\"><p class=\"cl-9098d97c\"><span class=\"cl-9098cb80\">braucht</span></p></td><td class=\"cl-9099198d\"><p class=\"cl-9098d97c\"><span class=\"cl-9098cb80\">he</span></p></td><td class=\"cl-90991982\"><p class=\"cl-9098d97d\"><span class=\"cl-9098cb80\">1</span></p></td><td class=\"cl-90991982\"><p class=\"cl-9098d97d\"><span class=\"cl-9098cb80\">3</span></p></td><td class=\"cl-90991982\"><p class=\"cl-9098d97d\"><span class=\"cl-9098cb80\">3</span></p></td><td class=\"cl-90991982\"><p class=\"cl-9098d97d\"><span class=\"cl-9098cb80\">0.00257</span></p></td><td class=\"cl-90991982\"><p class=\"cl-9098d97d\"><span class=\"cl-9098cb80\">96.501</span></p></td><td class=\"cl-9099198c\"><p class=\"cl-9098d97d\"><span class=\"cl-9098cb80\">0.166</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-909919aa\"><p class=\"cl-9098d97c\"><span class=\"cl-9098cb80\">braucht</span></p></td><td class=\"cl-909919a0\"><p class=\"cl-9098d97c\"><span class=\"cl-9098cb80\">medication</span></p></td><td class=\"cl-90991996\"><p class=\"cl-9098d97d\"><span class=\"cl-9098cb80\">1</span></p></td><td class=\"cl-90991996\"><p class=\"cl-9098d97d\"><span class=\"cl-9098cb80\">3</span></p></td><td class=\"cl-90991996\"><p class=\"cl-9098d97d\"><span class=\"cl-9098cb80\">3</span></p></td><td class=\"cl-90991996\"><p class=\"cl-9098d97d\"><span class=\"cl-9098cb80\">0.00257</span></p></td><td class=\"cl-90991996\"><p class=\"cl-9098d97d\"><span class=\"cl-9098cb80\">96.501</span></p></td><td class=\"cl-909919a1\"><p class=\"cl-9098d97d\"><span class=\"cl-9098cb80\">0.166</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-90991978\"><p class=\"cl-9098d97c\"><span class=\"cl-9098cb80\">braucht</span></p></td><td class=\"cl-9099198d\"><p class=\"cl-9098d97c\"><span class=\"cl-9098cb80\">needs</span></p></td><td class=\"cl-90991982\"><p class=\"cl-9098d97d\"><span class=\"cl-9098cb80\">1</span></p></td><td class=\"cl-90991982\"><p class=\"cl-9098d97d\"><span class=\"cl-9098cb80\">3</span></p></td><td class=\"cl-90991982\"><p class=\"cl-9098d97d\"><span class=\"cl-9098cb80\">3</span></p></td><td class=\"cl-90991982\"><p class=\"cl-9098d97d\"><span class=\"cl-9098cb80\">0.00257</span></p></td><td class=\"cl-90991982\"><p class=\"cl-9098d97d\"><span class=\"cl-9098cb80\">96.501</span></p></td><td class=\"cl-9099198c\"><p class=\"cl-9098d97d\"><span class=\"cl-9098cb80\">0.166</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-909919aa\"><p class=\"cl-9098d97c\"><span class=\"cl-9098cb80\">deutscher</span></p></td><td class=\"cl-909919a0\"><p class=\"cl-9098d97c\"><span class=\"cl-9098cb80\">german</span></p></td><td class=\"cl-90991996\"><p class=\"cl-9098d97d\"><span class=\"cl-9098cb80\">1</span></p></td><td class=\"cl-90991996\"><p class=\"cl-9098d97d\"><span class=\"cl-9098cb80\">3</span></p></td><td class=\"cl-90991996\"><p class=\"cl-9098d97d\"><span class=\"cl-9098cb80\">3</span></p></td><td class=\"cl-90991996\"><p class=\"cl-9098d97d\"><span class=\"cl-9098cb80\">0.00257</span></p></td><td class=\"cl-90991996\"><p class=\"cl-9098d97d\"><span class=\"cl-9098cb80\">96.501</span></p></td><td class=\"cl-909919a1\"><p class=\"cl-9098d97d\"><span class=\"cl-9098cb80\">0.166</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-909919bf\"><p class=\"cl-9098d97c\"><span class=\"cl-9098cb80\">er</span></p></td><td class=\"cl-909919b5\"><p class=\"cl-9098d97c\"><span class=\"cl-9098cb80\">he</span></p></td><td class=\"cl-909919b4\"><p class=\"cl-9098d97d\"><span class=\"cl-9098cb80\">1</span></p></td><td class=\"cl-909919b4\"><p class=\"cl-9098d97d\"><span class=\"cl-9098cb80\">3</span></p></td><td class=\"cl-909919b4\"><p class=\"cl-9098d97d\"><span class=\"cl-9098cb80\">3</span></p></td><td class=\"cl-909919b4\"><p class=\"cl-9098d97d\"><span class=\"cl-9098cb80\">0.00257</span></p></td><td class=\"cl-909919b4\"><p class=\"cl-9098d97d\"><span class=\"cl-9098cb80\">96.501</span></p></td><td class=\"cl-909919be\"><p class=\"cl-9098d97d\"><span class=\"cl-9098cb80\">0.166</span></p></td></tr></tbody></table></div>\n```\n:::\n:::\n\n\nThe results show that even using the very limited data base can produce some very reasonable results. In fact, based on the data that we used here, the first translations appear to be very sensible, but the mismatches also show that more data is required to disambiguate potential translations.\n\nWhile this method still requires manual correction, it is a very handy and useful tool for generating custom bilingual dictionaries that can be extended to any set of languages as long as these languages can be represented as distinct words and as long as parallel data is available. \n\n\n# Going further: crowd-sourced dictionaries with R and Git\n\nWhile it would go beyond the scope of this tutorial, it should be noted that the approach for creating dictionaries can be applied to crowed-sourced dictionaries. To do this, you could, e.g. upload your dictionary to a Git repository such as [GitHub](https://github.com/) or [GitLab](https://about.gitlab.com/) which would then allow everybody with an account on either of these platforms to add content to the dictionary. \n\n\n::: {.cell}\n::: {.cell-output-display}\n![](https://slcladal.github.io/images/git.png){width=50% style=\"float:right; padding:15px\"}\n:::\n:::\n\n\nTo add to the dictionary, contributors would simply have to fork the repository of the dictionary and then merge with the existing, original dictionary repository. The quality of the data would meanwhile remain under control of the owner of the original repository he they can decide on a case-by-case basis which change they would like to accept. In addition, and because Git is a version control environment, the owner could also go back to previous versions, if they think they erroneously accepted a change (merge).\n\nThis option is particularly interesting for the approach to creating dictionaries presented here because R Studio has an integrated and very easy to use pipeline to Git (see, e.g.,  [here](https://support.rstudio.com/hc/en-us/articles/200532077-Version-Control-with-Git-and-SVN) and [here](https://happygitwithr.com/rstudio-git-github.html))\n\nWe have reached the end of this tutorial and you now know how to create and modify networks in R and how you can highlight aspects of your data. \n\n<br><br>\n\n# Citation & Session Info {-}\n\nSchweinberger, Martin. 2022. *Lexicography with R*. Brisbane: The University of Queensland. url: https://slcladal.github.io/lex.html  (Version 2022.08.31).\n\n```\n@manual{schweinberger2022lex,\n  author = {Schweinberger, Martin},\n  title = {Lexicography with R},\n  note = {https://slcladal.github.io/lex.html},\n  year = {2022},\n  organization = \"The University of Queensland, Australia. School of Languages and Cultures},\n  address = {Brisbane},\n  edition = {2022.08.31}\n}\n```\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsessionInfo()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nR version 4.2.1 (2022-06-23)\nPlatform: x86_64-pc-linux-gnu (64-bit)\nRunning under: Ubuntu 22.04.1 LTS\n\nMatrix products: default\nBLAS:   /usr/lib/x86_64-linux-gnu/blas/libblas.so.3.10.0\nLAPACK: /usr/lib/x86_64-linux-gnu/lapack/liblapack.so.3.10.0\n\nlocale:\n [1] LC_CTYPE=en_AU.UTF-8       LC_NUMERIC=C              \n [3] LC_TIME=en_AU.UTF-8        LC_COLLATE=en_AU.UTF-8    \n [5] LC_MONETARY=en_AU.UTF-8    LC_MESSAGES=en_AU.UTF-8   \n [7] LC_PAPER=en_AU.UTF-8       LC_NAME=C                 \n [9] LC_ADDRESS=C               LC_TELEPHONE=C            \n[11] LC_MEASUREMENT=en_AU.UTF-8 LC_IDENTIFICATION=C       \n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] plyr_1.8.7      flextable_0.7.3 cluster_2.1.3   coop_0.6-3     \n[5] tidytext_0.3.3  udpipe_0.8.9    stringr_1.4.0   dplyr_1.0.9    \n\nloaded via a namespace (and not attached):\n [1] tidyselect_1.1.2  xfun_0.31         textdata_0.4.2    purrr_0.3.4      \n [5] lattice_0.20-45   vctrs_0.4.1       generics_0.1.3    htmltools_0.5.2  \n [9] SnowballC_0.7.0   yaml_2.3.5        base64enc_0.1-3   utf8_1.2.2       \n[13] rlang_1.0.4       pillar_1.7.0      glue_1.6.2        DBI_1.1.3        \n[17] rappdirs_0.3.3    gdtools_0.2.4     uuid_1.1-0        lifecycle_1.0.1  \n[21] zip_2.2.0         htmlwidgets_1.5.4 evaluate_0.15     knitr_1.39       \n[25] tzdb_0.3.0        fastmap_1.1.0     fansi_1.0.3       tokenizers_0.2.1 \n[29] Rcpp_1.0.8.3      readr_2.1.2       jsonlite_1.8.0    fs_1.5.2         \n[33] systemfonts_1.0.4 hms_1.1.1         digest_0.6.29     stringi_1.7.8    \n[37] grid_4.2.1        rprojroot_2.0.3   here_1.0.1        cli_3.3.0        \n[41] tools_4.2.1       magrittr_2.0.3    klippy_0.0.0.9500 tibble_3.1.7     \n[45] janeaustenr_0.1.5 tidyr_1.2.0       crayon_1.5.1      pkgconfig_2.0.3  \n[49] ellipsis_0.3.2    Matrix_1.4-1      data.table_1.14.2 xml2_1.3.3       \n[53] assertthat_0.2.1  rmarkdown_2.14    officer_0.4.3     rstudioapi_0.13  \n[57] R6_2.5.1          compiler_4.2.1   \n```\n:::\n:::\n\n\n\n\n\n# References{-}\n\n***\n\n[Back to top](#introduction)\n\n[Back to HOME](https://slcladal.github.io/index.html)\n\n***\n\n",
    "supporting": [
      "lex_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"site_libs/clipboard-1.7.1/clipboard.min.js\"></script>\n<link href=\"site_libs/primer-tooltips-1.4.0/build.css\" rel=\"stylesheet\" />\n<link href=\"site_libs/klippy-0.0.0.9500/css/klippy.min.css\" rel=\"stylesheet\" />\n<script src=\"site_libs/klippy-0.0.0.9500/js/klippy.min.js\"></script>\n<link href=\"site_libs/tabwid-1.0.0/tabwid.css\" rel=\"stylesheet\" />\n<link href=\"site_libs/tabwid-1.0.0/scrool.css\" rel=\"stylesheet\" />\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}