{
  "hash": "66f434ab907459194de08e16b96129a2",
  "result": {
    "markdown": "---\ntitle: \"Analyzing learner language using R\"\nauthor: \"Martin Schweinberger\"\ndate: \"2022-08-31\"\n---\n\n::: {.cell}\n::: {.cell-output-display}\n![](https://slcladal.github.io/images/uq1.jpg){width=100%}\n:::\n:::\n\n\n# Introduction{-}\n\nThis tutorial focuses on learner language and how to analyze differences between learners and L1 speakers of English using R. The aim of this tutorial is to showcase how to extract information from essays from learners and L1 speakers of English and how to analyze these essays. The aim is not to provide a fully-fledged analysis but rather to show and exemplify some common methods for data extraction, processing, and analysis.\n\n\n<div class=\"warning\" style='padding:0.1em; background-color:#f2f2f2; color:#51247a'>\n<span>\n<p style='margin-top:1em; text-align:center'>\nThe entire R Notebook for the tutorial can be downloaded [**here**](https://slcladal.github.io/content/llr.Rmd).  If you want to render the R Notebook on your machine, i.e. knitting the document to html or a pdf, you need to make sure that you have R and RStudio installed and you also need to download the [**bibliography file**](https://slcladal.github.io/content/bibliography.bib) and store it in the same folder where you store the Rmd file. <br><br>\n**[Here](https://colab.research.google.com/drive/1qgAA8iPZDjh0nmqPNtgr2HC71Y9qAuEC?usp=sharing)** is a **link to an interactive version of this tutorial on Google Colab**. The interactive tutorial is based on a Jupyter notebook of this tutorial. This interactive Jupyter notebook allows you to execute code yourself and - if you copy the Jupyter notebook - you can also change and edit the notebook, e.g. you can change code and upload your own data.<br></p>\n<p style='margin-left:1em;'>\n</p></span>\n</div>\n\n<br>\n\n\n**Preparation and session set up**\n\nThis tutorial is based on R. If you have not installed R or are new to it, you will find an introduction to and more information how to use R [here](https://slcladal.github.io/intror.html). For this tutorials, we need to install certain *packages* from an R *library* so that the scripts shown below are executed without errors. Before turning to the code below, please install the packages by running the code below this paragraph. If you have already installed the packages mentioned below, then you can skip ahead and ignore this section. To install the necessary packages, simply run the following code - it may take some time (between 1 and 5 minutes to install all of the packages so you do not need to worry if it takes some time).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# install packages\ninstall.packages(\"quanteda\")\ninstall.packages(\"flextable\")\ninstall.packages(\"quanteda.textstats\")\ninstall.packages(\"quanteda.textplots\")\ninstall.packages(\"tidyverse\")\ninstall.packages(\"tm\")\ninstall.packages(\"tidytext\")\ninstall.packages(\"tidyr\")\ninstall.packages(\"NLP\")\ninstall.packages(\"openNLP\")\ninstall.packages(\"openNLPdata\")\ninstall.packages(\"koRpus\")\ninstall.packages(\"stringi\")\ninstall.packages(\"hunspell\")\ninstall.packages(\"wordcloud2\")\ninstall.packages(\"pacman\")\n# install the language support package\nkoRpus::install.koRpus.lang(\"en\")\n# install klippy for copy-to-clipboard button in code chunks\ninstall.packages(\"remotes\")\nremotes::install_github(\"rlesur/klippy\")\n```\n:::\n\n\nNow that we have installed the packages, we can activate them as shown below.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# set options\noptions(stringsAsFactors = F)\noptions(scipen = 999)\noptions(max.print=1000)\noptions(java.parameters = c(\"-XX:+UseConcMarkSweepGC\", \"-Xmx8192m\"))\n#gc()\n# load packages\nlibrary(tidyverse)\nlibrary(flextable)\nlibrary(tm)\nlibrary(tidytext)\nlibrary(tidyr)\nlibrary(NLP)\nlibrary(openNLP)\nlibrary(quanteda)\nlibrary(quanteda.textstats)\nlibrary(quanteda.textplots)\nlibrary(koRpus)\nlibrary(koRpus.lang.en)\nlibrary(stringi)\nlibrary(hunspell)\nlibrary(wordcloud2)\nlibrary(pacman)\npacman::p_load_gh(\"trinker/entity\")\n# activate klippy for copy-to-clipboard button\nklippy::klippy()\n```\n\n::: {.cell-output-display}\n```{=html}\n<script>\n  addClassKlippyTo(\"pre.r, pre.markdown\");\n  addKlippy('left', 'top', 'auto', '1', 'Copy code', 'Copied!');\n</script>\n```\n:::\n:::\n\n\nOnce you have installed R and RStudio and once you have also initiated the session by executing the code shown above, you are good to go.\n\n**Loading data**\n\nWe use 7 essays written by learners from the [*International Corpus of Learner English* (ICLE)](https://uclouvain.be/en/research-institutes/ilc/cecl/icle.html) and two files containing a-level essays written by L1-English British students from [*The Louvain Corpus of Native English Essays* (LOCNESS)](https://uclouvain.be/en/research-institutes/ilc/cecl/locness.html) which was compiled by the *Centre for English Corpus Linguistics* (CECL), Universit√© catholique de Louvain, Belgium. The code chunk below loads the data from the LADAL repository on GitHub into R.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# load essays from l1 speakers\nns1 <- base::readRDS(url(\"https://slcladal.github.io/data/LCorpus/ns1.rda\", \"rb\"))\nns2 <- base::readRDS(url(\"https://slcladal.github.io/data/LCorpus/ns2.rda\", \"rb\"))\n# load essays from l2 speakers\nes <- base::readRDS(url(\"https://slcladal.github.io/data/LCorpus/es.rda\", \"rb\"))\nde <- base::readRDS(url(\"https://slcladal.github.io/data/LCorpus/de.rda\", \"rb\"))\nfr <- base::readRDS(url(\"https://slcladal.github.io/data/LCorpus/fr.rda\", \"rb\"))\nit <- base::readRDS(url(\"https://slcladal.github.io/data/LCorpus/it.rda\", \"rb\"))\npl <- base::readRDS(url(\"https://slcladal.github.io/data/LCorpus/pl.rda\", \"rb\"))\nru <- base::readRDS(url(\"https://slcladal.github.io/data/LCorpus/ru.rda\", \"rb\"))\n# inspect\nru %>%\n  # remove header\n  stringr::str_remove(., \"<[A-Z]{4,4}.*\") %>%\n  # remove empty elements\n  na_if(\"\") %>%\n  na.omit %>%\n  #show first 3 elements\n  head(3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"It is now a very wide spread opinion, that in the modern world there is no place for dreaming and imagination. Those who share this point of view usually say that at present we are so very much under the domination of science, industry, technology, ever-increasing tempo of our lives and so on, that neither dreaming nor imagination can possibly survive. Their usual argument is very simple - they suggest to their opponents to look at some samples of the modern art and to compare them to the masterpieces of the \\\"Old Masters\\\" of painting, music, literature.\"\n[2] \"As everything which is simple, the argument sounds very convincing. Of course, it is evident, that no modern writer, painter or musician can be compare to such names as Bach, Pushkin< Byron, Mozart, Rembrandt, Raffael et cetera. Modern pictures, in the majority of cases, seem to be merely repetitions or combinations of the images and methods of painting, invented very long before. The same is also true to modern verses, novels and songs.\"                                                                                                                        \n[3] \"But, I think, those, who put forward this argument, play - if I may put it like this - not fair game with their opponents, because such an approach presupposes the firm conviction, that dreaming and imagination can deal only with Arts, moreover, only with this \\\"well-established set\\\" of Arts, which includes music, painting, architecture, sculpture and literature. That is, a person, who follows the above-mentioned point of view tries to make his opponent take for granted the statement, the evidence of which is, to say the least, doubtful.\"                 \n```\n:::\n:::\n\n\nThe data inspection shows the first 3 text elements from the essay written a Russian learner of English to provide an idea of what the data look like. \n\nNow that we have loaded some data, we can go ahead and extract information from the texts and process the data to analyze differences between L1 speakers and learners of English.\n\n# Concordancing{-}\n\nConcordancing refers to the extraction of words or phrases from a given text or texts [@lindquist2009corpus]. Commonly, concordances are displayed in the form of key-word in contexts (KWIC) where the search term is shown with some preceding and following context. Thus, such displays are referred to as key word in context concordances. A more elaborate tutorial on how to perform concordancing with R is available [here](https://slcladal.github.io/kwics.html).\n\n\nConcordancing is helpful for seeing how a given term or phrased is used in the data, for inspecting how often a given word occurs in a text or a collection of texts, for extracting examples, and it also represents a basic procedure, and often the first step, in more sophisticated analyses. \n\nWe begin by creating KWIC displays of the term *problem* as shown below. To extract the kwic concordances, we use the `kwic` function from the `quanteda` package [cf. @benoit2018quanteda]. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# combine data from l1 speakers\nl1 <- c(ns1, ns2)\n# combine data from learners\nlearner <- c(de, es, fr, it, pl, ru)\n# extract kwic for term \"problem\" in learner data\nkwic <- quanteda::kwic(learner,               # the data in which to search\n                       pattern = \"problem.*\", # the pattern to look for\n                       valuetype = \"regex\",   # look for exact matches or patterns\n                       window = 10) %>%       # how much context to display (in elements) \n  # convert to table (called data.frame in R)\n  as.data.frame() %>%\n  # remove superfluous columns\n  dplyr::select(-to, -from, -pattern)\n# inspect\nhead(kwic)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  docname                                                     pre  keyword\n1  text12                      Many of the drug addits have legal problems\n2  text12     countries , like Spain , illegal . They have social problems\n3  text30     In our society there is a growing concern about the  problem\n4  text33 that once the availability of guns has been removed the  problem\n5  text33    honest way and remove any causes that could worsen a  problem\n6  text34       violence in our society . In order to analise the  problem\n                                                        post\n1       because they steal money for buying the drug that is\n2         too because people are afraid of them and the drug\n3       of violent crime . In fact , particular attention is\n4 of violence simply vanishes , but in this caotic situation\n5                    which is already particularly serious .\n6            in its complexity and allow people to live in a\n```\n:::\n:::\n\n\nThe output shows that the term *problem* occurs six times in the learner data.\n\nWe can also arrange the output according to what comes before or after the search term as shown below.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# take kwic\nkwic %>%\n  # arrange kwic alphabetically by what comes after the key term\n  dplyr::arrange(post)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  docname                                                          pre  keyword\n1  text12                           Many of the drug addits have legal problems\n2  text39 , greatest ideas were produced and solutions to many serious problems\n3  text34            violence in our society . In order to analise the  problem\n4  text33      that once the availability of guns has been removed the  problem\n5  text30          In our society there is a growing concern about the  problem\n6  text12          countries , like Spain , illegal . They have social problems\n7  text33         honest way and remove any causes that could worsen a  problem\n                                                         post\n1        because they steal money for buying the drug that is\n2 found . Most wonderful pieces of literature were created in\n3             in its complexity and allow people to live in a\n4  of violence simply vanishes , but in this caotic situation\n5        of violent crime . In fact , particular attention is\n6          too because people are afraid of them and the drug\n7                     which is already particularly serious .\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# take quick\nkwic %>%\n  # reverse the preceding context\n  dplyr::mutate(prerev = stringi::stri_reverse(pre)) %>%\n  # arrange kwic alphabetically by reversed preceding context\n  dplyr::arrange(prerev) %>%\n  # remove column with reversed preceding context\n  dplyr::select(-prerev)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  docname                                                          pre  keyword\n1  text33         honest way and remove any causes that could worsen a  problem\n2  text33      that once the availability of guns has been removed the  problem\n3  text34            violence in our society . In order to analise the  problem\n4  text30          In our society there is a growing concern about the  problem\n5  text12                           Many of the drug addits have legal problems\n6  text12          countries , like Spain , illegal . They have social problems\n7  text39 , greatest ideas were produced and solutions to many serious problems\n                                                         post\n1                     which is already particularly serious .\n2  of violence simply vanishes , but in this caotic situation\n3             in its complexity and allow people to live in a\n4        of violent crime . In fact , particular attention is\n5        because they steal money for buying the drug that is\n6          too because people are afraid of them and the drug\n7 found . Most wonderful pieces of literature were created in\n```\n:::\n:::\n\n\nWe can also combine concordancing with visualizations. For instance, use the `textplot_xray` function from the `quanteda.textplots` package to visualize where in some texts the term *people* and the term *imagination*  occurs.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# create kwics for people and imagination\nkwic_people <- quanteda::kwic(learner, pattern = c(\"people\", \"imagination\"))\n# generate x-ray plot\nquanteda.textplots::textplot_xray(kwic_people)\n```\n\n::: {.cell-output-display}\n![](llr_files/figure-html/conc4-1.png){width=672}\n:::\n:::\n\n\nWe can also search for phrases rather than individual words. To do this, we need to use the `phrase` function in the `pattern` argument as shown below. In the code chunk below, we look for any combination of the word *very* and any following word. It we would wish, we could of course also  sort (or order) the concordances as we have done above.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# generate kwic for phrases staring with very\nkwic <- quanteda::kwic(learner,                              # data\n                       pattern = phrase(\"^very [a-z]{1,}\"),  # search pattern\n                       valuetype = \"regex\") %>%              # type of pattern\n  # convert into a data frame\n  as.data.frame()\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n```{=html}\n<div class=\"tabwid\"><style>.cl-b5f1cee0{table-layout:auto;width:75%;}.cl-b5ed5fd6{font-family:'DejaVu Sans';font-size:12pt;font-weight:bold;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-b5ed5fe0{font-family:'DejaVu Sans';font-size:12pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-b5ed6f58{margin:0;text-align:center;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-b5ed6f62{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-b5ed9a32{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b5ed9a3c{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b5ed9a46{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b5ed9a47{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b5ed9a48{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b5ed9a49{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b5ed9a50{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b5ed9a51{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b5ed9a5a{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b5ed9a5b{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b5ed9a64{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b5ed9a65{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b5ed9a66{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b5ed9a6e{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b5ed9a6f{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b5ed9a78{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}</style><table class='cl-b5f1cee0'>\n```\n<caption class=\"Table Caption\">\n\nFirst 6 rows of the concordance for very + any other word in the learner data.\n\n</caption>\n```{=html}\n<thead><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-b5ed9a66\"><p class=\"cl-b5ed6f58\"><span class=\"cl-b5ed5fd6\">docname</span></p></td><td class=\"cl-b5ed9a78\"><p class=\"cl-b5ed6f62\"><span class=\"cl-b5ed5fd6\">from</span></p></td><td class=\"cl-b5ed9a78\"><p class=\"cl-b5ed6f62\"><span class=\"cl-b5ed5fd6\">to</span></p></td><td class=\"cl-b5ed9a6f\"><p class=\"cl-b5ed6f58\"><span class=\"cl-b5ed5fd6\">pre</span></p></td><td class=\"cl-b5ed9a6f\"><p class=\"cl-b5ed6f58\"><span class=\"cl-b5ed5fd6\">keyword</span></p></td><td class=\"cl-b5ed9a6f\"><p class=\"cl-b5ed6f58\"><span class=\"cl-b5ed5fd6\">post</span></p></td><td class=\"cl-b5ed9a6e\"><p class=\"cl-b5ed6f58\"><span class=\"cl-b5ed5fd6\">pattern</span></p></td></tr></thead><tbody><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-b5ed9a46\"><p class=\"cl-b5ed6f58\"><span class=\"cl-b5ed5fe0\">text3</span></p></td><td class=\"cl-b5ed9a32\"><p class=\"cl-b5ed6f62\"><span class=\"cl-b5ed5fe0\">193</span></p></td><td class=\"cl-b5ed9a32\"><p class=\"cl-b5ed6f62\"><span class=\"cl-b5ed5fe0\">194</span></p></td><td class=\"cl-b5ed9a3c\"><p class=\"cl-b5ed6f58\"><span class=\"cl-b5ed5fe0\">in black trousers and only</span></p></td><td class=\"cl-b5ed9a3c\"><p class=\"cl-b5ed6f58\"><span class=\"cl-b5ed5fe0\">very seldom</span></p></td><td class=\"cl-b5ed9a3c\"><p class=\"cl-b5ed6f58\"><span class=\"cl-b5ed5fe0\">in skirts , because she</span></p></td><td class=\"cl-b5ed9a47\"><p class=\"cl-b5ed6f58\"><span class=\"cl-b5ed5fe0\">^very [a-z]{1,}</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-b5ed9a51\"><p class=\"cl-b5ed6f58\"><span class=\"cl-b5ed5fe0\">text4</span></p></td><td class=\"cl-b5ed9a48\"><p class=\"cl-b5ed6f62\"><span class=\"cl-b5ed5fe0\">9</span></p></td><td class=\"cl-b5ed9a48\"><p class=\"cl-b5ed6f62\"><span class=\"cl-b5ed5fe0\">10</span></p></td><td class=\"cl-b5ed9a50\"><p class=\"cl-b5ed6f58\"><span class=\"cl-b5ed5fe0\">is admirable is that she's</span></p></td><td class=\"cl-b5ed9a50\"><p class=\"cl-b5ed6f58\"><span class=\"cl-b5ed5fe0\">very active</span></p></td><td class=\"cl-b5ed9a50\"><p class=\"cl-b5ed6f58\"><span class=\"cl-b5ed5fe0\">in doing sports and that</span></p></td><td class=\"cl-b5ed9a49\"><p class=\"cl-b5ed6f58\"><span class=\"cl-b5ed5fe0\">^very [a-z]{1,}</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-b5ed9a46\"><p class=\"cl-b5ed6f58\"><span class=\"cl-b5ed5fe0\">text4</span></p></td><td class=\"cl-b5ed9a32\"><p class=\"cl-b5ed6f62\"><span class=\"cl-b5ed5fe0\">27</span></p></td><td class=\"cl-b5ed9a32\"><p class=\"cl-b5ed6f62\"><span class=\"cl-b5ed5fe0\">28</span></p></td><td class=\"cl-b5ed9a3c\"><p class=\"cl-b5ed6f58\"><span class=\"cl-b5ed5fe0\">managed by her in a</span></p></td><td class=\"cl-b5ed9a3c\"><p class=\"cl-b5ed6f58\"><span class=\"cl-b5ed5fe0\">very simple</span></p></td><td class=\"cl-b5ed9a3c\"><p class=\"cl-b5ed6f58\"><span class=\"cl-b5ed5fe0\">way . She's very interested</span></p></td><td class=\"cl-b5ed9a47\"><p class=\"cl-b5ed6f58\"><span class=\"cl-b5ed5fe0\">^very [a-z]{1,}</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-b5ed9a51\"><p class=\"cl-b5ed6f58\"><span class=\"cl-b5ed5fe0\">text4</span></p></td><td class=\"cl-b5ed9a48\"><p class=\"cl-b5ed6f62\"><span class=\"cl-b5ed5fe0\">32</span></p></td><td class=\"cl-b5ed9a48\"><p class=\"cl-b5ed6f62\"><span class=\"cl-b5ed5fe0\">33</span></p></td><td class=\"cl-b5ed9a50\"><p class=\"cl-b5ed6f58\"><span class=\"cl-b5ed5fe0\">very simple way . She's</span></p></td><td class=\"cl-b5ed9a50\"><p class=\"cl-b5ed6f58\"><span class=\"cl-b5ed5fe0\">very interested</span></p></td><td class=\"cl-b5ed9a50\"><p class=\"cl-b5ed6f58\"><span class=\"cl-b5ed5fe0\">in cycling , swimming and</span></p></td><td class=\"cl-b5ed9a49\"><p class=\"cl-b5ed6f58\"><span class=\"cl-b5ed5fe0\">^very [a-z]{1,}</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-b5ed9a64\"><p class=\"cl-b5ed6f58\"><span class=\"cl-b5ed5fe0\">text5</span></p></td><td class=\"cl-b5ed9a5a\"><p class=\"cl-b5ed6f62\"><span class=\"cl-b5ed5fe0\">3</span></p></td><td class=\"cl-b5ed9a5a\"><p class=\"cl-b5ed6f62\"><span class=\"cl-b5ed5fe0\">4</span></p></td><td class=\"cl-b5ed9a5b\"><p class=\"cl-b5ed6f58\"><span class=\"cl-b5ed5fe0\">She's also</span></p></td><td class=\"cl-b5ed9a5b\"><p class=\"cl-b5ed6f58\"><span class=\"cl-b5ed5fe0\">very intelligent</span></p></td><td class=\"cl-b5ed9a5b\"><p class=\"cl-b5ed6f58\"><span class=\"cl-b5ed5fe0\">and because of that she</span></p></td><td class=\"cl-b5ed9a65\"><p class=\"cl-b5ed6f58\"><span class=\"cl-b5ed5fe0\">^very [a-z]{1,}</span></p></td></tr></tbody></table></div>\n```\n:::\n:::\n\n\n# Frequency lists{-}\n\nA useful procedure when dealing with texts is to extract frequency information. To exemplify how to extract frequency lists from texts, we will do this here using the L1 data.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nftb <- c(ns1, ns2) %>%\n  # remove punctuation\n  stringr::str_replace_all(., \"\\\\W\", \" \") %>%\n  # remove superfluous white spaces\n  stringr::str_squish() %>%\n  # convert to lower case\n  tolower() %>%\n  # split into words\n  stringr::str_split(\" \") %>%\n  # unlist\n  unlist() %>%\n  # convert into table\n  as.data.frame() %>%\n  # rename column\n  dplyr::rename(word = 1) %>%\n  # remove empty rows\n  dplyr::filter(word != \"\") %>%\n  # count words\n  dplyr::group_by(word) %>%\n  dplyr::summarise(freq = n()) %>%\n  # order by freq\n  dplyr::arrange(-freq)\n# inspect\nhead(ftb)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 √ó 2\n  word   freq\n  <chr> <int>\n1 the     650\n2 to      373\n3 of      320\n4 and     283\n5 is      186\n6 a       176\n```\n:::\n:::\n\n\nWe can easily remove stop words (words without lexical content) using the `anti_join` function as shown below.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nftb_wosw <- ftb %>%\n  # remove stop words\n  dplyr::anti_join(stop_words)\n# inspect\nhead(ftb_wosw)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 √ó 2\n  word       freq\n  <chr>     <int>\n1 transport    98\n2 people       85\n3 roads        80\n4 cars         69\n5 road         51\n6 system       50\n```\n:::\n:::\n\n\nWe can then visualize the results as a bar chart as shown below.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nftb_wosw %>%\n  # take 20 most frequent terms\n  head(20) %>%\n  # generate a plot\n  ggplot(aes(x = reorder(word, -freq), y = freq, label = freq)) +\n  # define type of plot\n  geom_bar(stat = \"identity\") +\n  # add labels\n  geom_text(vjust=1.6, color = \"white\") +\n  # display in black-and-white theme\n  theme_bw() +\n  # adapt x-axis tick labels\n  theme(axis.text.x = element_text(size=8, angle=90)) +\n  # adapt axes labels\n  labs(y = \"Frequnecy\", x = \"Word\")\n```\n\n::: {.cell-output-display}\n![](llr_files/figure-html/fr5-1.png){width=672}\n:::\n:::\n\n\nOr we can visualize the data as a word cloud (see below).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# create wordcloud\nwordcloud2(ftb_wosw[1:100,],    # define data to use\n           # define shape\n           shape = \"diamond\",\n           # define colors\n           color = scales::viridis_pal()(8))\n```\n\n::: {.cell-output-display}\n```{=html}\n<div id=\"htmlwidget-0b835a381e77374c5621\" style=\"width:100%;height:464px;\" class=\"wordcloud2 html-widget\"></div>\n<script type=\"application/json\" data-for=\"htmlwidget-0b835a381e77374c5621\">{\"x\":{\"word\":[\"transport\",\"people\",\"roads\",\"cars\",\"road\",\"system\",\"rail\",\"traffic\",\"public\",\"trains\",\"car\",\"government\",\"travel\",\"train\",\"cities\",\"major\",\"britain\",\"increasing\",\"british\",\"congestion\",\"due\",\"city\",\"increase\",\"bus\",\"main\",\"reduce\",\"time\",\"house\",\"service\",\"buses\",\"lead\",\"vehicles\",\"world\",\"country\",\"motorways\",\"services\",\"increased\",\"network\",\"pollution\",\"power\",\"vote\",\"amount\",\"companies\",\"party\",\"tax\",\"uk\",\"change\",\"democrasy\",\"expensive\",\"means\",\"motorway\",\"solution\",\"times\",\"building\",\"cost\",\"fares\",\"jams\",\"leads\",\"money\",\"political\",\"recent\",\"stations\",\"systems\",\"term\",\"town\",\"3\",\"built\",\"catch\",\"centres\",\"create\",\"encourage\",\"hours\",\"industry\",\"london\",\"parties\",\"past\",\"railway\",\"railways\",\"short\",\"votes\",\"walk\",\"2\",\"build\",\"bypass\",\"bypasses\",\"cycle\",\"drive\",\"efficient\",\"electoral\",\"environment\",\"fair\",\"greatly\",\"heavy\",\"journeys\",\"lines\",\"local\",\"majority\",\"modern\",\"newbury\",\"opposition\"],\"freq\":[98,85,80,69,51,50,48,45,41,36,35,32,31,25,24,23,22,21,20,20,19,18,17,16,16,16,16,15,15,13,13,13,13,12,12,12,11,11,11,11,11,10,10,10,10,10,9,9,9,9,9,9,9,8,8,8,8,8,8,8,8,8,8,8,8,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6],\"fontFamily\":\"Segoe UI\",\"fontWeight\":\"bold\",\"color\":[\"#440154FF\",\"#46337EFF\",\"#365C8DFF\",\"#277F8EFF\",\"#1FA187FF\",\"#4AC16DFF\",\"#9FDA3AFF\",\"#FDE725FF\"],\"minSize\":0,\"weightFactor\":1.83673469387755,\"backgroundColor\":\"white\",\"gridSize\":0,\"minRotation\":-0.785398163397448,\"maxRotation\":0.785398163397448,\"shuffle\":true,\"rotateRatio\":0.4,\"shape\":\"diamond\",\"ellipticity\":0.65,\"figBase64\":null,\"hover\":null},\"evals\":[],\"jsHooks\":{\"render\":[{\"code\":\"function(el,x){\\n                        console.log(123);\\n                        if(!iii){\\n                          window.location.reload();\\n                          iii = False;\\n\\n                        }\\n  }\",\"data\":null}]}}</script>\n```\n:::\n:::\n\n\n\n\n# Splitting texts into sentences{-}\n\nIt can be every useful to split texts into individual sentences. This can be done, e.g., to extract the average sentence length or simply to inspect or annotate individual sentences. To split a text into sentences, we clean the data by removing file identifiers and html tags as well as quotation marks within sentences. As we are dealing with several texts, we write a function that performs this task and that we can then apply to the individual texts.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncleanText <- function(x,...){\n  require(tokenizers)\n  # paste text together\n  x <- paste0(x)\n  # remove file identifiers\n  x <- stringr::str_remove_all(x, \"<.*?>\")\n  # remove quotation marks\n  x <- stringr::str_remove_all(x, fixed(\"\\\"\"))\n  # remove empty elements\n  x <- x[!x==\"\"]\n  # split text into sentences\n  x <- tokenize_sentences(x)\n  x <- unlist(x)\n}\n# clean texts\nns1_sen <- cleanText(ns1)\nns2_sen <- cleanText(ns2)\nde_sen <- cleanText(de)\nes_sen <- cleanText(es)\nfr_sen <- cleanText(fr)\nit_sen <- cleanText(it)\npl_sen <- cleanText(pl)\nru_sen <- cleanText(ru)\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n```{=html}\n<div class=\"tabwid\"><style>.cl-b648f67a{table-layout:auto;width:75%;}.cl-b6454aca{font-family:'DejaVu Sans';font-size:12pt;font-weight:bold;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-b6454ad4{font-family:'DejaVu Sans';font-size:12pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-b64559fc{margin:0;text-align:center;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-b6457c84{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b6457c98{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b6457c99{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b6457ca2{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}</style><table class='cl-b648f67a'>\n```\n<caption class=\"Table Caption\">\n\nFirst 6 sentences of the Russian learner data.\n\n</caption>\n```{=html}\n<thead><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-b6457ca2\"><p class=\"cl-b64559fc\"><span class=\"cl-b6454aca\">.</span></p></td></tr></thead><tbody><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-b6457c84\"><p class=\"cl-b64559fc\"><span class=\"cl-b6454ad4\">It is now a very wide spread opinion, that in the modern world there is no place for dreaming and imagination.</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-b6457c98\"><p class=\"cl-b64559fc\"><span class=\"cl-b6454ad4\">Those who share this point of view usually say that at present we are so very much under the domination of science, industry, technology, ever-increasing tempo of our lives and so on, that neither dreaming nor imagination can possibly survive.</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-b6457c84\"><p class=\"cl-b64559fc\"><span class=\"cl-b6454ad4\">Their usual argument is very simple - they suggest to their opponents to look at some samples of the modern art and to compare them to the masterpieces of the Old Masters of painting, music, literature.</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-b6457c98\"><p class=\"cl-b64559fc\"><span class=\"cl-b6454ad4\">As everything which is simple, the argument sounds very convincing.</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-b6457c99\"><p class=\"cl-b64559fc\"><span class=\"cl-b6454ad4\">Of course, it is evident, that no modern writer, painter or musician can be compare to such names as Bach, Pushkin&lt; Byron, Mozart, Rembrandt, Raffael et cetera.</span></p></td></tr></tbody></table></div>\n```\n:::\n:::\n\n\n\nNow that we have split the texts into individual sentences, we can easily extract and visualize the average sentence lengths of L1 speakers and learners of English.\n\n# Sentence length{-}\n\nThe most basic complexity measure is average sentence length. In the following, we will extract the average sentence length for L1-speakers and learners of English with different language backgrounds.\n\nWe can use the `count_words` function from the `tokenizers` package to count the words in each sentence. We apply the function to all texts and generate a table (a data frame) of the results and add the L1 of the speaker who produced the sentence.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# extract sentences lengths\nns1_sl <- tokenizers::count_words(ns1_sen)\nns2_sl <- tokenizers::count_words(ns2_sen)\nde_sl <- tokenizers::count_words(de_sen)\nes_sl <- tokenizers::count_words(es_sen)\nfr_sl <- tokenizers::count_words(fr_sen)\nit_sl <- tokenizers::count_words(it_sen)\npl_sl <- tokenizers::count_words(pl_sen)\nru_sl <- tokenizers::count_words(ru_sen)\n# create a data frame from the results\nsl_df <- data.frame(c(ns1_sl, ns2_sl, de_sl, es_sl, fr_sl, it_sl, pl_sl, ru_sl)) %>%\n  dplyr::rename(sentenceLength = 1) %>%\n  dplyr::mutate(l1 = c(rep(\"en\", length(ns1_sl)),\n                       rep(\"en\", length(ns2_sl)),\n                       rep(\"de\", length(de_sl)),\n                       rep(\"es\", length(es_sl)),\n                       rep(\"fr\", length(fr_sl)),\n                       rep(\"it\", length(it_sl)),\n                       rep(\"pl\", length(pl_sl)),\n                       rep(\"ru\", length(ru_sl))))\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n```{=html}\n<div class=\"tabwid\"><style>.cl-b656fedc{table-layout:auto;width:75%;}.cl-b652c056{font-family:'DejaVu Sans';font-size:12pt;font-weight:bold;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-b652c060{font-family:'DejaVu Sans';font-size:12pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-b652cf06{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-b652cf1a{margin:0;text-align:center;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-b6530052{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b6530066{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b6530070{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b6530071{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b653007a{background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b653007b{background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b6530084{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b6530085{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}</style><table class='cl-b656fedc'>\n```\n<caption class=\"Table Caption\">\n\nFirst 6 rows of the table holding the sentences lengths and the L1 of the speakers that produced them.\n\n</caption>\n```{=html}\n<thead><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-b6530085\"><p class=\"cl-b652cf06\"><span class=\"cl-b652c056\">sentenceLength</span></p></td><td class=\"cl-b6530084\"><p class=\"cl-b652cf1a\"><span class=\"cl-b652c056\">l1</span></p></td></tr></thead><tbody><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-b6530066\"><p class=\"cl-b652cf06\"><span class=\"cl-b652c060\">2</span></p></td><td class=\"cl-b6530052\"><p class=\"cl-b652cf1a\"><span class=\"cl-b652c060\">en</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-b6530071\"><p class=\"cl-b652cf06\"><span class=\"cl-b652c060\">17</span></p></td><td class=\"cl-b6530070\"><p class=\"cl-b652cf1a\"><span class=\"cl-b652c060\">en</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-b6530066\"><p class=\"cl-b652cf06\"><span class=\"cl-b652c060\">23</span></p></td><td class=\"cl-b6530052\"><p class=\"cl-b652cf1a\"><span class=\"cl-b652c060\">en</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-b6530071\"><p class=\"cl-b652cf06\"><span class=\"cl-b652c060\">17</span></p></td><td class=\"cl-b6530070\"><p class=\"cl-b652cf1a\"><span class=\"cl-b652c060\">en</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-b6530066\"><p class=\"cl-b652cf06\"><span class=\"cl-b652c060\">20</span></p></td><td class=\"cl-b6530052\"><p class=\"cl-b652cf1a\"><span class=\"cl-b652c060\">en</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-b653007b\"><p class=\"cl-b652cf06\"><span class=\"cl-b652c060\">34</span></p></td><td class=\"cl-b653007a\"><p class=\"cl-b652cf1a\"><span class=\"cl-b652c060\">en</span></p></td></tr></tbody></table></div>\n```\n:::\n:::\n\n\nNow, we can use the resulting table to create a box plot showing the results.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsl_df %>%\n  ggplot(aes(x = reorder(l1, -sentenceLength, mean), y = sentenceLength, fill = l1)) +\n  geom_boxplot() +\n  # adapt y-axis labels\n  labs(y = \"Sentence lenghts\") +\n  # adapt tick labels\n  scale_x_discrete(\"L1 of learners\", \n                   breaks = names(table(sl_df$l1)), \n                   labels = c(\"en\" = \"English\",\n                              \"de\" = \"German\",\n                              \"es\" = \"Spanish\",\n                              \"fr\" = \"French\",\n                              \"it\" = \"Italian\",\n                              \"pl\" = \"Polish\",\n                              \"ru\" = \"Russian\")) +\n  theme_bw() +\n  theme(legend.position = \"none\")\n```\n\n::: {.cell-output-display}\n![](llr_files/figure-html/senl8-1.png){width=672}\n:::\n:::\n\n\n# Extracting N-grams{-}\n\nIn a next step, we extract n-grams using the `tokens_ngrams` function from the `quanteda` package. In a first step, we take the sentence data, convert it to lower case and remove punctuation. Then we apply the `tokens_ngrams` function to extract the n-grams (in this case 2-grams).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nns1_tok <- ns1_sen %>%\n  tolower() %>%\n  quanteda::tokens(remove_punct = TRUE)\n# extract n-grams\nns1_2gram <- quanteda::tokens_ngrams(ns1_tok, n = 2)\n# inspect\nhead(ns1_2gram[[2]], 10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] \"the_basic\"        \"basic_dilema\"     \"dilema_facing\"    \"facing_the\"      \n [5] \"the_uk's\"         \"uk's_rail\"        \"rail_and\"         \"and_road\"        \n [9] \"road_transport\"   \"transport_system\"\n```\n:::\n:::\n\n\nWe can also extract tri-grams easily by changing the `n` argument in the `tokens_ngrams` function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# extract n-grams\nns1_3gram <- quanteda::tokens_ngrams(ns1_tok, n = 3)\n# inspect\nhead(ns1_3gram[[2]])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"the_basic_dilema\"    \"basic_dilema_facing\" \"dilema_facing_the\"  \n[4] \"facing_the_uk's\"     \"the_uk's_rail\"       \"uk's_rail_and\"      \n```\n:::\n:::\n\n\n\nWe now apply the same procedure to all texts as shown below.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nns1_tok <- ns1_sen %>% tolower() %>% quanteda::tokens(remove_punct = TRUE)\nns2_tok <- ns2_sen %>% tolower() %>% quanteda::tokens(remove_punct = TRUE)\nde_tok <- de_sen %>% tolower() %>% quanteda::tokens(remove_punct = TRUE)\nes_tok <- es_sen %>% tolower() %>% quanteda::tokens(remove_punct = TRUE)\nfr_tok <- fr_sen %>% tolower() %>% quanteda::tokens(remove_punct = TRUE)\nit_tok <- it_sen %>% tolower() %>% quanteda::tokens(remove_punct = TRUE)\npl_tok <- pl_sen %>% tolower() %>% quanteda::tokens(remove_punct = TRUE)\nru_tok <- ru_sen %>% tolower() %>% quanteda::tokens(remove_punct = TRUE)\n# extract n-grams\nns1_2gram <- as.vector(unlist(quanteda::tokens_ngrams(ns1_tok, n = 2)))\nns2_2gram <- as.vector(unlist(quanteda::tokens_ngrams(ns2_tok, n = 2)))\nde_2gram <- as.vector(unlist(quanteda::tokens_ngrams(de_tok, n = 2)))\nes_2gram <- as.vector(unlist(quanteda::tokens_ngrams(es_tok, n = 2)))\nfr_2gram <- as.vector(unlist(quanteda::tokens_ngrams(fr_tok, n = 2)))\nit_2gram <- as.vector(unlist(quanteda::tokens_ngrams(it_tok, n = 2)))\npl_2gram <- as.vector(unlist(quanteda::tokens_ngrams(pl_tok, n = 2)))\nru_2gram <- as.vector(unlist(quanteda::tokens_ngrams(ru_tok, n = 2)))\n```\n:::\n\n\n\nNext, we generate a table with the ngrams and the L1 background of the speaker that produced the bi-grams.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nngram_df <- c(ns1_2gram, ns2_2gram, de_2gram, es_2gram, \n              fr_2gram, it_2gram, pl_2gram, ru_2gram) %>%\n  as.data.frame() %>%\n  dplyr::rename(ngram = 1) %>%\n  dplyr::mutate(l1 = c(rep(\"en\", length(ns1_2gram)),\n                       rep(\"en\", length(ns2_2gram)),\n                       rep(\"de\", length(de_2gram)),\n                       rep(\"es\", length(es_2gram)),\n                       rep(\"fr\", length(fr_2gram)),\n                       rep(\"it\", length(it_2gram)),\n                       rep(\"pl\", length(pl_2gram)),\n                       rep(\"ru\", length(ru_2gram))),\n                learner = ifelse(l1 == \"en\", \"no\", \"yes\"))\n# inspect\nhead(ngram_df)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          ngram l1 learner\n1  transport_01 en      no\n2     the_basic en      no\n3  basic_dilema en      no\n4 dilema_facing en      no\n5    facing_the en      no\n6      the_uk's en      no\n```\n:::\n:::\n\n\nNow, we process the table further to add frequency information, i.e., how often a given n-gram occurs in each the language of speakers with distinct L1 backgrounds.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nngram_fdf <- ngram_df %>%\n  dplyr::group_by(ngram, learner) %>%\n  dplyr::summarise(freq = n()) %>%\n  dplyr::arrange(-freq)\n# inspect\nhead(ngram_fdf)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 √ó 3\n# Groups:   ngram [5]\n  ngram            learner  freq\n  <chr>            <chr>   <int>\n1 of_the           no         72\n2 to_the           no         40\n3 in_the           no         39\n4 public_transport no         35\n5 of_the           yes        33\n6 number_of        no         32\n```\n:::\n:::\n\n\nAs the word counts of the texts are quite different, we normalize the frequencies to per-1,000-word frequencies which are comparable across texts of different lengths.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nngram_nfdf <- ngram_fdf %>%\n  dplyr::group_by(ngram) %>%\n  dplyr::mutate(total_ngram = sum(freq)) %>%\n  dplyr::arrange(-total_ngram) %>%\n  # total by learner\n  dplyr::group_by(learner) %>%\n  dplyr::mutate(total_learner = sum(freq),\n                rfreq = freq/total_learner*1000)\n# inspect\nhead(ngram_nfdf, 10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 10 √ó 6\n# Groups:   learner [2]\n   ngram            learner  freq total_ngram total_learner rfreq\n   <chr>            <chr>   <int>       <int>         <int> <dbl>\n 1 of_the           no         72         105          9452  7.62\n 2 of_the           yes        33         105          3395  9.72\n 3 in_the           no         39          49          9452  4.13\n 4 in_the           yes        10          49          3395  2.95\n 5 to_the           no         40          47          9452  4.23\n 6 to_the           yes         7          47          3395  2.06\n 7 it_is            no         23          44          9452  2.43\n 8 it_is            yes        21          44          3395  6.19\n 9 public_transport no         35          35          9452  3.70\n10 number_of        no         32          35          9452  3.39\n```\n:::\n:::\n\n\nWe now reformat the table so that we have relative frequencies for both learners and L1 speakers even if a particular n-gram does not occur in the text produced by either a learner or a L1 speaker.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nngram_rel <- ngram_nfdf %>%\n  dplyr::select(ngram, learner, rfreq, total_ngram) %>%\n  tidyr::spread(learner, rfreq) %>%\n  dplyr::mutate(no = ifelse(is.na(no), 0, no),\n                yes = ifelse(is.na(yes), 0, yes)) %>%\n  tidyr::gather(learner, rfreq, no:yes) %>%\n  dplyr::arrange(-total_ngram)\n# inspect\nhead(ngram_rel)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 √ó 4\n  ngram  total_ngram learner rfreq\n  <chr>        <int> <chr>   <dbl>\n1 of_the         105 no       7.62\n2 of_the         105 yes      9.72\n3 in_the          49 no       4.13\n4 in_the          49 yes      2.95\n5 to_the          47 no       4.23\n6 to_the          47 yes      2.06\n```\n:::\n:::\n\n\nFinally, we visualize the most frequent n-grams in the data in a bar chart.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nngram_rel %>%\n  head(20) %>%\n  ggplot(aes(y = rfreq, x = reorder(ngram, -total_ngram), group = learner, fill = learner)) +\n  geom_bar(stat = \"identity\", position = position_dodge()) +\n  theme_bw() +\n  theme(axis.text.x = element_text(size=8, angle=90),\n        legend.position = \"top\") +\n  labs(y = \"Relative frequnecy\\n(per 1,000 words)\", x = \"n-gram\")\n```\n\n::: {.cell-output-display}\n![](llr_files/figure-html/ng10-1.png){width=672}\n:::\n:::\n\n\n\nWe can, of course also investigate only specific n-grams, e.g., n-grams containing a specific word such as *public* (below, we only show the first 6 n-grams containing *public* by using the `head` function).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nngram_rel %>%\n  dplyr::filter(stringr::str_detect(ngram, \"public\")) %>%\n  head()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 √ó 4\n  ngram            total_ngram learner rfreq\n  <chr>                  <int> <chr>   <dbl>\n1 public_transport          35 no      3.70 \n2 public_transport          35 yes     0    \n3 use_public                10 no      1.06 \n4 use_public                10 yes     0    \n5 of_public                  6 no      0.635\n6 of_public                  6 yes     0    \n```\n:::\n:::\n\n\nWe can also specify the order by adding the underscore as shown below.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nngram_rel %>%\n  dplyr::filter(stringr::str_detect(ngram, \"public_\")) %>%\n  head()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 √ó 4\n  ngram             total_ngram learner rfreq\n  <chr>                   <int> <chr>   <dbl>\n1 public_transport           35 no      3.70 \n2 public_transport           35 yes     0    \n3 public_action               1 no      0.106\n4 public_and                  1 no      0.106\n5 public_awareness            1 no      0.106\n6 public_opposition           1 no      0.106\n```\n:::\n:::\n\n\n# Differences in ngram use{-}\n\nNext, we will set out to identify differences in n-gram frequencies between learners and L1 speakers. In a first step, we transform the table so that we have separate columns for learners and L1-speakers. In addition, we also add columns containing all the information we need to perform Fisher's exact test to check if learners use certain n-grams significantly  more or less frequently compared to L1-speakers. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nsdif_ngram <- ngram_fdf %>%\n  tidyr::spread(learner, freq) %>%\n  dplyr::mutate(no = ifelse(is.na(no), 0, no),\n                yes = ifelse(is.na(yes), 0, yes)) %>%\n  dplyr::rename(l1speaker = no, \n                learner = yes) %>%\n  dplyr::mutate(total_ngram = l1speaker+learner) %>%\n  dplyr::ungroup() %>%\n  dplyr::mutate(total_learner = sum(learner),\n              total_l1 = sum(l1speaker)) %>%\n  dplyr::mutate(a = l1speaker,\n                b = learner) %>%\n  dplyr::mutate(c = total_l1-a,\n                d = total_learner-b)\n# inspect\nhead(sdif_ngram)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 √ó 10\n  ngram   l1speaker learner total_ngram total_learner total_l1     a     b     c\n  <chr>       <dbl>   <dbl>       <dbl>         <dbl>    <dbl> <dbl> <dbl> <dbl>\n1 -to_cr‚Ä¶         1       0           1          3395     9452     1     0  9451\n2 `_t             0       1           1          3395     9452     0     1  9452\n3 +_even          1       0           1          3395     9452     1     0  9451\n4 +_peop‚Ä¶         1       0           1          3395     9452     1     0  9451\n5 <_byron         0       1           1          3395     9452     0     1  9452\n6 ¬£_1mil‚Ä¶         1       0           1          3395     9452     1     0  9451\n# ‚Ä¶ with 1 more variable: d <dbl>\n```\n:::\n:::\n\n\nOn this re-arranged data set, we can now apply the Fisher's exact tests. As we are performing many different tests, we need to correct for multiple comparisons. To this end, we create a column which holds the Bonferroni corrected critical value (\\alpha .05). If a p-value is lower than the corrected critical value, then the learners and L1-speakers differ significantly in their use of that n-gram.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsdif_ngram <- sdif_ngram  %>%\n  # perform fishers exact test and extract estimate and p\n  dplyr::rowwise() %>%\n  dplyr::mutate(fisher_p = fisher.test(matrix(c(a,c,b,d), nrow= 2))$p.value,\n                oddsratio = fisher.test(matrix(c(a,c,b,d), nrow= 2))$estimate,\n                # calculate bonferroni correction\n                crit = .05/nrow(.),\n                sig_corr = ifelse(fisher_p < crit, \"p<.05\", \"n.s.\")) %>%\n  dplyr::arrange(fisher_p) %>%\n  dplyr::select(-total_ngram, -total_learner, -total_l1, -a, -b, -c, -d, -crit)\n# inspect\nhead(sdif_ngram)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 √ó 6\n# Rowwise: \n  ngram            l1speaker learner  fisher_p oddsratio sig_corr\n  <chr>                <dbl>   <dbl>     <dbl>     <dbl> <chr>   \n1 in_silence               0       8 0.0000236         0 n.s.    \n2 public_transport        35       0 0.0000276       Inf n.s.    \n3 silence_is               0       7 0.0000896         0 n.s.    \n4 of_all                   0       6 0.000339          0 n.s.    \n5 our_society              0       6 0.000339          0 n.s.    \n6 in_our                   0       5 0.00129           0 n.s.    \n```\n:::\n:::\n\n\nIn our case, there are no n-grams that differ significantly in their use by learners and L1-speakers once we have corrected for repeated testing as indicated by the *n.s.* (not significant) in the column called *sig_corr*.\n\n\n# Finding collocations{-}\n\nThere are various techniques for identifying collocations. To identify collocations without having a pre-defined target term, we can use the `textstat_collocations` function from the `quanteda.textstats` package [cf. @benoit2021package].\n\nHowever, before we can apply that function and start identifying collocations, we need to process the data to which we want to apply this function. In the present case, we will apply that function to the sentences in the L1 data which we extract in the code chunk below.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nns_sen <- c(ns1_sen, ns2_sen) %>%\n  tolower()\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n```{=html}\n<div class=\"tabwid\"><style>.cl-b93f96f4{table-layout:auto;width:95%;}.cl-b93a0388{font-family:'DejaVu Sans';font-size:12pt;font-weight:bold;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-b93a03a6{font-family:'DejaVu Sans';font-size:12pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-b93a1b5c{margin:0;text-align:center;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-b93a4a78{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b93a4a82{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b93a4a8c{background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b93a4a96{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}</style><table class='cl-b93f96f4'>\n```\n<caption class=\"Table Caption\">\n\nFirst 6 sentences in L1 data.\n\n</caption>\n```{=html}\n<thead><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-b93a4a96\"><p class=\"cl-b93a1b5c\"><span class=\"cl-b93a0388\">.</span></p></td></tr></thead><tbody><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-b93a4a78\"><p class=\"cl-b93a1b5c\"><span class=\"cl-b93a03a6\">transport 01</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-b93a4a82\"><p class=\"cl-b93a1b5c\"><span class=\"cl-b93a03a6\">the basic dilema facing the uk's rail and road transport system is the general rise in population.</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-b93a4a78\"><p class=\"cl-b93a1b5c\"><span class=\"cl-b93a03a6\">this leads to an increase in the number of commuters and transport users every year, consequently putting pressure on the uks transports network.</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-b93a4a82\"><p class=\"cl-b93a1b5c\"><span class=\"cl-b93a03a6\">the biggest worry to the system is the rapid rise of car users outside the major cities.</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-b93a4a78\"><p class=\"cl-b93a1b5c\"><span class=\"cl-b93a03a6\">most large cities have managed to incourage commuters to use public transport thus decreasing major conjestion in rush hour periods.</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-b93a4a8c\"><p class=\"cl-b93a1b5c\"><span class=\"cl-b93a03a6\">public transport is the obvious solution to to the increase in population if it is made cheep to commuters, clean, easy and efficient then it could take the strain of the overloaded british roads.</span></p></td></tr></tbody></table></div>\n```\n:::\n:::\n\n\n\nFrom the output shown above, we also see that splitting texts did not work perfectly as it produces some unwarranted artifacts like the \"sentences\" that consist of headings (e.g., *transport 01*). Fortunately, these errors do not really matter in the case of our example.\n\nNow that we have the L1 data split into sentences, we can tokenize these sentences and apply the `textstat_collocations` function which identifies collocations.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# create a token object\nns_tokens <- quanteda::tokens(ns_sen, remove_punct = TRUE)# %>%\n#  tokens_remove(stopwords(\"english\"))\n# extract collocations\nns_coll <- quanteda.textstats::textstat_collocations(ns_tokens, size = 2, min_count = 20)\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n```{=html}\n<div class=\"tabwid\"><style>.cl-b9586544{table-layout:auto;width:50%;}.cl-b953bbf2{font-family:'DejaVu Sans';font-size:12pt;font-weight:bold;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-b953bbfc{font-family:'DejaVu Sans';font-size:12pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-b953ce12{margin:0;text-align:center;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-b953ce1c{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-b953fbf8{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b953fbf9{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b953fc02{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b953fc0c{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b953fc0d{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b953fc16{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b953fc17{background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b953fc20{background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b953fc2a{background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b953fc34{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b953fc35{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b953fc3e{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}</style><table class='cl-b9586544'>\n```\n<caption class=\"Table Caption\">\n\nTop 6 collocations in teh L1 data.\n\n</caption>\n```{=html}\n<thead><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-b953fc34\"><p class=\"cl-b953ce12\"><span class=\"cl-b953bbf2\">collocation</span></p></td><td class=\"cl-b953fc35\"><p class=\"cl-b953ce1c\"><span class=\"cl-b953bbf2\">count</span></p></td><td class=\"cl-b953fc35\"><p class=\"cl-b953ce1c\"><span class=\"cl-b953bbf2\">count_nested</span></p></td><td class=\"cl-b953fc35\"><p class=\"cl-b953ce1c\"><span class=\"cl-b953bbf2\">length</span></p></td><td class=\"cl-b953fc35\"><p class=\"cl-b953ce1c\"><span class=\"cl-b953bbf2\">lambda</span></p></td><td class=\"cl-b953fc3e\"><p class=\"cl-b953ce1c\"><span class=\"cl-b953bbf2\">z</span></p></td></tr></thead><tbody><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-b953fc02\"><p class=\"cl-b953ce12\"><span class=\"cl-b953bbfc\">public transport</span></p></td><td class=\"cl-b953fbf8\"><p class=\"cl-b953ce1c\"><span class=\"cl-b953bbfc\">35</span></p></td><td class=\"cl-b953fbf8\"><p class=\"cl-b953ce1c\"><span class=\"cl-b953bbfc\">0</span></p></td><td class=\"cl-b953fbf8\"><p class=\"cl-b953ce1c\"><span class=\"cl-b953bbfc\">2</span></p></td><td class=\"cl-b953fbf8\"><p class=\"cl-b953ce1c\"><span class=\"cl-b953bbfc\">7.170227</span></p></td><td class=\"cl-b953fbf9\"><p class=\"cl-b953ce1c\"><span class=\"cl-b953bbfc\">14.89924</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-b953fc16\"><p class=\"cl-b953ce12\"><span class=\"cl-b953bbfc\">it is</span></p></td><td class=\"cl-b953fc0c\"><p class=\"cl-b953ce1c\"><span class=\"cl-b953bbfc\">23</span></p></td><td class=\"cl-b953fc0c\"><p class=\"cl-b953ce1c\"><span class=\"cl-b953bbfc\">0</span></p></td><td class=\"cl-b953fc0c\"><p class=\"cl-b953ce1c\"><span class=\"cl-b953bbfc\">2</span></p></td><td class=\"cl-b953fc0c\"><p class=\"cl-b953ce1c\"><span class=\"cl-b953bbfc\">3.119043</span></p></td><td class=\"cl-b953fc0d\"><p class=\"cl-b953ce1c\"><span class=\"cl-b953bbfc\">12.15265</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-b953fc02\"><p class=\"cl-b953ce12\"><span class=\"cl-b953bbfc\">of the</span></p></td><td class=\"cl-b953fbf8\"><p class=\"cl-b953ce1c\"><span class=\"cl-b953bbfc\">72</span></p></td><td class=\"cl-b953fbf8\"><p class=\"cl-b953ce1c\"><span class=\"cl-b953bbfc\">0</span></p></td><td class=\"cl-b953fbf8\"><p class=\"cl-b953ce1c\"><span class=\"cl-b953bbfc\">2</span></p></td><td class=\"cl-b953fbf8\"><p class=\"cl-b953ce1c\"><span class=\"cl-b953bbfc\">1.617862</span></p></td><td class=\"cl-b953fbf9\"><p class=\"cl-b953ce1c\"><span class=\"cl-b953bbfc\">11.45624</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-b953fc16\"><p class=\"cl-b953ce12\"><span class=\"cl-b953bbfc\">to use</span></p></td><td class=\"cl-b953fc0c\"><p class=\"cl-b953ce1c\"><span class=\"cl-b953bbfc\">21</span></p></td><td class=\"cl-b953fc0c\"><p class=\"cl-b953ce1c\"><span class=\"cl-b953bbfc\">0</span></p></td><td class=\"cl-b953fc0c\"><p class=\"cl-b953ce1c\"><span class=\"cl-b953bbfc\">2</span></p></td><td class=\"cl-b953fc0c\"><p class=\"cl-b953ce1c\"><span class=\"cl-b953bbfc\">3.458612</span></p></td><td class=\"cl-b953fc0d\"><p class=\"cl-b953ce1c\"><span class=\"cl-b953bbfc\">10.59222</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-b953fc02\"><p class=\"cl-b953ce12\"><span class=\"cl-b953bbfc\">number of</span></p></td><td class=\"cl-b953fbf8\"><p class=\"cl-b953ce1c\"><span class=\"cl-b953bbfc\">32</span></p></td><td class=\"cl-b953fbf8\"><p class=\"cl-b953ce1c\"><span class=\"cl-b953bbfc\">0</span></p></td><td class=\"cl-b953fbf8\"><p class=\"cl-b953ce1c\"><span class=\"cl-b953bbfc\">2</span></p></td><td class=\"cl-b953fbf8\"><p class=\"cl-b953ce1c\"><span class=\"cl-b953bbfc\">5.693830</span></p></td><td class=\"cl-b953fbf9\"><p class=\"cl-b953ce1c\"><span class=\"cl-b953bbfc\">10.06386</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-b953fc20\"><p class=\"cl-b953ce12\"><span class=\"cl-b953bbfc\">on the</span></p></td><td class=\"cl-b953fc17\"><p class=\"cl-b953ce1c\"><span class=\"cl-b953bbfc\">31</span></p></td><td class=\"cl-b953fc17\"><p class=\"cl-b953ce1c\"><span class=\"cl-b953bbfc\">0</span></p></td><td class=\"cl-b953fc17\"><p class=\"cl-b953ce1c\"><span class=\"cl-b953bbfc\">2</span></p></td><td class=\"cl-b953fc17\"><p class=\"cl-b953ce1c\"><span class=\"cl-b953bbfc\">2.103127</span></p></td><td class=\"cl-b953fc2a\"><p class=\"cl-b953ce1c\"><span class=\"cl-b953bbfc\">9.43355</span></p></td></tr></tbody></table></div>\n```\n:::\n:::\n\n\n\n\nThe resulting table shows collocations in L1 data descending by collocation strength.\n\n## Visualizing collocation networks{-}\n\nNetwork graphs are a very useful and flexible tool for visualizing relationships between elements such as words, personas, or authors. This section shows how to generate a network graph for collocations of the term *transport* using the `quanteda` package.\n\nIn a first step, we generate a document-feature matrix based on the sentences in the L1 data. A document-feature matrix shows how often elements (here these elements are the words that occur in the L1 data) occur in a selection of documents (here these documents are the sentences in the L1 data).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# create document-feature matrix\nns_dfm <- ns_sen %>% \n  #quanteda::dfm(remove_punct = TRUE) %>%\n    quanteda::dfm(remove = stopwords('english'), remove_punct = TRUE)# %>%\n    #quanteda::dfm_trim(min_termfreq = 10, verbose = FALSE)\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n```{=html}\n<div class=\"tabwid\"><style>.cl-b9778aaa{table-layout:auto;width:50%;}.cl-b973995e{font-family:'DejaVu Sans';font-size:12pt;font-weight:bold;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-b9739972{font-family:'DejaVu Sans';font-size:12pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-b973a53e{margin:0;text-align:center;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-b973a548{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-b973cc26{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b973cc27{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b973cc30{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b973cc31{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b973cc3a{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b973cc3b{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b973cc44{background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b973cc4e{background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b973cc4f{background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b973cc58{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b973cc59{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b973cc62{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}</style><table class='cl-b9778aaa'>\n```\n<caption class=\"Table Caption\">\n\nFirst 6 rows and columns of the document-feature matrix.\n\n</caption>\n```{=html}\n<thead><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-b973cc62\"><p class=\"cl-b973a53e\"><span class=\"cl-b973995e\">doc_id</span></p></td><td class=\"cl-b973cc58\"><p class=\"cl-b973a548\"><span class=\"cl-b973995e\">transport</span></p></td><td class=\"cl-b973cc58\"><p class=\"cl-b973a548\"><span class=\"cl-b973995e\">01</span></p></td><td class=\"cl-b973cc58\"><p class=\"cl-b973a548\"><span class=\"cl-b973995e\">basic</span></p></td><td class=\"cl-b973cc58\"><p class=\"cl-b973a548\"><span class=\"cl-b973995e\">dilema</span></p></td><td class=\"cl-b973cc58\"><p class=\"cl-b973a548\"><span class=\"cl-b973995e\">facing</span></p></td><td class=\"cl-b973cc59\"><p class=\"cl-b973a548\"><span class=\"cl-b973995e\">uk's</span></p></td></tr></thead><tbody><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-b973cc30\"><p class=\"cl-b973a53e\"><span class=\"cl-b9739972\">text1</span></p></td><td class=\"cl-b973cc26\"><p class=\"cl-b973a548\"><span class=\"cl-b9739972\">1</span></p></td><td class=\"cl-b973cc26\"><p class=\"cl-b973a548\"><span class=\"cl-b9739972\">1</span></p></td><td class=\"cl-b973cc26\"><p class=\"cl-b973a548\"><span class=\"cl-b9739972\">0</span></p></td><td class=\"cl-b973cc26\"><p class=\"cl-b973a548\"><span class=\"cl-b9739972\">0</span></p></td><td class=\"cl-b973cc26\"><p class=\"cl-b973a548\"><span class=\"cl-b9739972\">0</span></p></td><td class=\"cl-b973cc27\"><p class=\"cl-b973a548\"><span class=\"cl-b9739972\">0</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-b973cc3a\"><p class=\"cl-b973a53e\"><span class=\"cl-b9739972\">text2</span></p></td><td class=\"cl-b973cc31\"><p class=\"cl-b973a548\"><span class=\"cl-b9739972\">1</span></p></td><td class=\"cl-b973cc31\"><p class=\"cl-b973a548\"><span class=\"cl-b9739972\">0</span></p></td><td class=\"cl-b973cc31\"><p class=\"cl-b973a548\"><span class=\"cl-b9739972\">1</span></p></td><td class=\"cl-b973cc31\"><p class=\"cl-b973a548\"><span class=\"cl-b9739972\">1</span></p></td><td class=\"cl-b973cc31\"><p class=\"cl-b973a548\"><span class=\"cl-b9739972\">1</span></p></td><td class=\"cl-b973cc3b\"><p class=\"cl-b973a548\"><span class=\"cl-b9739972\">1</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-b973cc30\"><p class=\"cl-b973a53e\"><span class=\"cl-b9739972\">text3</span></p></td><td class=\"cl-b973cc26\"><p class=\"cl-b973a548\"><span class=\"cl-b9739972\">1</span></p></td><td class=\"cl-b973cc26\"><p class=\"cl-b973a548\"><span class=\"cl-b9739972\">0</span></p></td><td class=\"cl-b973cc26\"><p class=\"cl-b973a548\"><span class=\"cl-b9739972\">0</span></p></td><td class=\"cl-b973cc26\"><p class=\"cl-b973a548\"><span class=\"cl-b9739972\">0</span></p></td><td class=\"cl-b973cc26\"><p class=\"cl-b973a548\"><span class=\"cl-b9739972\">0</span></p></td><td class=\"cl-b973cc27\"><p class=\"cl-b973a548\"><span class=\"cl-b9739972\">0</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-b973cc3a\"><p class=\"cl-b973a53e\"><span class=\"cl-b9739972\">text4</span></p></td><td class=\"cl-b973cc31\"><p class=\"cl-b973a548\"><span class=\"cl-b9739972\">0</span></p></td><td class=\"cl-b973cc31\"><p class=\"cl-b973a548\"><span class=\"cl-b9739972\">0</span></p></td><td class=\"cl-b973cc31\"><p class=\"cl-b973a548\"><span class=\"cl-b9739972\">0</span></p></td><td class=\"cl-b973cc31\"><p class=\"cl-b973a548\"><span class=\"cl-b9739972\">0</span></p></td><td class=\"cl-b973cc31\"><p class=\"cl-b973a548\"><span class=\"cl-b9739972\">0</span></p></td><td class=\"cl-b973cc3b\"><p class=\"cl-b973a548\"><span class=\"cl-b9739972\">0</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-b973cc30\"><p class=\"cl-b973a53e\"><span class=\"cl-b9739972\">text5</span></p></td><td class=\"cl-b973cc26\"><p class=\"cl-b973a548\"><span class=\"cl-b9739972\">1</span></p></td><td class=\"cl-b973cc26\"><p class=\"cl-b973a548\"><span class=\"cl-b9739972\">0</span></p></td><td class=\"cl-b973cc26\"><p class=\"cl-b973a548\"><span class=\"cl-b9739972\">0</span></p></td><td class=\"cl-b973cc26\"><p class=\"cl-b973a548\"><span class=\"cl-b9739972\">0</span></p></td><td class=\"cl-b973cc26\"><p class=\"cl-b973a548\"><span class=\"cl-b9739972\">0</span></p></td><td class=\"cl-b973cc27\"><p class=\"cl-b973a548\"><span class=\"cl-b9739972\">0</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-b973cc4f\"><p class=\"cl-b973a53e\"><span class=\"cl-b9739972\">text6</span></p></td><td class=\"cl-b973cc44\"><p class=\"cl-b973a548\"><span class=\"cl-b9739972\">1</span></p></td><td class=\"cl-b973cc44\"><p class=\"cl-b973a548\"><span class=\"cl-b9739972\">0</span></p></td><td class=\"cl-b973cc44\"><p class=\"cl-b973a548\"><span class=\"cl-b9739972\">0</span></p></td><td class=\"cl-b973cc44\"><p class=\"cl-b973a548\"><span class=\"cl-b9739972\">0</span></p></td><td class=\"cl-b973cc44\"><p class=\"cl-b973a548\"><span class=\"cl-b9739972\">0</span></p></td><td class=\"cl-b973cc4e\"><p class=\"cl-b973a548\"><span class=\"cl-b9739972\">0</span></p></td></tr></tbody></table></div>\n```\n:::\n:::\n\n\n\nAs we want to generate a network graph of words that collocate with the term *organism*, we use the `calculateCoocStatistics` function to determine which words most strongly collocate with our target term (*organism*).  \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# load function for co-occurrence calculation\nsource(\"https://slcladal.github.io/rscripts/calculateCoocStatistics.R\")\n# define term\ncoocTerm <- \"transport\"\n# calculate co-occurrence statistics\ncoocs <- calculateCoocStatistics(coocTerm, ns_dfm, measure=\"LOGLIK\")\n# inspect results\ncoocs[1:10]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    public        use    traffic       rail     facing  commuters    cheaper \n113.171974  19.437311  10.508626   9.652830   9.382889   9.382889   9.382889 \n     roads       less      buses \n  9.080648   8.067363   6.702863 \n```\n:::\n:::\n\n\n\n\nWe now reduce the document-feature matrix to contain only the top 20 collocates of *transport* (plus our target word *transport*).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nredux_dfm <- dfm_select(ns_dfm, \n                        pattern = c(names(coocs)[1:10], \"transport\"))\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n```{=html}\n<div class=\"tabwid\"><style>.cl-ba0f22b6{table-layout:auto;width:50%;}.cl-ba0b1644{font-family:'DejaVu Sans';font-size:12pt;font-weight:bold;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-ba0b164e{font-family:'DejaVu Sans';font-size:12pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-ba0b1f2c{margin:0;text-align:center;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-ba0b1f36{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-ba0b3b88{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ba0b3b92{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ba0b3b9c{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ba0b3b9d{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ba0b3ba6{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ba0b3ba7{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ba0b3ba8{background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ba0b3bb0{background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ba0b3bba{background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ba0b3bbb{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ba0b3bbc{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ba0b3bc4{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}</style><table class='cl-ba0f22b6'>\n```\n<caption class=\"Table Caption\">\n\nFirst 6 rows and columns of the reduced feature co-occurrence matrix.\n\n</caption>\n```{=html}\n<thead><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-ba0b3bc4\"><p class=\"cl-ba0b1f2c\"><span class=\"cl-ba0b1644\">doc_id</span></p></td><td class=\"cl-ba0b3bbb\"><p class=\"cl-ba0b1f36\"><span class=\"cl-ba0b1644\">transport</span></p></td><td class=\"cl-ba0b3bbb\"><p class=\"cl-ba0b1f36\"><span class=\"cl-ba0b1644\">facing</span></p></td><td class=\"cl-ba0b3bbb\"><p class=\"cl-ba0b1f36\"><span class=\"cl-ba0b1644\">rail</span></p></td><td class=\"cl-ba0b3bbb\"><p class=\"cl-ba0b1f36\"><span class=\"cl-ba0b1644\">commuters</span></p></td><td class=\"cl-ba0b3bbb\"><p class=\"cl-ba0b1f36\"><span class=\"cl-ba0b1644\">use</span></p></td><td class=\"cl-ba0b3bbc\"><p class=\"cl-ba0b1f36\"><span class=\"cl-ba0b1644\">public</span></p></td></tr></thead><tbody><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-ba0b3b9c\"><p class=\"cl-ba0b1f2c\"><span class=\"cl-ba0b164e\">text1</span></p></td><td class=\"cl-ba0b3b88\"><p class=\"cl-ba0b1f36\"><span class=\"cl-ba0b164e\">1</span></p></td><td class=\"cl-ba0b3b88\"><p class=\"cl-ba0b1f36\"><span class=\"cl-ba0b164e\">0</span></p></td><td class=\"cl-ba0b3b88\"><p class=\"cl-ba0b1f36\"><span class=\"cl-ba0b164e\">0</span></p></td><td class=\"cl-ba0b3b88\"><p class=\"cl-ba0b1f36\"><span class=\"cl-ba0b164e\">0</span></p></td><td class=\"cl-ba0b3b88\"><p class=\"cl-ba0b1f36\"><span class=\"cl-ba0b164e\">0</span></p></td><td class=\"cl-ba0b3b92\"><p class=\"cl-ba0b1f36\"><span class=\"cl-ba0b164e\">0</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-ba0b3b9d\"><p class=\"cl-ba0b1f2c\"><span class=\"cl-ba0b164e\">text2</span></p></td><td class=\"cl-ba0b3ba6\"><p class=\"cl-ba0b1f36\"><span class=\"cl-ba0b164e\">1</span></p></td><td class=\"cl-ba0b3ba6\"><p class=\"cl-ba0b1f36\"><span class=\"cl-ba0b164e\">1</span></p></td><td class=\"cl-ba0b3ba6\"><p class=\"cl-ba0b1f36\"><span class=\"cl-ba0b164e\">1</span></p></td><td class=\"cl-ba0b3ba6\"><p class=\"cl-ba0b1f36\"><span class=\"cl-ba0b164e\">0</span></p></td><td class=\"cl-ba0b3ba6\"><p class=\"cl-ba0b1f36\"><span class=\"cl-ba0b164e\">0</span></p></td><td class=\"cl-ba0b3ba7\"><p class=\"cl-ba0b1f36\"><span class=\"cl-ba0b164e\">0</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-ba0b3b9c\"><p class=\"cl-ba0b1f2c\"><span class=\"cl-ba0b164e\">text3</span></p></td><td class=\"cl-ba0b3b88\"><p class=\"cl-ba0b1f36\"><span class=\"cl-ba0b164e\">1</span></p></td><td class=\"cl-ba0b3b88\"><p class=\"cl-ba0b1f36\"><span class=\"cl-ba0b164e\">0</span></p></td><td class=\"cl-ba0b3b88\"><p class=\"cl-ba0b1f36\"><span class=\"cl-ba0b164e\">0</span></p></td><td class=\"cl-ba0b3b88\"><p class=\"cl-ba0b1f36\"><span class=\"cl-ba0b164e\">1</span></p></td><td class=\"cl-ba0b3b88\"><p class=\"cl-ba0b1f36\"><span class=\"cl-ba0b164e\">0</span></p></td><td class=\"cl-ba0b3b92\"><p class=\"cl-ba0b1f36\"><span class=\"cl-ba0b164e\">0</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-ba0b3b9d\"><p class=\"cl-ba0b1f2c\"><span class=\"cl-ba0b164e\">text4</span></p></td><td class=\"cl-ba0b3ba6\"><p class=\"cl-ba0b1f36\"><span class=\"cl-ba0b164e\">0</span></p></td><td class=\"cl-ba0b3ba6\"><p class=\"cl-ba0b1f36\"><span class=\"cl-ba0b164e\">0</span></p></td><td class=\"cl-ba0b3ba6\"><p class=\"cl-ba0b1f36\"><span class=\"cl-ba0b164e\">0</span></p></td><td class=\"cl-ba0b3ba6\"><p class=\"cl-ba0b1f36\"><span class=\"cl-ba0b164e\">0</span></p></td><td class=\"cl-ba0b3ba6\"><p class=\"cl-ba0b1f36\"><span class=\"cl-ba0b164e\">0</span></p></td><td class=\"cl-ba0b3ba7\"><p class=\"cl-ba0b1f36\"><span class=\"cl-ba0b164e\">0</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-ba0b3b9c\"><p class=\"cl-ba0b1f2c\"><span class=\"cl-ba0b164e\">text5</span></p></td><td class=\"cl-ba0b3b88\"><p class=\"cl-ba0b1f36\"><span class=\"cl-ba0b164e\">1</span></p></td><td class=\"cl-ba0b3b88\"><p class=\"cl-ba0b1f36\"><span class=\"cl-ba0b164e\">0</span></p></td><td class=\"cl-ba0b3b88\"><p class=\"cl-ba0b1f36\"><span class=\"cl-ba0b164e\">0</span></p></td><td class=\"cl-ba0b3b88\"><p class=\"cl-ba0b1f36\"><span class=\"cl-ba0b164e\">1</span></p></td><td class=\"cl-ba0b3b88\"><p class=\"cl-ba0b1f36\"><span class=\"cl-ba0b164e\">1</span></p></td><td class=\"cl-ba0b3b92\"><p class=\"cl-ba0b1f36\"><span class=\"cl-ba0b164e\">1</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-ba0b3bb0\"><p class=\"cl-ba0b1f2c\"><span class=\"cl-ba0b164e\">text6</span></p></td><td class=\"cl-ba0b3ba8\"><p class=\"cl-ba0b1f36\"><span class=\"cl-ba0b164e\">1</span></p></td><td class=\"cl-ba0b3ba8\"><p class=\"cl-ba0b1f36\"><span class=\"cl-ba0b164e\">0</span></p></td><td class=\"cl-ba0b3ba8\"><p class=\"cl-ba0b1f36\"><span class=\"cl-ba0b164e\">0</span></p></td><td class=\"cl-ba0b3ba8\"><p class=\"cl-ba0b1f36\"><span class=\"cl-ba0b164e\">1</span></p></td><td class=\"cl-ba0b3ba8\"><p class=\"cl-ba0b1f36\"><span class=\"cl-ba0b164e\">0</span></p></td><td class=\"cl-ba0b3bba\"><p class=\"cl-ba0b1f36\"><span class=\"cl-ba0b164e\">1</span></p></td></tr></tbody></table></div>\n```\n:::\n:::\n\n\n\n\n\nNow, we can transform the document-feature matrix into a feature-co-occurrence matrix as shown below. A feature-co-occurrence matrix shows how often each element in that matrix co-occurs with every other element in that matrix.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntag_fcm <- fcm(redux_dfm)\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n```{=html}\n<div class=\"tabwid\"><style>.cl-ba26035a{table-layout:auto;width:50%;}.cl-ba2025ac{font-family:'DejaVu Sans';font-size:12pt;font-weight:bold;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-ba2025c0{font-family:'DejaVu Sans';font-size:12pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-ba203736{margin:0;text-align:center;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-ba20374a{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-ba206bac{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ba206bb6{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ba206bb7{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ba206bc0{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ba206bca{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ba206bcb{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ba206bd4{background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ba206bd5{background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ba206bde{background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ba206bdf{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ba206bf2{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ba206bfc{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}</style><table class='cl-ba26035a'>\n```\n<caption class=\"Table Caption\">\n\nFirst 6 rows and columns of the feature co-occurrence matrix.\n\n</caption>\n```{=html}\n<thead><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-ba206bfc\"><p class=\"cl-ba203736\"><span class=\"cl-ba2025ac\">doc_id</span></p></td><td class=\"cl-ba206bdf\"><p class=\"cl-ba20374a\"><span class=\"cl-ba2025ac\">transport</span></p></td><td class=\"cl-ba206bdf\"><p class=\"cl-ba20374a\"><span class=\"cl-ba2025ac\">facing</span></p></td><td class=\"cl-ba206bdf\"><p class=\"cl-ba20374a\"><span class=\"cl-ba2025ac\">rail</span></p></td><td class=\"cl-ba206bdf\"><p class=\"cl-ba20374a\"><span class=\"cl-ba2025ac\">commuters</span></p></td><td class=\"cl-ba206bdf\"><p class=\"cl-ba20374a\"><span class=\"cl-ba2025ac\">use</span></p></td><td class=\"cl-ba206bf2\"><p class=\"cl-ba20374a\"><span class=\"cl-ba2025ac\">public</span></p></td></tr></thead><tbody><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-ba206bb7\"><p class=\"cl-ba203736\"><span class=\"cl-ba2025c0\">transport</span></p></td><td class=\"cl-ba206bac\"><p class=\"cl-ba20374a\"><span class=\"cl-ba2025c0\">3</span></p></td><td class=\"cl-ba206bac\"><p class=\"cl-ba20374a\"><span class=\"cl-ba2025c0\">4</span></p></td><td class=\"cl-ba206bac\"><p class=\"cl-ba20374a\"><span class=\"cl-ba2025c0\">17</span></p></td><td class=\"cl-ba206bac\"><p class=\"cl-ba20374a\"><span class=\"cl-ba2025c0\">4</span></p></td><td class=\"cl-ba206bac\"><p class=\"cl-ba20374a\"><span class=\"cl-ba2025c0\">18</span></p></td><td class=\"cl-ba206bb6\"><p class=\"cl-ba20374a\"><span class=\"cl-ba2025c0\">38</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-ba206bc0\"><p class=\"cl-ba203736\"><span class=\"cl-ba2025c0\">facing</span></p></td><td class=\"cl-ba206bca\"><p class=\"cl-ba20374a\"><span class=\"cl-ba2025c0\">0</span></p></td><td class=\"cl-ba206bca\"><p class=\"cl-ba20374a\"><span class=\"cl-ba2025c0\">0</span></p></td><td class=\"cl-ba206bca\"><p class=\"cl-ba20374a\"><span class=\"cl-ba2025c0\">2</span></p></td><td class=\"cl-ba206bca\"><p class=\"cl-ba20374a\"><span class=\"cl-ba2025c0\">0</span></p></td><td class=\"cl-ba206bca\"><p class=\"cl-ba20374a\"><span class=\"cl-ba2025c0\">0</span></p></td><td class=\"cl-ba206bcb\"><p class=\"cl-ba20374a\"><span class=\"cl-ba2025c0\">0</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-ba206bb7\"><p class=\"cl-ba203736\"><span class=\"cl-ba2025c0\">rail</span></p></td><td class=\"cl-ba206bac\"><p class=\"cl-ba20374a\"><span class=\"cl-ba2025c0\">0</span></p></td><td class=\"cl-ba206bac\"><p class=\"cl-ba20374a\"><span class=\"cl-ba2025c0\">0</span></p></td><td class=\"cl-ba206bac\"><p class=\"cl-ba20374a\"><span class=\"cl-ba2025c0\">5</span></p></td><td class=\"cl-ba206bac\"><p class=\"cl-ba20374a\"><span class=\"cl-ba2025c0\">1</span></p></td><td class=\"cl-ba206bac\"><p class=\"cl-ba20374a\"><span class=\"cl-ba2025c0\">4</span></p></td><td class=\"cl-ba206bb6\"><p class=\"cl-ba20374a\"><span class=\"cl-ba2025c0\">2</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-ba206bc0\"><p class=\"cl-ba203736\"><span class=\"cl-ba2025c0\">commuters</span></p></td><td class=\"cl-ba206bca\"><p class=\"cl-ba20374a\"><span class=\"cl-ba2025c0\">0</span></p></td><td class=\"cl-ba206bca\"><p class=\"cl-ba20374a\"><span class=\"cl-ba2025c0\">0</span></p></td><td class=\"cl-ba206bca\"><p class=\"cl-ba20374a\"><span class=\"cl-ba2025c0\">0</span></p></td><td class=\"cl-ba206bca\"><p class=\"cl-ba20374a\"><span class=\"cl-ba2025c0\">0</span></p></td><td class=\"cl-ba206bca\"><p class=\"cl-ba20374a\"><span class=\"cl-ba2025c0\">1</span></p></td><td class=\"cl-ba206bcb\"><p class=\"cl-ba20374a\"><span class=\"cl-ba2025c0\">2</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-ba206bb7\"><p class=\"cl-ba203736\"><span class=\"cl-ba2025c0\">use</span></p></td><td class=\"cl-ba206bac\"><p class=\"cl-ba20374a\"><span class=\"cl-ba2025c0\">0</span></p></td><td class=\"cl-ba206bac\"><p class=\"cl-ba20374a\"><span class=\"cl-ba2025c0\">0</span></p></td><td class=\"cl-ba206bac\"><p class=\"cl-ba20374a\"><span class=\"cl-ba2025c0\">0</span></p></td><td class=\"cl-ba206bac\"><p class=\"cl-ba20374a\"><span class=\"cl-ba2025c0\">0</span></p></td><td class=\"cl-ba206bac\"><p class=\"cl-ba20374a\"><span class=\"cl-ba2025c0\">0</span></p></td><td class=\"cl-ba206bb6\"><p class=\"cl-ba20374a\"><span class=\"cl-ba2025c0\">16</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-ba206bd5\"><p class=\"cl-ba203736\"><span class=\"cl-ba2025c0\">public</span></p></td><td class=\"cl-ba206bd4\"><p class=\"cl-ba20374a\"><span class=\"cl-ba2025c0\">0</span></p></td><td class=\"cl-ba206bd4\"><p class=\"cl-ba20374a\"><span class=\"cl-ba2025c0\">0</span></p></td><td class=\"cl-ba206bd4\"><p class=\"cl-ba20374a\"><span class=\"cl-ba2025c0\">0</span></p></td><td class=\"cl-ba206bd4\"><p class=\"cl-ba20374a\"><span class=\"cl-ba2025c0\">0</span></p></td><td class=\"cl-ba206bd4\"><p class=\"cl-ba20374a\"><span class=\"cl-ba2025c0\">0</span></p></td><td class=\"cl-ba206bde\"><p class=\"cl-ba20374a\"><span class=\"cl-ba2025c0\">1</span></p></td></tr></tbody></table></div>\n```\n:::\n:::\n\n\n\n\nUsing the feature-co-occurrence matrix, we can generate the network graph which shows the terms that collocate with the target term *transport* with the edges representing the co-occurrence frequency. To generate this network graph, we use the `textplot_network` function from the `quanteda.textplots` package.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# generate network graph\nquanteda.textplots::textplot_network(tag_fcm, \n                                     min_freq = 1, \n                                     edge_alpha = 0.3, \n                                     edge_size = 5,\n                                     edge_color = \"gray80\",\n                                     vertex_labelsize = log(rowSums(tag_fcm)*15))\n```\n\n::: {.cell-output-display}\n![](llr_files/figure-html/dfm8-1.png){width=672}\n:::\n:::\n\n\n\n# Part-of-speech tagging{-}\n\nPart-of-speech tagging is a very useful procedure for many analyses. Here, we automatically identify parts of speech (word classes) in the text which, for a well-studied language like English, is approximately 95% accurate.\n\nThe code chunk below defines a function which applies this kind of tagging to any text fed into the function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nPOStag <- function(x){\n  # load necessary packages\n  require(\"stringr\")\n  require(\"NLP\")\n  require(\"openNLP\")\n  # define annotators\n  sent_token_annotator <- openNLP::Maxent_Sent_Token_Annotator()\n  word_token_annotator <- openNLP::Maxent_Word_Token_Annotator()\n  pos_tag_annotator <- openNLP::Maxent_POS_Tag_Annotator(language = \"en\", probs = FALSE)\n  # convert all file content to strings\n  strings <- lapply(x, function(x){\n    x <- as.String(x)  })\n  # loop over file contents\n  sapply(strings, function(x){\n    a <- NLP::annotate(x, list(sent_token_annotator, word_token_annotator))\n    p <- NLP::annotate(x, pos_tag_annotator, a)\n    w <- subset(p, type == \"word\")\n    tags <- sapply(w$features, '[[', \"POS\")\n    as <- sprintf(\"%s/%s\", x[w], tags)\n    at <- paste(as, collapse = \" \")\n    return(at)  \n    })\n  }\n```\n:::\n\n\nWe now apply this function to a test sentence to see if the function does what we want it to and to check the output format.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# generate test text\ntext <- \"It is now a very wide spread opinion, that in the modern world there is no place for dreaming and imagination.\"\n# apply pos-tag function to test text\ntagged_text <- POStag(text)\n# inspect result\ntagged_text\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"It/PRP is/VBZ now/RB a/DT very/RB wide/JJ spread/NN opinion/NN ,/, that/IN in/IN the/DT modern/JJ world/NN there/EX is/VBZ no/DT place/NN for/IN dreaming/VBG and/CC imagination/NN ./.\"\n```\n:::\n:::\n\n\nThe tags which you see here are from the tag set developed for the *Penn Treebank*, a corpus of English text with syntactic annotations. The tags are not always transparent, and this is very much the case for the word class we will be looking at - the tag for an adjective is `/JJ`!\n\nThe next step, we write a function that will clean our texts by removing tags and quotation marks as well as superfluous white spaces.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncomText <- function(x,...){\n  # paste text together\n  x <- paste0(x)\n  # remove file identifiers\n  x <- stringr::str_remove_all(x, \"<.*?>\")\n  # remove quotation marks\n  x <- stringr::str_remove_all(x, fixed(\"\\\"\"))\n  # remove superfluous white spaces\n  x <- stringr::str_squish(x)\n  # remove empty elements\n  x <- x[!x==\"\"]\n}\n```\n:::\n\n\n\nNow we apply the text cleaning function to the texts.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# combine texts\nns1_com <- comText(ns1_sen)\nns2_com <- comText(ns2_sen)\nde_com <- comText(de_sen)\nes_com <- comText(es_sen)\nfr_com <- comText(fr_sen)\nit_com <- comText(it_sen)\npl_com <- comText(pl_sen)\nru_com <- comText(ru_sen)\n```\n:::\n\n\n\nNow we apply the pos-tagging function to the texts.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# apply pos-tag function to data\nns1_pos <- as.vector(unlist(POStag(ns1_com)))\nns2_pos <- as.vector(unlist(POStag(ns2_com)))\nde_pos <- as.vector(unlist(POStag(de_com)))\nes_pos <- as.vector(unlist(POStag(es_com)))\nfr_pos <- as.vector(unlist(POStag(fr_com)))\nit_pos <- as.vector(unlist(POStag(it_com)))\npl_pos <- as.vector(unlist(POStag(pl_com)))\nru_pos <- as.vector(unlist(POStag(ru_com)))\n# inspect\nhead(ns1_pos)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Transport/NNP 01/CD\"                                                                                                                                                                                                                                                                                                         \n[2] \"The/DT basic/JJ dilema/NN facing/VBG the/DT UK/NNP 's/POS rail/NN and/CC road/NN transport/NN system/NN is/VBZ the/DT general/JJ rise/NN in/IN population/NN ./.\"                                                                                                                                                            \n[3] \"This/DT leads/VBZ to/TO an/DT increase/NN in/IN the/DT number/NN of/IN commuters/NNS and/CC transport/NN users/NNS every/DT year/NN ,/, consequently/RB putting/VBG pressure/NN on/IN the/DT UKs/NNP transports/VBZ network/NN ./.\"                                                                                          \n[4] \"The/DT biggest/JJS worry/NN to/TO the/DT system/NN is/VBZ the/DT rapid/JJ rise/NN of/IN car/NN users/NNS outside/IN the/DT major/JJ cities/NNS ./.\"                                                                                                                                                                          \n[5] \"Most/JJS large/JJ cities/NNS have/VBP managed/VBN to/TO incourage/VB commuters/NNS to/TO use/VB public/JJ transport/NN thus/RB decreasing/VBG major/JJ conjestion/NN in/IN Rush/NNP hour/NN periods/NNS ./.\"                                                                                                                 \n[6] \"Public/NNP transport/NN is/VBZ the/DT obvious/JJ solution/NN to/TO to/TO the/DT increase/NN in/IN population/NN if/IN it/PRP is/VBZ made/VBN cheep/NN to/TO commuters/NNS ,/, clean/JJ ,/, easy/JJ and/CC efficient/JJ then/RB it/PRP could/MD take/VB the/DT strain/NN of/IN the/DT overloaded/VBN British/JJ roads/NNS ./.\"\n```\n:::\n:::\n\n\nWe end up with pos-tagged texts where the pos-tags are added to each word (or symbol).\n\nIn the following section, we will use these pos-tags to identify potential differences between learners and L1-speakers of English.\n\n# Differences in pos-sequences{-}\n\nTo analyze differences in part-of-speech sequences between L1-speakers and learners of English,, we write a function that extracts pos-tag bigrams from the tagged texts. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# tokenize and extract pos tags\nposngram <- function(x,...){\n  x <- x %>%\n  stringr::str_remove_all(\"\\\\w*/\") %>%\n  quanteda::tokens(remove_punct = TRUE)  %>%\n    quanteda::tokens_ngrams(n = 2) %>%\n    stringr::str_remove_all(\"-\")\n  return(x)\n}\n```\n:::\n\n\n\nWe now apply the function to the pos-tagged texts.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# apply pos-tag function to data\nns1_posng <- as.vector(unlist(posngram(ns1_pos)))\nns2_posng <- as.vector(unlist(posngram(ns2_pos)))\nde_posng <- as.vector(unlist(posngram(de_pos)))\nes_posng <- as.vector(unlist(posngram(es_pos)))\nfr_posng <- as.vector(unlist(posngram(fr_pos)))\nit_posng <- as.vector(unlist(posngram(it_pos)))\npl_posng <- as.vector(unlist(posngram(pl_pos)))\nru_posng <- as.vector(unlist(posngram(ru_pos)))\n# inspect\nhead(ns1_posng)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"NNP_CD\" \"DT_JJ\"  \"JJ_NN\"  \"NN_VBG\" \"VBG_DT\" \"DT_NNP\"\n```\n:::\n:::\n\n\nIn a next step, we tabulate the results and add a column telling us about the L1 background of the speakers who have produced the texts.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nposngram_df <- c(ns1_posng, ns2_posng, de_posng, es_posng, fr_posng, \n                 it_posng, pl_posng, ru_posng) %>%\n  as.data.frame() %>%\n  # rename column\n  dplyr::rename(ngram = 1) %>%\n  # add l1\n  dplyr::mutate(l1 = c(rep(\"en\", length(ns1_posng)),\n                       rep(\"en\", length(ns2_posng)),\n                       rep(\"de\", length(de_posng)),\n                       rep(\"es\", length(es_posng)),\n                       rep(\"fr\", length(fr_posng)),\n                       rep(\"it\", length(it_posng)),\n                       rep(\"pl\", length(pl_posng)),\n                       rep(\"ru\", length(ru_posng))),\n                # add learner column\n                learner = ifelse(l1 == \"en\", \"no\", \"yes\")) %>%\n  # extract frequencies of ngrams\n  dplyr::group_by(ngram, learner) %>%\n  dplyr::summarise(freq = n()) %>%\n  dplyr::arrange(-freq)\n# inspect\nhead(posngram_df)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 √ó 3\n# Groups:   ngram [6]\n  ngram learner  freq\n  <chr> <chr>   <int>\n1 DT_NN no        495\n2 IN_DT no        418\n3 NN_IN no        417\n4 JJ_NN no        314\n5 DT_JJ no        243\n6 TO_VB no        237\n```\n:::\n:::\n\n\nNext, we transform the table and add all the information that we need to perform the Fisher's exact tests that we will use to determine if there are significant differences between L1 speakers and learners of English regarding their use of pos-sequences.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nposngram_df2 <- posngram_df %>%\n  tidyr::spread(learner, freq) %>%\n  dplyr::mutate(no = ifelse(is.na(no), 0, no),\n                yes = ifelse(is.na(yes), 0, yes)) %>%\n  dplyr::rename(l1speaker = no, \n                learner = yes) %>%\n  dplyr::mutate(total_ngram = l1speaker+learner) %>%\n  dplyr::ungroup() %>%\n  dplyr::mutate(total_learner = sum(learner),\n              total_l1 = sum(l1speaker)) %>%\n  dplyr::mutate(a = l1speaker,\n                b = learner) %>%\n  dplyr::mutate(c = total_l1-a,\n                d = total_learner-b)\n# inspect\nhead(posngram_df2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 √ó 10\n  ngram   l1speaker learner total_ngram total_learner total_l1     a     b     c\n  <chr>       <dbl>   <dbl>       <dbl>         <dbl>    <dbl> <dbl> <dbl> <dbl>\n1 `_RB            0       1           1          3518     9621     0     1  9621\n2 +_JJ            1       0           1          3518     9621     1     0  9620\n3 +_RB            1       0           1          3518     9621     1     0  9620\n4 <_NNP           0       1           1          3518     9621     0     1  9621\n5 $_CC            0       1           1          3518     9621     0     1  9621\n6 $_girl‚Ä¶         0       1           1          3518     9621     0     1  9621\n# ‚Ä¶ with 1 more variable: d <dbl>\n```\n:::\n:::\n\n\nOn this re-arranged data set, we can now apply the Fisher's exact tests. As we are performing many different tests, we need to correct for multiple comparisons. To this end, we create a column which holds the Bonferroni corrected critical value (\\alpha .05). If a p-value is lower than the corrected critical value, then the learners and L1-speakers differ significantly in their use of that n-gram.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsdif_posngram <- posngram_df2  %>%\n  # perform fishers exact test and extract estimate and p\n  dplyr::rowwise() %>%\n  dplyr::mutate(fisher_p = fisher.test(matrix(c(a,c,b,d), nrow= 2))$p.value,\n                oddsratio = fisher.test(matrix(c(a,c,b,d), nrow= 2))$estimate,\n                # calculate bonferroni correction\n                crit = .05/nrow(.),\n                sig_corr = ifelse(fisher_p < crit, \"p<.05\", \"n.s.\")) %>%\n  dplyr::arrange(fisher_p) %>%\n  dplyr::select(-total_ngram, -a, -b, -c, -d, -crit)\n# inspect\nhead(sdif_posngram)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 √ó 8\n# Rowwise: \n  ngram   l1speaker learner total_learner total_l1   fisher_p oddsratio sig_corr\n  <chr>       <dbl>   <dbl>         <dbl>    <dbl>      <dbl>     <dbl> <chr>   \n1 PRP_VBZ        43      55          3518     9621    1.13e-9     0.283 p<.05   \n2 NN_NNS        121      10          3518     9621    4.43e-8     4.47  p<.05   \n3 $_NN           32      40          3518     9621    2.82e-7     0.290 p<.05   \n4 IN_PRP         93      72          3518     9621    3.15e-6     0.467 p<.05   \n5 PRP_$          81      61          3518     9621    3.38e-5     0.481 p<.05   \n6 JJR_NNS        39       1          3518     9621    9.89e-5    14.3   n.s.    \n```\n:::\n:::\n\n\n\nWe can now check and compare the use of the the pos-tagged sequences that differ significantly between learners and L1 speakers of English using simple concordancing. We begin by checking the use in the L1-data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# combine l1 data\nl1_pos <- c(ns1_pos, ns2_pos)\n# combine l2 data\nl2_pos <- c(de_pos, es_pos, fr_pos, it_pos, pl_pos, ru_pos)\n# extract PRP_VBZ\nPRP_VBZ_l1 <-quanteda::kwic(quanteda::tokens(l1_pos), \n                            pattern = phrase(\"\\\\w* / PRP \\\\w* / VBZ\"), \n                            valuetype = \"regex\",\n                            window = 10) %>%\n  as.data.frame() %>%\n  # remove superfluous columns\n  dplyr::select(-from, -to, -docname, -pattern)\n# inspect results\nhead(PRP_VBZ_l1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                                     pre                keyword\n1     NN in / IN population / NN if / IN      it / PRP is / VBZ\n2  DT concrete / JJ jungle / NN yet / CC      it / PRP is / VBZ\n3 NN centres / NNS during / IN rush / NN ours / PRP comes / VBZ\n4                                        It / PRP removes / VBZ\n5                                            It / PRP has / VBZ\n6                                            It / PRP has / VBZ\n                                        post\n1    made / VBN cheep / NN to / TO commuters\n2        only / RB trying / VBG to / TO cope\n3        to / TO a / DT near / JJ standstill\n4 the / DT element / NN of / IN independence\n5      given / VBN us / PRP the / DT freedom\n6    reached / VBN the / DT stage / NN where\n```\n:::\n:::\n\n\nWe now turn to the learner data and also extract concordances for the same pos-sequence.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# extract PRP_VBZ\nPRP_VBZ_l2 <-quanteda::kwic(quanteda::tokens(l2_pos), \n                            pattern = phrase(\"\\\\w* / PRP \\\\w* / VBZ\"), \n                            valuetype = \"regex\", \n                            window = 10) %>%\n  as.data.frame() %>%\n  # remove superfluous columns\n  dplyr::select(-from, -to, -docname, -pattern)\n# inspect results\nhead(PRP_VBZ_l2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                                      pre               keyword\n1       NN why / WRB I / PRP admire / VBP    her / PRP is / VBZ\n2        WRB you / PRP look / VBP at / IN    her / PRP is / VBZ\n3                                           She / PRP has / VBZ\n4 $ shoulders / NNS and / CC usually / RB she / PRP wears / VBZ\n5                                            She / PRP is / VBZ\n6      IN skirts / NNS , / , because / IN she / PRP hates / VBZ\n                                        post\n1              her / PRP $ beauty / NN . / .\n2           really / RB brilliant / JJ . / .\n3 glistening / VBG dark / JJ brown / JJ hair\n4             a / DT hair / NN slide / NN or\n5      always / RB well / RB dressed / VBN ,\n6 wearing / VBG skirts / NNS and / CC tights\n```\n:::\n:::\n\n\n\n# Lexical diversity{-}\n\nAnother common measure used to asses the development of language learns is vocabulary size. Vocabulary size can be assessed with various measures that represent lexical diversity. In the present case, we will extract\n\n* `TTR`: *type-token ratio*\n* `C`: Herdan's C (see @tweedie1988lexdiv; sometimes referred to as LogTTR)\n* `R`: Guiraud's Root TTR (see @tweedie1988lexdiv)\n* `CTTR`: Carroll's Corrected TTR \n* `U`: Dugast's Uber Index (see @tweedie1988lexdiv)\n* `S`: Summer's index\n* `Maas`: Maas' indices\n\nThe formulas showing how the lexical diversity measures are calculated as well as additional information about the lexical diversity measures can be found [here](https://quanteda.io/reference/textstat_lexdiv.html).\n\nWhile we will extract all of these scores, we will only visualize Carroll's Corrected TTR to keep things simple. \n\n\\begin{equation}\n  CTTR =  \\frac{N_{Types}}{\\sqrt{2 N_{Tokens}}}\n\\end{equation}\n\nHowever, before we extract the lexical diversity measures, we split the data into individual essays.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncleanEss <- function(x){\n  x %>%\n  paste0(collapse = \" \") %>%\n  stringr::str_split(\"Transport [0-9]{1,2}\") %>%\n  unlist() %>%\n  stringr::str_squish() %>%\n  .[. != \"\"]\n}\n# apply function\nns1_ess <- cleanEss(ns1)\nns2_ess <- cleanEss(ns2)\nde_ess <- cleanEss(de)\nes_ess <- cleanEss(es)\nfr_ess <- cleanEss(fr)\nit_ess <- cleanEss(it)\npl_ess <- cleanEss(pl)\nru_ess <- cleanEss(ru)\n# inspect\nhead(ns1_ess, 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"The basic dilema facing the UK's rail and road transport system is the general rise in population. This leads to an increase in the number of commuters and transport users every year, consequently putting pressure on the UKs transports network. The biggest worry to the system is the rapid rise of car users outside the major cities. Most large cities have managed to incourage commuters to use public transport thus decreasing major conjestion in Rush hour periods. Public transport is the obvious solution to to the increase in population if it is made cheep to commuters, clean, easy and efficient then it could take the strain of the overloaded British roads. For commuters who regularly travel long distances rail transport should be made more appealing, more comfortable and cheaper. Motorways and other transport links are constantly being extended, widened and slowly turning the country into a concrete jungle yet it is only trying to cope with the increase in traffic, we are our own enemy! Another major problem created by the mass of vehicle transport is the pollution emitted into the atmosphere damaging the ozone layer, creating smog and forming acid rain. Tourturing the Earth we are living on. In concluding I wish to propose clean, efficient comfortable and cheap public transport for the near future.\"\n```\n:::\n:::\n\n\nIn a next step, we can apply the `lex.div` function from the `koRpus` package which calculates the different lexical diversity measures for us.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# extract lex. div. measures\nns1_lds <- lapply(ns1_ess, function(x){\n  x <- koRpus::lex.div(x, force.lang = 'en', # define language \n                       segment = 20,      # define segment width\n                       window = 20,       # define window width\n                       quiet = T,\n                       # define lex div measures\n                       measure=c(\"TTR\", \"C\", \"R\", \"CTTR\", \"U\", \"Maas\"),\n                       char=c(\"TTR\", \"C\", \"R\", \"CTTR\",\"U\", \"Maas\"))\n})\n# inspect\nns1_lds[1]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[1]]\n\nTotal number of tokens: 217 \nTotal number of types:  134\n\nType-Token Ratio\n               TTR: 0.62 \n\nTTR characteristics:\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.6140  0.6322  0.6429  0.6888  0.7129  0.9000 \n   SD\n 0.0852\n\n\nHerdan's C\n                 C: 0.91 \n\nC characteristics:\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.8614  0.9059  0.9131  0.9157  0.9164  0.9648 \n   SD\n 0.0186\n\n\nGuiraud's R\n                 R: 9.1 \n\nR characteristics:\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.789   5.459   6.484   6.596   8.158   9.080 \n   SD\n 1.8316\n\n\nCarroll's CTTR\n              CTTR: 6.43 \n\nCTTR characteristics:\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.265   3.860   4.585   4.664   5.769   6.420 \n   SD\n 1.2951\n\n\nUber Index\n                 U: 26.08 \n\nU characteristics:\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  5.041  21.104  22.588  23.564  26.226  36.992 \n   SD\n 4.5831\n\n\nMaas' Indices\n                 a: 0.2 \n              lgV0: 5.14 \n             lgeV0: 11.84 \n\nRelative vocabulary growth (first half to full text)\n                 a: 0 \n              lgV0: 1.83 \n                V': 0 (0 new types every 100 tokens)\n\nMaas Indices characteristics:\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.1644  0.1953  0.2104  0.2112  0.2177  0.4454 \n   SD\n 0.0392\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.185   4.106   4.506   4.428   4.955   5.223 \n   SD\n 0.7145\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  2.729   9.455  10.376  10.196  11.410  12.026 \n   SD\n 1.6452\n```\n:::\n:::\n\n\nWe now go ahead and extract the lexical diversity scores for the other essays.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlexDiv <- function(x){\n  lapply(x, function(y){\n    koRpus::lex.div(y, force.lang = 'en',  segment = 20, window = 20,  \n                    quiet = T, measure=c(\"TTR\", \"C\", \"R\", \"CTTR\", \"U\", \"Maas\"),\n                    char=c(\"TTR\", \"C\", \"R\", \"CTTR\",\"U\", \"Maas\"))\n  })\n}\n\n# extract lex. div. measures\nns2_lds <- lexDiv(ns2_ess)\nde_lds <- lexDiv(de_ess)\nes_lds <- lexDiv(es_ess)\nfr_lds <- lexDiv(fr_ess)\nit_lds <- lexDiv(it_ess)\npl_lds <- lexDiv(pl_ess)\nru_lds <- lexDiv(ru_ess)\n```\n:::\n\n\nIn a next step, we extract the CTTR values from L1-speakers and learners and put the results into a table.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncttr <- data.frame(c(as.vector(sapply(ns1_lds, '[', \"CTTR\")), \n                     as.vector(sapply(ns2_lds, '[', \"CTTR\")), \n                     as.vector(sapply(de_lds, '[', \"CTTR\")), \n                     as.vector(sapply(es_lds, '[', \"CTTR\")),\n                     as.vector(sapply(fr_lds, '[', \"CTTR\")), \n                     as.vector(sapply(it_lds, '[', \"CTTR\")), \n                     as.vector(sapply(pl_lds, '[', \"CTTR\")), \n                     as.vector(sapply(ru_lds, '[', \"CTTR\"))),\n          c(rep(\"en\", length(as.vector(sapply(ns1_lds, '[', \"CTTR\")))),\n            rep(\"en\", length(as.vector(sapply(ns2_lds, '[', \"CTTR\")))),\n            rep(\"de\", length(as.vector(sapply(de_lds, '[', \"CTTR\")))),\n            rep(\"es\", length(as.vector(sapply(es_lds, '[', \"CTTR\")))),\n            rep(\"fr\", length(as.vector(sapply(fr_lds, '[', \"CTTR\")))),\n            rep(\"it\", length(as.vector(sapply(it_lds, '[', \"CTTR\")))),\n            rep(\"pl\", length(as.vector(sapply(pl_lds, '[', \"CTTR\")))),\n            rep(\"ru\", length(as.vector(sapply(ru_lds, '[', \"CTTR\")))))) %>%\n  dplyr::rename(CTTR = 1,\n                l1 = 2)\n# inspect\nhead(cttr)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  CTTR l1\n1 6.43 en\n2 8.84 en\n3 8.20 en\n4 8.34 en\n5 7.34 en\n6 8.78 en\n```\n:::\n:::\n\n\nWe can now visualize the information in the table in the form of a dot plot to inspect potential differences with respect to the L1-background of speakers.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncttr %>%\n  dplyr::group_by(l1) %>%\n  dplyr::summarise(CTTR = mean(CTTR)) %>%\n  ggplot(aes(x = reorder(l1, CTTR, mean), y = CTTR)) +\n  geom_point() +\n  # adapt y-axis labels\n  labs(y = \"Lexical diversity (CTTR)\") +\n  # adapt tick labels\n  scale_x_discrete(\"L1 of learners\", \n                   breaks = names(table(cttr$l1)), \n                   labels = c(\"en\" = \"English\",\n                              \"de\" = \"German\",\n                              \"es\" = \"Spanish\",\n                              \"fr\" = \"French\",\n                              \"it\" = \"Italian\",\n                              \"pl\" = \"Polish\",\n                              \"ru\" = \"Russian\")) +\n  theme_bw() +\n  coord_cartesian(ylim = c(0, 15)) +\n  theme(legend.position = \"none\")\n```\n\n::: {.cell-output-display}\n![](llr_files/figure-html/ld7-1.png){width=672}\n:::\n:::\n\n\n# Readability{-}\n\nAnother measure to assess text quality or text complexity is *readability*. As with lexical diversity scores, the `textstat_readability` function from the `quanteda.textstats` package provides a multitude of different measures (see [here](https://quanteda.io/reference/textstat_readability.html) for the entire list of readability scores that can be extracted). In the following, we will focus on Flesch's Reading Ease Score exclusively [cf. @flesch1948new] (see below; ALS = average sentence length).\n\n\\begin{equation}\n  Flesch =  206.835‚àí(1.015 ASL)‚àí(84.6 \\frac{N_{Syllables}}{N_{Words}})\n\\end{equation}\n\nIn a first step, we extract the Flesch scores by applying the `textstat_readability` to the essays.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nns1_read <- quanteda.textstats::textstat_readability(ns1_ess)\nns2_read <- quanteda.textstats::textstat_readability(ns2_ess)\nde_read <- quanteda.textstats::textstat_readability(de_ess)\nes_read <- quanteda.textstats::textstat_readability(es_ess)\nfr_read <- quanteda.textstats::textstat_readability(fr_ess)\nit_read <- quanteda.textstats::textstat_readability(it_ess)\npl_read <- quanteda.textstats::textstat_readability(pl_ess)\nru_read <- quanteda.textstats::textstat_readability(ru_ess)\n# inspect\nns1_read\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   document   Flesch\n1     text1 43.12767\n2     text2 62.34563\n3     text3 63.16179\n4     text4 62.90455\n5     text5 53.53250\n6     text6 56.92020\n7     text7 53.89138\n8     text8 59.28742\n9     text9 62.26228\n10   text10 53.60807\n11   text11 58.24022\n12   text12 58.36792\n13   text13 55.85388\n14   text14 48.55222\n15   text15 55.41899\n16   text16 62.98538\n```\n:::\n:::\n\n\nNow, we generate a table with the results and the L1 of the speaker that produced the essay.\n \n\n::: {.cell}\n\n```{.r .cell-code}\nl1 <- c(rep(\"en\", nrow(ns1_read)), rep(\"en\", nrow(ns2_read)),\n        \"de\", \"es\", \"fr\", \"it\", \"pl\", \"ru\")\nread_l1 <- base::rbind(ns1_read, ns2_read, de_read, es_read, \n                    fr_read, it_read, pl_read, ru_read)\nread_l1 <- cbind(read_l1, l1) %>%\n  as.data.frame() %>%\n  dplyr::mutate(l1 = factor(l1, level = c(\"en\", \"de\", \"es\", \"fr\", \"it\", \"pl\", \"ru\"))) %>%\n  dplyr::group_by(l1) %>%\n  dplyr::summarise(Flesch = mean(Flesch))\n# inspect\nread_l1\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 7 √ó 2\n  l1    Flesch\n  <fct>  <dbl>\n1 en      56.7\n2 de      65.2\n3 es      57.6\n4 fr      66.4\n5 it      55.4\n6 pl      62.5\n7 ru      43.8\n```\n:::\n:::\n\n\nAs before, we can visualize the results to check for potential differences between L1-speakers and learners of English. In this case, we use bar charts to visualize the results.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nread_l1 %>%\n  ggplot(aes(x = l1, y = Flesch, label = round(Flesch, 1))) +\n  geom_bar(stat = \"identity\") +\n  geom_text(vjust=1.6, color = \"white\")+\n  # adapt tick labels\n  scale_x_discrete(\"L1 of learners\", \n                   breaks = names(table(read_l1$l1)), \n                   labels = c(\"en\" = \"English\",\n                              \"de\" = \"German\",\n                              \"es\" = \"Spanish\",\n                              \"fr\" = \"French\",\n                              \"it\" = \"Italian\",\n                              \"pl\" = \"Polish\",\n                              \"ru\" = \"Russian\")) +\n  theme_bw() +\n  coord_cartesian(ylim = c(0, 75)) +\n  theme(legend.position = \"none\")\n```\n\n::: {.cell-output-display}\n![](llr_files/figure-html/rd4-1.png){width=672}\n:::\n:::\n\n \n# Spelling errors{-}\n\nWe can also determine the number of spelling errors in L1 and learner texts by checking if words in a given text occur in a dictionary or not. To do this, we can use the `hunspell` function from the `hunspell` package. We can choose between different dictionaries (use `list_dictionaries()` to see which dictionaries are available) and we can specify words to ignore via the `ignore` argument.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# list words that are not in dict\nhunspell(ns1_ess, \n         format = c(\"text\"),\n         dict = dictionary(\"en_GB\"),\n         ignore = en_stats) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[1]]\n[1] \"dilema\"     \"UKs\"        \"incourage\"  \"conjestion\" \"Tourturing\"\n\n[[2]]\n[1] \"appealling\"\n\n[[3]]\n[1] \"dependance\" \"recieve\"    \"travell\"   \n\n[[4]]\n[1] \"ie\"          \"Improvent\"   \"maintanence\" \"theier\"      \"airplanes\"  \n[6] \"buisness\"    \"thier\"       \"ie\"          \"etc\"        \n\n[[5]]\n[1] \"tendancy\"     \"etc\"          \"HGV's\"        \"Eurotunnel\"   \"Eurotunnel's\"\n\n[[6]]\n[1] \"indaquacies\"      \"croweded\"         \"accomadating\"     \"roadsystem\"      \n[5] \"enviromentalists\" \"undergrouth\"      \"enviromentalists\" \"exponnentionally\"\n\n[[7]]\n[1] \"taffic\"    \"taffic\"    \"percieved\"\n\n[[8]]\n[1] \"notorously\" \"gars\"      \n\n[[9]]\n[1] \"seperate\"    \"secondy\"     \"Dwyford\"     \"disastorous\" \"railtrak\"   \n[6] \"anymore\"     \"loocally\"    \"offes\"      \n\n[[10]]\n[1] \"apparant\" \"persuede\" \"detere\"   \"overal\"  \n\n[[11]]\n[1] \"Tarmat\"\n\n[[12]]\n [1] \"Britains\"      \"streches\"      \"improoved\"     \"ammount\"      \n [5] \"soloution\"     \"privitisation\" \"bos\"           \"soloution\"    \n [9] \"improove\"      \"liase\"        \n\n[[13]]\n[1] \"abducters\" \"Bulger\"    \"enourmos\"  \"tyed\"      \"Britains\"  \"useage\"   \n[7] \"busses\"    \"useage\"   \n\n[[14]]\n [1] \"ment\"         \"accross\"      \"harmfull\"     \"byproducts\"   \"disel\"       \n [6] \"traveling\"    \"likelyhood\"   \"adverage\"     \"collegue\"     \"effectivly\"  \n[11] \"controll\"     \"incrasing\"    \"restablished\" \"councills\"   \n\n[[15]]\n[1] \"susstacial\" \"cataylitic\" \"alot\"       \"cataylitic\"\n\n[[16]]\n [1] \"ourselfs\"     \"ameander\"     \"illistrate\"   \"likly\"        \"firsty\"      \n [6] \"mannor\"       \"greeny\"       \"tipee's\"      \"somthing\"     \"thent\"       \n[11] \"westeren\"     \"beause\"       \"earilene\"     \"shorly\"       \"disasterous\" \n[16] \"nd\"           \"promblem\"     \"spiraling\"    \"intensifyed\"  \"privite\"     \n[21] \"companys\"     \"priviously\"   \"subsudised\"   \"privite\"      \"Unfortunatly\"\n[26] \"appethetic\"  \n```\n:::\n:::\n\n\nWe can check how many spelling mistakes and words are in a text as shown below.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nns1_nerr <- hunspell(ns1_ess, dict = dictionary(\"en_GB\")) %>%\n  unlist() %>%\n  length()\nns1_nw <- sum(tokenizers::count_words(ns1_ess))\n# inspect\nns1_nerr; ns1_nw\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 111\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 8499\n```\n:::\n:::\n\n\n\nTo check if L1 speakers and learners differ regrading the likelihood of making spelling errors, we apply the `hunspell` function to all texts and also extract the number of words for each text.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# ns1\nns1_nerr <- hunspell(ns1_ess, dict = dictionary(\"en_GB\")) %>%  unlist() %>% length()\nns1_nw <- sum(tokenizers::count_words(ns1_ess))\n# ns2\nns2_nerr <- hunspell(ns2_ess, dict = dictionary(\"en_GB\")) %>%  unlist() %>% length()\nns2_nw <- sum(tokenizers::count_words(ns2_ess))\n# de\nde_nerr <- hunspell(de_ess, dict = dictionary(\"en_GB\")) %>%  unlist() %>% length()\nde_nw <- sum(tokenizers::count_words(de_ess))\n# es\nes_nerr <- hunspell(es_ess, dict = dictionary(\"en_GB\")) %>%  unlist() %>% length()\nes_nw <- sum(tokenizers::count_words(es_ess))\n# fr\nfr_nerr <- hunspell(fr_ess, dict = dictionary(\"en_GB\")) %>%  unlist() %>% length()\nfr_nw <- sum(tokenizers::count_words(fr_ess))\n# it\nit_nerr <- hunspell(it_ess, dict = dictionary(\"en_GB\")) %>%  unlist() %>% length()\nit_nw <- sum(tokenizers::count_words(it_ess))\n# pl\npl_nerr <- hunspell(pl_ess, dict = dictionary(\"en_GB\")) %>%  unlist() %>% length()\npl_nw <- sum(tokenizers::count_words(pl_ess))\n# ru\nru_nerr <- hunspell(ru_ess, dict = dictionary(\"en_GB\")) %>%  unlist() %>% length()\nru_nw <- sum(tokenizers::count_words(ru_ess))\n```\n:::\n\n\nNow, we generate a table from the results.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nerr_tb <- c(ns1_nerr, ns2_nerr, de_nerr, es_nerr, fr_nerr, it_nerr, pl_nerr, ru_nerr) %>%\n  as.data.frame() %>%\n  # rename column\n  dplyr::rename(errors = 1) %>%\n  # add n of words\n  dplyr::mutate(words = c(ns1_nw, ns2_nw, de_nw, es_nw, fr_nw, it_nw, pl_nw, ru_nw)) %>%\n  # add l1\n  dplyr::mutate(l1 = c(\"en\", \"en\", \"de\", \"es\", \"fr\", \"it\", \"pl\", \"ru\")) %>%\n  # calculate rel freq\n  dplyr::mutate(freq = round(errors/words*1000, 1)) %>%\n  # summarise\n  dplyr::group_by(l1) %>%\n  dplyr::summarise(freq = mean(freq))\n# inspect\nhead(err_tb)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 √ó 2\n  l1     freq\n  <chr> <dbl>\n1 de     23.4\n2 en     17.0\n3 es     27.6\n4 fr     27.5\n5 it      9  \n6 pl      7.9\n```\n:::\n:::\n\n\nWe can now visualize the results.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nerr_tb %>%\n  ggplot(aes(x = reorder(l1, -freq), y = freq, label = freq)) +\n  geom_bar(stat = \"identity\") +\n  geom_text(vjust=1.6, color = \"white\") +\n  # adapt tick labels\n  scale_x_discrete(\"L1 of learners\", \n                   breaks = names(table(read_l1$l1)), \n                   labels = c(\"en\" = \"English\",\n                              \"de\" = \"German\",\n                              \"es\" = \"Spanish\",\n                              \"fr\" = \"French\",\n                              \"it\" = \"Italian\",\n                              \"pl\" = \"Polish\",\n                              \"ru\" = \"Russian\")) +\n  labs(y = \"Relative frequency\\n(per 1,000 words)\") +\n  theme_bw() +\n  coord_cartesian(ylim = c(0, 40)) +\n  theme(legend.position = \"none\")\n```\n\n::: {.cell-output-display}\n![](llr_files/figure-html/sp7-1.png){width=672}\n:::\n:::\n\n\n# Citation & Session Info {-}\n\nSchweinberger, Martin. 2022. *Analyzing learner language using R*. Brisbane: The University of Queensland. url: https://slcladal.github.io/llr.html (Version 2022.08.31).\n\n```\n@manual{schweinberger2022llr,\n  author = {Schweinberger, Martin},\n  title = {Analyzing learner language using R},\n  note = {https://slcladal.github.io/pwr.html},\n  year = {2022},\n  organization = \"The University of Queensland, Australia. School of Languages and Cultures},\n  address = {Brisbane},\n  edition = {2022.08.31}\n}\n```\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsessionInfo()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nR version 4.2.1 (2022-06-23)\nPlatform: x86_64-pc-linux-gnu (64-bit)\nRunning under: Ubuntu 22.04.1 LTS\n\nMatrix products: default\nBLAS:   /usr/lib/x86_64-linux-gnu/blas/libblas.so.3.10.0\nLAPACK: /usr/lib/x86_64-linux-gnu/lapack/liblapack.so.3.10.0\n\nlocale:\n [1] LC_CTYPE=en_AU.UTF-8          LC_NUMERIC=C                 \n [3] LC_TIME=en_AU.UTF-8           LC_COLLATE=en_AU.UTF-8       \n [5] LC_MONETARY=en_AU.UTF-8       LC_MESSAGES=en_AU.UTF-8      \n [7] LC_PAPER=en_AU.UTF-8          LC_NAME=en_AU.UTF-8          \n [9] LC_ADDRESS=en_AU.UTF-8        LC_TELEPHONE=en_AU.UTF-8     \n[11] LC_MEASUREMENT=en_AU.UTF-8    LC_IDENTIFICATION=en_AU.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] slam_0.1-50               Matrix_1.4-1             \n [3] tokenizers_0.2.1          entity_0.1.0             \n [5] pacman_0.5.1              wordcloud2_0.2.1         \n [7] hunspell_3.0.1            stringi_1.7.8            \n [9] koRpus.lang.en_0.1-4      koRpus_0.13-8            \n[11] sylly_0.1-6               quanteda.textplots_0.94.1\n[13] quanteda.textstats_0.95   quanteda_3.2.1           \n[15] openNLP_0.2-7             tidytext_0.3.3           \n[17] tm_0.7-8                  NLP_0.2-1                \n[19] flextable_0.7.3           forcats_0.5.1            \n[21] stringr_1.4.0             dplyr_1.0.9              \n[23] purrr_0.3.4               readr_2.1.2              \n[25] tidyr_1.2.0               tibble_3.1.7             \n[27] ggplot2_3.3.6             tidyverse_1.3.2          \n\nloaded via a namespace (and not attached):\n [1] googledrive_2.0.0    openNLPdata_1.5.3-4  colorspace_2.0-3    \n [4] ellipsis_0.3.2       ISOcodes_2022.01.10  base64enc_0.1-3     \n [7] fs_1.5.2             rstudioapi_0.13      farver_2.1.1        \n[10] SnowballC_0.7.0      ggrepel_0.9.1        fansi_1.0.3         \n[13] lubridate_1.8.0      xml2_1.3.3           knitr_1.39          \n[16] jsonlite_1.8.0       rJava_1.0-6          broom_1.0.0         \n[19] dbplyr_2.2.1         compiler_4.2.1       httr_1.4.3          \n[22] backports_1.4.1      assertthat_0.2.1     fastmap_1.1.0       \n[25] gargle_1.2.0         cli_3.3.0            htmltools_0.5.2     \n[28] tools_4.2.1          coda_0.19-4          gtable_0.3.0        \n[31] glue_1.6.2           fastmatch_1.1-3      Rcpp_1.0.8.3        \n[34] statnet.common_4.6.0 cellranger_1.1.0     vctrs_0.4.1         \n[37] xfun_0.31            stopwords_2.3        network_1.17.2      \n[40] rvest_1.0.2          nsyllable_1.0.1      lifecycle_1.0.1     \n[43] googlesheets4_1.0.0  klippy_0.0.0.9500    scales_1.2.0        \n[46] hms_1.1.1            parallel_4.2.1       yaml_2.3.5          \n[49] gdtools_0.2.4        zip_2.2.0            sylly.en_0.1-3      \n[52] rlang_1.0.4          pkgconfig_2.0.3      systemfonts_1.0.4   \n[55] evaluate_0.15        lattice_0.20-45      htmlwidgets_1.5.4   \n[58] labeling_0.4.2       tidyselect_1.1.2     magrittr_2.0.3      \n[61] R6_2.5.1             generics_0.1.3       sna_2.7             \n[64] DBI_1.1.3            pillar_1.7.0         haven_2.5.0         \n[67] withr_2.5.0          janeaustenr_0.1.5    modelr_0.1.8        \n[70] crayon_1.5.1         uuid_1.1-0           utf8_1.2.2          \n[73] tzdb_0.3.0           rmarkdown_2.14       officer_0.4.3       \n[76] grid_4.2.1           readxl_1.4.0         data.table_1.14.2   \n[79] reprex_2.0.1         digest_0.6.29        RcppParallel_5.1.5  \n[82] munsell_0.5.0        viridisLite_0.4.0   \n```\n:::\n:::\n\n\n\n\n\n***\n\n[Back to top](#introduction)\n\n[Back to HOME](https://slcladal.github.io/index.html)\n\n***\n\n# References {-}\n",
    "supporting": [
      "llr_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"site_libs/clipboard-1.7.1/clipboard.min.js\"></script>\n<link href=\"site_libs/primer-tooltips-1.4.0/build.css\" rel=\"stylesheet\" />\n<link href=\"site_libs/klippy-0.0.0.9500/css/klippy.min.css\" rel=\"stylesheet\" />\n<script src=\"site_libs/klippy-0.0.0.9500/js/klippy.min.js\"></script>\n<link href=\"site_libs/tabwid-1.0.0/tabwid.css\" rel=\"stylesheet\" />\n<link href=\"site_libs/tabwid-1.0.0/scrool.css\" rel=\"stylesheet\" />\n<script src=\"site_libs/htmlwidgets-1.5.4/htmlwidgets.js\"></script>\n<link href=\"site_libs/wordcloud2-0.0.1/wordcloud.css\" rel=\"stylesheet\" />\n<script src=\"site_libs/wordcloud2-0.0.1/wordcloud2-all.js\"></script>\n<script src=\"site_libs/wordcloud2-0.0.1/hover.js\"></script>\n<script src=\"site_libs/wordcloud2-binding-0.2.1/wordcloud2.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}