{
  "hash": "0ac3f3d850d614cbcedc3122528713e2",
  "result": {
    "markdown": "---\ntitle: \"Fixed-Effects Regression Modelling in R\"\nauthor: \"Martin Schweinberger\"\ndate: \"2022-08-31\"\n---\n\n::: {.cell}\n::: {.cell-output-display}\n![](https://slcladal.github.io/images/uq1.jpg){width=100%}\n:::\n:::\n\n\n# Introduction{-}\n\nThis tutorial focuses on different types of fixed-effects regression analysis using R.^[I'm extremely grateful to Stefan Thomas Gries who provided very helpful feedback and pointed out many errors in previous versions of this tutorial. All remaining errors are, of course, my own.] Regression models can be divided into fixed- and mixed-effects regression models (depending on whether the model has a random effect structure or not) and  this tutorial covers multiple fixed-effects linear, logistic, ordinal, and Poisson regression which differ in that they take different types of dependent or response variables. The versatility regarding being able to deal with many different response variables is one of the reasons why regression analysis is among the most widely used quantitative methods in the language sciences. Such regression models are used to assess if and how predictors (variables or interactions between variables) correlate with a certain response. \n\n[Check out this tutorial](https://slcladal.github.io/introreg.html) if you want to know more about the theoretical underpinnings and the conceptual basis of regression analysis.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](https://slcladal.github.io/images/yr_chili.jpg){width=15% style=\"float:right; padding:10px\"}\n:::\n:::\n\n\nThis tutorial is aimed at intermediate and advanced users of R with the aim of showcasing how to perform regression analysis using R. The aim is not to provide a fully-fledged analysis but rather to show and exemplify common regression types, model diagnostics, and model fitting using R. \n\n<br>\n<div class=\"warning\" style='padding:0.1em; background-color:#f2f2f2; color:#51247a'>\n<span>\n<p style='margin-top:1em; text-align:center'>\nThe entire R Notebook for the tutorial can be downloaded [**here**](https://slcladal.github.io/content/fixreg.Rmd).  If you want to render the R Notebook on your machine, i.e. knitting the document to html or a pdf, you need to make sure that you have R and RStudio installed and you also need to download the [**bibliography file**](https://slcladal.github.io/content/bibliography.bib) and store it in the same folder where you store the Rmd or the Rproj file. <br></p>\n<p style='margin-left:1em;'>\n</p></span>\n</div>\n\n<br>\n\n\n\n**Preparation and session set up**\n\nThis tutorial is based on R. If you have not installed R or are new to it, you will find an introduction to and more information how to use R [here](https://slcladal.github.io/intror.html). For this tutorials, we need to install certain *packages* from an R *library* so that the scripts shown below are executed without errors. Before turning to the code below, please install the packages by running the code below this paragraph. If you have already installed the packages mentioned below, then you can skip ahead and ignore this section. To install the necessary packages, simply run the following code - it may take some time (between 1 and 5 minutes to install all of the libraries so you do not need to worry if it takes some time).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# install\ninstall.packages(\"car\")\ninstall.packages(\"flextable\")\ninstall.packages(\"ggplot2\")\ninstall.packages(\"ggpubr\")\ninstall.packages(\"Hmisc\")\ninstall.packages(\"MASS\")\ninstall.packages(\"mclogit\")\ninstall.packages(\"ordinal\")\ninstall.packages(\"rms\")\ninstall.packages(\"robustbase\")\ninstall.packages(\"sjPlot\")\ninstall.packages(\"stringr\")\ninstall.packages(\"tibble\")\ninstall.packages(\"dplyr\")\ninstall.packages(\"vcd\")\ninstall.packages(\"vip\")\ninstall.packages(\"see\")\ninstall.packages(\"glmulti\")\ninstall.packages(\"performance\")\ninstall.packages(\"report\")\ninstall.packages(\"ggfortify\")\ninstall.packages(\"caret\")\ninstall.packages(\"msm\")\n# install klippy for copy-to-clipboard button in code chunks\ninstall.packages(\"remotes\")\nremotes::install_github(\"rlesur/klippy\")\n```\n:::\n\n\nNow that we have installed the packages, we activate them as shown below.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# set options\noptions(stringsAsFactors = F)          # no automatic data transformation\noptions(\"scipen\" = 100, \"digits\" = 12) # suppress math annotation\n# load packages\nlibrary(car)\nlibrary(flextable)\nlibrary(ggfortify)\nlibrary(ggplot2)\nlibrary(ggpubr)\nlibrary(Hmisc)\nlibrary(MASS)\nlibrary(ordinal)\nlibrary(rms)\nlibrary(sjPlot)\nlibrary(stringr)\nlibrary(vcd)\nlibrary(vip)\nlibrary(glmulti)\nlibrary(performance)\nlibrary(report)\nlibrary(dplyr)\n# activate klippy for copy-to-clipboard button\nklippy::klippy()\n```\n\n::: {.cell-output-display}\n```{=html}\n<script>\n  addClassKlippyTo(\"pre.r, pre.markdown\");\n  addKlippy('left', 'top', 'auto', '1', 'Copy code', 'Copied!');\n</script>\n```\n:::\n:::\n\n\n\nOnce you have installed R and RStudio and initiated the session by executing the code shown above, you are good to go.\n\n\n## Multiple Linear Regression{-}\n\nIn contrast to simple linear regression, which estimates the effect of a single predictor, multiple linear regression estimates the effect of various predictor (see the equation below). A multiple linear regression can thus test the effects of various predictors simultaneously.\n\n\\begin{equation}\n\nf_{(x)} = \\alpha + \\beta_{1}x_{i} + \\beta_{2}x_{i+1} + \\dots + \\beta_{n}x_{i+n} + \\epsilon\n\n\\end{equation}\n\nThere exists a wealth of literature focusing on multiple linear regressions and the concepts it is based on.  For instance, there are @achen1982interpreting, @bortz2006statistik, @crawley2005statistics, @faraway2002practical, @field2012discovering, @gries2021statistics, @levshina2015linguistics, @winter2019statistics and @wilcox2009basic to name just a few. Introductions to regression modeling in R are @baayen2008analyzing, @crawley2012r, @gries2021statistics, or @levshina2015linguistics.\n\nThe model diagnostics we are dealing with here are partly identical to the diagnostic methods discussed in the section on simple linear regression. Because of this overlap, diagnostics will only be described in more detail if they have not been described in the section on simple linear regression.\n\n***\n\n<div class=\"warning\" style='padding:0.1em; background-color:#f2f2f2; color:#51247a'>\n<span>\n<p style='margin-top:1em; text-align:center'>\n<b>EXCURSION</b></p>\n<p style='margin-left:1em;'>\n</p></span>\n</div>\n\n<div class=\"question\">` \n\n<details>\n  <summary>A note on sample size and power</summary>\nAlthough there appears to be a general assumption that 25 data points per variable level are sufficient, this is merely a general rule of thumb that is actually often incorrect. Such rules of thumb are inadequate because the required sample size depends on the number of variables in a given model, the size of the effect, the variance of the effect, and the desired power - in other words, the minimum necessary sample size relates to *statistical power* (see [here](https://slcladal.github.io/pwr.html) for a tutorial on power). If a model contains many variables, then this requires a larger sample size than a model which only uses very few predictors and if the effect that we want to find is small, then this also requires a larger sample than if the effect we are interested in is large. Finally, effects that are very robust and do not exhibit substantive variability require a much smaller sample size compared with effects that are spurious and vary notably Since the sample size depends on the effect size and variance as well as the number of variables, there is no one-size-fits-all answer to what the best sample size is.\n  \n  Another, slightly better but still incorrect, rule of thumb is that the more data, the better. This is not correct because models based on many cases are prone to report almost any effect as  significant (although with a tiny effect size). A discussion about this phenomenon led to a discussion between Adam Kilgariff and Stefan Gries (see [here for Adam's initial discussion note](https://www.degruyter.com/document/doi/10.1515/cllt.2005.1.2.263/html) and see [here for Stefan's response](https://www.degruyter.com/document/doi/10.1515/cllt.2005.1.2.277/html?lang=de)) resulting in a wider acknowledgement that effect sizes are more important than mere p-values. Also, given that there are procedures that can correct for overfitting, larger data sets are still preferable to data sets that are simply too small to warrant reliable results. In conclusion, it remains true that the sample size depends on the effect under investigation.\n</details>\n\n</div>`\n\n***\n\nDespite there being no ultimate rule of thumb, @field2012discovering[273-275], based on @green1991many, provide data-driven suggestions for the minimal size of data required for regression models that aim to find medium sized effects (k = number of predictors; categorical variables with more than two levels should be transformed into dummy variables):\n\n* If one is merely interested in the overall model fit (something I have not encountered), then the sample size should be at least 50 + k (k = number of predictors in model).\n\n*  If one is only interested in the effect of specific variables, then the sample size should be at least 104 + k (k = number of predictors in model).\n\n*  If one is only interested in both model fit and the effect of specific variables, then the sample size should be at least the higher value of 50 + k or 104 + k (k = number of predictors in model).\n\nYou will see in the R code below that there is already a function that tests whether the sample size is sufficient.\n\n### Example: Gifts and Availability{-}\n\nThe example we will go through here is taken from @field2012discovering. In this example, the research question is if the money that men spend on presents for women depends on the women's attractiveness and their relationship status. To answer this research question, we will implement a multiple linear regression and start by loading the data and inspect its structure and properties.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# load data\nmlrdata  <- base::readRDS(url(\"https://slcladal.github.io/data/mld.rda\", \"rb\"))\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n```{=html}\n<div class=\"tabwid\"><style>.cl-4ad5373e{table-layout:auto;width:75%;}.cl-4ad0cf5a{font-family:'DejaVu Sans';font-size:12pt;font-weight:bold;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-4ad0cf6e{font-family:'DejaVu Sans';font-size:12pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-4ad0e8a0{margin:0;text-align:center;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-4ad0e8aa{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-4ad12af4{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-4ad12b08{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-4ad12b09{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-4ad12b12{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-4ad12b1c{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-4ad12b1d{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-4ad12b26{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-4ad12b27{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-4ad12b30{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-4ad12b3a{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-4ad12b3b{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-4ad12b44{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}</style><table class='cl-4ad5373e'>\n```\n<caption class=\"Table Caption\">\n\nFirst 15 rows of the mlrdata.\n\n</caption>\n```{=html}\n<thead><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-4ad12b3b\"><p class=\"cl-4ad0e8a0\"><span class=\"cl-4ad0cf5a\">status</span></p></td><td class=\"cl-4ad12b3a\"><p class=\"cl-4ad0e8a0\"><span class=\"cl-4ad0cf5a\">attraction</span></p></td><td class=\"cl-4ad12b44\"><p class=\"cl-4ad0e8aa\"><span class=\"cl-4ad0cf5a\">money</span></p></td></tr></thead><tbody><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-4ad12b09\"><p class=\"cl-4ad0e8a0\"><span class=\"cl-4ad0cf6e\">Relationship</span></p></td><td class=\"cl-4ad12af4\"><p class=\"cl-4ad0e8a0\"><span class=\"cl-4ad0cf6e\">NotInterested</span></p></td><td class=\"cl-4ad12b08\"><p class=\"cl-4ad0e8aa\"><span class=\"cl-4ad0cf6e\">86.33</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-4ad12b1d\"><p class=\"cl-4ad0e8a0\"><span class=\"cl-4ad0cf6e\">Relationship</span></p></td><td class=\"cl-4ad12b12\"><p class=\"cl-4ad0e8a0\"><span class=\"cl-4ad0cf6e\">NotInterested</span></p></td><td class=\"cl-4ad12b1c\"><p class=\"cl-4ad0e8aa\"><span class=\"cl-4ad0cf6e\">45.58</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-4ad12b09\"><p class=\"cl-4ad0e8a0\"><span class=\"cl-4ad0cf6e\">Relationship</span></p></td><td class=\"cl-4ad12af4\"><p class=\"cl-4ad0e8a0\"><span class=\"cl-4ad0cf6e\">NotInterested</span></p></td><td class=\"cl-4ad12b08\"><p class=\"cl-4ad0e8aa\"><span class=\"cl-4ad0cf6e\">68.43</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-4ad12b1d\"><p class=\"cl-4ad0e8a0\"><span class=\"cl-4ad0cf6e\">Relationship</span></p></td><td class=\"cl-4ad12b12\"><p class=\"cl-4ad0e8a0\"><span class=\"cl-4ad0cf6e\">NotInterested</span></p></td><td class=\"cl-4ad12b1c\"><p class=\"cl-4ad0e8aa\"><span class=\"cl-4ad0cf6e\">52.93</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-4ad12b09\"><p class=\"cl-4ad0e8a0\"><span class=\"cl-4ad0cf6e\">Relationship</span></p></td><td class=\"cl-4ad12af4\"><p class=\"cl-4ad0e8a0\"><span class=\"cl-4ad0cf6e\">NotInterested</span></p></td><td class=\"cl-4ad12b08\"><p class=\"cl-4ad0e8aa\"><span class=\"cl-4ad0cf6e\">61.86</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-4ad12b1d\"><p class=\"cl-4ad0e8a0\"><span class=\"cl-4ad0cf6e\">Relationship</span></p></td><td class=\"cl-4ad12b12\"><p class=\"cl-4ad0e8a0\"><span class=\"cl-4ad0cf6e\">NotInterested</span></p></td><td class=\"cl-4ad12b1c\"><p class=\"cl-4ad0e8aa\"><span class=\"cl-4ad0cf6e\">48.47</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-4ad12b09\"><p class=\"cl-4ad0e8a0\"><span class=\"cl-4ad0cf6e\">Relationship</span></p></td><td class=\"cl-4ad12af4\"><p class=\"cl-4ad0e8a0\"><span class=\"cl-4ad0cf6e\">NotInterested</span></p></td><td class=\"cl-4ad12b08\"><p class=\"cl-4ad0e8aa\"><span class=\"cl-4ad0cf6e\">32.79</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-4ad12b1d\"><p class=\"cl-4ad0e8a0\"><span class=\"cl-4ad0cf6e\">Relationship</span></p></td><td class=\"cl-4ad12b12\"><p class=\"cl-4ad0e8a0\"><span class=\"cl-4ad0cf6e\">NotInterested</span></p></td><td class=\"cl-4ad12b1c\"><p class=\"cl-4ad0e8aa\"><span class=\"cl-4ad0cf6e\">35.91</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-4ad12b09\"><p class=\"cl-4ad0e8a0\"><span class=\"cl-4ad0cf6e\">Relationship</span></p></td><td class=\"cl-4ad12af4\"><p class=\"cl-4ad0e8a0\"><span class=\"cl-4ad0cf6e\">NotInterested</span></p></td><td class=\"cl-4ad12b08\"><p class=\"cl-4ad0e8aa\"><span class=\"cl-4ad0cf6e\">30.98</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-4ad12b1d\"><p class=\"cl-4ad0e8a0\"><span class=\"cl-4ad0cf6e\">Relationship</span></p></td><td class=\"cl-4ad12b12\"><p class=\"cl-4ad0e8a0\"><span class=\"cl-4ad0cf6e\">NotInterested</span></p></td><td class=\"cl-4ad12b1c\"><p class=\"cl-4ad0e8aa\"><span class=\"cl-4ad0cf6e\">44.82</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-4ad12b09\"><p class=\"cl-4ad0e8a0\"><span class=\"cl-4ad0cf6e\">Relationship</span></p></td><td class=\"cl-4ad12af4\"><p class=\"cl-4ad0e8a0\"><span class=\"cl-4ad0cf6e\">NotInterested</span></p></td><td class=\"cl-4ad12b08\"><p class=\"cl-4ad0e8aa\"><span class=\"cl-4ad0cf6e\">35.05</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-4ad12b1d\"><p class=\"cl-4ad0e8a0\"><span class=\"cl-4ad0cf6e\">Relationship</span></p></td><td class=\"cl-4ad12b12\"><p class=\"cl-4ad0e8a0\"><span class=\"cl-4ad0cf6e\">NotInterested</span></p></td><td class=\"cl-4ad12b1c\"><p class=\"cl-4ad0e8aa\"><span class=\"cl-4ad0cf6e\">64.49</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-4ad12b09\"><p class=\"cl-4ad0e8a0\"><span class=\"cl-4ad0cf6e\">Relationship</span></p></td><td class=\"cl-4ad12af4\"><p class=\"cl-4ad0e8a0\"><span class=\"cl-4ad0cf6e\">NotInterested</span></p></td><td class=\"cl-4ad12b08\"><p class=\"cl-4ad0e8aa\"><span class=\"cl-4ad0cf6e\">54.50</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-4ad12b1d\"><p class=\"cl-4ad0e8a0\"><span class=\"cl-4ad0cf6e\">Relationship</span></p></td><td class=\"cl-4ad12b12\"><p class=\"cl-4ad0e8a0\"><span class=\"cl-4ad0cf6e\">NotInterested</span></p></td><td class=\"cl-4ad12b1c\"><p class=\"cl-4ad0e8aa\"><span class=\"cl-4ad0cf6e\">61.48</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-4ad12b30\"><p class=\"cl-4ad0e8a0\"><span class=\"cl-4ad0cf6e\">Relationship</span></p></td><td class=\"cl-4ad12b27\"><p class=\"cl-4ad0e8a0\"><span class=\"cl-4ad0cf6e\">NotInterested</span></p></td><td class=\"cl-4ad12b26\"><p class=\"cl-4ad0e8aa\"><span class=\"cl-4ad0cf6e\">55.51</span></p></td></tr></tbody></table></div>\n```\n:::\n:::\n\n\nThe data set consist of three variables stored in three columns. The first column contains the relationship status of the present giver (in this study this were men), the second whether the man is interested in the woman (the present receiver in this study), and the third column represents the money spend on the present. The data set represents 100 cases and the mean amount of money spend on a present is 88.38 dollars. In a next step, we visualize the data to get a more detailed impression of the relationships between variables.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# create plots\np1 <- ggplot(mlrdata, aes(status, money)) +\n  geom_boxplot() + \n  theme_bw()\n# plot 2\np2 <- ggplot(mlrdata, aes(attraction, money)) +\n  geom_boxplot() +\n  theme_bw()\n# plot 3\np3 <- ggplot(mlrdata, aes(x = money)) +\n  geom_histogram(aes(y=..density..)) +            \n  theme_bw() +         \n  geom_density(alpha=.2, fill = \"gray50\") \n# plot 4\np4 <- ggplot(mlrdata, aes(status, money)) +\n  geom_boxplot(aes(fill = factor(status))) + \n  scale_fill_manual(values = c(\"grey30\", \"grey70\")) + \n  facet_wrap(~ attraction) + \n  guides(fill = \"none\") +\n  theme_bw()\n# show plots\nvip::grid.arrange(grobs = list(p1, p2, p3, p4), widths = c(1, 1), layout_matrix = rbind(c(1, 2), c(3, 4)))\n```\n\n::: {.cell-output-display}\n![](fixreg_files/figure-html/mlr3-1.png){width=672}\n:::\n:::\n\n\nThe upper left figure consists of a boxplot which shows how much money was spent by relationship status. The figure suggests that men spend more on women if they are not in a relationship. The next figure shows the relationship between the money spend on presents and whether or not the men were interested in the women.\n\nThe boxplot in the upper right panel suggests that men spend substantially more on women if the men are interested in them. The next figure depicts the distribution of the amounts of money spend on the presents for the women. In addition, the figure indicates the existence of two outliers (dots in the boxplot)\n\nThe histogram in the lower left panel shows that, although the mean amount of money spent on presents is 88.38 dollars, the distribution peaks around 50 dollars indicating that on average, men spend about 50 dollars on presents. Finally, we will plot the amount of money spend on presents against relationship status by attraction in order to check whether the money spent on presents is affected by an interaction between attraction and relationship status.\n\nThe boxplot in the lower right panel confirms the existence of an interaction (a non-additive term) as men only spend more money on  women if the men single *and* they are interested in the women. If men are not interested in the women, then the relationship has no effect as they spend an equal amount of money on the women regardless of whether they are in a relationship or not.\n\nWe will now start to implement the regression model. In a first step, we create two saturated models that contain all possible predictors (main effects and interactions). The two models are identical but one is generated with the `lm` and the other with the `glm` function as these functions offer different model parameters in their output.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm1.mlr = lm(                      # generate lm regression object\n  money ~ 1 + attraction*status,  # def. regression formula (1 = intercept)\n  data = mlrdata)                 # def. data\nm1.glm = glm(                     # generate glm regression object\n  money ~ 1 + attraction*status,  # def. regression formula (1 = intercept)\n  family = gaussian,              # def. linkage function\n  data = mlrdata)                 # def. data\n```\n:::\n\n\nAfter generating the saturated models we can now start with the model fitting. Model fitting refers to a process that aims at find the model that explains a maximum of variance with a minimum of predictors [see @field2012discovering 318]. Model fitting is therefore based on the *principle of parsimony* which is related to Occam's razor according to which explanations that require fewer assumptions are more likely to be true.\n\n### Automatic Model Fitting {-}\n\nIn this section, we will use a backward elimination procedure that uses decreases in AIC (*Akaike Information Criterion*) as the criterion to minimize the model in a step-wise manner. This procedure aims at finding the model with the lowest AIC values by evaluating - step-by-step - whether the removal of a predictor (term) leads to a lower AIC value.\n\nWe use this method here to show how to implement it but a better option would be to use the `glmulti` function from the `glmulti` package [see @calcagno2020glmulti] to find the best model. The `glmulti` function is better as it checks all possible models and the reports the model(s) with the best fit. This is advantages over forward or backward elimination as these procedures will often lead to different suggestions for the best model (based on the order in which predictors are added or removed). The tutorial on mixed-effects regression modelling shows how to implement  \n\n\nThe reason for avoiding automated model fitting is that the algorithm only checks if the AIC has decreased but not if the model is stable or reliable. Thus, automated model fitting has the problem that you can never be sure that the way that lead you to the final model is reliable and that all models were indeed stable. Imagine you want to climb down from a roof top and you have a ladder. The problem is that you do not know if and how many steps are broken. This is similar to using automated model fitting. In other sections, we will explore better methods to fit models (manual step-wise step-up and step-down procedures, for example).\n\nThe AIC is calculated using the equation below. The lower the AIC value, the better the balance between explained variance and the number of predictors. AIC values can and should only be compared for models that are fit on the same data set with the same (number of) cases (*LL* stands for *logged likelihood* or *LogLikelihood* and *k* represents the number of predictors in the model (including the intercept); the LL represents a measure of how good the model fits the data).\n\n\\begin{equation}\nAkaike Information Criterion (AIC) = -2LL + 2k\n\\end{equation}\n\nAn alternative to the AIC is the BIC (Bayesian Information Criterion). Both AIC and BIC penalize models for including variables in a model. The penalty of the BIC is bigger than the penalty of the AIC and it includes the number of cases in the model (*LL* stands for *logged likelihood* or *LogLikelihood*, *k* represents the number of predictors in the model (including the intercept), and *N* represents the number of cases in the model). \n\n\\begin{equation}\nBayesian Information Criterion (BIC) = -2LL + 2k * log(N)\n\\end{equation}\n\nInteractions are evaluated first and only if all insignificant interactions have been removed would the procedure start removing insignificant main effects (that are not part of significant interactions). Other model fitting procedures (forced entry, step-wise step up, hierarchical) are discussed during the implementation of other regression models. We cannot discuss all procedures here as model fitting is rather complex and a discussion of even the most common procedures would to lengthy and time consuming at this point. It is important to note though that there is not perfect model fitting procedure and automated approaches should be handled with care as they are likely to ignore violations of model parameters that can be detected during manual - but time consuming - model fitting procedures. As a general rule of thumb, it is advisable to fit models as carefully and deliberately as possible. We will now begin to fit the model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# automated AIC based model fitting\nstep(m1.mlr, direction = \"both\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nStart:  AIC=592.52\nmoney ~ 1 + attraction * status\n\n                    Df   Sum of Sq         RSS         AIC\n<none>                             34557.56428 592.5211556\n- attraction:status  1 24947.25481 59504.81909 644.8642395\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = money ~ 1 + attraction * status, data = mlrdata)\n\nCoefficients:\n                         (Intercept)               attractionNotInterested  \n                             99.1548                              -47.6628  \n                        statusSingle  attractionNotInterested:statusSingle  \n                             57.6928                              -63.1788  \n```\n:::\n:::\n\n\nThe automated model fitting procedure informs us that removing predictors has not caused a decrease in the AIC. The saturated model is thus also the final minimal adequate model. We will now inspect the final minimal model and go over the model report.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm2.mlr = lm(                       # generate lm regression object\n  money ~ (status + attraction)^2, # def. regression formula\n  data = mlrdata)                  # def. data\nm2.glm = glm(                      # generate glm regression object\n  money ~ (status + attraction)^2, # def. regression formula\n  family = gaussian,               # def. linkage function\n  data = mlrdata)                  # def. data\n# inspect final minimal model\nsummary(m2.mlr)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = money ~ (status + attraction)^2, data = mlrdata)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-45.0760 -14.2580   0.4596  11.9315  44.1424 \n\nCoefficients:\n                                         Estimate   Std. Error  t value\n(Intercept)                           99.15480000   3.79459947 26.13050\nstatusSingle                          57.69280000   5.36637403 10.75080\nattractionNotInterested              -47.66280000   5.36637403 -8.88175\nstatusSingle:attractionNotInterested -63.17880000   7.58919893 -8.32483\n                                                   Pr(>|t|)    \n(Intercept)                          < 0.000000000000000222 ***\nstatusSingle                         < 0.000000000000000222 ***\nattractionNotInterested                 0.00000000000003751 ***\nstatusSingle:attractionNotInterested    0.00000000000058085 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 18.9729973 on 96 degrees of freedom\nMultiple R-squared:  0.852041334,\tAdjusted R-squared:  0.847417626 \nF-statistic: 184.276619 on 3 and 96 DF,  p-value: < 0.0000000000000002220446\n```\n:::\n:::\n\n\nThe first element of the report is called *Call* and it reports the regression formula of the model. Then, the report provides the residual distribution (the range, median and quartiles of the residuals) which allows drawing inferences about the distribution of differences between observed and expected values. If the residuals are distributed non-normally, then this is a strong indicator that the model is unstable and unreliable because mathematical assumptions on which the model is based are violated.\n\nNext, the model summary reports the most important part: a table with model statistics of the fixed-effects structure of the model. The table contains the estimates (coefficients of the predictors), standard errors, t-values, and the p-values which show whether a predictor significantly correlates with the dependent variable that the model investigates.\n\nAll main effects (status and attraction) as well as the interaction between status and attraction is reported as being significantly correlated with the dependent variable (money). An interaction occurs if a correlation between the dependent variable and a predictor is affected by another predictor.\n\nThe top most term is called intercept and has a value of 99.15 which represents the base estimate to which all other estimates refer. To exemplify what this means, let us consider what the model would predict that a man would spend on a present if he interested in the woman but he is also in a relationship. The amount he would spend (based on the model would be 99.15 dollars (which is the intercept). This means that the intercept represents the predicted value if all predictors take the base or reference level. And since being in relationship but being interested are the case, and because the interaction does not apply, the predicted value in our example is exactly the intercept (see below). \n\n\n::: {.cell}\n\n```{.r .cell-code}\n#intercept  Single  NotInterested  Single:NotInterested\n99.15     + 57.69  + 0           + 0     # 156.8 single + interested\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 156.84\n```\n:::\n\n```{.r .cell-code}\n99.15     + 57.69  - 47.66       - 63.18 # 46.00 single + not interested\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 46\n```\n:::\n\n```{.r .cell-code}\n99.15     - 0      + 0           - 0     # 99.15 relationship + interested\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 99.15\n```\n:::\n\n```{.r .cell-code}\n99.15     - 0      - 47.66       - 0     # 51.49 relationship + not interested\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 51.49\n```\n:::\n:::\n\n\nNow, let us consider what a man would spend if he is in a relationship and he is not attracted to the women. In that case, the model predicts that the man would spend only  51.49 dollars on a present: the intercept (99.15) minus 47.66 because the man is not interested (and no additional subtraction because the interaction does not apply). \n\nWe can derive the same results easier using the `predict` function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# make prediction based on the model for original data\nprediction <- predict(m2.mlr, newdata = mlrdata)\n# inspect predictions\ntable(round(prediction,2))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n 46.01  51.49  99.15 156.85 \n    25     25     25     25 \n```\n:::\n:::\n\n\nBelow the table of coefficients, the regression summary reports model statistics that provide information about how well the model performs. The difference between the values and the values in the coefficients table is that the model statistics refer to the model as a whole rather than focusing on individual predictors.\n\nThe multiple R^2^-value is a measure of how much variance the model explains. A multiple R^2^-value of 0 would inform us that the model does not explain any variance while a value of .852 mean that the model explains 85.2 percent of the variance. A value of 1 would inform us that the model explains 100 percent of the variance and that the predictions of the model match the observed values perfectly. Multiplying the multiple R^2^-value thus provides the percentage of explained variance. Models that have a multiple R^2^-value equal or higher than .05 are deemed substantially significant [see @szmrecsanyi2006morphosyntactic 55]. It has been claimed that models should explain a minimum of 5 percent of variance but this is problematic as it is not uncommon for models to have very low explanatory power while still performing significantly and systematically better than chance. In addition, the total amount of variance is negligible in cases where one is interested in very weak but significant effects. It is much more important for model to perform significantly better than minimal base-line models because if this is not the case, then the model does not have any predictive and therefore no explanatory power.\n\nThe adjusted R^2^-value considers the amount of explained variance in light of the number of predictors in the model (it is thus somewhat similar to the AIC and BIC) and informs about how well the model would perform if it were applied to the population that the sample is drawn from. Ideally, the difference between multiple and adjusted R^2^-value should be very small as this means that the model is not overfitted. If, however, the difference between multiple and adjusted R^2^-value is substantial, then this would strongly suggest that the model is unstable and overfitted to the data while being inadequate for drawing inferences about the population. Differences between multiple and adjusted R^2^-values indicate that the data contains outliers that cause the distribution of the data on which the model is based to differ from the distributions that the model mathematically requires to provide reliable estimates. The difference between multiple and adjusted R^2^-value in our model is very small (85.2-84.7=.05) and should not cause concern.\n\nBefore continuing, we will calculate the confidence intervals of the coefficients.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# extract confidence intervals of the coefficients\nconfint(m2.mlr)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                                              2.5 %         97.5 %\n(Intercept)                           91.6225795890 106.6870204110\nstatusSingle                          47.0406317400  68.3449682600\nattractionNotInterested              -58.3149682600 -37.0106317400\nstatusSingle:attractionNotInterested -78.2432408219 -48.1143591781\n```\n:::\n\n```{.r .cell-code}\n# create and compare baseline- and minimal adequate model\nm0.mlr <- lm(money ~1, data = mlrdata)\nanova(m0.mlr, m2.mlr)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Variance Table\n\nModel 1: money ~ 1\nModel 2: money ~ (status + attraction)^2\n  Res.Df          RSS Df   Sum of Sq         F                 Pr(>F)    \n1     99 233562.28650                                                    \n2     96  34557.56428  3 199004.7222 184.27662 < 0.000000000000000222 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\nNow, we compare the final minimal adequate model to the base-line model to test whether then final model significantly outperforms the baseline model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# compare baseline- and minimal adequate model\nAnova(m0.mlr, m2.mlr, type = \"III\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnova Table (Type III tests)\n\nResponse: money\n                 Sum Sq Df    F value                 Pr(>F)    \n(Intercept) 781015.8300  1 2169.64133 < 0.000000000000000222 ***\nResiduals    34557.5643 96                                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\nThe comparison between the two model confirms that the minimal adequate model performs significantly better (makes significantly more accurate estimates of the outcome variable) compared with the baseline model.\n\n### Outlier Detection{-}\n\nAfter implementing the multiple regression, we now need to look for outliers and perform the model diagnostics by testing whether removing data points disproportionately decreases model fit. To begin with, we generate diagnostic plots.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# generate plots\nautoplot(m2.mlr) + \n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](fixreg_files/figure-html/mlr11-1.png){width=672}\n:::\n:::\n\n\nThe plots do not show severe problems such as funnel shaped patterns or drastic deviations from the diagonal line in Normal Q-Q plot (have a look at the explanation of what to look for and how to interpret these diagnostic plots in the section on simple linear regression) but data points 52, 64, and 83 are repeatedly indicated as potential outliers. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# determine a cutoff for data points that have D-values higher than 4/(n-k-1)\ncutoff <- 4/((nrow(mlrdata)-length(m2.mlr$coefficients)-2))\n# start plotting\npar(mfrow = c(1, 2))           # display plots in 3 rows/2 columns\nqqPlot(m2.mlr, main=\"QQ Plot\") # create qq-plot\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 52 83\n```\n:::\n\n```{.r .cell-code}\nplot(m2.mlr, which=4, cook.levels = cutoff); par(mfrow = c(1, 1))\n```\n\n::: {.cell-output-display}\n![](fixreg_files/figure-html/mlr12-1.png){width=672}\n:::\n:::\n\n\nThe graphs indicate that data points 52, 64, and 83 may be problematic. We will therefore statistically evaluate whether these data points need to be removed. In order to find out which data points require removal, we extract the influence measure statistics and add them to out data set.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# extract influence statistics\ninfl <- influence.measures(m2.mlr)\n# add infl. statistics to data\nmlrdata <- data.frame(mlrdata, infl[[1]], infl[[2]])\n# annotate too influential data points\nremove <- apply(infl$is.inf, 1, function(x) {\n  ifelse(x == TRUE, return(\"remove\"), return(\"keep\")) } )\n# add annotation to data\nmlrdata <- data.frame(mlrdata, remove)\n# number of rows before removing outliers\nnrow(mlrdata)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 100\n```\n:::\n\n```{.r .cell-code}\n# remove outliers\nmlrdata <- mlrdata[mlrdata$remove == \"keep\", ]\n# number of rows after removing outliers\nnrow(mlrdata)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 98\n```\n:::\n:::\n\n\nThe difference in row in the data set before and after removing data points indicate that two data points which represented outliers have been removed.\n\n\n\n<div class=\"warning\" style='padding:0.1em; background-color:#f2f2f2; color:#51247a'>\n<span>\n<p style='margin-top:1em; text-align:center'>\n<b>NOTE</b><br><br>In general, outliers should not simply be removed unless there are good reasons for it (this could be that the outliers represent measurement errors). If a data set contains outliers, one should rather switch to methods that are better at handling outliers, e.g. by using weights to account for data points with high leverage. One alternative would be to switch to a robust regression (see [here](https://slcladal.github.io/regression.html#16_Robust_Regression)). However, here we show how to proceed by removing outliers as this is a common, though potentially problematic, method of dealing with outliers. </p>\n<p style='margin-left:1em;'>\n</p></span>\n</div>\n\n<br>\n\n\n### Rerun Regression{-}\n\nAs we have decided to remove the outliers which means that we are now dealing with a different data set, we need to rerun the regression analysis. As the steps are identical to the regression analysis performed above, the steps will not be described in greater detail.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# recreate regression models on new data\nm0.mlr = lm(money ~ 1, data = mlrdata)\nm0.glm = glm(money ~ 1, family = gaussian, data = mlrdata)\nm1.mlr = lm(money ~ (status + attraction)^2, data = mlrdata)\nm1.glm = glm(money ~ status * attraction, family = gaussian,\n             data = mlrdata)\n# automated AIC based model fitting\nstep(m1.mlr, direction = \"both\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nStart:  AIC=570.29\nmoney ~ (status + attraction)^2\n\n                    Df   Sum of Sq         RSS         AIC\n<none>                             30411.31714 570.2850562\n- status:attraction  1 21646.86199 52058.17914 620.9646729\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = money ~ (status + attraction)^2, data = mlrdata)\n\nCoefficients:\n                         (Intercept)                          statusSingle  \n                          99.1548000                            55.8535333  \n             attractionNotInterested  statusSingle:attractionNotInterested  \n                         -47.6628000                           -59.4613667  \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# create new final models\nm2.mlr = lm(money ~ (status + attraction)^2, data = mlrdata)\nm2.glm = glm(money ~ status * attraction, family = gaussian,\n             data = mlrdata)\n# inspect final minimal model\nsummary(m2.mlr)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = money ~ (status + attraction)^2, data = mlrdata)\n\nResiduals:\n         Min           1Q       Median           3Q          Max \n-35.76416667 -13.50520000  -0.98948333  10.59887500  38.77166667 \n\nCoefficients:\n                                         Estimate   Std. Error  t value\n(Intercept)                           99.15480000   3.59735820 27.56323\nstatusSingle                          55.85353333   5.14015367 10.86612\nattractionNotInterested              -47.66280000   5.08743275 -9.36873\nstatusSingle:attractionNotInterested -59.46136667   7.26927504 -8.17982\n                                                   Pr(>|t|)    \n(Intercept)                          < 0.000000000000000222 ***\nstatusSingle                         < 0.000000000000000222 ***\nattractionNotInterested               0.0000000000000040429 ***\nstatusSingle:attractionNotInterested  0.0000000000013375166 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 17.986791 on 94 degrees of freedom\nMultiple R-squared:  0.857375902,\tAdjusted R-squared:  0.852824069 \nF-statistic: 188.358387 on 3 and 94 DF,  p-value: < 0.0000000000000002220446\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# extract confidence intervals of the coefficients\nconfint(m2.mlr)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                                              2.5 %         97.5 %\n(Intercept)                           92.0121609656 106.2974390344\nstatusSingle                          45.6476377202  66.0594289465\nattractionNotInterested              -57.7640169936 -37.5615830064\nstatusSingle:attractionNotInterested -73.8946826590 -45.0280506744\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# compare baseline with final model\nanova(m0.mlr, m2.mlr)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Variance Table\n\nModel 1: money ~ 1\nModel 2: money ~ (status + attraction)^2\n  Res.Df          RSS Df   Sum of Sq         F                 Pr(>F)    \n1     97 213227.06081                                                    \n2     94  30411.31714  3 182815.7437 188.35839 < 0.000000000000000222 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# compare baseline with final model\nAnova(m0.mlr, m2.mlr, type = \"III\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnova Table (Type III tests)\n\nResponse: money\n                 Sum Sq Df    F value                 Pr(>F)    \n(Intercept) 760953.2107  1 2352.07181 < 0.000000000000000222 ***\nResiduals    30411.3171 94                                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\n### Additional Model Diagnostics{-}\n\nAfter rerunning the regression analysis on the updated data set, we again create diagnostic plots in order to check whether there are potentially problematic data points.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# generate plots\nautoplot(m2.mlr) + \n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](fixreg_files/figure-html/mlr19-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# determine a cutoff for data points that have\n# D-values higher than 4/(n-k-1)\ncutoff <- 4/((nrow(mlrdata)-length(m2.mlr$coefficients)-2))\n# start plotting\npar(mfrow = c(1, 2))           # display plots in 1 row/2 columns\nqqPlot(m2.mlr, main=\"QQ Plot\") # create qq-plot\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n84 88 \n82 86 \n```\n:::\n\n```{.r .cell-code}\nplot(m2.mlr, which=4, cook.levels = cutoff); par(mfrow = c(1, 1))\n```\n\n::: {.cell-output-display}\n![](fixreg_files/figure-html/mlr20-1.png){width=672}\n:::\n:::\n\n\nAlthough the diagnostic plots indicate that additional points may be problematic, but these data points deviate substantially less from the trend than was the case with the data points that have already been removed. To make sure that retaining the data points that are deemed potentially problematic by the diagnostic plots, is acceptable, we extract diagnostic statistics and add them to the data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# add model diagnostics to the data\nmlrdata <- mlrdata %>%\n  dplyr::mutate(residuals = resid(m2.mlr),\n                standardized.residuals = rstandard(m2.mlr),\n                studentized.residuals = rstudent(m2.mlr),\n                cooks.distance = cooks.distance(m2.mlr),\n                dffit = dffits(m2.mlr),\n                leverage = hatvalues(m2.mlr),\n                covariance.ratios = covratio(m2.mlr),\n                fitted = m2.mlr$fitted.values)\n```\n:::\n\n\nWe can now use these diagnostic statistics to create more precise diagnostic plots.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# plot 5\np5 <- ggplot(mlrdata,\n             aes(studentized.residuals)) +\n  theme(legend.position = \"none\")+\n  geom_histogram(aes(y=..density..),\n                 binwidth = .2,\n                 colour=\"black\",\n                 fill=\"gray90\") +\n  labs(x = \"Studentized Residual\", y = \"Density\") +\n  stat_function(fun = dnorm,\n                args = list(mean = mean(mlrdata$studentized.residuals, na.rm = TRUE),\n                            sd = sd(mlrdata$studentized.residuals, na.rm = TRUE)),\n                colour = \"red\", size = 1) +\n  theme_bw(base_size = 8)\n# plot 6\np6 <- ggplot(mlrdata, aes(fitted, studentized.residuals)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", colour = \"Red\")+\n  theme_bw(base_size = 8)+\n  labs(x = \"Fitted Values\",\n       y = \"Studentized Residual\")\n# plot 7\np7 <- qplot(sample = mlrdata$studentized.residuals, stat=\"qq\") +\n  theme_bw(base_size = 8) +\n  labs(x = \"Theoretical Values\",\n       y = \"Observed Values\")\nvip::grid.arrange(p5, p6, p7, nrow = 1)\n```\n\n::: {.cell-output-display}\n![](fixreg_files/figure-html/mlr22-1.png){width=672}\n:::\n:::\n\n\nThe new diagnostic plots do not indicate outliers that require removal. With respect to such data points the following parameters should be considered:\n\n1) Data points with standardized residuals > 3.29 should be removed [@field2012discovering 269]\n\n2) If more than 1 percent of data points have standardized residuals exceeding values > 2.58, then the error rate of the model is unacceptable [@field2012discovering 269].\n\n3) If more than 5 percent of data points have standardized residuals exceeding values   > 1.96, then the error rate of the model is unacceptable [@field2012discovering 269]\n\n4) In addition, data points with Cook's D-values > 1 should be removed [@field2012discovering 269]\n\n5) Also, data points with leverage values higher than $3(k + 1)/N$ or $2(k + 1)/N$ (k = Number of predictors, N = Number of cases in model) should be removed [@field2012discovering 270]\n\n6) There should not be (any) autocorrelation among predictors. This means that independent variables cannot be correlated with itself (for instance, because data points come from the same subject). If there is autocorrelation among predictors, then a Repeated Measures Design or a (hierarchical) mixed-effects model should be implemented instead.\n\n7) Predictors cannot substantially correlate with each other (multicollinearity) (see the [subsection on (multi-)collinearity](https://slcladal.github.io/regression.html#Multicollinearity) in the section of multiple binomial logistic regression for more details about (multi-)collinearity). If a model contains predictors that have variance inflation factors (VIF) > 10 the model is unreliable [@myers1990classical] and predictors causing such VIFs should be removed. Indeed, even VIFs of 2.5 can be problematic [@szmrecsanyi2006morphosyntactic 215] Indeed, @zuur2010protocol propose that variables with VIFs exceeding 3 should be removed! \n\n\n<div class=\"warning\" style='padding:0.1em; background-color:#f2f2f2; color:#51247a'>\n<span>\n<p style='margin-top:1em; text-align:center'>\n<b>NOTE</b><br><br>However, (multi-)collinearity is only an issue if one is interested in interpreting regression results!  If the interpretation is irrelevant because what is relevant is prediction(!), then it does not matter if the model contains collinear predictors! See @gries2021statistics for a more elaborate explanation.</p>\n<p style='margin-left:1em;'>\n</p></span>\n</div>\n\n<br>\n\n8) The mean value of VIFs should be ~ 1 [@bowerman1990linear].\n\nThe following code chunk evaluates these criteria.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# 1: optimal = 0\n# (listed data points should be removed)\nwhich(mlrdata$standardized.residuals > 3.29)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nnamed integer(0)\n```\n:::\n\n```{.r .cell-code}\n# 2: optimal = 1\n# (listed data points should be removed)\nstdres_258 <- as.vector(sapply(mlrdata$standardized.residuals, function(x) {\nifelse(sqrt((x^2)) > 2.58, 1, 0) } ))\n(sum(stdres_258) / length(stdres_258)) * 100\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0\n```\n:::\n\n```{.r .cell-code}\n# 3: optimal = 5\n# (listed data points should be removed)\nstdres_196 <- as.vector(sapply(mlrdata$standardized.residuals, function(x) {\nifelse(sqrt((x^2)) > 1.96, 1, 0) } ))\n(sum(stdres_196) / length(stdres_196)) * 100\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 6.12244897959\n```\n:::\n\n```{.r .cell-code}\n# 4: optimal = 0\n# (listed data points should be removed)\nwhich(mlrdata$cooks.distance > 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nnamed integer(0)\n```\n:::\n\n```{.r .cell-code}\n# 5: optimal = 0\n# (data points should be removed if cooks distance is close to 1)\nwhich(mlrdata$leverage >= (3*mean(mlrdata$leverage)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nnamed integer(0)\n```\n:::\n\n```{.r .cell-code}\n# 6: checking autocorrelation:\n# Durbin-Watson test (optimal: high p-value)\ndwt(m2.mlr)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n lag  Autocorrelation D-W Statistic p-value\n   1 -0.0143324675649  1.9680423527   0.622\n Alternative hypothesis: rho != 0\n```\n:::\n\n```{.r .cell-code}\n# 7: test multicollinearity 1\nvif(m2.mlr)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                        statusSingle              attractionNotInterested \n                                2.00                                 1.96 \nstatusSingle:attractionNotInterested \n                                2.96 \n```\n:::\n\n```{.r .cell-code}\n# 8: test multicollinearity 2\n1/vif(m2.mlr)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                        statusSingle              attractionNotInterested \n                      0.500000000000                       0.510204081633 \nstatusSingle:attractionNotInterested \n                      0.337837837838 \n```\n:::\n\n```{.r .cell-code}\n# 9: mean vif should not exceed 1\nmean(vif(m2.mlr))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2.30666666667\n```\n:::\n:::\n\n\nExcept for the mean VIF value (2.307) which should not exceed 1, all diagnostics are acceptable. We will now test whether the sample size is sufficient for our model. With respect to the minimal sample size and based on @green1991many, @field2012discovering[273-274] offer the following rules of thumb for an adequate sample size (k = number of predictors; categorical predictors with more than two levels should be recoded as dummy variables):\n\n* if you are interested in the overall model: 50 + 8k (k = number of predictors)\n\n* if you are interested in individual predictors: 104 + k\n\n* if you are interested in both: take the higher value!\n\n### Evaluation of Sample Size{-}\n\nAfter performing the diagnostics, we will now test whether the sample size is adequate and what the values of R would be based on a random distribution in order to be able to estimate how likely a $\\beta$-error is given the present sample size [see @field2012discovering 274]. Beta errors (or $\\beta$-errors) refer to the erroneous assumption that a predictor is not significant (based on the analysis and given the sample) although it does have an effect in the population. In other words, $\\beta$-error means to overlook a significant effect because of weaknesses of the analysis. The test statistics ranges between 0 and 1 where lower values are better. If the values approximate 1, then there is serious concern as the model is not reliable given the sample size. In such cases, unfortunately, the best option is to increase the sample size.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# load functions\nsource(\"https://slcladal.github.io/rscripts/SampleSizeMLR.r\")\nsource(\"https://slcladal.github.io/rscripts/ExpR.r\")\n# check if sample size is sufficient\nsmplesz(m2.mlr)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Sample too small: please increase your sample by  9  data points\"\n```\n:::\n\n```{.r .cell-code}\n# check beta-error likelihood\nexpR(m2.mlr)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Based on the sample size expect a false positive correlation of 0.0309 between the predictors and the predicted\"\n```\n:::\n:::\n\n\nThe function `smplesz` reports that the sample size is insufficient by 9 data points according to @green1991many. The likelihood of $\\beta$-errors, however, is very small (0.0309). As a last step, we summarize the results of the regression analysis.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# tabulate model results\nsjPlot::tab_model(m0.glm, m2.glm)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table style=\"border-collapse:collapse; border:none;\">\n<tr>\n<th style=\"border-top: double; text-align:center; font-style:normal; font-weight:bold; padding:0.2cm;  text-align:left; \">&nbsp;</th>\n<th colspan=\"3\" style=\"border-top: double; text-align:center; font-style:normal; font-weight:bold; padding:0.2cm; \">money</th>\n<th colspan=\"3\" style=\"border-top: double; text-align:center; font-style:normal; font-weight:bold; padding:0.2cm; \">money</th>\n</tr>\n<tr>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  text-align:left; \">Predictors</td>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  \">Estimates</td>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  \">CI</td>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  \">p</td>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  \">Estimates</td>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  \">CI</td>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  col7\">p</td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; \">(Intercept)</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">88.12</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">78.72&nbsp;&ndash;&nbsp;97.52</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"><strong>&lt;0.001</strong></td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">99.15</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">92.10&nbsp;&ndash;&nbsp;106.21</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  col7\"><strong>&lt;0.001</strong></td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; \">status [Single]</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"></td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"></td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"></td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">55.85</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">45.78&nbsp;&ndash;&nbsp;65.93</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  col7\"><strong>&lt;0.001</strong></td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; \">attraction<br>[NotInterested]</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"></td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"></td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"></td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">&#45;47.66</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">&#45;57.63&nbsp;&ndash;&nbsp;-37.69</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  col7\"><strong>&lt;0.001</strong></td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; \">status [Single] *<br>attraction<br>[NotInterested]</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"></td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"></td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"></td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">&#45;59.46</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">&#45;73.71&nbsp;&ndash;&nbsp;-45.21</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  col7\"><strong>&lt;0.001</strong></td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; padding-top:0.1cm; padding-bottom:0.1cm; border-top:1px solid;\">Observations</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left; border-top:1px solid;\" colspan=\"3\">98</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left; border-top:1px solid;\" colspan=\"3\">98</td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; padding-top:0.1cm; padding-bottom:0.1cm;\">R<sup>2</sup></td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left;\" colspan=\"3\">0.000</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left;\" colspan=\"3\">0.857</td>\n</tr>\n\n</table>\n\n`````\n:::\n:::\n\n\n\nAdditionally, we can inspect the summary of the regression model as shown below to extract additional information. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(m2.mlr)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = money ~ (status + attraction)^2, data = mlrdata)\n\nResiduals:\n         Min           1Q       Median           3Q          Max \n-35.76416667 -13.50520000  -0.98948333  10.59887500  38.77166667 \n\nCoefficients:\n                                         Estimate   Std. Error  t value\n(Intercept)                           99.15480000   3.59735820 27.56323\nstatusSingle                          55.85353333   5.14015367 10.86612\nattractionNotInterested              -47.66280000   5.08743275 -9.36873\nstatusSingle:attractionNotInterested -59.46136667   7.26927504 -8.17982\n                                                   Pr(>|t|)    \n(Intercept)                          < 0.000000000000000222 ***\nstatusSingle                         < 0.000000000000000222 ***\nattractionNotInterested               0.0000000000000040429 ***\nstatusSingle:attractionNotInterested  0.0000000000013375166 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 17.986791 on 94 degrees of freedom\nMultiple R-squared:  0.857375902,\tAdjusted R-squared:  0.852824069 \nF-statistic: 188.358387 on 3 and 94 DF,  p-value: < 0.0000000000000002220446\n```\n:::\n:::\n\n\nAlthough @field2012discovering suggest that the main effects of the predictors involved in the interaction should not be interpreted, they are interpreted here to illustrate how the results of a multiple linear regression can be reported. \n\n\nWe can use the `reports` package [@report] to summarize the analysis.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nreport::report(m2.mlr)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nWe fitted a linear model (estimated using OLS) to predict money with status and attraction (formula: money ~ (status + attraction)^2). The model explains a statistically significant and substantial proportion of variance (R2 = 0.86, F(3, 94) = 188.36, p < .001, adj. R2 = 0.85). The model's intercept, corresponding to status = Relationship and attraction = Interested, is at 99.15 (95% CI [92.01, 106.30], t(94) = 27.56, p < .001). Within this model:\n\n  - The effect of status [Single] is statistically significant and positive (beta = 55.85, 95% CI [45.65, 66.06], t(94) = 10.87, p < .001; Std. beta = 1.19, 95% CI [0.97, 1.41])\n  - The effect of attraction [NotInterested] is statistically significant and negative (beta = -47.66, 95% CI [-57.76, -37.56], t(94) = -9.37, p < .001; Std. beta = -1.02, 95% CI [-1.23, -0.80])\n  - The interaction effect of attraction [NotInterested] on status [Single] is statistically significant and negative (beta = -59.46, 95% CI [-73.89, -45.03], t(94) = -8.18, p < .001; Std. beta = -1.27, 95% CI [-1.58, -0.96])\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using the Wald approximation.\n```\n:::\n:::\n\n\nWe can use this output to write up a final report: \n\n\nA multiple linear regression was fitted to the data using an automated, step-wise, AIC-based (Akaike's Information Criterion) procedure. The model fitting arrived at a final minimal model. During the model diagnostics, two outliers were detected and removed. Further diagnostics did not find other issues after the removal.\n\nThe final minimal adequate regression model is based on 98 data points and performs highly significantly better than a minimal baseline model (multiple R^2^: .857, adjusted R^2^: .853, F-statistic (3, 94): 154.4, AIC: 850.4, BIC: 863.32, p<.001^$***$^). The final minimal adequate regression model reports *attraction* and *status* as significant main effects. The relationship status of men correlates highly significantly and positively with the amount of money spend on the women's presents (SE: 5.14, t-value: 10.87, p<.001^$***$^). This shows that men spend 156.8 dollars on presents if they are single while they spend 99,15 dollars if they are in a relationship. Whether men are attracted to women also correlates highly significantly and positively with the money they spend on women (SE: 5.09, t-values: -9.37, p<.001^$***$^). If men are not interested in women, they spend 47.66 dollar less on a present for women compared with women the men are interested in.\n\nFurthermore, the final minimal adequate regression model reports a highly significant interaction between relationship *status* and *attraction* (SE: 7.27, t-value: -8.18, p<.001^$***$^): If men are single but they are not interested in a women, a man would spend only 59.46 dollars on a present compared to all other constellations.\n\n\n## Robust Regression{-}\n\nRobust regression represents an alternative to linear regression models when the data contains outliers that should not be removed. As such, robust regressions can handle overly influential data points (outliers) and they allow us to retain outliers rather than removing them by adding weights [@rousseeuw2005robust]. Thus, robust regressions are used when there are outliers present in the data and we can thus not use traditional models but we do not have a good reason for removing the outliers. \n\n\n> Robust regressions allow us to handle overly influential data points (outliers) by using weights. Thus, robust regressions enable us to retain all data points.\n\n\nWe begin by loading a data set (the mlrdata set which have used for multiple linear regression).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# load data\nrobustdata  <- base::readRDS(url(\"https://slcladal.github.io/data/mld.rda\", \"rb\"))\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n```{=html}\n<div class=\"tabwid\"><style>.cl-4e542942{table-layout:auto;width:75%;}.cl-4e50c8ce{font-family:'DejaVu Sans';font-size:12pt;font-weight:bold;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-4e50c8d8{font-family:'DejaVu Sans';font-size:12pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-4e50d58a{margin:0;text-align:center;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-4e50d594{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-4e510870{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-4e51087a{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-4e51087b{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-4e510884{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-4e5108c0{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-4e5108c1{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-4e5108ca{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-4e5108d4{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-4e5108d5{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-4e5108d6{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-4e5108de{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-4e5108df{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}</style><table class='cl-4e542942'>\n```\n<caption class=\"Table Caption\">\n\nFirst 15 rows of the robustdata.\n\n</caption>\n```{=html}\n<thead><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-4e5108de\"><p class=\"cl-4e50d58a\"><span class=\"cl-4e50c8ce\">status</span></p></td><td class=\"cl-4e5108d6\"><p class=\"cl-4e50d58a\"><span class=\"cl-4e50c8ce\">attraction</span></p></td><td class=\"cl-4e5108df\"><p class=\"cl-4e50d594\"><span class=\"cl-4e50c8ce\">money</span></p></td></tr></thead><tbody><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-4e51087b\"><p class=\"cl-4e50d58a\"><span class=\"cl-4e50c8d8\">Relationship</span></p></td><td class=\"cl-4e510870\"><p class=\"cl-4e50d58a\"><span class=\"cl-4e50c8d8\">NotInterested</span></p></td><td class=\"cl-4e51087a\"><p class=\"cl-4e50d594\"><span class=\"cl-4e50c8d8\">86.33</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-4e5108c1\"><p class=\"cl-4e50d58a\"><span class=\"cl-4e50c8d8\">Relationship</span></p></td><td class=\"cl-4e510884\"><p class=\"cl-4e50d58a\"><span class=\"cl-4e50c8d8\">NotInterested</span></p></td><td class=\"cl-4e5108c0\"><p class=\"cl-4e50d594\"><span class=\"cl-4e50c8d8\">45.58</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-4e51087b\"><p class=\"cl-4e50d58a\"><span class=\"cl-4e50c8d8\">Relationship</span></p></td><td class=\"cl-4e510870\"><p class=\"cl-4e50d58a\"><span class=\"cl-4e50c8d8\">NotInterested</span></p></td><td class=\"cl-4e51087a\"><p class=\"cl-4e50d594\"><span class=\"cl-4e50c8d8\">68.43</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-4e5108c1\"><p class=\"cl-4e50d58a\"><span class=\"cl-4e50c8d8\">Relationship</span></p></td><td class=\"cl-4e510884\"><p class=\"cl-4e50d58a\"><span class=\"cl-4e50c8d8\">NotInterested</span></p></td><td class=\"cl-4e5108c0\"><p class=\"cl-4e50d594\"><span class=\"cl-4e50c8d8\">52.93</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-4e51087b\"><p class=\"cl-4e50d58a\"><span class=\"cl-4e50c8d8\">Relationship</span></p></td><td class=\"cl-4e510870\"><p class=\"cl-4e50d58a\"><span class=\"cl-4e50c8d8\">NotInterested</span></p></td><td class=\"cl-4e51087a\"><p class=\"cl-4e50d594\"><span class=\"cl-4e50c8d8\">61.86</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-4e5108c1\"><p class=\"cl-4e50d58a\"><span class=\"cl-4e50c8d8\">Relationship</span></p></td><td class=\"cl-4e510884\"><p class=\"cl-4e50d58a\"><span class=\"cl-4e50c8d8\">NotInterested</span></p></td><td class=\"cl-4e5108c0\"><p class=\"cl-4e50d594\"><span class=\"cl-4e50c8d8\">48.47</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-4e51087b\"><p class=\"cl-4e50d58a\"><span class=\"cl-4e50c8d8\">Relationship</span></p></td><td class=\"cl-4e510870\"><p class=\"cl-4e50d58a\"><span class=\"cl-4e50c8d8\">NotInterested</span></p></td><td class=\"cl-4e51087a\"><p class=\"cl-4e50d594\"><span class=\"cl-4e50c8d8\">32.79</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-4e5108c1\"><p class=\"cl-4e50d58a\"><span class=\"cl-4e50c8d8\">Relationship</span></p></td><td class=\"cl-4e510884\"><p class=\"cl-4e50d58a\"><span class=\"cl-4e50c8d8\">NotInterested</span></p></td><td class=\"cl-4e5108c0\"><p class=\"cl-4e50d594\"><span class=\"cl-4e50c8d8\">35.91</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-4e51087b\"><p class=\"cl-4e50d58a\"><span class=\"cl-4e50c8d8\">Relationship</span></p></td><td class=\"cl-4e510870\"><p class=\"cl-4e50d58a\"><span class=\"cl-4e50c8d8\">NotInterested</span></p></td><td class=\"cl-4e51087a\"><p class=\"cl-4e50d594\"><span class=\"cl-4e50c8d8\">30.98</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-4e5108c1\"><p class=\"cl-4e50d58a\"><span class=\"cl-4e50c8d8\">Relationship</span></p></td><td class=\"cl-4e510884\"><p class=\"cl-4e50d58a\"><span class=\"cl-4e50c8d8\">NotInterested</span></p></td><td class=\"cl-4e5108c0\"><p class=\"cl-4e50d594\"><span class=\"cl-4e50c8d8\">44.82</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-4e51087b\"><p class=\"cl-4e50d58a\"><span class=\"cl-4e50c8d8\">Relationship</span></p></td><td class=\"cl-4e510870\"><p class=\"cl-4e50d58a\"><span class=\"cl-4e50c8d8\">NotInterested</span></p></td><td class=\"cl-4e51087a\"><p class=\"cl-4e50d594\"><span class=\"cl-4e50c8d8\">35.05</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-4e5108c1\"><p class=\"cl-4e50d58a\"><span class=\"cl-4e50c8d8\">Relationship</span></p></td><td class=\"cl-4e510884\"><p class=\"cl-4e50d58a\"><span class=\"cl-4e50c8d8\">NotInterested</span></p></td><td class=\"cl-4e5108c0\"><p class=\"cl-4e50d594\"><span class=\"cl-4e50c8d8\">64.49</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-4e51087b\"><p class=\"cl-4e50d58a\"><span class=\"cl-4e50c8d8\">Relationship</span></p></td><td class=\"cl-4e510870\"><p class=\"cl-4e50d58a\"><span class=\"cl-4e50c8d8\">NotInterested</span></p></td><td class=\"cl-4e51087a\"><p class=\"cl-4e50d594\"><span class=\"cl-4e50c8d8\">54.50</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-4e5108c1\"><p class=\"cl-4e50d58a\"><span class=\"cl-4e50c8d8\">Relationship</span></p></td><td class=\"cl-4e510884\"><p class=\"cl-4e50d58a\"><span class=\"cl-4e50c8d8\">NotInterested</span></p></td><td class=\"cl-4e5108c0\"><p class=\"cl-4e50d594\"><span class=\"cl-4e50c8d8\">61.48</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-4e5108d5\"><p class=\"cl-4e50d58a\"><span class=\"cl-4e50c8d8\">Relationship</span></p></td><td class=\"cl-4e5108d4\"><p class=\"cl-4e50d58a\"><span class=\"cl-4e50c8d8\">NotInterested</span></p></td><td class=\"cl-4e5108ca\"><p class=\"cl-4e50d594\"><span class=\"cl-4e50c8d8\">55.51</span></p></td></tr></tbody></table></div>\n```\n:::\n:::\n\n\nWe first fit an ordinary linear model (and although we know from the section on multiple regression that the interaction between status and attraction is significant, we will disregard this for now as this will help to explain the weighing procedure which is the focus of this section).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# create model\nslm <- lm(money ~ status+attraction, data = robustdata)\n# inspect model\nsummary(slm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = money ~ status + attraction, data = robustdata)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-60.87070 -15.78645  -2.61010  13.88770  59.93710 \n\nCoefficients:\n                            Estimate   Std. Error   t value\n(Intercept)             114.94950000   4.28993616  26.79515\nstatusSingle             26.10340000   4.95359160   5.26959\nattractionNotInterested -79.25220000   4.95359160 -15.99894\n                                      Pr(>|t|)    \n(Intercept)             < 0.000000000000000222 ***\nstatusSingle                     0.00000082576 ***\nattractionNotInterested < 0.000000000000000222 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 24.767958 on 97 degrees of freedom\nMultiple R-squared:  0.745229335,\tAdjusted R-squared:  0.739976331 \nF-statistic: 141.867286 on 2 and 97 DF,  p-value: < 0.0000000000000002220446\n```\n:::\n:::\n\n\nWe now check whether the model is well fitted using diagnostic plots.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# generate plots\nautoplot(slm) + \n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) \n```\n\n::: {.cell-output-display}\n![](fixreg_files/figure-html/rr4-1.png){width=672}\n:::\n:::\n\n\n\nThe diagnostic plots indicate that there are three outliers in the data (data points 52, 83 and possibly 64). Therefore, we need to evaluate if the outliers severely affect the fit of the model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrobustdata[c(52, 64, 83),]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   status    attraction  money\n52 Single NotInterested   0.93\n64 Single NotInterested  84.28\n83 Single    Interested 200.99\n```\n:::\n:::\n\n\nWe can now calculate Cook's distance and standardized residuals check if the values of the potentially problematic points have unacceptably high values (-2 < ok < 2).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nCooksDistance <- cooks.distance(slm)\nStandardizedResiduals <- stdres(slm)\na <- cbind(robustdata, CooksDistance, StandardizedResiduals)\na[CooksDistance > 4/100, ]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n         status    attraction  money   CooksDistance StandardizedResiduals\n1  Relationship NotInterested  86.33 0.0444158829857         2.07565427025\n52       Single NotInterested   0.93 0.0641937458854        -2.49535435377\n65       Single NotInterested  12.12 0.0427613629698        -2.03662765573\n67       Single NotInterested  13.28 0.0407877963315        -1.98907421786\n83       Single    Interested 200.99 0.0622397126545         2.45708203516\n84       Single    Interested 193.69 0.0480020776213         2.15782333134\n88       Single    Interested 193.78 0.0481663678409         2.16151282221\n```\n:::\n:::\n\n\nWe will calculate the absolute value and reorder the table so that it is easier to check the values.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nAbsoluteStandardizedResiduals <- abs(StandardizedResiduals)\na <- cbind(robustdata, CooksDistance, StandardizedResiduals, AbsoluteStandardizedResiduals)\nasorted <- a[order(-AbsoluteStandardizedResiduals), ]\nasorted[1:10, ]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n         status    attraction  money   CooksDistance StandardizedResiduals\n52       Single NotInterested   0.93 0.0641937458854        -2.49535435377\n83       Single    Interested 200.99 0.0622397126545         2.45708203516\n88       Single    Interested 193.78 0.0481663678409         2.16151282221\n84       Single    Interested 193.69 0.0480020776213         2.15782333134\n1  Relationship NotInterested  86.33 0.0444158829857         2.07565427025\n65       Single NotInterested  12.12 0.0427613629698        -2.03662765573\n67       Single NotInterested  13.28 0.0407877963315        -1.98907421786\n78       Single    Interested 188.76 0.0394313968241         1.95572122040\n21 Relationship NotInterested  81.90 0.0369837409025         1.89404933081\n24 Relationship NotInterested  81.56 0.0364414260634         1.88011125419\n   AbsoluteStandardizedResiduals\n52                 2.49535435377\n83                 2.45708203516\n88                 2.16151282221\n84                 2.15782333134\n1                  2.07565427025\n65                 2.03662765573\n67                 1.98907421786\n78                 1.95572122040\n21                 1.89404933081\n24                 1.88011125419\n```\n:::\n:::\n\n\nAs Cook's distance and the standardized residuals do have unacceptable values, we re-calculate the linear model as a robust regression and inspect the results\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# create robust regression model\nrmodel <- robustbase::lmrob(money ~ status + attraction, data = robustdata)\n# inspect model\nsummary(rmodel)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nrobustbase::lmrob(formula = money ~ status + attraction, data = robustdata)\n \\--> method = \"MM\"\nResiduals:\n         Min           1Q       Median           3Q          Max \n-61.14269797 -15.20405742  -1.48712073  14.43502548  62.42342866 \n\nCoefficients:\n                            Estimate   Std. Error   t value\n(Intercept)             113.18405742   3.89777743  29.03810\nstatusSingle             25.38251392   5.08841085   4.98830\nattractionNotInterested -76.49387337   5.06626478 -15.09867\n                                      Pr(>|t|)    \n(Intercept)             < 0.000000000000000222 ***\nstatusSingle                      0.0000026725 ***\nattractionNotInterested < 0.000000000000000222 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nRobust residual standard error: 22.349751 \nMultiple R-squared:  0.740716956,\tAdjusted R-squared:  0.735370914 \nConvergence in 11 IRWLS iterations\n\nRobustness weights: \n 10 weights are ~= 1. The remaining 90 ones are summarized as\n       Min.     1st Qu.      Median        Mean     3rd Qu.        Max. \n0.415507115 0.856134372 0.947485761 0.889078637 0.986192098 0.998890516 \nAlgorithmic parameters: \n           tuning.chi                    bb            tuning.psi \n1.5476399999999999046 0.5000000000000000000 4.6850610000000001421 \n           refine.tol               rel.tol             scale.tol \n0.0000001000000000000 0.0000001000000000000 0.0000000001000000000 \n            solve.tol           eps.outlier                 eps.x \n0.0000001000000000000 0.0010000000000000000 0.0000000000018189894 \n    warn.limit.reject     warn.limit.meanrw \n0.5000000000000000000 0.5000000000000000000 \n     nResample         max.it       best.r.s       k.fast.s          k.max \n           500             50              2              1            200 \n   maxit.scale      trace.lev            mts     compute.rd fast.s.large.n \n           200              0           1000              0           2000 \n                  psi           subsampling                   cov \n           \"bisquare\"         \"nonsingular\"         \".vcov.avar1\" \ncompute.outlier.stats \n                 \"SM\" \nseed : int(0) \n```\n:::\n:::\n\n\n\nThe output shows that both status and attraction are significant but, as we have seen above, the effect that really matters is the interaction between status and attraction. \n\nWe will briefly check the weights to understand the process of weighing better. The idea of weighing is to downgrade data points that are too influential while not punishing data points that have a good fit and are thus less influential. This means that the problematic data points should have lower weights than other data points (the maximum is 1 - so points can only be made \"lighter\"). \n\n\n::: {.cell}\n\n```{.r .cell-code}\nhweights <- data.frame(status = robustdata$status, resid = rmodel$resid, weight = rmodel$rweights)\nhweights2 <- hweights[order(rmodel$rweights), ]\nhweights2[1:15, ]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n         status          resid         weight\n83       Single  62.4234286579 0.415507114929\n52       Single -61.1426979713 0.434323488879\n88       Single  55.2134286579 0.521220438134\n84       Single  55.1234286579 0.522529018160\n78       Single  50.1934286579 0.593234264142\n65       Single -49.9526979713 0.596626236587\n1  Relationship  49.6398159519 0.601024799801\n67       Single -48.7926979713 0.612874510784\n21 Relationship  45.2098159519 0.661914443015\n24 Relationship  44.8698159519 0.666467525203\n39 Relationship -43.8940574189 0.679427923638\n79       Single  40.8234286579 0.719104302439\n58       Single -40.5226979713 0.722893398846\n89       Single  39.9734286579 0.729766935693\n95       Single  39.8234286579 0.731633318260\n```\n:::\n:::\n\n\nThe values of the weights support our assumption that those data points that were deemed too influential are made *lighter* as they now only have weights of 0.415507114929 and 0.434323488879 respectively. This was, however, not the focus of this sections as this section merely served to introduce the concept of weights and how they can be used in the context of a robust linear regression.\n\n\n## Logistic Regression{-}\n\nLogistic regression is a multivariate analysis technique that builds on and is very similar in terms of its implementation to linear regression but logistic regressions take dependent variables that represent nominal rather than numeric scaling [@harrell2015regression]. The difference requires that the linear regression must be modified in certain ways to avoid producing non-sensical outcomes. The most fundamental difference between logistic and linear regressions is that logistic regression work on the probabilities of an outcome (the likelihood), rather than the outcome itself. In addition, the likelihoods on which the logistic regression works must be logged (logarithmized) in order to avoid produce predictions that produce values greater than 1 (instance occurs) and 0 (instance does not occur). You can check this by logging the values from -10 to 10 using the `plogis` function as shown below.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nround(plogis(-10:10), 5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] 0.00005 0.00012 0.00034 0.00091 0.00247 0.00669 0.01799 0.04743 0.11920\n[10] 0.26894 0.50000 0.73106 0.88080 0.95257 0.98201 0.99331 0.99753 0.99909\n[19] 0.99966 0.99988 0.99995\n```\n:::\n:::\n\n\nIf we visualize these logged values, we get an S-shaped curve which reflects the logistic function.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](fixreg_files/figure-html/blm0b-1.png){width=672}\n:::\n:::\n\n\n\nTo understand what this mean, we will use a very simple example. In this example, we want to see whether the height of men affect their likelihood of being in a relationship. The data we use represents a data set consisting of two variables: height and relationship.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](fixreg_files/figure-html/blm1-1.png){width=672}\n:::\n:::\n\n\nThe left panel of the Figure above shows that a linear model would predict values for the relationship status, which represents a factor (0 = Single and 1 = In a Relationship), that are nonsensical because values above 1 or below 0 do not make sense. In contrast to a linear regression, which predicts actual values, such as the frequencies of prepositions in a certain text, a logistic regression predicts *probabilities* of events (for example, being in a relationship) rather than actual values. The center panel shows the predictions of a logistic regression and we see that a logistic regression also has an intercept and a (very steep) slope but that the regression line also predicts values that are above 1 and below 0. However, when we log the predicted values we these predicted values are transformed into probabilities with values between 0 and 1. And the logged regression line has a S-shape which reflects the logistic function. Furthermore, we can then find the optimal line (the line with the lowest residual deviance) by comparing the sum of residuals - just as we did for a simple linear model and that way, we find the regression line for a logistic regression. \n\n### Example: EH in Kiwi English{-}\n\nTo exemplify how to implement a logistic regression in R [see @agresti1996introduction; @agresti2011categorical] for very good and thorough introductions to this topic], we will analyze the use of the discourse particle *eh* in New Zealand English and test which factors correlate with its occurrence. The data set represents speech units in a corpus that were coded for the speaker who uttered a given speech unit, the gender, ethnicity, and age of that speaker and whether or not the speech unit contained an *eh*. To begin with, we clean the current work space, set option, install and activate relevant packages, load customized functions, and load the example data set.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# load data\nblrdata  <- base::readRDS(url(\"https://slcladal.github.io/data/bld.rda\", \"rb\"))\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n```{=html}\n<div class=\"tabwid\"><style>.cl-4fa831da{table-layout:auto;width:75%;}.cl-4fa2c650{font-family:'DejaVu Sans';font-size:12pt;font-weight:bold;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-4fa2c66e{font-family:'DejaVu Sans';font-size:12pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-4fa2dc62{margin:0;text-align:center;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-4fa2dc6c{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-4fa32cee{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-4fa32d02{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-4fa32d03{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-4fa32d0c{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-4fa32d16{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-4fa32d20{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-4fa32d21{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-4fa32d2a{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-4fa32d34{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-4fa32d3e{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-4fa32d3f{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-4fa32d48{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}</style><table class='cl-4fa831da'>\n```\n<caption class=\"Table Caption\">\n\nFirst 15 rows of the blrdata.\n\n</caption>\n```{=html}\n<thead><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-4fa32d3e\"><p class=\"cl-4fa2dc62\"><span class=\"cl-4fa2c650\">ID</span></p></td><td class=\"cl-4fa32d3f\"><p class=\"cl-4fa2dc62\"><span class=\"cl-4fa2c650\">Gender</span></p></td><td class=\"cl-4fa32d3f\"><p class=\"cl-4fa2dc62\"><span class=\"cl-4fa2c650\">Age</span></p></td><td class=\"cl-4fa32d3f\"><p class=\"cl-4fa2dc62\"><span class=\"cl-4fa2c650\">Ethnicity</span></p></td><td class=\"cl-4fa32d48\"><p class=\"cl-4fa2dc6c\"><span class=\"cl-4fa2c650\">EH</span></p></td></tr></thead><tbody><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-4fa32d03\"><p class=\"cl-4fa2dc62\"><span class=\"cl-4fa2c66e\">&lt;S1A-001#M&gt;</span></p></td><td class=\"cl-4fa32d02\"><p class=\"cl-4fa2dc62\"><span class=\"cl-4fa2c66e\">Men</span></p></td><td class=\"cl-4fa32d02\"><p class=\"cl-4fa2dc62\"><span class=\"cl-4fa2c66e\">Young</span></p></td><td class=\"cl-4fa32d02\"><p class=\"cl-4fa2dc62\"><span class=\"cl-4fa2c66e\">Pakeha</span></p></td><td class=\"cl-4fa32cee\"><p class=\"cl-4fa2dc6c\"><span class=\"cl-4fa2c66e\">0</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-4fa32d20\"><p class=\"cl-4fa2dc62\"><span class=\"cl-4fa2c66e\">&lt;S1A-001#M&gt;</span></p></td><td class=\"cl-4fa32d0c\"><p class=\"cl-4fa2dc62\"><span class=\"cl-4fa2c66e\">Men</span></p></td><td class=\"cl-4fa32d0c\"><p class=\"cl-4fa2dc62\"><span class=\"cl-4fa2c66e\">Young</span></p></td><td class=\"cl-4fa32d0c\"><p class=\"cl-4fa2dc62\"><span class=\"cl-4fa2c66e\">Pakeha</span></p></td><td class=\"cl-4fa32d16\"><p class=\"cl-4fa2dc6c\"><span class=\"cl-4fa2c66e\">1</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-4fa32d03\"><p class=\"cl-4fa2dc62\"><span class=\"cl-4fa2c66e\">&lt;S1A-001#M&gt;</span></p></td><td class=\"cl-4fa32d02\"><p class=\"cl-4fa2dc62\"><span class=\"cl-4fa2c66e\">Men</span></p></td><td class=\"cl-4fa32d02\"><p class=\"cl-4fa2dc62\"><span class=\"cl-4fa2c66e\">Young</span></p></td><td class=\"cl-4fa32d02\"><p class=\"cl-4fa2dc62\"><span class=\"cl-4fa2c66e\">Pakeha</span></p></td><td class=\"cl-4fa32cee\"><p class=\"cl-4fa2dc6c\"><span class=\"cl-4fa2c66e\">0</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-4fa32d20\"><p class=\"cl-4fa2dc62\"><span class=\"cl-4fa2c66e\">&lt;S1A-001#M&gt;</span></p></td><td class=\"cl-4fa32d0c\"><p class=\"cl-4fa2dc62\"><span class=\"cl-4fa2c66e\">Men</span></p></td><td class=\"cl-4fa32d0c\"><p class=\"cl-4fa2dc62\"><span class=\"cl-4fa2c66e\">Young</span></p></td><td class=\"cl-4fa32d0c\"><p class=\"cl-4fa2dc62\"><span class=\"cl-4fa2c66e\">Pakeha</span></p></td><td class=\"cl-4fa32d16\"><p class=\"cl-4fa2dc6c\"><span class=\"cl-4fa2c66e\">0</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-4fa32d03\"><p class=\"cl-4fa2dc62\"><span class=\"cl-4fa2c66e\">&lt;S1A-001#M&gt;</span></p></td><td class=\"cl-4fa32d02\"><p class=\"cl-4fa2dc62\"><span class=\"cl-4fa2c66e\">Men</span></p></td><td class=\"cl-4fa32d02\"><p class=\"cl-4fa2dc62\"><span class=\"cl-4fa2c66e\">Young</span></p></td><td class=\"cl-4fa32d02\"><p class=\"cl-4fa2dc62\"><span class=\"cl-4fa2c66e\">Pakeha</span></p></td><td class=\"cl-4fa32cee\"><p class=\"cl-4fa2dc6c\"><span class=\"cl-4fa2c66e\">1</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-4fa32d20\"><p class=\"cl-4fa2dc62\"><span class=\"cl-4fa2c66e\">&lt;S1A-001#M&gt;</span></p></td><td class=\"cl-4fa32d0c\"><p class=\"cl-4fa2dc62\"><span class=\"cl-4fa2c66e\">Men</span></p></td><td class=\"cl-4fa32d0c\"><p class=\"cl-4fa2dc62\"><span class=\"cl-4fa2c66e\">Young</span></p></td><td class=\"cl-4fa32d0c\"><p class=\"cl-4fa2dc62\"><span class=\"cl-4fa2c66e\">Pakeha</span></p></td><td class=\"cl-4fa32d16\"><p class=\"cl-4fa2dc6c\"><span class=\"cl-4fa2c66e\">1</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-4fa32d03\"><p class=\"cl-4fa2dc62\"><span class=\"cl-4fa2c66e\">&lt;S1A-001#M&gt;</span></p></td><td class=\"cl-4fa32d02\"><p class=\"cl-4fa2dc62\"><span class=\"cl-4fa2c66e\">Men</span></p></td><td class=\"cl-4fa32d02\"><p class=\"cl-4fa2dc62\"><span class=\"cl-4fa2c66e\">Young</span></p></td><td class=\"cl-4fa32d02\"><p class=\"cl-4fa2dc62\"><span class=\"cl-4fa2c66e\">Pakeha</span></p></td><td class=\"cl-4fa32cee\"><p class=\"cl-4fa2dc6c\"><span class=\"cl-4fa2c66e\">0</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-4fa32d20\"><p class=\"cl-4fa2dc62\"><span class=\"cl-4fa2c66e\">&lt;S1A-001#M&gt;</span></p></td><td class=\"cl-4fa32d0c\"><p class=\"cl-4fa2dc62\"><span class=\"cl-4fa2c66e\">Men</span></p></td><td class=\"cl-4fa32d0c\"><p class=\"cl-4fa2dc62\"><span class=\"cl-4fa2c66e\">Young</span></p></td><td class=\"cl-4fa32d0c\"><p class=\"cl-4fa2dc62\"><span class=\"cl-4fa2c66e\">Pakeha</span></p></td><td class=\"cl-4fa32d16\"><p class=\"cl-4fa2dc6c\"><span class=\"cl-4fa2c66e\">0</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-4fa32d03\"><p class=\"cl-4fa2dc62\"><span class=\"cl-4fa2c66e\">&lt;S1A-001#M&gt;</span></p></td><td class=\"cl-4fa32d02\"><p class=\"cl-4fa2dc62\"><span class=\"cl-4fa2c66e\">Men</span></p></td><td class=\"cl-4fa32d02\"><p class=\"cl-4fa2dc62\"><span class=\"cl-4fa2c66e\">Young</span></p></td><td class=\"cl-4fa32d02\"><p class=\"cl-4fa2dc62\"><span class=\"cl-4fa2c66e\">Pakeha</span></p></td><td class=\"cl-4fa32cee\"><p class=\"cl-4fa2dc6c\"><span class=\"cl-4fa2c66e\">0</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-4fa32d20\"><p class=\"cl-4fa2dc62\"><span class=\"cl-4fa2c66e\">&lt;S1A-001#M&gt;</span></p></td><td class=\"cl-4fa32d0c\"><p class=\"cl-4fa2dc62\"><span class=\"cl-4fa2c66e\">Men</span></p></td><td class=\"cl-4fa32d0c\"><p class=\"cl-4fa2dc62\"><span class=\"cl-4fa2c66e\">Young</span></p></td><td class=\"cl-4fa32d0c\"><p class=\"cl-4fa2dc62\"><span class=\"cl-4fa2c66e\">Pakeha</span></p></td><td class=\"cl-4fa32d16\"><p class=\"cl-4fa2dc6c\"><span class=\"cl-4fa2c66e\">1</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-4fa32d03\"><p class=\"cl-4fa2dc62\"><span class=\"cl-4fa2c66e\">&lt;S1A-001#M&gt;</span></p></td><td class=\"cl-4fa32d02\"><p class=\"cl-4fa2dc62\"><span class=\"cl-4fa2c66e\">Men</span></p></td><td class=\"cl-4fa32d02\"><p class=\"cl-4fa2dc62\"><span class=\"cl-4fa2c66e\">Young</span></p></td><td class=\"cl-4fa32d02\"><p class=\"cl-4fa2dc62\"><span class=\"cl-4fa2c66e\">Pakeha</span></p></td><td class=\"cl-4fa32cee\"><p class=\"cl-4fa2dc6c\"><span class=\"cl-4fa2c66e\">0</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-4fa32d20\"><p class=\"cl-4fa2dc62\"><span class=\"cl-4fa2c66e\">&lt;S1A-001#M&gt;</span></p></td><td class=\"cl-4fa32d0c\"><p class=\"cl-4fa2dc62\"><span class=\"cl-4fa2c66e\">Men</span></p></td><td class=\"cl-4fa32d0c\"><p class=\"cl-4fa2dc62\"><span class=\"cl-4fa2c66e\">Young</span></p></td><td class=\"cl-4fa32d0c\"><p class=\"cl-4fa2dc62\"><span class=\"cl-4fa2c66e\">Pakeha</span></p></td><td class=\"cl-4fa32d16\"><p class=\"cl-4fa2dc6c\"><span class=\"cl-4fa2c66e\">0</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-4fa32d03\"><p class=\"cl-4fa2dc62\"><span class=\"cl-4fa2c66e\">&lt;S1A-001#M&gt;</span></p></td><td class=\"cl-4fa32d02\"><p class=\"cl-4fa2dc62\"><span class=\"cl-4fa2c66e\">Men</span></p></td><td class=\"cl-4fa32d02\"><p class=\"cl-4fa2dc62\"><span class=\"cl-4fa2c66e\">Young</span></p></td><td class=\"cl-4fa32d02\"><p class=\"cl-4fa2dc62\"><span class=\"cl-4fa2c66e\">Pakeha</span></p></td><td class=\"cl-4fa32cee\"><p class=\"cl-4fa2dc6c\"><span class=\"cl-4fa2c66e\">1</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-4fa32d20\"><p class=\"cl-4fa2dc62\"><span class=\"cl-4fa2c66e\">&lt;S1A-001#M&gt;</span></p></td><td class=\"cl-4fa32d0c\"><p class=\"cl-4fa2dc62\"><span class=\"cl-4fa2c66e\">Men</span></p></td><td class=\"cl-4fa32d0c\"><p class=\"cl-4fa2dc62\"><span class=\"cl-4fa2c66e\">Young</span></p></td><td class=\"cl-4fa32d0c\"><p class=\"cl-4fa2dc62\"><span class=\"cl-4fa2c66e\">Pakeha</span></p></td><td class=\"cl-4fa32d16\"><p class=\"cl-4fa2dc6c\"><span class=\"cl-4fa2c66e\">1</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-4fa32d2a\"><p class=\"cl-4fa2dc62\"><span class=\"cl-4fa2c66e\">&lt;S1A-001#M&gt;</span></p></td><td class=\"cl-4fa32d21\"><p class=\"cl-4fa2dc62\"><span class=\"cl-4fa2c66e\">Men</span></p></td><td class=\"cl-4fa32d21\"><p class=\"cl-4fa2dc62\"><span class=\"cl-4fa2c66e\">Young</span></p></td><td class=\"cl-4fa32d21\"><p class=\"cl-4fa2dc62\"><span class=\"cl-4fa2c66e\">Pakeha</span></p></td><td class=\"cl-4fa32d34\"><p class=\"cl-4fa2dc6c\"><span class=\"cl-4fa2c66e\">0</span></p></td></tr></tbody></table></div>\n```\n:::\n:::\n\n\nThe summary of the data show that the data set contains 25,821 observations of five variables. The variable `ID` contains strings that represent a combination file and speaker of a speech unit. The second variable represents the gender, the third the age, and the fourth the ethnicity of speakers. The fifth variable represents whether or not a speech unit contained the discourse particle *eh*. \n\nNext, we factorize the variables in our data set. In other words, we specify that the strings represent variable levels and define new reference levels because as a default `R` will use the variable level which first occurs in alphabet ordering as the reference level for each variable, we redefine the variable levels for Age and Ethnicity.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nblrdata <- blrdata %>%\n  # factorize variables\n  dplyr::mutate(Age = factor(Age),\n                Gender = factor(Gender),\n                Ethnicity = factor(Ethnicity),\n                ID = factor(ID),\n                EH = factor(EH)) %>%\n  # relevel Age (Reference = Young) and Ethnicity (Reference= Pakeha))\n  dplyr::mutate(Age = relevel(Age, \"Young\"),\n                Ethnicity = relevel(Ethnicity, \"Pakeha\"))\n```\n:::\n\n\nAfter preparing the data, we will now plot the data to get an overview of potential relationships between variables.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nblrdata %>%\n  dplyr::mutate(EH = ifelse(EH == \"0\", 0, 1)) %>%\n  ggplot(aes(Age, EH, color = Gender)) +\n  facet_wrap(~Ethnicity) +\n  stat_summary(fun = mean, geom = \"point\") +\n  stat_summary(fun.data = mean_cl_boot, geom = \"errorbar\", width = 0.2) +\n  theme_set(theme_bw(base_size = 10)) +\n  theme(legend.position = \"top\") +\n  labs(x = \"\", y = \"Observed Probabilty of eh\") +\n  scale_color_manual(values = c(\"gray20\", \"gray70\"))\n```\n\n::: {.cell-output-display}\n![](fixreg_files/figure-html/blm6-1.png){width=672}\n:::\n:::\n\n\nWith respect to main effects, the Figure above indicates that men use *eh* more frequently than women, that young speakers use it more frequently compared with old speakers, and that speakers that are descendants of European settlers (Pakeha) use *eh* more frequently compared with Maori (the native inhabitants of New Zealand).\n\nThe plots in the lower panels do not indicate significant interactions between use of *eh* and the Age, Gender, and Ethnicity of speakers. In a next step, we will start building the logistic regression model.\n\n### Model Building{-}\n\nAs a first step, we need to define contrasts and use the `datadist` function to store aspects of our variables that can be accessed later when plotting and summarizing the model. Contrasts define what and how variable levels should be compared and therefore influences how the results of the regression analysis are presented. In this case, we use treatment contrasts which are in-built. Treatment contrasts mean that we assess the significance of levels of a predictor against a baseline which is the reference level of a predictor. @field2012discovering [414-427] and @gries2021statistics provide very good and accessible explanations of contrasts and how to manually define contrasts if you would like to know more. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# set contrasts\noptions(contrasts  =c(\"contr.treatment\", \"contr.poly\"))\n# create distance matrix\nblrdata.dist <- datadist(blrdata)\n# include distance matrix in options\noptions(datadist = \"blrdata.dist\")\n```\n:::\n\n\nNext, we generate a minimal model that predicts the use of *eh* solely based on the intercept.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# baseline glm model\nm0.glm = glm(EH ~ 1, family = binomial, data = blrdata)\n```\n:::\n\n\n### Model Fitting{-}\n\nWe will now start with the model fitting procedure. In the present case, we will not use a manual step-wise procedure as above but we will use the the `glmulti` function from the `glmulti` package [see @calcagno2020glmulti] to select the best model. The  `glmulti` function computes all possible models (when we choose exhaustive screen by setting the method to `\"h\"`) and then reports those models that have the best information criterion values, i.e. values which provide information about the most parsimonious models (models that explain a maxi,um amount of variance with a minimum number of predictors). \n\n\n::: {.cell}\n\n```{.r .cell-code}\nblr.glmulti <- glmulti(y = EH ~ Gender + Age + Ethnicity, # formula\n                       crit = aicc,                     # information crit. (aic, bic, aicc, qaic, qaicc)\n                       data = blrdata,                  # data\n                       family = binomial,               # model type\n                       method = \"h\",                    # screening type\n                       fitfunc = glm,                   # fit function\n                       level = 2)                       # 2 = with intercations (1 = without)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nInitialization...\nTASK: Exhaustive screening of candidate set.\nFitting...\nCompleted.\n```\n:::\n\n```{.r .cell-code}\n# inspect\nprint(blr.glmulti)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nglmulti.analysis\nMethod: h / Fitting: glm / IC used: aicc\nLevel: 2 / Marginality: FALSE\nFrom 18 models:\nBest IC: 32145.5418216606\nBest model:\n[1] \"EH ~ 1 + Gender + Age\"\nEvidence weight: 0.293712170021837\nWorst IC: 33009.7548480748\n4 models within 2 IC units.\n7 models to reach 95% of evidence weight.\n```\n:::\n:::\n\n\nAs the results inform us that there are 4 additional models that perform similarly well (`4 models within 2 IC units.`) we inspect these 4 models.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nweightable(blr.glmulti)[1:4,] %>%\n  regulartable() %>%\n  autofit()\n```\n\n::: {.cell-output-display}\n```{=html}\n<div class=\"tabwid\"><style>.cl-5124f3a4{}.cl-512243ca{font-family:'DejaVu Sans';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-51224c3a{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-51224c44{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-512262ce{width:307pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-512262d8{width:115pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-512262d9{width:111.5pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-512262da{width:111.5pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-512262e2{width:307pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-512262ec{width:115pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-512262ed{width:111.5pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-512262f6{width:307pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-512262f7{width:115pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}</style><table class='cl-5124f3a4'>\n```\n\n```{=html}\n<thead><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-512262f6\"><p class=\"cl-51224c3a\"><span class=\"cl-512243ca\">model</span></p></td><td class=\"cl-512262ed\"><p class=\"cl-51224c44\"><span class=\"cl-512243ca\">aicc</span></p></td><td class=\"cl-512262f7\"><p class=\"cl-51224c44\"><span class=\"cl-512243ca\">weights</span></p></td></tr></thead><tbody><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-512262ce\"><p class=\"cl-51224c3a\"><span class=\"cl-512243ca\">EH ~ 1 + Gender + Age</span></p></td><td class=\"cl-512262d9\"><p class=\"cl-51224c44\"><span class=\"cl-512243ca\">32,145.5418217</span></p></td><td class=\"cl-512262d8\"><p class=\"cl-51224c44\"><span class=\"cl-512243ca\">0.293712170022</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-512262ce\"><p class=\"cl-51224c3a\"><span class=\"cl-512243ca\">EH ~ 1 + Gender + Age + Ethnicity + Ethnicity:Age</span></p></td><td class=\"cl-512262d9\"><p class=\"cl-51224c44\"><span class=\"cl-512243ca\">32,146.4745618</span></p></td><td class=\"cl-512262d8\"><p class=\"cl-51224c44\"><span class=\"cl-512243ca\">0.184238336451</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-512262ce\"><p class=\"cl-51224c3a\"><span class=\"cl-512243ca\">EH ~ 1 + Gender + Age + Ethnicity</span></p></td><td class=\"cl-512262d9\"><p class=\"cl-51224c44\"><span class=\"cl-512243ca\">32,147.2814269</span></p></td><td class=\"cl-512262d8\"><p class=\"cl-51224c44\"><span class=\"cl-512243ca\">0.123075457179</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-512262e2\"><p class=\"cl-51224c3a\"><span class=\"cl-512243ca\">EH ~ 1 + Gender + Age + Age:Gender</span></p></td><td class=\"cl-512262da\"><p class=\"cl-51224c44\"><span class=\"cl-512243ca\">32,147.4182015</span></p></td><td class=\"cl-512262ec\"><p class=\"cl-51224c44\"><span class=\"cl-512243ca\">0.114940009116</span></p></td></tr></tbody></table></div>\n```\n:::\n:::\n\n\nThe results show that a model with only *Gender* and *Age* is the most parsimonious model and we decide to consider this model our final minimal adequate model.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# baseline glm model\nlr.glm <- glm(EH ~ Age + Gender, family = binomial, data = blrdata)\nlr.lrm <- lrm(EH ~ Age + Gender, data = blrdata, x = T, y = T, linear.predictors = T)\n# inspect\nsummary(lr.glm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nglm(formula = EH ~ Age + Gender, family = binomial, data = blrdata)\n\nDeviance Residuals: \n         Min            1Q        Median            3Q           Max  \n-1.080454338  -0.915641628  -0.770357585   1.277555296   1.837151604  \n\nCoefficients:\n                 Estimate    Std. Error   z value               Pr(>|z|)    \n(Intercept) -0.2323829796  0.0222629000 -10.43813 < 0.000000000000000222 ***\nAgeOld      -0.8305365551  0.0335166085 -24.77985 < 0.000000000000000222 ***\nGenderWomen -0.4201134305  0.0272517594 -15.41601 < 0.000000000000000222 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 33007.75469  on 25820  degrees of freedom\nResidual deviance: 32139.54089  on 25818  degrees of freedom\nAIC: 32145.54089\n\nNumber of Fisher Scoring iterations: 4\n```\n:::\n:::\n\n\n### Model Diagnostics {-}\n\nWe will now check for incomplete information, complete separation, and  (multi-)collinearity.\n\n\n**Incomplete information** means that the data does not contain all combinations of the predictor or the dependent variable. This is important because if the data does not contain cases of all combinations, it will be unable to provide accurate results.\n\n\n**Complete separation** is a related phenomenon where a model will assume that it has found a perfect predictor (because one level of a predictor correlates purely with hits or with fails but not both hits and fails of the dependent variable). In such cases the model overestimates the effect of that predictor and the results of that model are no longer reliable. For example, if *eh* was only used by young speakers in the data, the model would jump on that fact and say: \n\n> Ha! If there is an old speaker, that means that that speaker will never ever and under no circumstances say *eh* - I can therefore ignore all other factors! \n\n**Multicollinearity**  means that predictors in a model can be predicted by other predictors in the model (this means that they *share* variance with other predictors). If this is the case, the model results are unreliable because the presence of absence of one predictor has substantive effects on at least one other predictor. \n\nTo check whether the final minimal model contains predictors that correlate with each other, we extract variance inflation factors (VIF). If a model contains predictors that have variance inflation factors (VIF) > 10 the model is unreliable [@myers1990classical]. @gries2021statistics shows that a VIF of 10 means that that predictor is explainable (predictable) from the other predictors in the model with an R^2^ of .9 (a VIF of 5 means that predictor is explainable (predictable) from the other predictors in the model with an R^2^ of .8).Indeed, predictors with VIF values greater than 4 are usually already problematic but, for large data sets, even VIFs greater than 2 can lead inflated standard errors ([Jaeger 2013](http://wiki.bcs.rochester.edu/HlpLab/LSA2013Regression?action=AttachFile&do=view&target=LSA13-Lecture6-CommonIssuesAndSolutions.pdf)). Also, VIFs of 2.5 can be problematic [@szmrecsanyi2006morphosyntactic 215] and [@zuur2010protocol] proposes that variables with VIFs exceeding 3 should be removed.\n\n\n<div class=\"warning\" style='padding:0.1em; background-color:#f2f2f2; color:#51247a'>\n<span>\n<p style='margin-top:1em; text-align:center'>\n<b>NOTE</b><br><br>However, (multi-)collinearity is only an issue if one is interested in interpreting regression results!  If the interpretation is irrelevant because what is relevant is prediction(!), then it does not matter if the model contains collinear predictors! See @gries2021statistics or the excursion below for a more elaborate explanation.</p>\n<p style='margin-left:1em;'>\n</p></span>\n</div>\n\n<br>\n\n\n***\n\n<div class=\"warning\" style='padding:0.1em; background-color:#f2f2f2; color:#51247a'>\n<span>\n<p style='margin-top:1em; text-align:center'>\n<b>EXCURSION</b></p>\n<p style='margin-left:1em;'>\n</p></span>\n</div>\n\n<div class=\"question\">` \n\nWhat is multicollinearity?<br>\n\n<details>\n  <summary>Answer</summary>\n  \n  <br>\n  \n  During the workshop on mixed-effects modeling, we talked about (multi-)collinearity and someone asked if collinearity reflected shared variance (what I thought) or predictability of variables (what the other person thought). Both answers are correct! We will see below why...\n  \n  ***\n  \n  <div class=\"warning\" style='padding:0.1em; background-color:#f2f2f2; color:#51247a'>\n  <span>\n  <p style='margin-top:1em; text-align:center'>\n  (Multi-)collinearity reflects the predictability of predictors based on the values of other predictors!</p></span></div>\n  \n  \n  ***\n  \n  To test this, I generate a data set with 4 independent variables `a`, `b`, `c`, and `d` as well as two potential response variables `r1` (which is random) and `r2` (where the first 50 data points are the same as in `r1` but for the second 50 data points I have added a value of 50 to the data points 51 to 100 from `r1`). This means that the predictors `a` and `d` should both strongly correlate with `r2`. \n  \n\n  ::: {.cell}\n  \n  ```{.r .cell-code}\n  # load packages\n  library(dplyr)\n  library(rms)\n  # create data set\n  # responses\n  # 100 random numbers\n  r1 <- rnorm(100, 50, 10)\n  # 50 smaller + 50 larger numbers\n  r2 <- c(r1[1:50], r1[51:100] + 50)\n  # predictors\n  a <- c(rep(\"1\", 50), rep (\"0\", 50))\n  b <- rep(c(rep(\"1\", 25), rep (\"0\", 25)), 2)\n  c <- rep(c(rep(\"1\", 10), rep(\"0\", 10)), 5)\n  d <- c(rep(\"1\", 47), rep (\"0\", 3), rep (\"0\", 47), rep (\"1\", 3))\n  # create data set\n  df <- data.frame(r1, r2, a, b, c, d)\n  ```\n  :::\n\n  ::: {.cell}\n  ::: {.cell-output-display}\n  ```{=html}\n  <div class=\"tabwid\"><style>.cl-513f9100{table-layout:auto;width:75%;}.cl-513bcbf6{font-family:'DejaVu Sans';font-size:12pt;font-weight:bold;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-513bcc00{font-family:'DejaVu Sans';font-size:12pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-513bd524{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-513bd525{margin:0;text-align:center;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-513bf0d6{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-513bf0e0{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-513bf0e1{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-513bf0ea{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-513bf0eb{background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-513bf0ec{background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-513bf0f4{background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-513bf0f5{background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-513bf0f6{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-513bf0fe{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-513bf0ff{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-513bf100{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-513bf108{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-513bf109{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-513bf10a{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-513bf10b{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}</style><table class='cl-513f9100'>\n  ```\n  <caption class=\"Table Caption\">\n  \n  First 10 rows of the mlrdata.\n  \n  </caption>\n  ```{=html}\n  <thead><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-513bf10b\"><p class=\"cl-513bd524\"><span class=\"cl-513bcbf6\">r1</span></p></td><td class=\"cl-513bf109\"><p class=\"cl-513bd524\"><span class=\"cl-513bcbf6\">r2</span></p></td><td class=\"cl-513bf108\"><p class=\"cl-513bd525\"><span class=\"cl-513bcbf6\">a</span></p></td><td class=\"cl-513bf108\"><p class=\"cl-513bd525\"><span class=\"cl-513bcbf6\">b</span></p></td><td class=\"cl-513bf108\"><p class=\"cl-513bd525\"><span class=\"cl-513bcbf6\">c</span></p></td><td class=\"cl-513bf10a\"><p class=\"cl-513bd525\"><span class=\"cl-513bcbf6\">d</span></p></td></tr></thead><tbody><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-513bf0e0\"><p class=\"cl-513bd524\"><span class=\"cl-513bcc00\">55.3805120133</span></p></td><td class=\"cl-513bf0d6\"><p class=\"cl-513bd524\"><span class=\"cl-513bcc00\">55.3805120133</span></p></td><td class=\"cl-513bf0ea\"><p class=\"cl-513bd525\"><span class=\"cl-513bcc00\">1</span></p></td><td class=\"cl-513bf0ea\"><p class=\"cl-513bd525\"><span class=\"cl-513bcc00\">1</span></p></td><td class=\"cl-513bf0ea\"><p class=\"cl-513bd525\"><span class=\"cl-513bcc00\">1</span></p></td><td class=\"cl-513bf0e1\"><p class=\"cl-513bd525\"><span class=\"cl-513bcc00\">1</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-513bf0fe\"><p class=\"cl-513bd524\"><span class=\"cl-513bcc00\">50.5169933942</span></p></td><td class=\"cl-513bf100\"><p class=\"cl-513bd524\"><span class=\"cl-513bcc00\">50.5169933942</span></p></td><td class=\"cl-513bf0f6\"><p class=\"cl-513bd525\"><span class=\"cl-513bcc00\">1</span></p></td><td class=\"cl-513bf0f6\"><p class=\"cl-513bd525\"><span class=\"cl-513bcc00\">1</span></p></td><td class=\"cl-513bf0f6\"><p class=\"cl-513bd525\"><span class=\"cl-513bcc00\">1</span></p></td><td class=\"cl-513bf0ff\"><p class=\"cl-513bd525\"><span class=\"cl-513bcc00\">1</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-513bf0e0\"><p class=\"cl-513bd524\"><span class=\"cl-513bcc00\">67.5135734932</span></p></td><td class=\"cl-513bf0d6\"><p class=\"cl-513bd524\"><span class=\"cl-513bcc00\">67.5135734932</span></p></td><td class=\"cl-513bf0ea\"><p class=\"cl-513bd525\"><span class=\"cl-513bcc00\">1</span></p></td><td class=\"cl-513bf0ea\"><p class=\"cl-513bd525\"><span class=\"cl-513bcc00\">1</span></p></td><td class=\"cl-513bf0ea\"><p class=\"cl-513bd525\"><span class=\"cl-513bcc00\">1</span></p></td><td class=\"cl-513bf0e1\"><p class=\"cl-513bd525\"><span class=\"cl-513bcc00\">1</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-513bf0fe\"><p class=\"cl-513bd524\"><span class=\"cl-513bcc00\">60.1733973728</span></p></td><td class=\"cl-513bf100\"><p class=\"cl-513bd524\"><span class=\"cl-513bcc00\">60.1733973728</span></p></td><td class=\"cl-513bf0f6\"><p class=\"cl-513bd525\"><span class=\"cl-513bcc00\">1</span></p></td><td class=\"cl-513bf0f6\"><p class=\"cl-513bd525\"><span class=\"cl-513bcc00\">1</span></p></td><td class=\"cl-513bf0f6\"><p class=\"cl-513bd525\"><span class=\"cl-513bcc00\">1</span></p></td><td class=\"cl-513bf0ff\"><p class=\"cl-513bd525\"><span class=\"cl-513bcc00\">1</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-513bf0e0\"><p class=\"cl-513bd524\"><span class=\"cl-513bcc00\">72.3678161278</span></p></td><td class=\"cl-513bf0d6\"><p class=\"cl-513bd524\"><span class=\"cl-513bcc00\">72.3678161278</span></p></td><td class=\"cl-513bf0ea\"><p class=\"cl-513bd525\"><span class=\"cl-513bcc00\">1</span></p></td><td class=\"cl-513bf0ea\"><p class=\"cl-513bd525\"><span class=\"cl-513bcc00\">1</span></p></td><td class=\"cl-513bf0ea\"><p class=\"cl-513bd525\"><span class=\"cl-513bcc00\">1</span></p></td><td class=\"cl-513bf0e1\"><p class=\"cl-513bd525\"><span class=\"cl-513bcc00\">1</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-513bf0fe\"><p class=\"cl-513bd524\"><span class=\"cl-513bcc00\">52.4309002788</span></p></td><td class=\"cl-513bf100\"><p class=\"cl-513bd524\"><span class=\"cl-513bcc00\">52.4309002788</span></p></td><td class=\"cl-513bf0f6\"><p class=\"cl-513bd525\"><span class=\"cl-513bcc00\">1</span></p></td><td class=\"cl-513bf0f6\"><p class=\"cl-513bd525\"><span class=\"cl-513bcc00\">1</span></p></td><td class=\"cl-513bf0f6\"><p class=\"cl-513bd525\"><span class=\"cl-513bcc00\">1</span></p></td><td class=\"cl-513bf0ff\"><p class=\"cl-513bd525\"><span class=\"cl-513bcc00\">1</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-513bf0e0\"><p class=\"cl-513bd524\"><span class=\"cl-513bcc00\">55.6904236277</span></p></td><td class=\"cl-513bf0d6\"><p class=\"cl-513bd524\"><span class=\"cl-513bcc00\">55.6904236277</span></p></td><td class=\"cl-513bf0ea\"><p class=\"cl-513bd525\"><span class=\"cl-513bcc00\">1</span></p></td><td class=\"cl-513bf0ea\"><p class=\"cl-513bd525\"><span class=\"cl-513bcc00\">1</span></p></td><td class=\"cl-513bf0ea\"><p class=\"cl-513bd525\"><span class=\"cl-513bcc00\">1</span></p></td><td class=\"cl-513bf0e1\"><p class=\"cl-513bd525\"><span class=\"cl-513bcc00\">1</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-513bf0fe\"><p class=\"cl-513bd524\"><span class=\"cl-513bcc00\">44.2366484882</span></p></td><td class=\"cl-513bf100\"><p class=\"cl-513bd524\"><span class=\"cl-513bcc00\">44.2366484882</span></p></td><td class=\"cl-513bf0f6\"><p class=\"cl-513bd525\"><span class=\"cl-513bcc00\">1</span></p></td><td class=\"cl-513bf0f6\"><p class=\"cl-513bd525\"><span class=\"cl-513bcc00\">1</span></p></td><td class=\"cl-513bf0f6\"><p class=\"cl-513bd525\"><span class=\"cl-513bcc00\">1</span></p></td><td class=\"cl-513bf0ff\"><p class=\"cl-513bd525\"><span class=\"cl-513bcc00\">1</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-513bf0e0\"><p class=\"cl-513bd524\"><span class=\"cl-513bcc00\">55.5987335816</span></p></td><td class=\"cl-513bf0d6\"><p class=\"cl-513bd524\"><span class=\"cl-513bcc00\">55.5987335816</span></p></td><td class=\"cl-513bf0ea\"><p class=\"cl-513bd525\"><span class=\"cl-513bcc00\">1</span></p></td><td class=\"cl-513bf0ea\"><p class=\"cl-513bd525\"><span class=\"cl-513bcc00\">1</span></p></td><td class=\"cl-513bf0ea\"><p class=\"cl-513bd525\"><span class=\"cl-513bcc00\">1</span></p></td><td class=\"cl-513bf0e1\"><p class=\"cl-513bd525\"><span class=\"cl-513bcc00\">1</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-513bf0ec\"><p class=\"cl-513bd524\"><span class=\"cl-513bcc00\">58.3120761737</span></p></td><td class=\"cl-513bf0eb\"><p class=\"cl-513bd524\"><span class=\"cl-513bcc00\">58.3120761737</span></p></td><td class=\"cl-513bf0f4\"><p class=\"cl-513bd525\"><span class=\"cl-513bcc00\">1</span></p></td><td class=\"cl-513bf0f4\"><p class=\"cl-513bd525\"><span class=\"cl-513bcc00\">1</span></p></td><td class=\"cl-513bf0f4\"><p class=\"cl-513bd525\"><span class=\"cl-513bcc00\">1</span></p></td><td class=\"cl-513bf0f5\"><p class=\"cl-513bd525\"><span class=\"cl-513bcc00\">1</span></p></td></tr></tbody></table></div>\n  ```\n  :::\n  :::\n\n  \n  \n  Here are the visualizations of r1 and r2\n  \n\n  ::: {.cell}\n  ::: {.cell-output-display}\n  ![](fixreg_files/figure-html/data2-1.png){width=672}\n  :::\n  \n  ::: {.cell-output-display}\n  ![](fixreg_files/figure-html/data2-2.png){width=672}\n  :::\n  :::\n\n  \n  **Fit first model**\n  \n  Now, I fit a first model. As the response is random, we do not expect any of the predictors to have a significant effect and we expect the R^2^ to be rather low.\n  \n\n  ::: {.cell}\n  \n  ```{.r .cell-code}\n  m1 <- lm(r1 ~ a + b + c + d, data = df)\n  # inspect model\n  summary(m1)\n  ```\n  \n  ::: {.cell-output .cell-output-stdout}\n  ```\n  \n  Call:\n  lm(formula = r1 ~ a + b + c + d, data = df)\n  \n  Residuals:\n            Min            1Q        Median            3Q           Max \n  -25.076373713  -7.774694324   0.933388851   6.630369791  22.021652947 \n  \n  Coefficients:\n                  Estimate   Std. Error  t value             Pr(>|t|)    \n  (Intercept) 46.077236782  2.059313248 22.37505 < 0.0000000000000002 ***\n  a1           5.957479586  4.593681927  1.29689             0.197812    \n  b1           5.148440925  2.098541787  2.45334             0.015977 *  \n  c1          -0.213502165  2.188893729 -0.09754             0.922504    \n  d1          -5.232737413  4.515342995 -1.15888             0.249410    \n  ---\n  Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n  \n  Residual standard error: 10.4927089 on 95 degrees of freedom\n  Multiple R-squared:  0.075627922,\tAdjusted R-squared:  0.0367069924 \n  F-statistic: 1.94311705 on 4 and 95 DF,  p-value: 0.109586389\n  ```\n  :::\n  :::\n\n  \n  We now check for (multi-)collinearity using the `vif` function from the `rms` package [@rms]. Variables `a` and `d` should have high variance inflation factor values (vif-values) because they overlap very much!\n  \n\n  ::: {.cell}\n  \n  ```{.r .cell-code}\n  # extract vifs\n  rms::vif(m1)\n  ```\n  \n  ::: {.cell-output .cell-output-stdout}\n  ```\n             a1            b1            c1            d1 \n  4.79166666667 1.00000000000 1.08796296296 4.62962962963 \n  ```\n  :::\n  :::\n\n  \n  Variables `a` and `d` do indeed have high vif-values.\n  \n  **Fit second model**\n  \n  We now fit a second model to the response which has higher values for the latter part of the response. Both `a` and `d` strongly correlate with the response. **But** because `a` and `d` are collinear, `d` should not be reported as being significant by the model. The R^2^ of the model should be rather high (given the correlation between the response r2 and `a` and `d`).\n  \n\n  ::: {.cell}\n  \n  ```{.r .cell-code}\n  m2 <- lm(r2 ~ a + b + c + d, data = df)\n  # inspect model\n  summary(m2)\n  ```\n  \n  ::: {.cell-output .cell-output-stdout}\n  ```\n  \n  Call:\n  lm(formula = r2 ~ a + b + c + d, data = df)\n  \n  Residuals:\n            Min            1Q        Median            3Q           Max \n  -25.076373713  -7.774694324   0.933388851   6.630369791  22.021652947 \n  \n  Coefficients:\n                   Estimate    Std. Error  t value               Pr(>|t|)    \n  (Intercept)  96.077236782   2.059313248 46.65499 < 0.000000000000000222 ***\n  a1          -44.042520414   4.593681927 -9.58763  0.0000000000000012574 ***\n  b1            5.148440925   2.098541787  2.45334               0.015977 *  \n  c1           -0.213502165   2.188893729 -0.09754               0.922504    \n  d1           -5.232737413   4.515342995 -1.15888               0.249410    \n  ---\n  Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n  \n  Residual standard error: 10.4927089 on 95 degrees of freedom\n  Multiple R-squared:  0.851726565,\tAdjusted R-squared:  0.845483473 \n  F-statistic: 136.427041 on 4 and 95 DF,  p-value: < 0.0000000000000002220446\n  ```\n  :::\n  :::\n\n  \n  Again, we extract the vif-values.\n  \n\n  ::: {.cell}\n  \n  ```{.r .cell-code}\n  # extract vifs\n  rms::vif(m2)\n  ```\n  \n  ::: {.cell-output .cell-output-stdout}\n  ```\n             a1            b1            c1            d1 \n  4.79166666667 1.00000000000 1.08796296296 4.62962962963 \n  ```\n  :::\n  :::\n\n  \n  The vif-values are identical which shows that what matters is if the variables are predictable. To understand how we arrive at vif-values, we inspect the model matrix.\n  \n\n  ::: {.cell}\n  \n  ```{.r .cell-code}\n  # inspect model matrix\n  mm <- model.matrix(m2)\n  ```\n  :::\n\n  ::: {.cell}\n  ::: {.cell-output-display}\n  ```{=html}\n  <div class=\"tabwid\"><style>.cl-5162d2a0{table-layout:auto;width:75%;}.cl-515e8e66{font-family:'DejaVu Sans';font-size:12pt;font-weight:bold;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-515e8e70{font-family:'DejaVu Sans';font-size:12pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-515e9d20{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-515ed60a{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-515ed614{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-515ed615{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-515ed616{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-515ed61e{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-515ed61f{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-515ed628{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-515ed629{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-515ed632{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-515ed633{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-515ed634{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-515ed63c{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}</style><table class='cl-5162d2a0'>\n  ```\n  <caption class=\"Table Caption\">\n  \n  First 10 rows of the mlrdata.\n  \n  </caption>\n  ```{=html}\n  <thead><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-515ed633\"><p class=\"cl-515e9d20\"><span class=\"cl-515e8e66\">(Intercept)</span></p></td><td class=\"cl-515ed634\"><p class=\"cl-515e9d20\"><span class=\"cl-515e8e66\">a1</span></p></td><td class=\"cl-515ed634\"><p class=\"cl-515e9d20\"><span class=\"cl-515e8e66\">b1</span></p></td><td class=\"cl-515ed634\"><p class=\"cl-515e9d20\"><span class=\"cl-515e8e66\">c1</span></p></td><td class=\"cl-515ed63c\"><p class=\"cl-515e9d20\"><span class=\"cl-515e8e66\">d1</span></p></td></tr></thead><tbody><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-515ed615\"><p class=\"cl-515e9d20\"><span class=\"cl-515e8e70\">1</span></p></td><td class=\"cl-515ed60a\"><p class=\"cl-515e9d20\"><span class=\"cl-515e8e70\">1</span></p></td><td class=\"cl-515ed60a\"><p class=\"cl-515e9d20\"><span class=\"cl-515e8e70\">1</span></p></td><td class=\"cl-515ed60a\"><p class=\"cl-515e9d20\"><span class=\"cl-515e8e70\">1</span></p></td><td class=\"cl-515ed614\"><p class=\"cl-515e9d20\"><span class=\"cl-515e8e70\">1</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-515ed61f\"><p class=\"cl-515e9d20\"><span class=\"cl-515e8e70\">1</span></p></td><td class=\"cl-515ed616\"><p class=\"cl-515e9d20\"><span class=\"cl-515e8e70\">1</span></p></td><td class=\"cl-515ed616\"><p class=\"cl-515e9d20\"><span class=\"cl-515e8e70\">1</span></p></td><td class=\"cl-515ed616\"><p class=\"cl-515e9d20\"><span class=\"cl-515e8e70\">1</span></p></td><td class=\"cl-515ed61e\"><p class=\"cl-515e9d20\"><span class=\"cl-515e8e70\">1</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-515ed615\"><p class=\"cl-515e9d20\"><span class=\"cl-515e8e70\">1</span></p></td><td class=\"cl-515ed60a\"><p class=\"cl-515e9d20\"><span class=\"cl-515e8e70\">1</span></p></td><td class=\"cl-515ed60a\"><p class=\"cl-515e9d20\"><span class=\"cl-515e8e70\">1</span></p></td><td class=\"cl-515ed60a\"><p class=\"cl-515e9d20\"><span class=\"cl-515e8e70\">1</span></p></td><td class=\"cl-515ed614\"><p class=\"cl-515e9d20\"><span class=\"cl-515e8e70\">1</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-515ed61f\"><p class=\"cl-515e9d20\"><span class=\"cl-515e8e70\">1</span></p></td><td class=\"cl-515ed616\"><p class=\"cl-515e9d20\"><span class=\"cl-515e8e70\">1</span></p></td><td class=\"cl-515ed616\"><p class=\"cl-515e9d20\"><span class=\"cl-515e8e70\">1</span></p></td><td class=\"cl-515ed616\"><p class=\"cl-515e9d20\"><span class=\"cl-515e8e70\">1</span></p></td><td class=\"cl-515ed61e\"><p class=\"cl-515e9d20\"><span class=\"cl-515e8e70\">1</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-515ed615\"><p class=\"cl-515e9d20\"><span class=\"cl-515e8e70\">1</span></p></td><td class=\"cl-515ed60a\"><p class=\"cl-515e9d20\"><span class=\"cl-515e8e70\">1</span></p></td><td class=\"cl-515ed60a\"><p class=\"cl-515e9d20\"><span class=\"cl-515e8e70\">1</span></p></td><td class=\"cl-515ed60a\"><p class=\"cl-515e9d20\"><span class=\"cl-515e8e70\">1</span></p></td><td class=\"cl-515ed614\"><p class=\"cl-515e9d20\"><span class=\"cl-515e8e70\">1</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-515ed61f\"><p class=\"cl-515e9d20\"><span class=\"cl-515e8e70\">1</span></p></td><td class=\"cl-515ed616\"><p class=\"cl-515e9d20\"><span class=\"cl-515e8e70\">1</span></p></td><td class=\"cl-515ed616\"><p class=\"cl-515e9d20\"><span class=\"cl-515e8e70\">1</span></p></td><td class=\"cl-515ed616\"><p class=\"cl-515e9d20\"><span class=\"cl-515e8e70\">1</span></p></td><td class=\"cl-515ed61e\"><p class=\"cl-515e9d20\"><span class=\"cl-515e8e70\">1</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-515ed615\"><p class=\"cl-515e9d20\"><span class=\"cl-515e8e70\">1</span></p></td><td class=\"cl-515ed60a\"><p class=\"cl-515e9d20\"><span class=\"cl-515e8e70\">1</span></p></td><td class=\"cl-515ed60a\"><p class=\"cl-515e9d20\"><span class=\"cl-515e8e70\">1</span></p></td><td class=\"cl-515ed60a\"><p class=\"cl-515e9d20\"><span class=\"cl-515e8e70\">1</span></p></td><td class=\"cl-515ed614\"><p class=\"cl-515e9d20\"><span class=\"cl-515e8e70\">1</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-515ed61f\"><p class=\"cl-515e9d20\"><span class=\"cl-515e8e70\">1</span></p></td><td class=\"cl-515ed616\"><p class=\"cl-515e9d20\"><span class=\"cl-515e8e70\">1</span></p></td><td class=\"cl-515ed616\"><p class=\"cl-515e9d20\"><span class=\"cl-515e8e70\">1</span></p></td><td class=\"cl-515ed616\"><p class=\"cl-515e9d20\"><span class=\"cl-515e8e70\">1</span></p></td><td class=\"cl-515ed61e\"><p class=\"cl-515e9d20\"><span class=\"cl-515e8e70\">1</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-515ed615\"><p class=\"cl-515e9d20\"><span class=\"cl-515e8e70\">1</span></p></td><td class=\"cl-515ed60a\"><p class=\"cl-515e9d20\"><span class=\"cl-515e8e70\">1</span></p></td><td class=\"cl-515ed60a\"><p class=\"cl-515e9d20\"><span class=\"cl-515e8e70\">1</span></p></td><td class=\"cl-515ed60a\"><p class=\"cl-515e9d20\"><span class=\"cl-515e8e70\">1</span></p></td><td class=\"cl-515ed614\"><p class=\"cl-515e9d20\"><span class=\"cl-515e8e70\">1</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-515ed61f\"><p class=\"cl-515e9d20\"><span class=\"cl-515e8e70\">1</span></p></td><td class=\"cl-515ed616\"><p class=\"cl-515e9d20\"><span class=\"cl-515e8e70\">1</span></p></td><td class=\"cl-515ed616\"><p class=\"cl-515e9d20\"><span class=\"cl-515e8e70\">1</span></p></td><td class=\"cl-515ed616\"><p class=\"cl-515e9d20\"><span class=\"cl-515e8e70\">1</span></p></td><td class=\"cl-515ed61e\"><p class=\"cl-515e9d20\"><span class=\"cl-515e8e70\">1</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-515ed615\"><p class=\"cl-515e9d20\"><span class=\"cl-515e8e70\">1</span></p></td><td class=\"cl-515ed60a\"><p class=\"cl-515e9d20\"><span class=\"cl-515e8e70\">1</span></p></td><td class=\"cl-515ed60a\"><p class=\"cl-515e9d20\"><span class=\"cl-515e8e70\">1</span></p></td><td class=\"cl-515ed60a\"><p class=\"cl-515e9d20\"><span class=\"cl-515e8e70\">0</span></p></td><td class=\"cl-515ed614\"><p class=\"cl-515e9d20\"><span class=\"cl-515e8e70\">1</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-515ed61f\"><p class=\"cl-515e9d20\"><span class=\"cl-515e8e70\">1</span></p></td><td class=\"cl-515ed616\"><p class=\"cl-515e9d20\"><span class=\"cl-515e8e70\">1</span></p></td><td class=\"cl-515ed616\"><p class=\"cl-515e9d20\"><span class=\"cl-515e8e70\">1</span></p></td><td class=\"cl-515ed616\"><p class=\"cl-515e9d20\"><span class=\"cl-515e8e70\">0</span></p></td><td class=\"cl-515ed61e\"><p class=\"cl-515e9d20\"><span class=\"cl-515e8e70\">1</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-515ed615\"><p class=\"cl-515e9d20\"><span class=\"cl-515e8e70\">1</span></p></td><td class=\"cl-515ed60a\"><p class=\"cl-515e9d20\"><span class=\"cl-515e8e70\">1</span></p></td><td class=\"cl-515ed60a\"><p class=\"cl-515e9d20\"><span class=\"cl-515e8e70\">1</span></p></td><td class=\"cl-515ed60a\"><p class=\"cl-515e9d20\"><span class=\"cl-515e8e70\">0</span></p></td><td class=\"cl-515ed614\"><p class=\"cl-515e9d20\"><span class=\"cl-515e8e70\">1</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-515ed61f\"><p class=\"cl-515e9d20\"><span class=\"cl-515e8e70\">1</span></p></td><td class=\"cl-515ed616\"><p class=\"cl-515e9d20\"><span class=\"cl-515e8e70\">1</span></p></td><td class=\"cl-515ed616\"><p class=\"cl-515e9d20\"><span class=\"cl-515e8e70\">1</span></p></td><td class=\"cl-515ed616\"><p class=\"cl-515e9d20\"><span class=\"cl-515e8e70\">0</span></p></td><td class=\"cl-515ed61e\"><p class=\"cl-515e9d20\"><span class=\"cl-515e8e70\">1</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-515ed628\"><p class=\"cl-515e9d20\"><span class=\"cl-515e8e70\">1</span></p></td><td class=\"cl-515ed629\"><p class=\"cl-515e9d20\"><span class=\"cl-515e8e70\">1</span></p></td><td class=\"cl-515ed629\"><p class=\"cl-515e9d20\"><span class=\"cl-515e8e70\">1</span></p></td><td class=\"cl-515ed629\"><p class=\"cl-515e9d20\"><span class=\"cl-515e8e70\">0</span></p></td><td class=\"cl-515ed632\"><p class=\"cl-515e9d20\"><span class=\"cl-515e8e70\">1</span></p></td></tr></tbody></table></div>\n  ```\n  :::\n  :::\n\n  \n  We now fit a linear model in which we predict `d` from the other predictors in the model matrix.\n  \n\n  ::: {.cell}\n  \n  ```{.r .cell-code}\n  mt <- lm(mm[,5] ~ mm[,1:4])\n  summary(mt)$r.squared\n  ```\n  \n  ::: {.cell-output .cell-output-stdout}\n  ```\n  [1] 0.784\n  ```\n  :::\n  :::\n\n  The R^2^ shows that the values of `d` are explained to 78.4 percent by the values of the other predictors in the model.\n  \n  Now, we can write a function [taken from @gries2021statistics] that converts this R^2^ value \n  \n\n  ::: {.cell}\n  \n  ```{.r .cell-code}\n  R2.to.VIF <- function(some.modelmatrix.r2) {\n  return(1/(1-some.modelmatrix.r2)) } \n  R2.to.VIF(0.784)\n  ```\n  \n  ::: {.cell-output .cell-output-stdout}\n  ```\n  [1] 4.62962962963\n  ```\n  :::\n  :::\n\n  \n  The function outputs the vif-value of `d`. This shows that the vif-value of `d` represents its predictability from the other predictors in the model matrix which represents the amount of shared variance between `d` and the other predictors in the model.\n  \n</details>\n\n</div>`\n\n***\n\n<br>\n\n\nWe start by We will now check for incomplete information and complete separation.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# check incomplete information and complete separation\nifelse(min(ftable(blrdata$Age, blrdata$Gender, blrdata$EH)) == 0, \"not possible\", \"possible\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"possible\"\n```\n:::\n:::\n\n\nAs the table does not contain any 0s, we can rule out incomplete information and complete separation. We continue to check for  (multi-)collinearity.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nifelse(max(vif(lr.glm)) <= 3,  \"vifs ok\", \"WARNING: high vifs!\") # VIFs ok\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"vifs ok\"\n```\n:::\n:::\n\n\nIn addition, predictors with 1/VIF values $<$ .1 must be removed (data points with values above .2 are considered problematic) [@menard1995applied] and the mean value of VIFs should be $~$ 1 [@bowerman1990linear].\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(vif(lr.glm))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1.00481494539\n```\n:::\n:::\n\n\n\nAs the variance inflation factors are below 3, we do not need to worry about  (multi-)collinearity. \n\n### Model Evaluation {-}\n\nNow, that we have confirmed that our model is not suffering from incomplete information, complete separation, and  (multi-)collinearity we can check if the final minimal model significantly improves model fit compared to the null model. For this, we calculate a Model Likelihood Ratio Test.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# check if adding Age significantly improves model fit\nanova(lr.glm, m0.glm, test = \"Chi\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Deviance Table\n\nModel 1: EH ~ Age + Gender\nModel 2: EH ~ 1\n  Resid. Df  Resid. Dev Df     Deviance               Pr(>Chi)    \n1     25818 32139.54089                                           \n2     25820 33007.75469 -2 -868.2138011 < 0.000000000000000222 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n\n```{.r .cell-code}\nAnova(lr.glm, test = \"LR\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Deviance Table (Type II tests)\n\nResponse: EH\n          LR Chisq Df             Pr(>Chisq)    \nAge    668.6350712  1 < 0.000000000000000222 ***\nGender 237.3199140  1 < 0.000000000000000222 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\nThe final model significantly improves model fit which means that we have now arrived at the final minimal adequate model. To elaborate, the code above provides three values: a $\\chi$^2^, the degrees of freedom, and a p-value. The p-value is lower than .05 and the results of the Model Likelihood Ratio Test therefore confirm that the final minimal adequate model performs significantly better than the initial minimal base-line model. \n\nIn a next step, we calculate pseudo-R^2^ values which represent the amount of residual variance that is explained by the final minimal adequate model. We cannot use the ordinary R^2^ because the model works on the logged probabilities rather than the values of the dependent variable.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# calculate pseudo R^2\n# number of cases\nmodelChi <- lr.glm$null.deviance - lr.glm$deviance\nncases <- length(fitted(lr.glm))\nR2.hl <- modelChi/lr.glm$null.deviance\nR.cs <- 1 - exp ((lr.glm$deviance - lr.glm$null.deviance)/ncases)\nR.n <- R.cs /( 1- ( exp (-(lr.glm$null.deviance/ ncases))))\n# function for extracting pseudo-R^2\nlogisticPseudoR2s <- function(LogModel) {\n  dev <- LogModel$deviance\n\tnullDev <- LogModel$null.deviance\n\tmodelN <-  length(LogModel$fitted.values)\n\tR.l <-  1 -  dev / nullDev\n\tR.cs <- 1- exp ( -(nullDev - dev) / modelN)\n\tR.n <- R.cs / ( 1 - ( exp (-(nullDev / modelN))))\n\tcat(\"Pseudo R^2 for logistic regression\\n\")\n\tcat(\"Hosmer and Lemeshow R^2  \", round(R.l, 3), \"\\n\")\n\tcat(\"Cox and Snell R^2        \", round(R.cs, 3), \"\\n\")\n\tcat(\"Nagelkerke R^2           \", round(R.n, 3),    \"\\n\") }\nlogisticPseudoR2s(lr.glm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPseudo R^2 for logistic regression\nHosmer and Lemeshow R^2   0.026 \nCox and Snell R^2         0.033 \nNagelkerke R^2            0.046 \n```\n:::\n:::\n\n\nThe low pseudo-R^2^ values show that our model has very low explanatory power. For instance, the value of Hosmer and Lemeshow R^2^ (0.026) \"is the proportional reduction in the absolute value of the log-likelihood measure and as such it is a measure of how much the badness of fit improves as a result of the inclusion of the predictor variables\" [@field2012discovering 317]. In essence, all the pseudo-R^2^ values are measures of how substantive the model is (how much better it is compared to a baseline model). Next, we extract the confidence intervals for the coefficients of the model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# extract the confidence intervals for the coefficients\nconfint(lr.glm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                      2.5 %          97.5 %\n(Intercept) -0.276050866670 -0.188778707810\nAgeOld      -0.896486392279 -0.765095825382\nGenderWomen -0.473530977637 -0.366703827307\n```\n:::\n:::\n\n\nDespite having low explanatory and predictive power, the age of speakers and their gender are significant as the confidence intervals of the coefficients do not overlap with 0.\n\n### Effect Size{-}\n\nIn a next step, we compute odds ratios and their confidence intervals. Odds Ratios represent a common measure of effect size and can be used to compare effect sizes across models. Odds ratios rang between 0 and infinity. Values of 1 indicate that there is no effect. The further away the values are from 1, the stronger the effect. If the values are lower than 1, then the variable level correlates negatively with the occurrence of the outcome (the probability decreases) while values above 1 indicate a positive correlation and show that the variable level causes an increase in the probability of the outcome (the occurrence of EH).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nexp(lr.glm$coefficients) # odds ratios\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   (Intercept)         AgeOld    GenderWomen \n0.792642499264 0.435815384592 0.656972294902 \n```\n:::\n\n```{.r .cell-code}\nexp(confint(lr.glm))     # confidence intervals of the odds ratios\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWaiting for profiling to be done...\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                     2.5 %         97.5 %\n(Intercept) 0.758774333456 0.827969709653\nAgeOld      0.408000698619 0.465289342309\nGenderWomen 0.622799290871 0.693014866732\n```\n:::\n:::\n\n\nThe odds ratios confirm that older speakers use *eh* significantly less often compared with younger speakers and that women use *eh* less frequently than men as the confidence intervals of the odds rations do not overlap with 1. In a next step, we calculate the prediction accuracy of the model.\n\n### Prediction Accuracy{-}\n\nIn order to calculate the prediction accuracy of the model, we generate a variable called *Prediction* that contains the predictions of pour model and which we add to the data. Then, we use the `confusionMatrix` function from the `caret` package [@caret] to extract the prediction accuracy. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# create variable with contains the prediction of the model\nblrdata <- blrdata %>%\n  dplyr::mutate(Prediction = predict(lr.glm, type = \"response\"),\n                Prediction = ifelse(Prediction > .5, 1, 0),\n                Prediction = factor(Prediction, levels = c(\"0\", \"1\")),\n                EH = factor(EH, levels = c(\"0\", \"1\")))\n# create a confusion matrix with compares observed against predicted values\ncaret::confusionMatrix(blrdata$Prediction, blrdata$EH)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nConfusion Matrix and Statistics\n\n          Reference\nPrediction     0     1\n         0 17114  8707\n         1     0     0\n                                                    \n               Accuracy : 0.66279385                \n                 95% CI : (0.656990096, 0.668560948)\n    No Information Rate : 0.66279385                \n    P-Value [Acc > NIR] : 0.5029107                 \n                                                    \n                  Kappa : 0                         \n                                                    \n Mcnemar's Test P-Value : < 0.00000000000000022     \n                                                    \n            Sensitivity : 1.00000000                \n            Specificity : 0.00000000                \n         Pos Pred Value : 0.66279385                \n         Neg Pred Value :        NaN                \n             Prevalence : 0.66279385                \n         Detection Rate : 0.66279385                \n   Detection Prevalence : 1.00000000                \n      Balanced Accuracy : 0.50000000                \n                                                    \n       'Positive' Class : 0                         \n                                                    \n```\n:::\n:::\n\n\nWe can see that out model has never predicted the use of *eh* which is common when dealing with rare phenomena. This is expected as the event s so rare that the probability of it not occurring substantively outweighs the probability of it occurring. As such, the prediction accuracy of our model is not significantly better compared to the prediction accuracy of the baseline model which is the no-information rate (NIR)) (p = 0.5029).\n\nWe can use the `plot_model` function from the `sjPlot` package [@sjPlot] to visualize the effects.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# predicted probability\nefp1 <- plot_model(lr.glm, type = \"pred\", terms = c(\"Age\"), axis.lim = c(0, 1)) \n# predicted percentage\nefp2 <- plot_model(lr.glm, type = \"pred\", terms = c(\"Gender\"), axis.lim = c(0, 1)) \ngrid.arrange(efp1, efp2, nrow = 1)\n```\n\n::: {.cell-output-display}\n![](fixreg_files/figure-html/blm27-1.png){width=672}\n:::\n:::\n\n\nAnd we can also combine the visualization of the effects in a single plot as shown below.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsjPlot::plot_model(lr.glm, type = \"pred\", terms = c(\"Age\", \"Gender\"), axis.lim = c(0, 1)) +\n  theme(legend.position = \"top\") +\n  labs(x = \"\", y = \"Predicted Probabilty of eh\", title = \"\") +\n  scale_color_manual(values = c(\"gray20\", \"gray70\"))\n```\n\n::: {.cell-output-display}\n![](fixreg_files/figure-html/blm28-1.png){width=672}\n:::\n:::\n\n\n\n### Outlier detection{-}\n\nIn order to detect potential outliers, we will calculate diagnostic parameters and add these to our data set.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninfl <- influence.measures(lr.glm) # calculate influence statistics\nblrdata <- data.frame(blrdata, infl[[1]], infl[[2]]) # add influence statistics\n```\n:::\n\n\nIn a next step, we use these diagnostic parameters to check if there are data points which should be removed as they unduly affect the model fit.  \n\n\n### Sample Size{-}\n\nWe now check whether the sample size is sufficient for our analysis [@green1991many].\n\n* if you are interested in the overall model: 50 + 8k (k = number of predictors)\n\n* if you are interested in individual predictors: 104 + k\n\n* if you are interested in both: take the higher value!\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# function to evaluate sample size\nsmplesz <- function(x) {\n  ifelse((length(x$fitted) < (104 + ncol(summary(x)$coefficients)-1)) == TRUE,\n    return(\n      paste(\"Sample too small: please increase your sample by \",\n      104 + ncol(summary(x)$coefficients)-1 - length(x$fitted),\n      \" data points\", collapse = \"\")),\n    return(\"Sample size sufficient\")) }\n# apply unction to model\nsmplesz(lr.glm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Sample size sufficient\"\n```\n:::\n:::\n\n\nAccording to rule of thumb provided in @green1991many, the sample size is sufficient for our analysis.\n\n### Summarizing Results{-}\n\nAs a final step, we summarize our findings in tabulated form.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsjPlot::tab_model(lr.glm)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table style=\"border-collapse:collapse; border:none;\">\n<tr>\n<th style=\"border-top: double; text-align:center; font-style:normal; font-weight:bold; padding:0.2cm;  text-align:left; \">&nbsp;</th>\n<th colspan=\"3\" style=\"border-top: double; text-align:center; font-style:normal; font-weight:bold; padding:0.2cm; \">EH</th>\n</tr>\n<tr>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  text-align:left; \">Predictors</td>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  \">Odds Ratios</td>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  \">CI</td>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  \">p</td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; \">(Intercept)</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.79</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.76&nbsp;&ndash;&nbsp;0.83</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"><strong>&lt;0.001</strong></td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; \">Age [Old]</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.44</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.41&nbsp;&ndash;&nbsp;0.47</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"><strong>&lt;0.001</strong></td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; \">Gender [Women]</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.66</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.62&nbsp;&ndash;&nbsp;0.69</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"><strong>&lt;0.001</strong></td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; padding-top:0.1cm; padding-bottom:0.1cm; border-top:1px solid;\">Observations</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left; border-top:1px solid;\" colspan=\"3\">25821</td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; padding-top:0.1cm; padding-bottom:0.1cm;\">R<sup>2</sup> Tjur</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left;\" colspan=\"3\">0.032</td>\n</tr>\n\n</table>\n\n`````\n:::\n:::\n\n\n***\n\nA more detailed summary table can be retrieved as follows:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# load function\nsource(\"https://slcladal.github.io/rscripts/blrsummary.r\")\n# calculate accuracy \npredict.acc <- caret::confusionMatrix(blrdata$Prediction, blrdata$EH)\npredict.acc <- predict.acc[3]$overall[[1]]\n# create summary table\nblrsummarytb <- blrsummary(lr.glm, lr.lrm, predict.acc) \n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n```{=html}\n<div class=\"tabwid\"><style>.cl-54669676{table-layout:auto;width:75%;}.cl-5460e834{font-family:'DejaVu Sans';font-size:12pt;font-weight:bold;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-5460e848{font-family:'DejaVu Sans';font-size:12pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-546105d0{margin:0;text-align:center;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-5461644e{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-54616458{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-5461646c{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-5461646d{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-54616476{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-54616480{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-5461648a{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-5461648b{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-54616494{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-5461649e{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-5461649f{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-546164a8{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}</style><table class='cl-54669676'>\n```\n<caption class=\"Table Caption\">\n\nResults of the binomial logistic regression analysis.\n\n</caption>\n```{=html}\n<thead><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-5461649f\"><p class=\"cl-546105d0\"><span class=\"cl-5460e834\">Statistics</span></p></td><td class=\"cl-5461649e\"><p class=\"cl-546105d0\"><span class=\"cl-5460e834\">Estimate</span></p></td><td class=\"cl-5461649e\"><p class=\"cl-546105d0\"><span class=\"cl-5460e834\">VIF</span></p></td><td class=\"cl-5461649e\"><p class=\"cl-546105d0\"><span class=\"cl-5460e834\">OddsRatio</span></p></td><td class=\"cl-5461649e\"><p class=\"cl-546105d0\"><span class=\"cl-5460e834\">CI(2.5%)</span></p></td><td class=\"cl-5461649e\"><p class=\"cl-546105d0\"><span class=\"cl-5460e834\">CI(97.5%)</span></p></td><td class=\"cl-5461649e\"><p class=\"cl-546105d0\"><span class=\"cl-5460e834\">Std. Error</span></p></td><td class=\"cl-5461649e\"><p class=\"cl-546105d0\"><span class=\"cl-5460e834\">z value</span></p></td><td class=\"cl-5461649e\"><p class=\"cl-546105d0\"><span class=\"cl-5460e834\">Pr(&gt;|z|)</span></p></td><td class=\"cl-546164a8\"><p class=\"cl-546105d0\"><span class=\"cl-5460e834\">Significance</span></p></td></tr></thead><tbody><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-54616458\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\">(Intercept)</span></p></td><td class=\"cl-5461644e\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\">-0.23</span></p></td><td class=\"cl-5461644e\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\"></span></p></td><td class=\"cl-5461644e\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\">0.79</span></p></td><td class=\"cl-5461644e\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\">0.76</span></p></td><td class=\"cl-5461644e\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\">0.83</span></p></td><td class=\"cl-5461644e\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\">0.02</span></p></td><td class=\"cl-5461644e\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\">-10.44</span></p></td><td class=\"cl-5461644e\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\">0</span></p></td><td class=\"cl-5461646c\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\">p &lt; .001***</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-54616476\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\">AgeOld</span></p></td><td class=\"cl-5461646d\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\">-0.83</span></p></td><td class=\"cl-5461646d\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\">1</span></p></td><td class=\"cl-5461646d\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\">0.44</span></p></td><td class=\"cl-5461646d\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\">0.41</span></p></td><td class=\"cl-5461646d\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\">0.47</span></p></td><td class=\"cl-5461646d\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\">0.03</span></p></td><td class=\"cl-5461646d\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\">-24.78</span></p></td><td class=\"cl-5461646d\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\">0</span></p></td><td class=\"cl-54616480\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\">p &lt; .001***</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-54616458\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\">GenderWomen</span></p></td><td class=\"cl-5461644e\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\">-0.42</span></p></td><td class=\"cl-5461644e\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\">1</span></p></td><td class=\"cl-5461644e\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\">0.66</span></p></td><td class=\"cl-5461644e\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\">0.62</span></p></td><td class=\"cl-5461644e\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\">0.69</span></p></td><td class=\"cl-5461644e\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\">0.03</span></p></td><td class=\"cl-5461644e\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\">-15.42</span></p></td><td class=\"cl-5461644e\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\">0</span></p></td><td class=\"cl-5461646c\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\">p &lt; .001***</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-54616476\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\">Model statistics</span></p></td><td class=\"cl-5461646d\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\"></span></p></td><td class=\"cl-5461646d\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\"></span></p></td><td class=\"cl-5461646d\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\"></span></p></td><td class=\"cl-5461646d\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\"></span></p></td><td class=\"cl-5461646d\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\"></span></p></td><td class=\"cl-5461646d\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\"></span></p></td><td class=\"cl-5461646d\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\"></span></p></td><td class=\"cl-5461646d\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\"></span></p></td><td class=\"cl-54616480\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\">Value</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-54616458\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\">Number of cases in model</span></p></td><td class=\"cl-5461644e\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\"></span></p></td><td class=\"cl-5461644e\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\"></span></p></td><td class=\"cl-5461644e\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\"></span></p></td><td class=\"cl-5461644e\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\"></span></p></td><td class=\"cl-5461644e\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\"></span></p></td><td class=\"cl-5461644e\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\"></span></p></td><td class=\"cl-5461644e\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\"></span></p></td><td class=\"cl-5461644e\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\"></span></p></td><td class=\"cl-5461646c\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\">25821</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-54616476\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\">Observed misses</span></p></td><td class=\"cl-5461646d\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\"></span></p></td><td class=\"cl-5461646d\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\"></span></p></td><td class=\"cl-5461646d\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\"></span></p></td><td class=\"cl-5461646d\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\"></span></p></td><td class=\"cl-5461646d\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\"></span></p></td><td class=\"cl-5461646d\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\"></span></p></td><td class=\"cl-5461646d\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\"></span></p></td><td class=\"cl-5461646d\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\">0 :</span></p></td><td class=\"cl-54616480\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\">17114</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-54616458\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\">Observed successes</span></p></td><td class=\"cl-5461644e\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\"></span></p></td><td class=\"cl-5461644e\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\"></span></p></td><td class=\"cl-5461644e\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\"></span></p></td><td class=\"cl-5461644e\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\"></span></p></td><td class=\"cl-5461644e\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\"></span></p></td><td class=\"cl-5461644e\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\"></span></p></td><td class=\"cl-5461644e\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\"></span></p></td><td class=\"cl-5461644e\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\">1 :</span></p></td><td class=\"cl-5461646c\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\">8707</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-54616476\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\">Null deviance</span></p></td><td class=\"cl-5461646d\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\"></span></p></td><td class=\"cl-5461646d\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\"></span></p></td><td class=\"cl-5461646d\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\"></span></p></td><td class=\"cl-5461646d\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\"></span></p></td><td class=\"cl-5461646d\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\"></span></p></td><td class=\"cl-5461646d\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\"></span></p></td><td class=\"cl-5461646d\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\"></span></p></td><td class=\"cl-5461646d\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\"></span></p></td><td class=\"cl-54616480\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\">33007.75</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-54616458\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\">Residual deviance</span></p></td><td class=\"cl-5461644e\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\"></span></p></td><td class=\"cl-5461644e\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\"></span></p></td><td class=\"cl-5461644e\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\"></span></p></td><td class=\"cl-5461644e\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\"></span></p></td><td class=\"cl-5461644e\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\"></span></p></td><td class=\"cl-5461644e\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\"></span></p></td><td class=\"cl-5461644e\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\"></span></p></td><td class=\"cl-5461644e\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\"></span></p></td><td class=\"cl-5461646c\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\">32139.54</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-54616476\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\">R2 (Nagelkerke)</span></p></td><td class=\"cl-5461646d\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\"></span></p></td><td class=\"cl-5461646d\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\"></span></p></td><td class=\"cl-5461646d\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\"></span></p></td><td class=\"cl-5461646d\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\"></span></p></td><td class=\"cl-5461646d\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\"></span></p></td><td class=\"cl-5461646d\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\"></span></p></td><td class=\"cl-5461646d\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\"></span></p></td><td class=\"cl-5461646d\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\"></span></p></td><td class=\"cl-54616480\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\">0.046</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-54616458\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\">R2 (Hosmer &amp; Lemeshow)</span></p></td><td class=\"cl-5461644e\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\"></span></p></td><td class=\"cl-5461644e\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\"></span></p></td><td class=\"cl-5461644e\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\"></span></p></td><td class=\"cl-5461644e\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\"></span></p></td><td class=\"cl-5461644e\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\"></span></p></td><td class=\"cl-5461644e\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\"></span></p></td><td class=\"cl-5461644e\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\"></span></p></td><td class=\"cl-5461644e\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\"></span></p></td><td class=\"cl-5461646c\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\">0.026</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-54616476\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\">R2 (Cox &amp; Snell)</span></p></td><td class=\"cl-5461646d\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\"></span></p></td><td class=\"cl-5461646d\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\"></span></p></td><td class=\"cl-5461646d\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\"></span></p></td><td class=\"cl-5461646d\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\"></span></p></td><td class=\"cl-5461646d\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\"></span></p></td><td class=\"cl-5461646d\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\"></span></p></td><td class=\"cl-5461646d\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\"></span></p></td><td class=\"cl-5461646d\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\"></span></p></td><td class=\"cl-54616480\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\">0.033</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-54616458\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\">C</span></p></td><td class=\"cl-5461644e\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\"></span></p></td><td class=\"cl-5461644e\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\"></span></p></td><td class=\"cl-5461644e\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\"></span></p></td><td class=\"cl-5461644e\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\"></span></p></td><td class=\"cl-5461644e\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\"></span></p></td><td class=\"cl-5461644e\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\"></span></p></td><td class=\"cl-5461644e\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\"></span></p></td><td class=\"cl-5461644e\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\"></span></p></td><td class=\"cl-5461646c\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\">0.602</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-54616476\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\">Somers' Dxy</span></p></td><td class=\"cl-5461646d\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\"></span></p></td><td class=\"cl-5461646d\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\"></span></p></td><td class=\"cl-5461646d\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\"></span></p></td><td class=\"cl-5461646d\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\"></span></p></td><td class=\"cl-5461646d\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\"></span></p></td><td class=\"cl-5461646d\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\"></span></p></td><td class=\"cl-5461646d\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\"></span></p></td><td class=\"cl-5461646d\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\"></span></p></td><td class=\"cl-54616480\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\">0.203</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-54616458\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\">AIC</span></p></td><td class=\"cl-5461644e\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\"></span></p></td><td class=\"cl-5461644e\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\"></span></p></td><td class=\"cl-5461644e\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\"></span></p></td><td class=\"cl-5461644e\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\"></span></p></td><td class=\"cl-5461644e\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\"></span></p></td><td class=\"cl-5461644e\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\"></span></p></td><td class=\"cl-5461644e\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\"></span></p></td><td class=\"cl-5461644e\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\"></span></p></td><td class=\"cl-5461646c\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\">32145.54</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-54616476\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\">Prediction accuracy</span></p></td><td class=\"cl-5461646d\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\"></span></p></td><td class=\"cl-5461646d\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\"></span></p></td><td class=\"cl-5461646d\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\"></span></p></td><td class=\"cl-5461646d\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\"></span></p></td><td class=\"cl-5461646d\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\"></span></p></td><td class=\"cl-5461646d\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\"></span></p></td><td class=\"cl-5461646d\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\"></span></p></td><td class=\"cl-5461646d\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\"></span></p></td><td class=\"cl-54616480\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\">0.66%</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-5461648b\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\">Model Likelihood Ratio Test</span></p></td><td class=\"cl-5461648a\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\"></span></p></td><td class=\"cl-5461648a\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\"></span></p></td><td class=\"cl-5461648a\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\"></span></p></td><td class=\"cl-5461648a\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\"></span></p></td><td class=\"cl-5461648a\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\"></span></p></td><td class=\"cl-5461648a\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\">Model L.R.: 868.21</span></p></td><td class=\"cl-5461648a\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\">df: 2</span></p></td><td class=\"cl-5461648a\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\">p-value: 0</span></p></td><td class=\"cl-54616494\"><p class=\"cl-546105d0\"><span class=\"cl-5460e848\">sig: p &lt; .001***</span></p></td></tr></tbody></table></div>\n```\n:::\n:::\n\n\n**R^2^ (Hosmer & Lemeshow)**\n\nHosmer and Lemeshow's R^2^ \"is the proportional reduction in the absolute value of the log-likelihood  measure and as such it is a measure of how much the badness of fit improves  as a result of the inclusion of the predictor variables. It can vary between 0 (indicating that the predictors are useless at predicting the outcome variable) and 1 (indicating that the model predicts the outcome variable perfectly)\" [@field2012discovering 317].\n\n**R^2^ (Cox & Snell)**\n\n \"Cox and Snell's R^2^ (1989) is based on the deviance of the model (-2LL(new)\n and the deviance of the baseline model (-2LL(baseline), and the sample size,\n n [...]. However, this statistic never reaches its theoretical maximum of 1.\n\n**R2 (Nagelkerke)**\n\n Since R^2^ (Cox & Snell) never reaches its theoretical maximum of 1,\n Nagelkerke (1991) suggested Nagelkerke's R^2^ [@field2012discovering 317-318].\n\n**Somers D~xy~**\n\n Somers D~xy~ is a rank correlation between predicted probabilities and observed\n responses ranges between 0 (randomness) and 1 (perfect prediction). Somers' D~xy~ should have a value higher than .5 for the model to be meaningful [@baayen2008analyzing 204].\n\n**C**\n\n C is an index of concordance between the predicted probability and the\n observed response. When C takes the value 0.5, the predictions are random,\n when it is 1, prediction is perfect. A value above 0.8 indicates that the\n model may have some real predictive capacity [@baayen2008analyzing 204].\n\n**Akaike information criteria (AIC)**\n\nAkaike information criteria (AlC = -2LL + 2k) provide a value that reflects a ratio between the number of predictors in the model and the variance that is explained by these predictors. Changes in AIC can serve as a measure of whether the inclusion of a variable leads to a significant increase in the amount of variance that is explained by the model. \"You can think of this as the price you pay for something: you get a better value of R^2^, but you pay a higher price, and was that higher price worth it?  These information criteria help you to decide. The BIC is the same as the AIC but adjusts the penalty included in the AlC (i.e., 2k) by the number of cases: BlC = -2LL + 2k x log(n) in which n is the number of cases in the model\" [@field2012discovering 318].\n\nWe can use the `reports` package [@report] to summarize the analysis.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nreport::report(lr.glm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nWe fitted a logistic model (estimated using ML) to predict EH with Age and Gender (formula: EH ~ Age + Gender). The model's explanatory power is weak (Tjur's R2 = 0.03). The model's intercept, corresponding to Age = Young and Gender = Men, is at -0.23 (95% CI [-0.28, -0.19], p < .001). Within this model:\n\n  - The effect of Age [Old] is statistically significant and negative (beta = -0.83, 95% CI [-0.90, -0.77], p < .001; Std. beta = -0.83, 95% CI [-0.90, -0.77])\n  - The effect of Gender [Women] is statistically significant and negative (beta = -0.42, 95% CI [-0.47, -0.37], p < .001; Std. beta = -0.42, 95% CI [-0.47, -0.37])\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using \n```\n:::\n:::\n\n\nWe can use this output to write up a final report: \n\nWe fitted a logistic model (estimated using ML) to predict the use of the utterance-final discourse particle *eh* with Age and Gender (formula: EH ~ Age + Gender). The model's explanatory power is weak (Tjur's R^2^ = 0.03). The model's intercept, corresponding to Age = Young and Gender = Men, is at -0.23 (95% CI [-0.28, -0.19], p < .001). Within this model:\n\n* The effect of Age [Old] is statistically significant and negative (beta = -0.83, 95% CI [-0.90, -0.77], p < .001; Std. beta = -0.83, 95% CI [-0.90, -0.77])\n\n* The effect of Gender [Women] is statistically significant and negative (beta = -0.42, 95% CI [-0.47, -0.37], p < .001; Std. beta = -0.42, 95% CI [-0.47, -0.37])\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using \n\n## Ordinal Regression{-}\n\nOrdinal regression is very similar to multiple linear regression but takes an ordinal dependent variable [@agresti2010analysis]. For this reason, ordinal regression is one of the key methods in analysing Likert data. \n\nTo see how an ordinal regression is implemented in R, we load and inspect the ordinaldata data set. The data set consists of 400 observations of students that were either educated at this school (Internal = 1) or not (Internal = 0). Some of the students have been abroad (Exchange = 1) while other have not (Exchange = 0). In addition, the data set contains the students' final score of a language test (FinalScore) and the dependent variable which the recommendation of a committee for an additional, very prestigious program. The recommendation has three levels (*very likely*, *somewhat likely*, and *unlikely*) and reflects the committees assessment of whether the student is likely to succeed in the program.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# load data\nordata  <- base::readRDS(url(\"https://slcladal.github.io/data/ord.rda\", \"rb\")) %>%\n  dplyr::rename(Recommend = 1, \n              Internal = 2, \n              Exchange = 3, \n              FinalScore = 4) %>%\n  dplyr::mutate(FinalScore = round(FinalScore, 2))\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n```{=html}\n<div class=\"tabwid\"><style>.cl-565cfb0a{table-layout:auto;width:75%;}.cl-56597b74{font-family:'DejaVu Sans';font-size:12pt;font-weight:bold;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-56597b7e{font-family:'DejaVu Sans';font-size:12pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-56598560{margin:0;text-align:center;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-5659856a{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-5659b03a{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-5659b044{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-5659b045{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-5659b04e{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-5659b058{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-5659b062{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-5659b063{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-5659b06c{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-5659b06d{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-5659b076{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-5659b080{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-5659b0bc{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}</style><table class='cl-565cfb0a'>\n```\n<caption class=\"Table Caption\">\n\nFirst 15 rows of the ordata.\n\n</caption>\n```{=html}\n<thead><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-5659b080\"><p class=\"cl-56598560\"><span class=\"cl-56597b74\">Recommend</span></p></td><td class=\"cl-5659b076\"><p class=\"cl-5659856a\"><span class=\"cl-56597b74\">Internal</span></p></td><td class=\"cl-5659b076\"><p class=\"cl-5659856a\"><span class=\"cl-56597b74\">Exchange</span></p></td><td class=\"cl-5659b0bc\"><p class=\"cl-5659856a\"><span class=\"cl-56597b74\">FinalScore</span></p></td></tr></thead><tbody><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-5659b044\"><p class=\"cl-56598560\"><span class=\"cl-56597b7e\">very likely</span></p></td><td class=\"cl-5659b045\"><p class=\"cl-5659856a\"><span class=\"cl-56597b7e\">0</span></p></td><td class=\"cl-5659b045\"><p class=\"cl-5659856a\"><span class=\"cl-56597b7e\">0</span></p></td><td class=\"cl-5659b03a\"><p class=\"cl-5659856a\"><span class=\"cl-56597b7e\">3.26</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-5659b058\"><p class=\"cl-56598560\"><span class=\"cl-56597b7e\">somewhat likely</span></p></td><td class=\"cl-5659b04e\"><p class=\"cl-5659856a\"><span class=\"cl-56597b7e\">1</span></p></td><td class=\"cl-5659b04e\"><p class=\"cl-5659856a\"><span class=\"cl-56597b7e\">0</span></p></td><td class=\"cl-5659b062\"><p class=\"cl-5659856a\"><span class=\"cl-56597b7e\">3.21</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-5659b044\"><p class=\"cl-56598560\"><span class=\"cl-56597b7e\">unlikely</span></p></td><td class=\"cl-5659b045\"><p class=\"cl-5659856a\"><span class=\"cl-56597b7e\">1</span></p></td><td class=\"cl-5659b045\"><p class=\"cl-5659856a\"><span class=\"cl-56597b7e\">1</span></p></td><td class=\"cl-5659b03a\"><p class=\"cl-5659856a\"><span class=\"cl-56597b7e\">3.94</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-5659b058\"><p class=\"cl-56598560\"><span class=\"cl-56597b7e\">somewhat likely</span></p></td><td class=\"cl-5659b04e\"><p class=\"cl-5659856a\"><span class=\"cl-56597b7e\">0</span></p></td><td class=\"cl-5659b04e\"><p class=\"cl-5659856a\"><span class=\"cl-56597b7e\">0</span></p></td><td class=\"cl-5659b062\"><p class=\"cl-5659856a\"><span class=\"cl-56597b7e\">2.81</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-5659b044\"><p class=\"cl-56598560\"><span class=\"cl-56597b7e\">somewhat likely</span></p></td><td class=\"cl-5659b045\"><p class=\"cl-5659856a\"><span class=\"cl-56597b7e\">0</span></p></td><td class=\"cl-5659b045\"><p class=\"cl-5659856a\"><span class=\"cl-56597b7e\">0</span></p></td><td class=\"cl-5659b03a\"><p class=\"cl-5659856a\"><span class=\"cl-56597b7e\">2.53</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-5659b058\"><p class=\"cl-56598560\"><span class=\"cl-56597b7e\">unlikely</span></p></td><td class=\"cl-5659b04e\"><p class=\"cl-5659856a\"><span class=\"cl-56597b7e\">0</span></p></td><td class=\"cl-5659b04e\"><p class=\"cl-5659856a\"><span class=\"cl-56597b7e\">1</span></p></td><td class=\"cl-5659b062\"><p class=\"cl-5659856a\"><span class=\"cl-56597b7e\">2.59</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-5659b044\"><p class=\"cl-56598560\"><span class=\"cl-56597b7e\">somewhat likely</span></p></td><td class=\"cl-5659b045\"><p class=\"cl-5659856a\"><span class=\"cl-56597b7e\">0</span></p></td><td class=\"cl-5659b045\"><p class=\"cl-5659856a\"><span class=\"cl-56597b7e\">0</span></p></td><td class=\"cl-5659b03a\"><p class=\"cl-5659856a\"><span class=\"cl-56597b7e\">2.56</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-5659b058\"><p class=\"cl-56598560\"><span class=\"cl-56597b7e\">somewhat likely</span></p></td><td class=\"cl-5659b04e\"><p class=\"cl-5659856a\"><span class=\"cl-56597b7e\">0</span></p></td><td class=\"cl-5659b04e\"><p class=\"cl-5659856a\"><span class=\"cl-56597b7e\">0</span></p></td><td class=\"cl-5659b062\"><p class=\"cl-5659856a\"><span class=\"cl-56597b7e\">2.73</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-5659b044\"><p class=\"cl-56598560\"><span class=\"cl-56597b7e\">unlikely</span></p></td><td class=\"cl-5659b045\"><p class=\"cl-5659856a\"><span class=\"cl-56597b7e\">0</span></p></td><td class=\"cl-5659b045\"><p class=\"cl-5659856a\"><span class=\"cl-56597b7e\">0</span></p></td><td class=\"cl-5659b03a\"><p class=\"cl-5659856a\"><span class=\"cl-56597b7e\">3.00</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-5659b058\"><p class=\"cl-56598560\"><span class=\"cl-56597b7e\">somewhat likely</span></p></td><td class=\"cl-5659b04e\"><p class=\"cl-5659856a\"><span class=\"cl-56597b7e\">1</span></p></td><td class=\"cl-5659b04e\"><p class=\"cl-5659856a\"><span class=\"cl-56597b7e\">0</span></p></td><td class=\"cl-5659b062\"><p class=\"cl-5659856a\"><span class=\"cl-56597b7e\">3.50</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-5659b044\"><p class=\"cl-56598560\"><span class=\"cl-56597b7e\">unlikely</span></p></td><td class=\"cl-5659b045\"><p class=\"cl-5659856a\"><span class=\"cl-56597b7e\">1</span></p></td><td class=\"cl-5659b045\"><p class=\"cl-5659856a\"><span class=\"cl-56597b7e\">1</span></p></td><td class=\"cl-5659b03a\"><p class=\"cl-5659856a\"><span class=\"cl-56597b7e\">3.65</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-5659b058\"><p class=\"cl-56598560\"><span class=\"cl-56597b7e\">somewhat likely</span></p></td><td class=\"cl-5659b04e\"><p class=\"cl-5659856a\"><span class=\"cl-56597b7e\">0</span></p></td><td class=\"cl-5659b04e\"><p class=\"cl-5659856a\"><span class=\"cl-56597b7e\">0</span></p></td><td class=\"cl-5659b062\"><p class=\"cl-5659856a\"><span class=\"cl-56597b7e\">2.84</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-5659b044\"><p class=\"cl-56598560\"><span class=\"cl-56597b7e\">very likely</span></p></td><td class=\"cl-5659b045\"><p class=\"cl-5659856a\"><span class=\"cl-56597b7e\">0</span></p></td><td class=\"cl-5659b045\"><p class=\"cl-5659856a\"><span class=\"cl-56597b7e\">1</span></p></td><td class=\"cl-5659b03a\"><p class=\"cl-5659856a\"><span class=\"cl-56597b7e\">3.90</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-5659b058\"><p class=\"cl-56598560\"><span class=\"cl-56597b7e\">somewhat likely</span></p></td><td class=\"cl-5659b04e\"><p class=\"cl-5659856a\"><span class=\"cl-56597b7e\">0</span></p></td><td class=\"cl-5659b04e\"><p class=\"cl-5659856a\"><span class=\"cl-56597b7e\">0</span></p></td><td class=\"cl-5659b062\"><p class=\"cl-5659856a\"><span class=\"cl-56597b7e\">2.68</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-5659b06d\"><p class=\"cl-56598560\"><span class=\"cl-56597b7e\">unlikely</span></p></td><td class=\"cl-5659b063\"><p class=\"cl-5659856a\"><span class=\"cl-56597b7e\">1</span></p></td><td class=\"cl-5659b063\"><p class=\"cl-5659856a\"><span class=\"cl-56597b7e\">0</span></p></td><td class=\"cl-5659b06c\"><p class=\"cl-5659856a\"><span class=\"cl-56597b7e\">3.57</span></p></td></tr></tbody></table></div>\n```\n:::\n:::\n\n\nIn a first step, we need to relevel the ordinal variable to represent an ordinal factor (or a progression from \"unlikely\" over \"somewhat likely\" to \"very likely\". And we will also factorize Internal and Exchange to make it easier to interpret the output later on.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# relevel data\nordata <- ordata %>%\n  dplyr::mutate(Recommend = factor(Recommend, \n                           levels=c(\"unlikely\", \"somewhat likely\", \"very likely\"),\n                           labels=c(\"unlikely\",  \"somewhat likely\",  \"very likely\"))) %>%\n  dplyr::mutate(Exchange = ifelse(Exchange == 1, \"Exchange\", \"NoExchange\")) %>%\n  dplyr::mutate(Internal = ifelse(Internal == 1, \"Internal\", \"External\"))\n```\n:::\n\n\nNow that the dependent variable is releveled, we check the distribution of the variable levels by tabulating the data. To get a better understanding of the data we create frequency tables across variables rather than viewing the variables in isolation.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## three way cross tabs (xtabs) and flatten the table\nftable(xtabs(~ Exchange + Recommend + Internal, data = ordata))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                           Internal External Internal\nExchange   Recommend                                 \nExchange   unlikely                       25        6\n           somewhat likely                12        4\n           very likely                     7        3\nNoExchange unlikely                      175       14\n           somewhat likely                98       26\n           very likely                    20       10\n```\n:::\n:::\n\n\nWe also check the mean and standard deviation of the final score as final score is a numeric variable and cannot be tabulated (unless we convert it to a factor).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(ordata$FinalScore); sd(ordata$FinalScore)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n1.900000 2.720000 2.990000 2.998925 3.270000 4.000000 \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.397940933861\n```\n:::\n:::\n\n\nThe lowest score is 1.9 and the highest score is a 4.0 with a mean of approximately 3. Finally, we inspect the distributions graphically.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# visualize data\nggplot(ordata, aes(x = Recommend, y = FinalScore)) +\n  geom_boxplot(size = .75) +\n  geom_jitter(alpha = .5) +\n  facet_grid(Exchange ~ Internal, margins = TRUE) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1))\n```\n\n::: {.cell-output-display}\n![](fixreg_files/figure-html/orr6-1.png){width=672}\n:::\n:::\n\n\nWe see that we have only few students that have taken part in an exchange program and there are also only few internal students overall. With respect to recommendations, only few students are considered to very likely succeed in the program. We can now start with the modeling by using the `polr` function. To make things easier for us, we will only consider the main effects here as this tutorial only aims to how to implement an ordinal regression but not how it should be done in a proper study - then, the model fitting and diagnostic procedures would have to be performed accurately, of course. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# fit ordered logit model and store results 'm'\nm <- polr(Recommend ~ Internal + Exchange + FinalScore, data = ordata, Hess=TRUE)\n# summarize model\nsummary(m)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCall:\npolr(formula = Recommend ~ Internal + Exchange + FinalScore, \n    data = ordata, Hess = TRUE)\n\nCoefficients:\n                          Value  Std. Error     t value\nInternalInternal   1.0476639460 0.265789134 3.941710973\nExchangeNoExchange 0.0586810767 0.297858822 0.197009698\nFinalScore         0.6157435926 0.260631275 2.362508462\n\nIntercepts:\n                            Value       Std. Error  t value    \nunlikely|somewhat likely    2.261997623 0.882173604 2.564118460\nsomewhat likely|very likely 4.357441880 0.904467838 4.817685824\n\nResidual Deviance: 717.024871356 \nAIC: 727.024871356 \n```\n:::\n:::\n\n\nThe results show that having studied here at this school increases the chances of receiving a positive recommendation but that having been on an exchange has a negative but insignificant effect on the recommendation. The final score also correlates positively with a positive recommendation but not as much as having studied here.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## store table\n(ctable <- coef(summary(m)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                                      Value     Std. Error        t value\nInternalInternal            1.0476639460469 0.265789134041 3.941710972596\nExchangeNoExchange          0.0586810766831 0.297858821982 0.197009698396\nFinalScore                  0.6157435925546 0.260631274948 2.362508462104\nunlikely|somewhat likely    2.2619976233665 0.882173604268 2.564118459704\nsomewhat likely|very likely 4.3574418799106 0.904467837679 4.817685824069\n```\n:::\n:::\n\n\nAs the regression report does not provide p-values, we have to calculate them separately (after having calculated them, we add them to the coefficient table).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## calculate and store p values\np <- pnorm(abs(ctable[, \"t value\"]), lower.tail = FALSE) * 2\n## combined table\n(ctable <- cbind(ctable, \"p value\" = p))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                                      Value     Std. Error        t value\nInternalInternal            1.0476639460469 0.265789134041 3.941710972596\nExchangeNoExchange          0.0586810766831 0.297858821982 0.197009698396\nFinalScore                  0.6157435925546 0.260631274948 2.362508462104\nunlikely|somewhat likely    2.2619976233665 0.882173604268 2.564118459704\nsomewhat likely|very likely 4.3574418799106 0.904467837679 4.817685824069\n                                        p value\nInternalInternal            0.00008090242989074\nExchangeNoExchange          0.84381994829785212\nFinalScore                  0.01815172703306605\nunlikely|somewhat likely    0.01034382345525988\nsomewhat likely|very likely 0.00000145232812832\n```\n:::\n:::\n\nAs predicted, Exchange does not have a significant effect but FinalScore and Internal both correlate significantly with the likelihood of receiving a positive recommendation.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# extract profiled confidence intervals\nci <- confint(m)\n# calculate odds ratios and combine them with profiled CIs\nexp(cbind(OR = coef(m), ci))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                              OR          2.5 %        97.5 %\nInternalInternal   2.85098328212 1.695837799597 4.81711408266\nExchangeNoExchange 1.06043698872 0.595033205649 1.91977108408\nFinalScore         1.85103250193 1.113625249822 3.09849059342\n```\n:::\n:::\n\n\nThe odds ratios show that internal students are 2.85 times or 285 percent as likely as non-internal students to receive positive evaluations and that a 1-point increase in the test score lead to a 1.85 times or 185 percent increase in the chances of receiving a positive recommendation. The effect of an exchange is slightly negative but, as we have seen above, not significant.\n\n\n\n## Poisson Regression{-}\n\nThis section is based on [this tutorials](https://stats.idre.ucla.edu/r/dae/poisson-regression/) on how to perform a Poisson regression in R. \n\n\n<div class=\"warning\" style='padding:0.1em; background-color:#f2f2f2; color:#51247a'>\n<span>\n<p style='margin-top:1em; text-align:center'>\n<br>Poisson regressions are used to analyze data where the dependent variable represents counts.</p>\n<p style='margin-left:1em;'>\n</p></span>\n</div>\n\n<br>\n\nThis applied particularly to counts that are based on observations of something that is measured in set intervals. For instances the number of pauses in two-minute-long conversations. Poisson regressions are particularly appealing when dealing with rare events, i.e. when something only occurs very infrequently. In such cases, normal linear regressions do not work because the instances that do occur are automatically considered outliers. Therefore, it is useful to check if the data conform to a Poisson distribution.\n\nHowever, the tricky thing about Poisson regressions is that the data has to conform to the Poisson distribution which is, according to my experience, rarely the case, unfortunately. The Gaussian Normal Distribution is very flexible because it is defined by two parameters, the mean (mu, i.e. $\\mu$) and the standard deviation (sigma, i.e. $\\sigma$). This allows the normal distribution to take very different shapes (for example, very high and slim (compressed) or very wide and flat). In contrast, the Poisson is defined by only one parameter (lambda, i.e. $\\lambda$) which mean that if we have a mean of 2, then the standard deviation is also 2 (actually we would have to say that the mean is $\\lambda$ and the standard deviation is also  $\\lambda$ or $\\lambda$ = $\\mu$ = $\\sigma$). This is much trickier for natural data as this means that the Poisson distribution is very rigid.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](fixreg_files/figure-html/pr0-1.png){width=672}\n:::\n:::\n\n\nAs we can see, as $\\lambda$ takes on higher values, the distribution becomes wider and flatter - a compressed distribution with a high mean can therefore not be Poisson-distributed. We will now start by loading the data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# load data\npoissondata  <- base::readRDS(url(\"https://slcladal.github.io/data/prd.rda\", \"rb\"))\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n```{=html}\n<div class=\"tabwid\"><style>.cl-578ec5f8{table-layout:auto;width:75%;}.cl-578a4866{font-family:'DejaVu Sans';font-size:12pt;font-weight:bold;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-578a487a{font-family:'DejaVu Sans';font-size:12pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-578a5b76{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-578a5b80{margin:0;text-align:center;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-578a9c80{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-578a9c8a{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-578a9c8b{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-578a9c94{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-578a9c9e{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-578a9ca8{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-578a9cb2{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-578a9cb3{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-578a9cbc{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-578a9cc6{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-578a9cd0{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-578a9cda{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-578a9cdb{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-578a9ce4{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-578a9cee{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-578a9cf8{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}</style><table class='cl-578ec5f8'>\n```\n<caption class=\"Table Caption\">\n\nFirst 15 rows of the poissondata.\n\n</caption>\n```{=html}\n<thead><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-578a9cf8\"><p class=\"cl-578a5b76\"><span class=\"cl-578a4866\">Id</span></p></td><td class=\"cl-578a9ce4\"><p class=\"cl-578a5b76\"><span class=\"cl-578a4866\">Pauses</span></p></td><td class=\"cl-578a9cee\"><p class=\"cl-578a5b80\"><span class=\"cl-578a4866\">Language</span></p></td><td class=\"cl-578a9cdb\"><p class=\"cl-578a5b76\"><span class=\"cl-578a4866\">Alcohol</span></p></td></tr></thead><tbody><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-578a9c8b\"><p class=\"cl-578a5b76\"><span class=\"cl-578a487a\">45</span></p></td><td class=\"cl-578a9c94\"><p class=\"cl-578a5b76\"><span class=\"cl-578a487a\">0</span></p></td><td class=\"cl-578a9c80\"><p class=\"cl-578a5b80\"><span class=\"cl-578a487a\">German</span></p></td><td class=\"cl-578a9c8a\"><p class=\"cl-578a5b76\"><span class=\"cl-578a487a\">41</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-578a9ca8\"><p class=\"cl-578a5b76\"><span class=\"cl-578a487a\">108</span></p></td><td class=\"cl-578a9c9e\"><p class=\"cl-578a5b76\"><span class=\"cl-578a487a\">0</span></p></td><td class=\"cl-578a9cb3\"><p class=\"cl-578a5b80\"><span class=\"cl-578a487a\">Russian</span></p></td><td class=\"cl-578a9cb2\"><p class=\"cl-578a5b76\"><span class=\"cl-578a487a\">41</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-578a9c8b\"><p class=\"cl-578a5b76\"><span class=\"cl-578a487a\">15</span></p></td><td class=\"cl-578a9c94\"><p class=\"cl-578a5b76\"><span class=\"cl-578a487a\">0</span></p></td><td class=\"cl-578a9c80\"><p class=\"cl-578a5b80\"><span class=\"cl-578a487a\">German</span></p></td><td class=\"cl-578a9c8a\"><p class=\"cl-578a5b76\"><span class=\"cl-578a487a\">44</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-578a9ca8\"><p class=\"cl-578a5b76\"><span class=\"cl-578a487a\">67</span></p></td><td class=\"cl-578a9c9e\"><p class=\"cl-578a5b76\"><span class=\"cl-578a487a\">0</span></p></td><td class=\"cl-578a9cb3\"><p class=\"cl-578a5b80\"><span class=\"cl-578a487a\">German</span></p></td><td class=\"cl-578a9cb2\"><p class=\"cl-578a5b76\"><span class=\"cl-578a487a\">42</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-578a9c8b\"><p class=\"cl-578a5b76\"><span class=\"cl-578a487a\">153</span></p></td><td class=\"cl-578a9c94\"><p class=\"cl-578a5b76\"><span class=\"cl-578a487a\">0</span></p></td><td class=\"cl-578a9c80\"><p class=\"cl-578a5b80\"><span class=\"cl-578a487a\">German</span></p></td><td class=\"cl-578a9c8a\"><p class=\"cl-578a5b76\"><span class=\"cl-578a487a\">40</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-578a9ca8\"><p class=\"cl-578a5b76\"><span class=\"cl-578a487a\">51</span></p></td><td class=\"cl-578a9c9e\"><p class=\"cl-578a5b76\"><span class=\"cl-578a487a\">0</span></p></td><td class=\"cl-578a9cb3\"><p class=\"cl-578a5b80\"><span class=\"cl-578a487a\">Russian</span></p></td><td class=\"cl-578a9cb2\"><p class=\"cl-578a5b76\"><span class=\"cl-578a487a\">42</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-578a9c8b\"><p class=\"cl-578a5b76\"><span class=\"cl-578a487a\">164</span></p></td><td class=\"cl-578a9c94\"><p class=\"cl-578a5b76\"><span class=\"cl-578a487a\">0</span></p></td><td class=\"cl-578a9c80\"><p class=\"cl-578a5b80\"><span class=\"cl-578a487a\">German</span></p></td><td class=\"cl-578a9c8a\"><p class=\"cl-578a5b76\"><span class=\"cl-578a487a\">46</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-578a9ca8\"><p class=\"cl-578a5b76\"><span class=\"cl-578a487a\">133</span></p></td><td class=\"cl-578a9c9e\"><p class=\"cl-578a5b76\"><span class=\"cl-578a487a\">0</span></p></td><td class=\"cl-578a9cb3\"><p class=\"cl-578a5b80\"><span class=\"cl-578a487a\">German</span></p></td><td class=\"cl-578a9cb2\"><p class=\"cl-578a5b76\"><span class=\"cl-578a487a\">40</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-578a9c8b\"><p class=\"cl-578a5b76\"><span class=\"cl-578a487a\">2</span></p></td><td class=\"cl-578a9c94\"><p class=\"cl-578a5b76\"><span class=\"cl-578a487a\">0</span></p></td><td class=\"cl-578a9c80\"><p class=\"cl-578a5b80\"><span class=\"cl-578a487a\">German</span></p></td><td class=\"cl-578a9c8a\"><p class=\"cl-578a5b76\"><span class=\"cl-578a487a\">33</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-578a9ca8\"><p class=\"cl-578a5b76\"><span class=\"cl-578a487a\">53</span></p></td><td class=\"cl-578a9c9e\"><p class=\"cl-578a5b76\"><span class=\"cl-578a487a\">0</span></p></td><td class=\"cl-578a9cb3\"><p class=\"cl-578a5b80\"><span class=\"cl-578a487a\">German</span></p></td><td class=\"cl-578a9cb2\"><p class=\"cl-578a5b76\"><span class=\"cl-578a487a\">46</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-578a9c8b\"><p class=\"cl-578a5b76\"><span class=\"cl-578a487a\">1</span></p></td><td class=\"cl-578a9c94\"><p class=\"cl-578a5b76\"><span class=\"cl-578a487a\">0</span></p></td><td class=\"cl-578a9c80\"><p class=\"cl-578a5b80\"><span class=\"cl-578a487a\">German</span></p></td><td class=\"cl-578a9c8a\"><p class=\"cl-578a5b76\"><span class=\"cl-578a487a\">40</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-578a9ca8\"><p class=\"cl-578a5b76\"><span class=\"cl-578a487a\">128</span></p></td><td class=\"cl-578a9c9e\"><p class=\"cl-578a5b76\"><span class=\"cl-578a487a\">0</span></p></td><td class=\"cl-578a9cb3\"><p class=\"cl-578a5b80\"><span class=\"cl-578a487a\">English</span></p></td><td class=\"cl-578a9cb2\"><p class=\"cl-578a5b76\"><span class=\"cl-578a487a\">38</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-578a9c8b\"><p class=\"cl-578a5b76\"><span class=\"cl-578a487a\">16</span></p></td><td class=\"cl-578a9c94\"><p class=\"cl-578a5b76\"><span class=\"cl-578a487a\">0</span></p></td><td class=\"cl-578a9c80\"><p class=\"cl-578a5b80\"><span class=\"cl-578a487a\">German</span></p></td><td class=\"cl-578a9c8a\"><p class=\"cl-578a5b76\"><span class=\"cl-578a487a\">44</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-578a9ca8\"><p class=\"cl-578a5b76\"><span class=\"cl-578a487a\">106</span></p></td><td class=\"cl-578a9c9e\"><p class=\"cl-578a5b76\"><span class=\"cl-578a487a\">0</span></p></td><td class=\"cl-578a9cb3\"><p class=\"cl-578a5b80\"><span class=\"cl-578a487a\">German</span></p></td><td class=\"cl-578a9cb2\"><p class=\"cl-578a5b76\"><span class=\"cl-578a487a\">37</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-578a9cc6\"><p class=\"cl-578a5b76\"><span class=\"cl-578a487a\">89</span></p></td><td class=\"cl-578a9cda\"><p class=\"cl-578a5b76\"><span class=\"cl-578a487a\">0</span></p></td><td class=\"cl-578a9cbc\"><p class=\"cl-578a5b80\"><span class=\"cl-578a487a\">German</span></p></td><td class=\"cl-578a9cd0\"><p class=\"cl-578a5b76\"><span class=\"cl-578a487a\">40</span></p></td></tr></tbody></table></div>\n```\n:::\n:::\n\n\nWe will clean the data by factorizing Id which is currently considered a numeric variable rather than a factor.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# process data\npoissondata <- poissondata %>%\n  mutate(Id = factor(Id, levels = 1:200, labels = 1:200))\n# inspect data\nstr(poissondata)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n'data.frame':\t200 obs. of  4 variables:\n $ Id      : Factor w/ 200 levels \"1\",\"2\",\"3\",\"4\",..: 45 108 15 67 153 51 164 133 2 53 ...\n $ Pauses  : int  0 0 0 0 0 0 0 0 0 0 ...\n $ Language: chr  \"German\" \"Russian\" \"German\" \"German\" ...\n $ Alcohol : int  41 41 44 42 40 42 46 40 33 46 ...\n```\n:::\n:::\n\n\nFirst, we check if the conditions for a Poisson regression are met.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# output the results\ngf = vcd::goodfit(poissondata$Pauses, \n                  type= \"poisson\", \n                  method= \"ML\")\n# inspect results\nsummary(gf)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\t Goodness-of-fit test for poisson distribution\n\n                           X^2 df            P(> X^2)\nLikelihood Ratio 33.0122916717  5 0.00000374234139957\n```\n:::\n:::\n\n\nIf the p-values is smaller than .05, then data is not Poisson distributed which means that it differs significantly from a Poisson distribution and is very likely over-dispersed. We will check the divergence from a Poisson distribution visually by plotting the observed counts against the expected counts if the data were Poisson distributed.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(gf,main=\"Count data vs Poisson distribution\")\n```\n\n::: {.cell-output-display}\n![](fixreg_files/figure-html/pr5-1.png){width=672}\n:::\n:::\n\n\nAlthough the goodfit function reported that the data differs significantly from the Poisson distribution, the fit is rather good. We can use an additional Levene's test to check if variance homogeneity is given. \n\n<div class=\"warning\" style='padding:0.1em; background-color:#f2f2f2; color:#51247a'>\n<span>\n<p style='margin-top:1em; text-align:center'>\n<b>NOTE</b><br><br>The use of Levene's test to check if the model is heteroscedastic is generally not recommended as it is too lax when dealing with few observations (because in such cases it does not have the power to identify heteroscedasticity) while it is too harsh when dealing with many observations (when heteroscedasticity typically is not a severe problem).</p>\n<p style='margin-left:1em;'>\n</p></span>\n</div>\n\n\n<br>\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# check homogeneity\nleveneTest(poissondata$Pauses, poissondata$Language, center = mean)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLevene's Test for Homogeneity of Variance (center = mean)\n       Df  F value        Pr(>F)    \ngroup   2 17.15274 0.00000013571 ***\n      197                           \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\nThe Levene's test indicates that variance homogeneity is also violated. Since both the approximation to a Poisson distribution and variance homogeneity are violated, we should switch either to a quasi-Poisson model or a negative binomial model. However, as we are only interested in how to implement a Poisson model here, we continue despite the fact that this could not be recommended if we were actually interested in accurate results based on a reliable model. \n\nIn a next step, we summarize Progression by inspecting the means and standard deviations of the individual variable levels.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# extract mean and standard deviation\nwith(poissondata, tapply(Pauses, Language, function(x) {\n  sprintf(\"M (SD) = %1.2f (%1.2f)\", mean(x), sd(x))\n}))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n               English                 German                Russian \n\"M (SD) = 1.00 (1.28)\" \"M (SD) = 0.24 (0.52)\" \"M (SD) = 0.20 (0.40)\" \n```\n:::\n:::\n\n\nNow, we visualize the data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# plot data\nggplot(poissondata, aes(Pauses, fill = Language)) +\n  geom_histogram(binwidth=.5, position=\"dodge\") +\n  scale_fill_manual(values=c(\"gray30\", \"gray50\", \"gray70\"))\n```\n\n::: {.cell-output-display}\n![](fixreg_files/figure-html/pr8-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# calculate Poisson regression\nm1.poisson <- glm(Pauses ~ Language + Alcohol, family=\"poisson\", data=poissondata)\n# inspect model\nsummary(m1.poisson)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nglm(formula = Pauses ~ Language + Alcohol, family = \"poisson\", \n    data = poissondata)\n\nDeviance Residuals: \n         Min            1Q        Median            3Q           Max  \n-2.204338080  -0.843641817  -0.510586515   0.255772098   2.679576560  \n\nCoefficients:\n                     Estimate    Std. Error  z value         Pr(>|z|)    \n(Intercept)     -4.1632652529  0.6628774832 -6.28060 0.00000000033728 ***\nLanguageGerman  -0.7140499158  0.3200148750 -2.23130         0.025661 *  \nLanguageRussian -1.0838591456  0.3582529824 -3.02540         0.002483 ** \nAlcohol          0.0701523975  0.0105992050  6.61865 0.00000000003625 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 287.6722345  on 199  degrees of freedom\nResidual deviance: 189.4496199  on 196  degrees of freedom\nAIC: 373.5045031\n\nNumber of Fisher Scoring iterations: 6\n```\n:::\n:::\n\n\nIn addition to the Estimates for the coefficients, we could also calculate the confidence intervals for the coefficients (LL stands for lower limit and UL for upper limit in the table below).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# calculate model\ncov.m1 <- sandwich::vcovHC(m1.poisson, type=\"HC0\")\n# extract standard error\nstd.err <- sqrt(diag(cov.m1))\n# extract robust estimates\nr.est <- cbind(Estimate= coef(m1.poisson), \n               \"Robust SE\" = std.err,\n               \"Pr(>|z|)\" = 2 * pnorm(abs(coef(m1.poisson)/std.err),\n                                      lower.tail=FALSE),\nLL = coef(m1.poisson) - 1.96 * std.err,\nUL = coef(m1.poisson) + 1.96 * std.err)\n# inspect data\nr.est\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                        Estimate       Robust SE                 Pr(>|z|)\n(Intercept)     -4.1632652529178 0.6480942940026 0.0000000001328637743948\nLanguageGerman  -0.7140499157783 0.2986422497410 0.0168031204011183932234\nLanguageRussian -1.0838591456208 0.3210481575684 0.0007354744824167306532\nAlcohol          0.0701523974937 0.0104351647012 0.0000000000178397516955\n                              LL               UL\n(Intercept)     -5.4335300691629 -2.8930004366727\nLanguageGerman  -1.2993887252708 -0.1287111062859\nLanguageRussian -1.7131135344549 -0.4546047567867\nAlcohol          0.0496994746793  0.0906053203082\n```\n:::\n:::\n\n\nWe can now calculate the p-value of the model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwith(m1.poisson, cbind(res.deviance = deviance, df = df.residual,\n  p = pchisq(deviance, df.residual, lower.tail=FALSE)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     res.deviance  df              p\n[1,] 189.44961991 196 0.618227445717\n```\n:::\n:::\n\n\nNow, we check, if removing Language leads to a significant decrease in model fit.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# remove Language from the model\nm2.poisson <- update(m1.poisson, . ~ . -Language)\n# check if dropping Language causes a significant decrease in model fit\nanova(m2.poisson, m1.poisson, test=\"Chisq\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Deviance Table\n\nModel 1: Pauses ~ Alcohol\nModel 2: Pauses ~ Language + Alcohol\n  Resid. Df  Resid. Dev Df   Deviance   Pr(>Chi)    \n1       198 204.0213018                             \n2       196 189.4496199  2 14.5716819 0.00068517 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\nWe now calculate robust coefficients using the `msm` package [@msm].\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# get estimates\ns <- msm::deltamethod(list(~ exp(x1), ~ exp(x2), ~ exp(x3), ~ exp(x4)), \n                                                coef(m1.poisson), cov.m1)\n# exponentiate old estimates dropping the p values\nrexp.est <- exp(r.est[, -3])\n# replace SEs with estimates for exponentiated coefficients\nrexp.est[, \"Robust SE\"] <- s\n# display results\nrexp.est\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                       Estimate       Robust SE               LL\n(Intercept)     0.0155566784084 0.0100821945101 0.00436765044896\nLanguageGerman  0.4896571063601 0.1462322998451 0.27269843575906\nLanguageRussian 0.3382875026084 0.1086065794408 0.18030353649622\nAlcohol         1.0726716412682 0.0111935052470 1.05095521026088\n                             UL\n(Intercept)     0.0554097096208\nLanguageGerman  0.8792279322820\nLanguageRussian 0.6346987787641\nAlcohol         1.0948368101199\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# extract predicted values\n(s1 <- data.frame(Alcohol = mean(poissondata$Alcohol),\n  Language = factor(1:3, levels = 1:3, labels = names(table(poissondata$Language)))))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  Alcohol Language\n1  52.645  English\n2  52.645   German\n3  52.645  Russian\n```\n:::\n\n```{.r .cell-code}\n# show results\npredict(m1.poisson, s1, type=\"response\", se.fit=TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$fit\n             1              2              3 \n0.624944591447 0.306008560283 0.211410945109 \n\n$se.fit\n              1               2               3 \n0.0862811728183 0.0883370633684 0.0705010813453 \n\n$residual.scale\n[1] 1\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n## calculate and store predicted values\npoissondata$Predicted <- predict(m1.poisson, type=\"response\")\n## order by program and then by math\npoissondata <- poissondata[with(poissondata, order(Language, Alcohol)), ]\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n## create the plot\nggplot(poissondata, aes(x = Alcohol, y = Predicted, colour = Language)) +\n  geom_point(aes(y = Pauses), alpha=.5, \n             position=position_jitter(h=.2)) +\n  geom_line(size = 1) +\n  labs(x = \"Alcohol (ml)\", y = \"Expected number of pauses\") +\n  scale_color_manual(values=c(\"gray30\", \"gray50\", \"gray70\"))\n```\n\n::: {.cell-output-display}\n![](fixreg_files/figure-html/pr16-1.png){width=672}\n:::\n:::\n\n\n\nThat's it for this tutorial. We hope that you have enjoyed this tutorial and learned how to perform regression analysis including model fitting and model diagnostics as well as reporting regression results.\n\n\n# Citation & Session Info {-}\n\nSchweinberger, Martin. 2022. *Fixed-Effects Regression Modelling in R*. Brisbane: The University of Queensland. url: https://slcladal.github.io/fixreg.html (Version 2022.08.31).\n\n```\n@manual{schweinberger2022fixreg,\n  author = {Schweinberger, Martin},\n  title = {Fixed-Effects Regression Modelling in R},\n  note = {https://slcladal.github.io/fixreg.html},\n  year = {2022},\n  organization = \"The University of Queensland, Australia. School of Languages and Cultures},\n  address = {Brisbane},\n  edition = {2022.08.31}\n}\n```\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsessionInfo()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nR version 4.2.1 (2022-06-23)\nPlatform: x86_64-pc-linux-gnu (64-bit)\nRunning under: Ubuntu 22.04.1 LTS\n\nMatrix products: default\nBLAS:   /usr/lib/x86_64-linux-gnu/blas/libblas.so.3.10.0\nLAPACK: /usr/lib/x86_64-linux-gnu/lapack/liblapack.so.3.10.0\n\nlocale:\n [1] LC_CTYPE=en_AU.UTF-8          LC_NUMERIC=C                 \n [3] LC_TIME=en_AU.UTF-8           LC_COLLATE=en_AU.UTF-8       \n [5] LC_MONETARY=en_AU.UTF-8       LC_MESSAGES=en_AU.UTF-8      \n [7] LC_PAPER=en_AU.UTF-8          LC_NAME=en_AU.UTF-8          \n [9] LC_ADDRESS=en_AU.UTF-8        LC_TELEPHONE=en_AU.UTF-8     \n[11] LC_MEASUREMENT=en_AU.UTF-8    LC_IDENTIFICATION=en_AU.UTF-8\n\nattached base packages:\n[1] grid      stats     graphics  grDevices utils     datasets  methods  \n[8] base     \n\nother attached packages:\n [1] dplyr_1.0.9        report_0.5.1       performance_0.9.1  glmulti_1.0.8     \n [5] leaps_3.1          rJava_1.0-6        vip_0.3.2          vcd_1.4-10        \n [9] stringr_1.4.0      sjPlot_2.8.10      rms_6.3-0          SparseM_1.81      \n[13] ordinal_2019.12-10 MASS_7.3-58.1      Hmisc_4.7-1        Formula_1.2-4     \n[17] survival_3.4-0     lattice_0.20-45    ggpubr_0.4.0       ggfortify_0.4.14  \n[21] ggplot2_3.3.6      flextable_0.7.3    car_3.1-0          carData_3.0-5     \n\nloaded via a namespace (and not attached):\n  [1] uuid_1.1-0           backports_1.4.1      systemfonts_1.0.4   \n  [4] plyr_1.8.7           splines_4.2.1        listenv_0.8.0       \n  [7] TH.data_1.1-1        digest_0.6.29        foreach_1.5.2       \n [10] htmltools_0.5.2      fansi_1.0.3          magrittr_2.0.3      \n [13] checkmate_2.1.0      cluster_2.1.3        globals_0.15.1      \n [16] recipes_1.0.1        gower_1.0.0          modelr_0.1.8        \n [19] officer_0.4.3        sandwich_3.0-2       hardhat_1.2.0       \n [22] jpeg_0.1-9           colorspace_2.0-3     xfun_0.31           \n [25] crayon_1.5.1         jsonlite_1.8.0       lme4_1.1-30         \n [28] iterators_1.0.14     zoo_1.8-10           glue_1.6.2          \n [31] gtable_0.3.0         ipred_0.9-13         emmeans_1.7.5       \n [34] MatrixModels_0.5-0   sjstats_0.18.1       sjmisc_2.8.9        \n [37] future.apply_1.9.0   DEoptimR_1.0-11      abind_1.4-5         \n [40] scales_1.2.0         mvtnorm_1.1-3        DBI_1.1.3           \n [43] rstatix_0.7.0        ggeffects_1.1.2      Rcpp_1.0.8.3        \n [46] xtable_1.8-4         htmlTable_2.4.1      klippy_0.0.0.9500   \n [49] proxy_0.4-27         foreign_0.8-82       stats4_4.2.1        \n [52] lava_1.6.10          prodlim_2019.11.13   datawizard_0.4.1    \n [55] htmlwidgets_1.5.4    RColorBrewer_1.1-3   ellipsis_0.3.2      \n [58] pkgconfig_2.0.3      farver_2.1.1         nnet_7.3-17         \n [61] deldir_1.0-6         utf8_1.2.2           caret_6.0-93        \n [64] reshape2_1.4.4       tidyselect_1.1.2     labeling_0.4.2      \n [67] rlang_1.0.4          effectsize_0.7.0     munsell_0.5.0       \n [70] tools_4.2.1          cli_3.3.0            generics_0.1.3      \n [73] sjlabelled_1.2.0     broom_1.0.0          evaluate_0.15       \n [76] fastmap_1.1.0        yaml_2.3.5           ModelMetrics_1.2.2.2\n [79] knitr_1.39           zip_2.2.0            robustbase_0.95-0   \n [82] purrr_0.3.4          future_1.26.1        nlme_3.1-158        \n [85] quantreg_5.93        xml2_1.3.3           compiler_4.2.1      \n [88] rstudioapi_0.13      png_0.1-7            e1071_1.7-11        \n [91] ggsignif_0.6.3       tibble_3.1.7         stringi_1.7.8       \n [94] parameters_0.18.1    gdtools_0.2.4        Matrix_1.4-1        \n [97] nloptr_2.0.3         vctrs_0.4.1          msm_1.6.9           \n[100] pillar_1.7.0         lifecycle_1.0.1      lmtest_0.9-40       \n[103] ucminf_1.1-4         estimability_1.4     data.table_1.14.2   \n[106] cowplot_1.1.1        insight_0.18.0       R6_2.5.1            \n[109] latticeExtra_0.6-30  gridExtra_2.3        parallelly_1.32.0   \n[112] codetools_0.2-18     polspline_1.1.20     boot_1.3-28         \n[115] assertthat_0.2.1     withr_2.5.0          multcomp_1.4-19     \n[118] expm_0.999-6         parallel_4.2.1       mgcv_1.8-40         \n[121] bayestestR_0.12.1    timeDate_3043.102    rpart_4.1.16        \n[124] class_7.3-20         tidyr_1.2.0          coda_0.19-4         \n[127] minqa_1.2.4          rmarkdown_2.14       pROC_1.18.0         \n[130] lubridate_1.8.0      numDeriv_2016.8-1.1  base64enc_0.1-3     \n[133] interp_1.1-2        \n```\n:::\n:::\n\n\n\n***\n\n[Back to top](#introduction)\n\n[Back to HOME](https://slcladal.github.io/index.html)\n\n***\n\n\n# References{-}\n\n\n\n",
    "supporting": [
      "fixreg_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"site_libs/clipboard-1.7.1/clipboard.min.js\"></script>\n<link href=\"site_libs/primer-tooltips-1.4.0/build.css\" rel=\"stylesheet\" />\n<link href=\"site_libs/klippy-0.0.0.9500/css/klippy.min.css\" rel=\"stylesheet\" />\n<script src=\"site_libs/klippy-0.0.0.9500/js/klippy.min.js\"></script>\n<link href=\"site_libs/tabwid-1.0.0/tabwid.css\" rel=\"stylesheet\" />\n<link href=\"site_libs/tabwid-1.0.0/scrool.css\" rel=\"stylesheet\" />\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}