{
  "hash": "91271770b977507908525483754def2e",
  "result": {
    "markdown": "---\ntitle: \"Introduction to Lexical Similarity\"\nauthor: \"Dattatreya Majumdar\"\ndate: \"2022-08-31\"\n---\n\n::: {.cell}\n::: {.cell-output-display}\n![](https://slcladal.github.io/images/uq1.jpg){width=100%}\n:::\n:::\n\n\n# Introduction{-}\n\nThis tutorial introduces Text Similarity [see @zahrotun2016comparison; @li2013distance], i.e. how close or similar two pieces of text are with respect to either their use of words or characters (lexical similarity) or in terms of meaning (semantic similarity).This tutorial is aimed at beginners and intermediate users of R with the aim of showcasing how to assess the similarity of texts in R. The aim is not to provide a fully-fledged analysis but rather to show and exemplify selected useful methods associated with assessing text similarity. \n\n\n<div class=\"warning\" style='padding:0.1em; background-color:#f2f2f2; color:#51247a'>\n<span>\n<p style='margin-top:1em; text-align:center'>\nThe entire R Notebook for the tutorial can be downloaded [**here**](https://slcladal.github.io/content/lexsim.Rmd).  If you want to render the R Notebook on your machine, i.e. knitting the document to html or a pdf, you need to make sure that you have R and RStudio installed and you also need to download the [**bibliography file**](https://slcladal.github.io/content/bibliography.bib) and store it in the same folder where you store the Rmd file. <br></p>\n<p style='margin-left:1em;'>\n</p></span>\n</div>\n\n<br>\n\n*Lexical Similarity* provides a measure of the similarity of two texts based on the intersection of the word sets of same or different languages. A lexical similarity of 1 suggests that there is complete overlap between the vocabularies while a score of 0 suggests that there are no common words in the two texts. There are several different ways of evaluating lexical similarity such as Jaccard Similarity, Cosine Similarity, Levenshtein Distance etc.\n\n*Semantic Similarity* on the other hand measures the similarity between two texts based on their meaning rather than their lexicographical similarity. Semantic similarity is highly useful for summarizing texts and extracting key attributes from large documents or document collections. Semantic Similarity can be evaluated using methods such as *Latent Semantic Analysis* (LSA), *Normalised Google Distance* (NGD), *Salient Semantic Analysis* (SSA) etc.  \n\nAs a part of this tutorial we will focus primarily on Lexical Similarity. We begin with a brief overview of relevant concepts and then show different measures can be implemented in R.\n\n## Jaccard Similarity{-}\n\nThe Jaccard similarity is defined as an intersection of two texts divided by the union of that two documents. In other words it can be expressed as the number of common words over the total number of the words in the two texts or documents. The Jaccard similarity of two documents ranges from 0 to 1, where 0 signifies no similarity and 1 signifies complete overlap.The mathematical representation of the Jaccard Similarity is shown below: -\n\n\\begin{equation}\nJ(A,B) = \\frac{|A \\bigcap B|}{|A \\bigcup B |} = \\frac{|A \\bigcap B|}{|A| + |B| - |A \\bigcap B|}\n\\end{equation}\n\n## Cosine Similarity{-}\n\nIn case of cosine similarity the two documents are represented in a n-dimensional vector space with each word represented in a vector form. Thus the cosine similarity metric measures the cosine of the angle between two n-dimensional vectors projected in a multi-dimensional space. The cosine similarity ranges from 0 to 1. A value closer to 0 indicates less similarity whereas a score closer to 1 indicates more similarity.The mathematical representation of the Cosine Similarity is shown below: -\n\n\\begin{equation}\nsimilarity = cos(\\theta) = \\frac{A \\cdot B}{||A|| ||B||} = \\frac{\\sum_{i=1}^{n} A_{i} B_{i}}{\\sqrt{\\sum_{i=1}^{n} A_{i}^{2}} \\sqrt{\\sum_{i=1}^{n} B_{i}^{2}}}\n\\end{equation}\n\n\n## Levenshtein Distance{-}\n\nLevenshtein distance comparison is generally carried out between two words. It determines the minimum number of single character edits required to change one word to another. The higher the number of edits more are the texts different from each other.An edit is defined by either an insertion of a character, a deletion of character or a replacement of a character. For two words *a* and *b* with lengths *i* and *j* the Levenshtein distance is defined as follows: -\n\n\\begin{equation}\nlev_{a,b}(i,j) = \n\\begin{cases}\n    max(i,j) & \\quad \\text{if min(i,j) = 0,}\\\\\n    min \\begin{cases}\n      lev_{a,b}(i-1,j)+1  \\\\\n      lev_{a,b}(i, j-1)+1  & \\text{otherwise.}\\\\\n      lev_{a,b}(i-1,j-1)+1_{(a_{i} \\neq b_{j})} \\\\\n  \\end{cases}\n  \\end{cases}\n\\end{equation}\n\n\n## Preparation and session set up{-}\n\n\nThis tutorial is based on R. If you have not installed R or are new to it, you will find an introduction to and more information how to use R [here](https://slcladal.github.io/intror.html). For this tutorials, we need to install certain *packages* from an R *library* so that the scripts shown below are executed without errors. Before turning to the code below, please install the packages by running the code below this paragraph. If you have already installed the packages mentioned below, then you can skip ahead ignore this section. To install the necessary packages, simply run the following code - it may take some time (between 1 and 5 minutes to install all of the packages so you do not need to worry if it takes some time).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# set options\noptions(stringsAsFactors = F)\n# install libraries\ninstall.packages(\"stringdist\")\ninstall.packages(\"hashr\")\ninstall.packages(\"tidyverse\")\ninstall.packages(\"flextable\")\n# install klippy for copy-to-clipboard button in code chunks\ninstall.packages(\"remotes\")\nremotes::install_github(\"rlesur/klippy\")\n```\n:::\n\n\nNow that we have installed the packages, we activate them as shown below.\n\n\n::: {.cell}\n\n```{.r .klippy .cell-code}\n# set options\noptions(stringsAsFactors = F)          # no automatic data transformation\noptions(\"scipen\" = 100, \"digits\" = 12) # suppress math annotation\n# activate packages\nlibrary(stringdist)\nlibrary(hashr)\nlibrary(tidyverse)\nlibrary(flextable)\n# activate klippy for copy-to-clipboard button\nklippy::klippy()\n```\n\n::: {.cell-output-display}\n```{=html}\n<script>\n  addClassKlippyTo(\"pre.r, pre.markdown\");\n  addKlippy('left', 'top', 'auto', '1', 'Copy code', 'Copied!');\n</script>\n```\n:::\n:::\n\n\n\nOnce you have installed R and RStudio and initiated the session by executing the code shown above, you are good to go.\n\n# Measuring Similarity in R{-}\n\nFor evaluating the similarity scores and the edit distance for the above discussed methods in R we have installed the *stringdist* package and will be primarily using two functions in that: *stringdist* and *stringsim*. We are also utilising the *hashr* package so that Jaccard and cosine similarity are evaluated word wise instead of letter wise. The sentence is tokenised and the corresponding list of words are hashed so that the sentences are transformed into a list of integers.For the Jaccard and the Cosine similarity we will be using the same set of texts whereas for the Levenshtein edit distance we will take 3 pairs of words to illustrate *insert*, *delete* and *replace* operations.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntext1 = \"The quick brown fox jumped over the wall\"\ntext2 = \"The fast brown fox leaped over the wall\"\ninsert_ex = c(\"Marta\",\"Martha\")\ndel_ex = c(\"Genome\",\"Gnome\")\nrep_ex = c(\"Tim\",\"Tom\")\n```\n:::\n\n\n## Jaccard Similarity{-}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Using the seq_dist function along with hash function to calculate the Jaccard similarity word-wise\njac_sim_score = seq_dist(hash(strsplit(text1, \"\\\\s+\")), hash(strsplit(text2, \"\\\\s+\")), method = \"jaccard\",q=2)\nprint(paste0(\"The Jaccard similarity for the two texts is \",jac_sim_score))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"The Jaccard similarity for the two texts is 0.727272727272727\"\n```\n:::\n:::\n\n\n## Cosine Similarity{-}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Using the seq_dist function along with hash function to calculate the Jaccard similarity word-wise\ncos_sim_score = seq_dist(hash(strsplit(text1, \"\\\\s+\")), hash(strsplit(text2, \"\\\\s+\")), method = \"cosine\",q=2)\nprint(paste0(\"The Cosine similarity for the two texts is \",cos_sim_score))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"The Cosine similarity for the two texts is 0.571428571428572\"\n```\n:::\n:::\n\n\n## Levenshtein distance{-}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Insert edit\nins_edit = stringdist(insert_ex[1],insert_ex[2],method = \"lv\")\nprint(paste0(\"The insert edit distance for \",insert_ex[1],\" and \",insert_ex[2],\" is \",ins_edit))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"The insert edit distance for Marta and Martha is 1\"\n```\n:::\n\n```{.r .cell-code}\n# Delete edit\ndel_edit = stringdist(del_ex[1],del_ex[2],method = \"lv\")\nprint(paste0(\"The delete edit distance for \",del_ex[1],\" and \",del_ex[2],\" is \",del_edit))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"The delete edit distance for Genome and Gnome is 1\"\n```\n:::\n\n```{.r .cell-code}\n# Replace edit\nrep_edit = stringdist(rep_ex[1],rep_ex[2],method = \"lv\")\nprint(paste0(\"The replace edit distance for \",rep_ex[1],\" and \",rep_ex[2],\" is \",rep_edit))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"The replace edit distance for Tim and Tom is 1\"\n```\n:::\n:::\n\n\n# Concluding remarks{-}\n\nAs shown above, the Jaccard and Cosine similarity scores are different which is important to note when using different measures to determine similarity. The differences are primarily primarily caused because Jaccard takes only the unique words in the two texts into consideration whereas the Cosine similarity approach takes the total length of the vectors into consideration. For the Levenshtein edit distance, the examples provided above show that for the first case we have to insert an extra *h*, for the second we have to delete an *e* and for the last case we need to replace *i* with *o*. Thus, for all the pairs taken into account here the edit distance is 1.\n\n# Citation & Session Info {-}\n\nMajumdar, Dattatreya. 2022. *Lexical Text Similarity using R*. Brisbane: The University of Queensland. url: https://slcladal.github.io/lexsim.html (Version 2022.08.31).\n\n```\n@manual{Majumdar2022ta,\n  author = {Majumdar, Dattatreya},\n  title = {Text Analysis and Distant Reading using R},\n  note = {https://slcladal.github.io/lexsim.html},\n  year = {2022},\n  organization = \"The University of Queensland, Australia. School of Languages and Cultures},\n  address = {Brisbane},\n  edition = {2022.08.31}\n}\n```\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsessionInfo()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nR version 4.2.1 (2022-06-23)\nPlatform: x86_64-pc-linux-gnu (64-bit)\nRunning under: Ubuntu 22.04.1 LTS\n\nMatrix products: default\nBLAS:   /usr/lib/x86_64-linux-gnu/blas/libblas.so.3.10.0\nLAPACK: /usr/lib/x86_64-linux-gnu/lapack/liblapack.so.3.10.0\n\nlocale:\n [1] LC_CTYPE=en_AU.UTF-8       LC_NUMERIC=C              \n [3] LC_TIME=en_AU.UTF-8        LC_COLLATE=en_AU.UTF-8    \n [5] LC_MONETARY=en_AU.UTF-8    LC_MESSAGES=en_AU.UTF-8   \n [7] LC_PAPER=en_AU.UTF-8       LC_NAME=C                 \n [9] LC_ADDRESS=C               LC_TELEPHONE=C            \n[11] LC_MEASUREMENT=en_AU.UTF-8 LC_IDENTIFICATION=C       \n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] flextable_0.7.3  forcats_0.5.1    stringr_1.4.0    dplyr_1.0.9     \n [5] purrr_0.3.4      readr_2.1.2      tidyr_1.2.0      tibble_3.1.7    \n [9] ggplot2_3.3.6    tidyverse_1.3.2  hashr_0.1.4      stringdist_0.9.8\n\nloaded via a namespace (and not attached):\n [1] Rcpp_1.0.8.3        lubridate_1.8.0     assertthat_0.2.1   \n [4] digest_0.6.29       utf8_1.2.2          R6_2.5.1           \n [7] cellranger_1.1.0    backports_1.4.1     reprex_2.0.1       \n[10] evaluate_0.15       httr_1.4.3          pillar_1.7.0       \n[13] gdtools_0.2.4       rlang_1.0.4         uuid_1.1-0         \n[16] googlesheets4_1.0.0 readxl_1.4.0        data.table_1.14.2  \n[19] rstudioapi_0.13     klippy_0.0.0.9500   rmarkdown_2.14     \n[22] googledrive_2.0.0   htmlwidgets_1.5.4   munsell_0.5.0      \n[25] broom_1.0.0         compiler_4.2.1      modelr_0.1.8       \n[28] xfun_0.31           systemfonts_1.0.4   base64enc_0.1-3    \n[31] pkgconfig_2.0.3     htmltools_0.5.2     tidyselect_1.1.2   \n[34] fansi_1.0.3         crayon_1.5.1        tzdb_0.3.0         \n[37] dbplyr_2.2.1        withr_2.5.0         grid_4.2.1         \n[40] jsonlite_1.8.0      gtable_0.3.0        lifecycle_1.0.1    \n[43] DBI_1.1.3           magrittr_2.0.3      scales_1.2.0       \n[46] zip_2.2.0           cli_3.3.0           stringi_1.7.8      \n[49] fs_1.5.2            xml2_1.3.3          ellipsis_0.3.2     \n[52] generics_0.1.3      vctrs_0.4.1         tools_4.2.1        \n[55] glue_1.6.2          officer_0.4.3       hms_1.1.1          \n[58] parallel_4.2.1      fastmap_1.1.0       yaml_2.3.5         \n[61] colorspace_2.0-3    gargle_1.2.0        rvest_1.0.2        \n[64] knitr_1.39          haven_2.5.0        \n```\n:::\n:::\n\n\n***\n\n[Back to top](#introduction)\n\n[Back to HOME](https://slcladal.github.io/index.html)\n\n***\n\n\n# References{-}\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"site_libs/clipboard-1.7.1/clipboard.min.js\"></script>\n<link href=\"site_libs/primer-tooltips-1.4.0/build.css\" rel=\"stylesheet\" />\n<link href=\"site_libs/klippy-0.0.0.9500/css/klippy.min.css\" rel=\"stylesheet\" />\n<script src=\"site_libs/klippy-0.0.0.9500/js/klippy.min.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}