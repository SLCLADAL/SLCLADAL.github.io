---
title: "Practical Overview of Selected Text Analytics Methods"
author: "Martin Schweinberger"
date: ""
output:
  bookdown::html_document2
bibliography: bibliography.bib
link-citations: yes
---



```{r uq1, echo=F, eval = T, fig.cap="", message=FALSE, warning=FALSE, out.width='100%'}
knitr::include_graphics("https://slcladal.github.io/images/uq1.jpg")
```

# Introduction{-}

```{r diff, echo=FALSE, out.width= "15%", out.extra='style="float:right; padding:10px"'}
knitr::include_graphics("https://slcladal.github.io/images/gy_chili.jpg")
```

This tutorial introduces Text Analysis [see @bernard1998text; @kabanoff1997introduction; @popping2000computer], i.e. computer-based analysis of language data or the (semi-)automated extraction of information from text. 


<div class="warning" style='padding:0.5em; background-color:rgba(215,209,204,.3); color:#51247a'>
<span>
<p style='margin-top:1em; text-align:center'>
Please cite as: <br>Schweinberger, Martin. 2023. *Practical Overview of Selected Text Analytics Methods*. Brisbane: The Language Technology and Data Analysis Laboratory (LADAL). url: https://ladal.edu.au/textanalysis.html (Version 2023.09.24).<br>
</p>
</span>
</div>
<br>

Most of the applications of Text Analysis are based upon a relatively limited number of key procedures or concepts (e.g. concordancing, word frequencies, annotation or tagging, collocation, text classification, Sentiment Analysis, Entity Extraction, Topic Modeling, etc.). In the following, we will explore these procedures and introduce some basic tools that help you perform the introduced tasks. 


<div class="warning" style='padding:0.1em; background-color:#f2f2f2; color:#51247a'>
<span>
<p style='margin-top:1em; text-align:center'>
To be able to follow this tutorial, we suggest you check out and familiarize yourself with the content of the following **R Basics** tutorials:<br>
</p>
<p style='margin-top:1em; text-align:left'>
<ul>
  <li>[Getting started with R](https://ladal.edu.au/intror.html) </li>
  <li>[Loading, saving, and generating data in R](https://ladal.edu.au/load.html) </li>
  <li>[String Processing in R](https://ladal.edu.au/string.html) </li>
  <li>[Regular Expressions in R](https://ladal.edu.au/regex.html) </li>
</ul>
</p>
<p style='margin-top:1em; text-align:center'>
Click [**here**](https://ladal.edu.au/content/kwics.Rmd)^[If you want to render the R Notebook on your machine, i.e. knitting the document to html or a pdf, you need to make sure that you have R and RStudio installed and you also need to download the [**bibliography file**](https://ladal.edu.au/content/bibliography.bib) and store it in the same folder where you store the Rmd file.] to download the **entire R Notebook** for this tutorial.<br><br>
[![Binder](https://mybinder.org/badge_logo.svg)](https://binderhub.atap-binder.cloud.edu.au/v2/gh/SLCLADAL/interactive-notebooks-environment/main?urlpath=git-pull%3Frepo%3Dhttps%253A%252F%252Fgithub.com%252FSLCLADAL%252Finteractive-notebooks%26urlpath%3Dlab%252Ftree%252Finteractive-notebooks%252Fnotebooks%252Ftextanalysis_cb.ipynb%26branch%3Dmain)<br>
Click [**here**](https://binderhub.atap-binder.cloud.edu.au/v2/gh/SLCLADAL/interactive-notebooks-environment/main?urlpath=git-pull%3Frepo%3Dhttps%253A%252F%252Fgithub.com%252FSLCLADAL%252Finteractive-notebooks%26urlpath%3Dlab%252Ftree%252Finteractive-notebooks%252Fnotebooks%252Ftextanalysis_cb.ipynb%26branch%3Dmain) to open an interactive Jupyter notebook that allows you to execute, change, and edit the code as well as to upload your own data. <br>
</p>
<p style='margin-left:1em;'>
</p></span>
</div>

<br>

**Preparation and session set up**


This tutorial is based on R. If you're new to R or haven't installed it yet, you can find an introduction and installation instructions [here](https://ladal.edu.au/intror.html). To ensure the scripts below run smoothly, we need to install specific R packages from a library. If you've already installed these packages, you can skip this section. To install them, run the code below (which may take 1 to 5 minutes).

```{r prep1, echo=T, eval = F, message=FALSE, warning=FALSE}
# install packages
install.packages("quanteda")
install.packages("dplyr")
install.packages("stringr")
install.packages("ggplot2")
install.packages("tm")
install.packages("udpipe")
install.packages("tidytext")
install.packages("wordcloud2")
install.packages("quanteda.textstats")
install.packages("quanteda.textplots")
install.packages("ggraph")
install.packages("flextable")
# install klippy for copy-to-clipboard button in code chunks
install.packages("remotes")
remotes::install_github("rlesur/klippy")
```

Once all packages are installed, you can activate them bu executing (running) the code chunk below.

```{r prep2, message=FALSE, warning=FALSE} 
# load packages
library(dplyr)
library(stringr)
library(ggplot2)
library(flextable)
library(quanteda)
library(tm)
library(udpipe)
library(tidytext)
library(wordcloud2)
library(flextable)
library(quanteda.textstats)
library(quanteda.textplots)
library(ggraph)
library(tidyr)
# activate klippy for copy-to-clipboard button
klippy::klippy()
```

Once you have  initiated the session by executing the code shown above, you are good to go.

# Concordancing{-}

In Text Analysis, concordancing refers to the extraction of words from a given text or texts [@lindquist2009corpus]. Commonly, concordances are displayed in the form of key-word in contexts (KWIC) where the search term is shown with some preceding and following context. Thus, such displays are referred to as key word in context concordances. A more elaborate tutorial on how to perform concordancing with R is available [here](https://ladal.edu.au/kwics.html).

<br>

<div class="warning" style='padding:0.1em; background-color: rgba(251,184,0,.5); color:#51247a'>
<span>
<p style='margin-top:1em; text-align:center'>
Concordancing is a text analysis technique that retrieves and displays occurrences of a chosen word or phrase within a text or dataset, showing the surrounding context. It's used to examine word usage, context, and linguistic patterns for research and language analysis purposes.<br>
</p>
<p style='margin-left:1em;'>
</p></span>
</div>

<br> 

```{r antconc, echo=FALSE, out.width= "60%", out.extra='style="float:right; padding:10px"'}
knitr::include_graphics("https://slcladal.github.io/images/AntConcConcordance.png")
```

Concordancing is a valuable tool that helps us understand how a term is used in the data, examine word frequency, extract examples, and serves as a fundamental step for more advanced language data analyses.

In the following section, we'll use R to explore text, using Lewis Carroll's *Alice's Adventures in Wonderland* as our example text. We'll start by loading the text data, which is available from the LADAL GitHub repository for this tutorial. If you're interested in loading your own data, you can refer to [this tutorial](https://ladal.edu.au/intror.html#Working_with_text). 

We start by loading our example text.

```{r conc1, message=FALSE, warning=FALSE}
# load text
text <- base::readRDS(url("https://slcladal.github.io/data/alice.rda", "rb"))
```

```{r conc1b, echo = F, message=FALSE, warning=FALSE}
# inspect data
text %>%
  as.data.frame() %>%
  head() %>%
  flextable() %>%
  flextable::set_table_properties(width = .75, layout = "autofit") %>%
  flextable::theme_zebra() %>%
  flextable::fontsize(size = 12) %>%
  flextable::fontsize(size = 12, part = "header") %>%
  flextable::align_text_col(align = "center") %>%
  flextable::set_caption(caption = "First text elements of the example text")  %>%
  flextable::border_outer()
```

The data still consists of short text snippets which is why we collapse these snippets and then split the collapsed data into chapters. 


```{r conc2, message=FALSE, warning=FALSE}
# combine and split into chapters
text_chapters <- text %>%
  # paste all texts together into one long text
  paste0(collapse = " ") %>%
  # replace Chapter I to Chapter XVI with qwertz 
  stringr::str_replace_all("(CHAPTER [XVI]{1,7}\\.{0,1}) ", "qwertz\\1") %>%
  # convert text to lower case
  tolower() %>%  
  # split the long text into chapters
  stringr::str_split("qwertz") %>%
  # unlist the result (convert into simple vector)
  unlist()
```

```{r conc2b, echo = F, message=FALSE, warning=FALSE}
# inspect data
text_chapters %>%
  substr(start=1, stop=500) %>%
  as.data.frame() %>%
  head() %>%
  flextable() %>%
  flextable::set_table_properties(width = .95, layout = "autofit") %>%
  flextable::theme_zebra() %>%
  flextable::fontsize(size = 12) %>%
  flextable::fontsize(size = 12, part = "header") %>%
  flextable::align_text_col(align = "center") %>%
  flextable::set_caption(caption = "First 500 characters of the first 6 chapters of  the example text")  %>%
  flextable::border_outer()
```

After dividing the data into chapters, we conduct concordancing and extract KWICs (**K**ey**W**ord **I**n **C**ontext). This is accomplished using the `kwic` function from the `quanteda` package, which requires three main arguments: the data (x), the search pattern (pattern), and the window size.

To begin, we'll create KWICs for the term *alice* using the `kwic` function from the `quanteda` package, as demonstrated below. 

<br>

<div class="warning" style='padding:0.1em; background-color: rgba(251,184,0,.5); color:#51247a'>
<span>
<p style='margin-top:1em; text-align:center'>
The `kwic` function in the `quanteda` package extracts <b>K</b>ey<b>W</b>ord <b>I</b>n <b>C</b>ontext (KWIC) information. Its main arguments are `x` (text data), `pattern` (search term), and `window` (context size) to display words around the pattern.<br>
</p>
<p style='margin-left:1em;'>
</p></span>
</div>

<br>


```{r conc3, message=FALSE, warning=FALSE}
# create kwic
kwic_alice <- quanteda::kwic(x = text_chapters, # define text(s) 
                             # define pattern
                             pattern = "alice",
                             # define window size
                             window = 5) %>%
  # convert into a data frame
  as.data.frame() %>%
  # remove superfluous columns
  dplyr::select(-to, -from, -pattern)
```

```{r conc3b, echo = F, message=FALSE, warning=FALSE}
# inspect data
kwic_alice %>%
  as.data.frame() %>%
  head() %>%
  flextable() %>%
  flextable::set_table_properties(width = .95, layout = "autofit") %>%
  flextable::theme_zebra() %>%
  flextable::fontsize(size = 12) %>%
  flextable::fontsize(size = 12, part = "header") %>%
  flextable::align_text_col(align = "center") %>%
  flextable::set_caption(caption = "First 10 concordances of *alice*  in the example text")  %>%
  flextable::border_outer()
```

In our search, we have the flexibility to utilize regular expressions, allowing us to extract not only straightforward terms like *alice* but also more intricate and even abstract patterns. An abstract pattern may involve only a part of the term being specified. For example, if we specify *walk*, we can retrieve words like *walking*, *walker,* *walked*, and *walks* that contain this sequence. To effectively capture such abstract patterns, we employ what are known as *regular expressions*.

When incorporating a regular expression in the `pattern` argument, it's crucial to specify the `valuetype` as `regex`, as illustrated below.

```{r conc5, message=FALSE, warning=FALSE}
# create kwic
kwic_walk <- quanteda::kwic(x = text_chapters, 
                          pattern = "walk.*",
                          window = 5,
                          valuetype = "regex") %>%
  # convert into a data frame
  as.data.frame() %>%
  # remove superfluous columns
  dplyr::select(-to, -from, -pattern)
```


```{r conc5b, echo = F, message=FALSE, warning=FALSE}
# inspect data
kwic_walk %>%
  as.data.frame() %>%
  head(10) %>%
  flextable() %>%
  flextable::set_table_properties(width = .95, layout = "autofit") %>%
  flextable::theme_zebra() %>%
  flextable::fontsize(size = 12) %>%
  flextable::fontsize(size = 12, part = "header") %>%
  flextable::align_text_col(align = "center") %>%
  flextable::set_caption(caption = "First 10 cleaned Concordances of *walk* in the example text")  %>%
  flextable::border_outer()
```


When searching for expressions that represent phrases consisting of multiple elements, like *poor alice*, it's essential to explicitly specify in the `pattern` argument that we are searching for a `phrase`. 

```{r conc7, message=FALSE, warning=FALSE}
# create kwic
kwic_pooralice <- quanteda::kwic(x = text_chapters, 
                          pattern = quanteda::phrase("poor alice"),
                          window = 5) %>%
  # convert into a data frame
  as.data.frame() %>%
  # remove superfluous columns
  dplyr::select(-to, -from, -pattern)
```



```{r conc8b, echo = F, message=FALSE, warning=FALSE}
# inspect data
kwic_pooralice %>%
  as.data.frame() %>%
  head() %>%
  flextable() %>%
  flextable::set_table_properties(width = .95, layout = "autofit") %>%
  flextable::theme_zebra() %>%
  flextable::fontsize(size = 12) %>%
  flextable::fontsize(size = 12, part = "header") %>%
  flextable::align_text_col(align = "center") %>%
  flextable::set_caption(caption = "First cleaned concordances of the phrase *poor alice* in the example text")  %>%
  flextable::border_outer()
```


We could continue our analysis by exploring in greater detail how the phrase *poor alice* is used in context, perhaps by adjusting the context window size or conducting similar investigations. However, for now, we'll shift our focus to learning how to extract and work with word frequencies.

# Word Frequency{-}

Frequency information is a cornerstone of text analytics, underpinning nearly all analytical methods. Identifying the most common words within a text is a fundamental technique in text analytics, serving as the bedrock of text analysis. This frequency data is typically organized into word frequency lists, which consist of word forms and their corresponding frequencies within a given text or collection of texts.

Given the paramount importance of extracting word frequency lists, we will proceed to demonstrate how to do so. In the first step, we'll continue with our example text, convert the chapters to lowercase, eliminate non-word symbols (including punctuation), and then break down the text (the chapters) into individual words.

```{r wf1, message=FALSE, warning=FALSE}
# process the text and save result in "text_words"
text_words <- text  %>%
  # convert all text to lowercase
  tolower() %>%
  # remove non-word characters, keeping spaces
  str_replace_all("[^[:alpha:][:space:]]*", "")  %>%
  # remove punctuation
  tm::removePunctuation() %>%
  # squish consecutive spaces into a single space
  stringr::str_squish() %>%
  # split the text into individual words, separated by spaces
  stringr::str_split(" ") %>%
  # unlist the result into a single vector of words
  unlist()

```


```{r wf1b, echo = F, message=FALSE, warning=FALSE}
# inspect data
text_words %>%
  as.data.frame() %>%
  head(15) %>%
  flextable() %>%
  flextable::set_table_properties(width = .5, layout = "autofit") %>%
  flextable::theme_zebra() %>%
  flextable::fontsize(size = 12) %>%
  flextable::fontsize(size = 12, part = "header") %>%
  flextable::align_text_col(align = "center") %>%
  flextable::set_caption(caption = "First 15 words in the example text")  %>%
  flextable::border_outer()
```

With our word vector in hand, let's effortlessly construct a table that showcases a word frequency list, as demonstrated below.

```{r wf2, message=FALSE, warning=FALSE}
# Create a word frequency table from the 'text_words' data
wfreq <- text_words %>%
  # count the frequency of each unique word
  table() %>%
  # convert the frequency table into a data frame
  as.data.frame() %>%
  # arrange the data frame rows in descending order of word frequency
  arrange(desc(Freq)) %>%
  # rename the columns for clarity
  dplyr::rename(word = 1,
                frequency = 2)

```


```{r wf2b, echo = F, message=FALSE, warning=FALSE}
# inspect data
wfreq %>%
  as.data.frame() %>%
  head(15) %>%
  flextable() %>%
  flextable::set_table_properties(width = .5, layout = "autofit") %>%
  flextable::theme_zebra() %>%
  flextable::fontsize(size = 12) %>%
  flextable::fontsize(size = 12, part = "header") %>%
  flextable::align_text_col(align = "center") %>%
  flextable::set_caption(caption = "Top 15 words in the example text by frequency.")  %>%
  flextable::border_outer()
```

The most common words often consist of function words that may lack significance. To enhance our analysis, we'll eliminate these function words, often referred to as *stopwords*, from the frequency list. Let's take a look at the refined list without stopwords.

```{r wf4, message=FALSE, warning=FALSE}
# create table wo stopwords
wfreq_wostop <- wfreq %>%
  anti_join(tidytext::stop_words, by = "word") %>%
  dplyr::filter(word != "")
```



```{r wf5b, echo = F, message=FALSE, warning=FALSE}
# inspect data
wfreq_wostop %>%
  as.data.frame() %>%
  head(15) %>%
  flextable() %>%
  flextable::set_table_properties(width = .5, layout = "autofit") %>%
  flextable::theme_zebra() %>%
  flextable::fontsize(size = 12) %>%
  flextable::fontsize(size = 12, part = "header") %>%
  flextable::align_text_col(align = "center") %>%
  flextable::set_caption(caption = "Top 15 lexical words in the example text by frequency.")  %>%
  flextable::border_outer()
```


Word frequency lists can be presented visually in several ways, with bar graphs being the most common and intuitive choice for visualization.

```{r wf6, message=FALSE, warning=FALSE}
wfreq_wostop %>%
  head(10) %>%
  ggplot(aes(x = reorder(word, -frequency, mean), y = frequency)) +
  geom_bar(stat = "identity") +
  labs(title = "10 most frequent non-stop words \nin the example text",
       x = "") +
  theme(axis.text.x = element_text(angle = 45, size = 12, hjust = 1))
```

## Wordclouds{-}

Alternatively, word frequency lists can be visually represented as word clouds, though they provide less detailed information. Word clouds are visual representations where words appear larger based on their frequency, offering a quick visual summary of word importance in a dataset.

```{r wc1, message=FALSE, warning=FALSE}
# create a word cloud visualization
text %>%
  # Convert text data to a quanteda corpus
  quanteda::corpus() %>%
  # tokenize the corpus, removing punctuation
  quanteda::tokens(remove_punct = TRUE) %>%
  # remove English stopwords
  quanteda::tokens_remove(stopwords("english")) %>%
  # create a document-feature matrix (DFM)
  quanteda::dfm() %>%
  # generate a word cloud using textplot_wordcloud
  quanteda.textplots::textplot_wordcloud(
    # maximum words to display in the word cloud
    max_words = 150,
    # determine the maximum size of words
    max_size = 10,
    # determine the minimum size of words
    min_size = 1.5,
    # Define a color palette for the word cloud
    color = scales::viridis_pal(option = "A")(10))  

```


<br>

<div class="warning" style='padding:0.1em; background-color: rgba(251,184,0,.5); color:#51247a'>
<span>
<p style='margin-top:1em; text-align:center'>
The `textplot_wordcloud` function creates a word cloud visualization of text data in R. Its main arguments are `x` (a Document-Feature Matrix or DFM), `max_words` (maximum words to display), and `color` (color palette for the word cloud).<br>
</p>
<p style='margin-left:1em;'>
</p></span>
</div>

<br>


Another form of word clouds, known as *comparison clouds*, is helpful in discerning disparities between texts. For instance, we can load various texts and assess how they vary in terms of word frequencies. To illustrate this, we'll load Herman Melville's *Moby Dick*, George Orwell's *1984*, and Charles Darwin's *Origin*. 

First, we'll load these texts and combine them into single documents.

```{r wc2, message=FALSE, warning=FALSE}
# load data
orwell_sep <- base::readRDS(url("https://slcladal.github.io/data/orwell.rda", "rb"))
orwell <- orwell_sep %>%
  paste0(collapse = " ")
melville_sep <- base::readRDS(url("https://slcladal.github.io/data/melville.rda", "rb"))
melville <- melville_sep %>%
  paste0(collapse = " ")
darwin_sep <- base::readRDS(url("https://slcladal.github.io/data/darwin.rda", "rb"))
darwin <- darwin_sep %>%
  paste0(collapse = " ")
```

Now, we generate a corpus object from these texts and create a variable with the author name.

```{r wc3, message=FALSE, warning=FALSE}
corp_dom <- quanteda::corpus(c(darwin, melville, orwell)) 
attr(corp_dom, "docvars")$Author = c("Darwin", "Melville", "Orwell")
```

Now, we can remove so-called *stopwords* (non-lexical function words) and punctuation and generate the comparison cloud.

```{r wc4, message=FALSE, warning=FALSE}
# create a comparison word cloud for a corpus
corp_dom %>%
  # tokenize the corpus, removing punctuation, symbols, and numbers
  quanteda::tokens(remove_punct = TRUE,
                   remove_symbols = TRUE,
                   remove_numbers = TRUE) %>%
  # remove English stopwords
  quanteda::tokens_remove(stopwords("english")) %>%
  # create a Document-Feature Matrix (DFM)
  quanteda::dfm() %>%
  # group the DFM by the 'Author' column from 'corp_dom'
  quanteda::dfm_group(groups = corp_dom$Author) %>%
  # trim the DFM, keeping terms that occur at least 10 times
  quanteda::dfm_trim(min_termfreq = 10, verbose = FALSE) %>%
  # generate a comparison word cloud
  quanteda.textplots::textplot_wordcloud(
    # create a comparison word cloud
    comparison = TRUE,  
    # set colors for different groups
    color = c("darkgray", "orange", "purple"),  
    # define the maximum number of words to display in the word cloud
    max_words = 150)  
```


## Frequency changes{-}

We can also explore how the term *alice* is used throughout the chapters of our example text. To begin, let's extract the word count for each chapter.

```{r wf13, message=FALSE, warning=FALSE}
# extract the number of words per chapter
Words <- text_chapters %>%
  # split each chapter into words based on spaces
  stringr::str_split(" ")  %>%
  # measure the length (number of words) in each chapter
  lengths()
# display the resulting data, which contains the word counts per chapter
Words
```

Next, we extract the number of matches in each chapter.

```{r wf14, message=FALSE, warning=FALSE}
# extract the number of matches of "alice" per chapter
Matches <- text_chapters %>%
  # count the number of times "alice" appears in each chapter
  stringr::str_count("alice")
# display the resulting data, which shows the number of matches of "alice" per chapter
Matches
```

Now, we extract the names of the chapters and create a table with the chapter names and the relative frequency of matches per 1,000 words.

```{r wf15, message=FALSE, warning=FALSE}
# extract chapters
Chapters <- paste0("chapter ", 0:(length(text_chapters)-1))
Chapters
```

Next, we combine the information in a single data frame and add a column containing the relative frequency of *alice* in each chapter.

```{r wf16, message=FALSE, warning=FALSE}
# create table of results
tb <- data.frame(Chapters, Matches, Words) %>%
  # create new variable with the relative frequency
  dplyr::mutate(Frequency = round(Matches/Words*1000, 2))
```

```{r wf17b, echo = F, message=FALSE, warning=FALSE}
# inspect data
tb %>%
  as.data.frame() %>%
  head(15) %>%
  flextable() %>%
  flextable::set_table_properties(width = .5, layout = "autofit") %>%
  flextable::theme_zebra() %>%
  flextable::fontsize(size = 12) %>%
  flextable::fontsize(size = 12, part = "header") %>%
  flextable::align_text_col(align = "center") %>%
  flextable::set_caption(caption = "Words and their (relative) freqeuncy across in the example text by frequency.")  %>%
  flextable::border_outer()
```


Now, let's visualize the relative frequencies of our search term in each chapter.

```{r wf18, echo=T, eval = T, message=FALSE, warning=FALSE}
# create a plot using ggplot
ggplot(tb, aes(x = Chapters, y = Frequency, group = 1)) + 
  # add a smoothed line (trendline) in purple color
  geom_smooth(color = "purple") +
  # add a line plot in dark gray color
  geom_line(color = "darkgray") +         
  # remove fill from the legend
  guides(color = guide_legend(override.aes = list(fill = NA))) +
  # set a white and black theme
  theme_bw() +
  # rotate x-axis text by 45 degrees and adjust alignment
  theme(axis.text.x = element_text(angle = 45, hjust = 1))+
  # customize the y-axis label
  scale_y_continuous(name = "Relative Frequency (per 1,000 words)")

```

## Dispersion plots{-}

To show when in a text or in a collection of texts certain terms occur, we can use *dispersion plots*. The `quanteda` package offers a very easy-to-use function `textplot_xray` to generate dispersion plots.

```{r dp, warning=F, message=F}
# add chapter names
names(text_chapters) <- Chapters
# generate corpus from chapters
text_corpus <- quanteda::corpus(text_chapters)
# generate dispersion plots
quanteda.textplots::textplot_xray(kwic(text_corpus, pattern = "alice"),
                                  kwic(text_corpus, pattern = "hatter"),
                                  sort = T)
```

We can modify the plot by saving it into an object and then use `ggplot` to modify it appearance.

```{r dp2, warning=F, message=F}
# generate and save dispersion plots
dp <- quanteda.textplots::textplot_xray(kwic(text_corpus, pattern = "alice"),
                                        kwic(text_corpus, pattern = "cat"))
# modify plot
dp + aes(color = keyword) + 
  scale_color_manual(values = c('red', 'blue')) +
  theme(legend.position = "none")
```

## Over- and underuse{-}

Frequency data serves as a valuable lens through which we can explore the essence of a text. For instance, when we examine private dialogues, we often encounter higher occurrences of second-person pronouns compared to more formal text types like scripted monologues or speeches. This insight holds the potential to aid in text classification and assessing text formality.

To illustrate, consider the following statistics: the counts of second-person pronouns, *you* and *your*, as well as the total word count excluding these pronouns in private dialogues versus scripted monologues within the Irish segment of the International Corpus of English (ICE). Additionally, the tables provide the percentage of second-person pronouns in both text types, enabling us to discern whether private dialogues indeed contain more of these pronouns compared to scripted monologues, such as speeches.

```{r ou1, eval=T, echo=F, message=FALSE, warning=FALSE, paged.print=FALSE}
# create a matrix 'numbers' with rows and columns
numbers <- matrix(c("you, your", "6761", "659", 
                    "Other words", "259625", "105295", 
                    "Percent", "2.60", "0.63"), 
                  byrow = TRUE, nrow = 3)
# assign column names to the matrix
colnames(numbers) <- c("", "Private dialogues", "Scripted monologues")
```

```{r ou1b, echo = F, message=FALSE, warning=FALSE}
# inspect data
ndf <- numbers %>%
  as.data.frame()
colnames(ndf)[1] <- "."
ndf %>%
  flextable() %>%
  flextable::set_table_properties(width = .5, layout = "autofit") %>%
  flextable::theme_zebra() %>%
  flextable::fontsize(size = 12) %>%
  flextable::fontsize(size = 12, part = "header") %>%
  flextable::align_text_col(align = "center") %>%
  flextable::set_caption(caption = "Use of 2nd person pronouns (and all other words) in ICE Ireland.")  %>%
  flextable::border_outer()
```


This straightforward example highlights that second-person pronouns constitute 2.6 percent of all words in private dialogues, yet they represent only 0.63 percent in scripted speeches. To vividly illustrate such variations, we can employ association and mosaic plots, which offer effective visual presentations.

```{r ou2, message=FALSE, warning=FALSE, paged.print=FALSE}
# create a matrix 'd' with the specified values and dimensions
d <- matrix(c(6761, 659, 259625, 105295), nrow = 2, byrow = TRUE)
# assign column names to the matrix
colnames(d) <- c("D", "M")
# assign row names to the matrix
rownames(d) <- c("you, your", "Other words")
# generate an association plot using 'assocplot' function
assocplot(d)
```

In an association plot, bars above the dashed line signify relative overuse, while bars below indicate relative underuse. Accordingly, the plot reveals that in monologues, there's an underuse of *you* and *your* and an overuse of *other words*. Conversely, in dialogues, the opposite patterns emerge: an overuse of *you* and *your* and an underuse of *other words*. This visual representation helps us grasp the distinctive word usage patterns between these text types.


# N-grams, Collocations, and  Keyness{-}

Collocations are like linguistic buddies. They're those word pairs that just seem to go hand in hand, like *Merry Christmas*. You see, these words have a special relationship â€“ they occur together way more often than if words were just randomly strung together in a sentence.

Now, let's talk about N-grams. Think of them as close companions to collocates. N-grams represent groups of words that snuggle up next to each other in texts. Bi-grams, for instance, are word pairs that frequently cozy up together, tri-grams involve trios, and so on.

Creating N-gram lists, especially bi-grams, is surprisingly easy. In our example text adventure, we'll craft a bi-gram list by doing something quite straightforward: taking each word and introducing it to the next word in line. It's a fun way to discover meaningful word pairs in your text!

```{r coll2, message=FALSE, warning=FALSE, paged.print=FALSE}
# create data frame
text_bigrams <- data.frame(text_words[1:length(text_words)-1], 
                       text_words[2:length(text_words)]) %>%
  dplyr::rename(Word1 = 1,
                Word2 = 2) %>%
  dplyr::mutate(Bigram = paste0(Word1, " ", Word2)) %>%
  dplyr::group_by(Bigram) %>%
  dplyr::summarise(Frequency = n()) %>%
  dplyr::arrange(-Frequency)
```


```{r coll3b, echo = F, message=FALSE, warning=FALSE}
# inspect data
text_bigrams %>%
  as.data.frame() %>%
  head(15) %>%
  flextable() %>%
  flextable::set_table_properties(width = .5, layout = "autofit") %>%
  flextable::theme_zebra() %>%
  flextable::fontsize(size = 12) %>%
  flextable::fontsize(size = 12, part = "header") %>%
  flextable::align_text_col(align = "center") %>%
  flextable::set_caption(caption = "Top 10 most frequent bigrams and their (relative) freqeuncy in the example text")  %>%
  flextable::border_outer()
```

Both N-grams and collocations are not only an important concept in language teaching but they are also fundamental in Text Analysis and many other research areas working with language data. Unfortunately, words that collocate do not have to be immediately adjacent but can also encompass several slots which makes it harder to retrieve of collocates that are not adjacent- We will find out how to identify non-adjacent collocates in the next section.

## Finding collocations{-}

Numerous methods exist for uncovering collocations. When we want to identify collocations without a predefined target term, we can turn to the `textstat_collocations` function within the `quanteda.textstats` package.


<br>

<div class="warning" style='padding:0.1em; background-color: rgba(251,184,0,.5); color:#51247a'>
<span>
<p style='margin-top:1em; text-align:center'>
Collocations are word combinations or sequences of words that frequently occur together in a language due to their inherent association and natural co-occurrence. They are used in language analysis and text processing to identify meaningful patterns, gain insights into language usage, improve language models, and enhance natural language understanding.<br>
</p>
<p style='margin-left:1em;'>
</p></span>
</div>

<br>

However, before we can effectively employ this function to identify collocations, we must first prepare the data to which we intend to apply it. In our current scenario, we will utilize this function to analyze sentences extracted from the example text, as demonstrated in the following code chunk.

```{r coll5}
text_sentences <- text %>%
  tolower() %>%
  paste0(collapse= " ") %>%
  stringr::str_split(fixed(".")) %>%
  unlist() %>%
  tm::removePunctuation() %>%
  stringr::str_squish()
```


```{r coll6b, echo = F, message=FALSE, warning=FALSE}
# inspect data
text_sentences %>%
  as.data.frame() %>%
  head(15) %>%
  flextable() %>%
  flextable::set_table_properties(width = .95, layout = "autofit") %>%
  flextable::theme_zebra() %>%
  flextable::fontsize(size = 12) %>%
  flextable::fontsize(size = 12, part = "header") %>%
  flextable::align_text_col(align = "center") %>%
  flextable::set_caption(caption = "First 10 sentences in the example text")  %>%
  flextable::border_outer()
```


In the displayed results above, we observe that segmenting texts solely by full stops has its limitations, occasionally generating unexpected artifacts like "sentences" comprised of single characters. This occurs, for instance, due to the abbreviation "H.M.S. Beagle," the ship used by Darwin during his exploration of the southern hemisphere. Luckily, these errors do not significantly impact our current example.

With our example text now divided into sentences, our next step is to tokenize these sentences and employ the `textstat_collocations` function, which adeptly detects collocations.

```{r coll7, warning=F, message=F}
# create a token object
text_tokens <- tokens(text_sentences, remove_punct = TRUE) %>%
  # remove stopwords
  tokens_remove(stopwords("english"))
# extract collocations
text_coll <- textstat_collocations(text_tokens, size = 2, min_count = 20)
```


```{r coll8b, echo = F, message=FALSE, warning=FALSE}
# inspect data
text_coll %>%
  as.data.frame() %>%
  head(15) %>%
  flextable() %>%
  flextable::set_table_properties(width = .5, layout = "autofit") %>%
  flextable::theme_zebra() %>%
  flextable::fontsize(size = 12) %>%
  flextable::fontsize(size = 12, part = "header") %>%
  flextable::align_text_col(align = "center") %>%
  flextable::set_caption(caption = "Top 10 collocations in the example text")  %>%
  flextable::border_outer()
```



The resulting table shows collocations in the example text descending by collocation strength.

## Visualizing Collocation Networks{-}

Network graphs are versatile and powerful for visualizing relationships between various elements, be it words, characters, or authors. This section demonstrates the creation of a network graph to explore collocations of the term *alice* using the `quanteda` package.

To begin, we'll construct a document-feature matrix based on the sentences in our example text. This matrix illustrates the frequency of elements (in our case, words from the example text) occurring within a set of documents (in our case, the sentences in the example text).

Network analysis involves representing and analyzing relationships between interconnected entities using nodes (representing entities) and edges (representing relationships). In this context, a network graph will help us visualize how words or elements are connected based on their co-occurrence patterns, shedding light on their relationships and associations within the text.

```{r dfm1, message=F, warning=F}
# create document-feature matrix
text_dfm <- text_sentences %>% 
    quanteda::dfm(remove = stopwords(), remove_punct = TRUE) %>%
    quanteda::dfm_trim(min_termfreq = 10, verbose = FALSE)
```

```{r dfm1b, echo = F, message=FALSE, warning=FALSE}
# inspect data
text_dfm[1:6, 1:6] %>%
  as.data.frame() %>%
  head(15) %>%
  flextable() %>%
  flextable::set_table_properties(width = .5, layout = "autofit") %>%
  flextable::theme_zebra() %>%
  flextable::fontsize(size = 12) %>%
  flextable::fontsize(size = 12, part = "header") %>%
  flextable::align_text_col(align = "center") %>%
  flextable::set_caption(caption = "First 6 rows and columns of the document-feature matrix.")  %>%
  flextable::border_outer()
```


As we want to generate a network graph of words that collocate with the term *organism*, we use the `calculateCoocStatistics` function to determine which words most strongly collocate with our target term (*organism*).  

```{r dfm3, message=F, warning=F}
# load function for co-occurrence calculation
source("https://slcladal.github.io/rscripts/calculateCoocStatistics.R")
# define term
coocTerm <- "alice"
# calculate co-occurrence statistics
coocs <- calculateCoocStatistics(coocTerm, text_dfm, measure="LOGLIK")
# inspect results
coocs[1:20]
```

We will now narrow down the document-feature matrix to focus exclusively on the top 20 collocates of the word organism, while retaining our primary keyword *organism*. In other words, we're homing in on the 20 most closely associated words that frequently appear alongside organism. This process enables us to explore the key elements and their relationships in the context of organism, providing valuable insights into how this word is used and connected within the text.

```{r dfm4}
redux_dfm <- dfm_select(text_dfm, 
                        pattern = c(names(coocs)[1:20], "alice"))
```



```{r dfm4b, echo = F, message=FALSE, warning=FALSE}
# inspect data
redux_dfm[1:6, 1:6] %>%
  as.data.frame() %>%
  head(15) %>%
  flextable() %>%
  flextable::set_table_properties(width = .5, layout = "autofit") %>%
  flextable::theme_zebra() %>%
  flextable::fontsize(size = 12) %>%
  flextable::fontsize(size = 12, part = "header") %>%
  flextable::align_text_col(align = "center") %>%
  flextable::set_caption(caption = "First 6 rows and columns of the reduced feature co-occurrence matrix.")  %>%
  flextable::border_outer()
```


Now, we can transform the document-feature matrix into a feature-co-occurrence matrix as shown below. A feature-co-occurrence matrix shows how often each element in that matrix co-occurs with every other element in that matrix.

```{r dfm6}
tag_fcm <- fcm(redux_dfm)
```


```{r dfm6b, echo = F, message=FALSE, warning=FALSE}
# inspect data
tag_fcm[1:6, 1:6] %>%
  as.data.frame() %>%
  head(15) %>%
  flextable() %>%
  flextable::set_table_properties(width = .5, layout = "autofit") %>%
  flextable::theme_zebra() %>%
  flextable::fontsize(size = 12) %>%
  flextable::fontsize(size = 12, part = "header") %>%
  flextable::align_text_col(align = "center") %>%
  flextable::set_caption(caption = "First 6 rows and columns of the feature co-occurrence matrix.")  %>%
  flextable::border_outer()
```


Using the feature-co-occurrence matrix, we can generate the network graph which shows the terms that collocate with the target term *alice* with the edges representing the co-occurrence frequency. To generate this network graph, we use the `textplot_network` function from the `quanteda.textplots` package.

```{r dfm8, message=F, warning=F}
# generate network graph
textplot_network(tag_fcm, 
                 min_freq = 1, 
                 edge_alpha = 0.1, 
                 edge_size = 5,
                 edge_color = "purple",
                 vertex_labelsize = log(rowSums(tag_fcm))*2)
```

## Keyness{-}

Another valuable technique for automated text summarization is keyword extraction. This approach revolves around pinpointing words closely associated with a specific text. In simpler terms, keyness analysis strives to identify words that distinctly represent the content of a given text.


<br>

<div class="warning" style='padding:0.1em; background-color: rgba(251,184,0,.5); color:#51247a'>
<span>
<p style='margin-top:1em; text-align:center'>
Keyness is a statistical measure that assesses how prominently a term stands out in a specific context by comparing its frequency to what's expected based on background data. It helps identify significant terms in text analysis, aiding tasks like text summarization and content categorization.<br>
</p>
<p style='margin-left:1em;'>
</p></span>
</div>

<br>


In the following section, we'll extract keywords from the works of Charles Darwin's Origin, Herman Melville's Moby Dick, and George Orwell's 1984. Our journey begins by constructing a weighted document feature matrix using the corpus containing these three texts.

To create this corpus, we'll utilize text objects, each comprising numerous elements, as opposed to the objects that previously held the combined texts. This approach allows us to delve into a more granular analysis of the individual texts.

Keyness is a statistical measure that evaluates the significance of a term's frequency within a specific text or context. It helps identify words that are particularly representative of the content or theme of a given text. In text analysis, keyness is a crucial concept for uncovering and highlighting the most meaningful and contextually relevant words or phrases within a body of text.

```{r key0, message=F, warning=F}
corp_dom <- quanteda::corpus(c(darwin_sep, orwell_sep, melville_sep)) 
attr(corp_dom, "docvars")$Author = c(rep("Darwin", length(darwin_sep)), 
                                     rep("Orwell", length(orwell_sep)),
                                     rep("Melville", length(melville_sep)))
```

Moving forward, our next step involves generating the document feature matrix and refining it by eliminating stopwords and specific words of interest. Furthermore, we organize the document feature matrix by author, allowing us to structure and analyze the data effectively.


```{r key1, message=F, warning=F}
dfm_authors <- corp_dom %>%
  quanteda::tokens(remove_punct = TRUE) %>%
  quanteda::tokens_remove(quanteda::stopwords("english")) %>%
  quanteda::tokens_remove(c("now", "one", "like", "may", "can")) %>%
  quanteda::dfm() %>%
  quanteda::dfm_group(groups = Author) %>%
  quanteda::dfm_weight(scheme = "prop")
```

In a next step, we use the `textstat_frequency` function from the `quanteda` package to extract the most frequent non-stopwords in the three texts.
 
```{r key2, message=F, warning=F}
# Calculate relative frequency by president
freq_weight <- quanteda.textstats::textstat_frequency(dfm_authors, 
                                                      n = 10,
                                                      groups = dfm_authors$Author)
```



```{r key3b, echo = F, message=FALSE, warning=FALSE}
# inspect data
freq_weight %>%
  as.data.frame() %>%
  head(15) %>%
  flextable() %>%
  flextable::set_table_properties(width = .5, layout = "autofit") %>%
  flextable::theme_zebra() %>%
  flextable::fontsize(size = 12) %>%
  flextable::fontsize(size = 12, part = "header") %>%
  flextable::align_text_col(align = "center") %>%
  flextable::set_caption(caption = "Most common words across three texts.")  %>%
  flextable::border_outer()
```


Now, we can simply plot the most common words and most indicative non-stop words in the three texts.
 
```{r key4, message=F, warning=F}
ggplot(freq_weight, aes(nrow(freq_weight):1, frequency)) +
     geom_point() +
     facet_wrap(~ group, scales = "free") +
     coord_flip() +
     scale_x_continuous(breaks = nrow(freq_weight):1,
                        labels = freq_weight$feature) +
     labs(x = NULL, y = "Relative frequency")
```

# Text Classification{-}

Text classification involves methods for categorizing text into predefined groups, like languages, genres, or authors. These categorizations usually rely on the frequency of word types, important terms, phonetic elements, and other linguistic characteristics such as sentence length and words per line.

Like many other text analysis methods, text classification often starts with a training dataset already marked with the necessary labels. You can create these training datasets and their associated features manually or opt for pre-built training sets offered by specific software or tools.


<br>

<div class="warning" style='padding:0.1em; background-color: rgba(251,184,0,.5); color:#51247a'>
<span>
<p style='margin-top:1em; text-align:center'>
Text classification is a machine learning task where text documents are categorized into predefined classes or labels based on their content. It involves training a model on labeled data to learn patterns and then using that model to classify new, unlabeled documents. Text classification has numerous applications, such as spam detection, sentiment analysis, and topic categorization.<br>
</p>
<p style='margin-left:1em;'>
</p></span>
</div>

<br>


In the upcoming example, we'll use phoneme frequency to classify a text. To get started, we'll load a German text and break it down into its constituent phonetic elements.

```{r tc1, message=FALSE, warning=FALSE}
# read in German text
German <- readLines("https://slcladal.github.io/data/phonemictext1.txt") %>%
  stringr::str_remove_all(" ") %>%
  stringr::str_split("") %>%
  unlist()
# inspect data
head(German, 20)
```

We now do the same for three other texts - an English and a Spanish text as well as one text in a language that we will determine using classification.

```{r tc2, message=FALSE, warning=FALSE}
# read in texts
English <- readLines("https://slcladal.github.io/data/phonemictext2.txt")
Spanish <- readLines("https://slcladal.github.io/data/phonemictext3.txt")
Unknown <- readLines("https://slcladal.github.io/data/phonemictext4.txt")
# clean, split texts into phonemes, unlist and convert them into vectors
English <- as.vector(unlist(strsplit(gsub(" ", "", English), "")))
Spanish <- as.vector(unlist(strsplit(gsub(" ", "", Spanish), "")))
Unknown <- as.vector(unlist(strsplit(gsub(" ", "", Unknown), "")))
# inspect data
head(English, 20)
```
We will now create a table that represents the phonemes and their frequencies in each of the 4 texts. In addition, we will add the language and simply the column names.

```{r tc3, echo=T, eval = T, message=FALSE, warning=FALSE}
# create data tables
German <- data.frame(names(table(German)), as.vector(table(German)))
English <- data.frame(names(table(English)), as.vector(table(English)))
Spanish <- data.frame(names(table(Spanish)), as.vector(table(Spanish)))
Unknown <- data.frame(names(table(Unknown)), as.vector(table(Unknown)))
# add column with language
German$Language <- "German"
English$Language <- "English"
Spanish$Language <- "Spanish"
Unknown$Language <- "Unknown"
# simplify column names
colnames(German)[1:2] <- c("Phoneme", "Frequency")
colnames(English)[1:2] <- c("Phoneme", "Frequency")
colnames(Spanish)[1:2] <- c("Phoneme", "Frequency")
colnames(Unknown)[1:2] <- c("Phoneme", "Frequency")
# combine all tables into a single table
classdata <- rbind(German, English, Spanish, Unknown) 
```


```{r tc3b, echo = F, message=FALSE, warning=FALSE}
# inspect data
classdata %>%
  as.data.frame() %>%
  head(10) %>%
  flextable() %>%
  flextable::set_table_properties(width = .5, layout = "autofit") %>%
  flextable::theme_zebra() %>%
  flextable::fontsize(size = 12) %>%
  flextable::fontsize(size = 12, part = "header") %>%
  flextable::align_text_col(align = "center") %>%
  flextable::set_caption(caption = "First 10 lines of the class data.")  %>%
  flextable::border_outer()
```


Now, we group the data so that we see, how often each phoneme is used in each language.

```{r tc5, echo=T, eval = T, message=FALSE, warning=FALSE}
# convert into wide format
classdw <- classdata %>%
  tidyr::spread(Phoneme, Frequency) %>%
  replace(is.na(.), 0)
```

```{r tc6b, echo = F, message=FALSE, warning=FALSE}
# inspect data
classdw[, 1:6] %>%
  as.data.frame() %>%
  head(10) %>%
  flextable() %>%
  flextable::set_table_properties(width = .5, layout = "autofit") %>%
  flextable::theme_zebra() %>%
  flextable::fontsize(size = 12) %>%
  flextable::fontsize(size = 12, part = "header") %>%
  flextable::align_text_col(align = "center") %>%
  flextable::set_caption(caption = "Overview of the class data in wide format.")  %>%
  flextable::border_outer()
```



Next, we must reshape our data to reflect the frequency of each phoneme categorized by language. This transformation aligns with our classifier's design, which will employ *Language* as the dependent variable and utilize phoneme frequencies as predictors.

```{r tc8, echo=T, eval = T, message=FALSE, warning=FALSE}
numvar <- colnames(classdw)[2:length(colnames(classdw))]
classdw[numvar] <- lapply(classdw[numvar], as.numeric)
# function for normalizing numeric variables
normalize <- function(x) { (x-min(x))/(max(x)-min(x))   }
# apply normalization
 classdw[numvar] <- as.data.frame(lapply(classdw[numvar], normalize))
```



```{r tc9b, echo = F, message=FALSE, warning=FALSE}
# inspect data
classdw[, 1:6] %>%
  as.data.frame() %>%
  head(10) %>%
  flextable() %>%
  flextable::set_table_properties(width = .5, layout = "autofit") %>%
  flextable::theme_zebra() %>%
  flextable::fontsize(size = 12) %>%
  flextable::fontsize(size = 12, part = "header") %>%
  flextable::align_text_col(align = "center") %>%
  flextable::set_caption(caption = "Overview of the probabilities.")  %>%
  flextable::border_outer()
```


Before turning to the actual classification, we will use a cluster analysis to see which texts the unknown text is most similar with.

```{r tc10, echo=T, eval = T, message=FALSE, warning=FALSE}
# remove language column
textm <- classdw[,2:ncol(classdw)]
# add languages as row names
rownames(textm) <- classdw[,1]
# create distance matrix
distmtx <- dist(textm)
# perform clustering
clustertexts <- hclust(distmtx, method="ward.D")  
# visualize cluster result
plot(clustertexts, hang = .25,main = "")           
```

As indicated by the cluster analysis, the unidentified text forms a cluster alongside the English texts, strongly suggesting that the unknown text is likely in English.

Before we dive into the actual classification process, we'll partition the data into two distinct sets: one excluding *Unknown* (our training set) and the other containing only *Unknown* (our test set). This segmentation allows us to train our model effectively and subsequently test its accuracy.

```{r tc11, echo=T, eval = T, message=FALSE, warning=FALSE}
# create training set
train <- classdw %>%
  filter(Language != "Unknown")
# create test set
test <- classdw %>%
  filter(Language == "Unknown")
```



```{r tc12b, echo = F, message=FALSE, warning=FALSE}
# inspect data
classdw[, 1:6] %>%
  as.data.frame() %>%
  head(10) %>%
  flextable() %>%
  flextable::set_table_properties(width = .5, layout = "autofit") %>%
  flextable::theme_zebra() %>%
  flextable::fontsize(size = 12) %>%
  flextable::fontsize(size = 12, part = "header") %>%
  flextable::align_text_col(align = "center") %>%
  flextable::set_caption(caption = "Overview of the training set probabilities.")  %>%
  flextable::border_outer()
```


In the final stage, we can put our classifier into action. Our chosen classifier is a k-nearest neighbor classifier, which operates on the principle of classifying an unknown element based on its proximity to the clusters within the training set.

```{r message=FALSE, warning=FALSE}
# set seed for reproducibility
set.seed(12345)
# apply k-nearest-neighbor (knn) classifier
prediction <- class::knn(train[,2:ncol(train)], 
                         test[,2:ncol(test)], 
                         cl = train[, 1], 
                         k = 3)
# inspect the result
prediction
```

Using the phoneme frequencies present in the unknown text, our knn-classifier confidently predicts that the text is in English. This prediction aligns with reality, as the text is, indeed, a section of the Wikipedia article for Aldous Huxley's *Brave New World*. It's worth noting that the training texts encompassed German, English, and Spanish translations of a subsection from Wikipedia's article on Hermann Hesse's *Steppenwolf*.


# Part-of-Speech tagging{-}

One widely used method for enhancing text data is part-of-speech tagging, which involves identifying the word type to which each word belongs. In the following section, we will apply part-of-speech tags to a brief English text.

Part-of-speech tagging is the process of assigning grammatical categories (such as noun, verb, adjective, etc.) to individual words in a text. It provides valuable insights into the syntactic and grammatical structure of a text, making it easier to analyze and extract meaningful information.


<br>

<div class="warning" style='padding:0.1em; background-color: rgba(251,184,0,.5); color:#51247a'>
<span>
<p style='margin-top:1em; text-align:center'>
Part-of-speech tagging (POS tagging) is a natural language processing task where each word in a text is assigned a grammatical category, such as noun, verb, adjective, etc. It involves using linguistic patterns and context to determine the appropriate part of speech for each word. POS tagging is crucial for various language analysis tasks, including information retrieval, text summarization, and grammar analysis.<br>
</p>
<p style='margin-left:1em;'>
</p></span>
</div>

<br>



We start by selecting a portion of our example text.

```{r udi1a, message=FALSE, warning=FALSE}
# load text
sample  <- base::readRDS(url("https://slcladal.github.io/data/alice.rda", "rb")) %>%
  .[1:10] %>%
  paste0(collapse = " ")
# inspect
substr(sample, 1, 200)
```

With our text ready for analysis, our next step is to download a pre-trained language model.

```{r udi1b, eval = F, message=FALSE, warning=FALSE}
# download language model
m_eng	<- udpipe::udpipe_download_model(language = "english-ewt")
```

Once you've downloaded a model previously, you also have the option to load it directly from the location where you've stored it on your computer. In my case, I've placed the model in a folder labeled *udpipemodels*.

```{r udi1c, message=FALSE, warning=FALSE}
# load language model from your computer after you have downloaded it once
m_eng <- udpipe_load_model(here::here("udpipemodels", "english-ewt-ud-2.5-191206.udpipe"))
```

We can now use the model to annotate out text.

```{r udi1d, message=FALSE, warning=FALSE}
# tokenise, tag, dependency parsing
text_anndf <- udpipe::udpipe_annotate(m_eng, x = sample) %>%
  as.data.frame() %>%
  dplyr::select(-sentence)
# inspect
head(text_anndf, 10)
```

It can be useful to extract only the words and their pos-tags and convert them back into a text format (rather than a tabular format). 

```{r udi2, message=FALSE, warning=FALSE}
tagged_text <- paste(text_anndf$token, "/", text_anndf$xpos, collapse = " ", sep = "")
# inspect tagged text
substr(tagged_text, 1, 200)
```

We could use the pos-tagged data to study differences in the distribution of word classes across different registers. or to find certain syntactic patterns in a collection of texts. 

# Names Entity Recognition {-}

Named Entity Recognition (NER), also known as named entity extraction or entity extraction, is a text analysis technique that automatically identifies and extracts named entities from text, such as people, locations, brands, and more.

NER involves the process of extracting textual elements with characteristics commonly associated with proper nouns (e.g., locations, individuals, organizations) rather than other parts of speech. These characteristics may include non-sentence initial capitalization. Named entities are frequently retrieved in automated summarization and topic modeling.

NER can be accomplished through straightforward feature extraction, like extracting all non-sentence-initial capitalized words, or with the aid of training sets. Utilizing training setsâ€”texts annotated to identify entities and non-entitiesâ€”proves more effective when dealing with unknown or inconsistently capitalized data.


<br>

<div class="warning" style='padding:0.1em; background-color: rgba(251,184,0,.5); color:#51247a'>
<span>
<p style='margin-top:1em; text-align:center'>
Named Entity Recognition (NER) is a natural language processing task that identifies and classifies words or phrases within text into predefined categories, such as persons, locations, organizations, and more. It employs contextual clues and language patterns to recognize these named entities. NER is essential for various applications, including information extraction, text summarization, and knowledge graph construction.<br>
</p>
<p style='margin-left:1em;'>
</p></span>
</div>

<br>


In this context, we will leverage the results obtained from part-of-speech tagging to extract terms tagged as named entities (the label `PROPN` in the `upos` column).

```{r ner1, message=FALSE, warning=FALSE}
# tokenise, tag, dependency parsing
ner_df <- text_anndf %>%
  dplyr::filter(upos == "PROPN") %>%
  dplyr::select(token_id, token, lemma, upos, feats)
# inspect
head(ner_df)
```

The obtained results can be further processed and categorized into various types such as persons, locations, dates, and other entities. This initial insight should provide you with a starting point for your analysis and exploration.

# Dependency Parsing Using UDPipe{-}

In addition to part-of-speech tagging, we can create visual representations illustrating the syntactic relationships between the various components of a sentence. 


<br>

<div class="warning" style='padding:0.1em; background-color: rgba(251,184,0,.5); color:#51247a'>
<span>
<p style='margin-top:1em; text-align:center'>
Dependency parsing is a linguistic analysis technique that reveals the grammatical structure of sentences by identifying how words relate to one another. It establishes hierarchical relationships, such as subject-verb, modifier-noun, or object-verb connections, within a sentence. Dependency parsing is fundamental for understanding sentence syntax, semantic roles, and linguistic relationships, playing a critical role in various natural language processing tasks like sentiment analysis, information extraction, and machine translation.<br>
</p>
<p style='margin-left:1em;'>
</p></span>
</div>

<br>


To achieve this, we first construct an object containing a sentence (in this instance, the sentence *John gave Mary a kiss*), and subsequently, we utilize the `textplot_dependencyparser` function to plot or visualize the dependencies.

```{r udi3, message=FALSE, warning=FALSE}
# parse text
sent <- udpipe::udpipe_annotate(m_eng, x = "John gave Mary a kiss.") %>%
  as.data.frame()
# inspect
head(sent)
```

We now generate the plot.

```{r udi5, message=FALSE, warning=FALSE}
# generate dependency plot
dplot <- textplot::textplot_dependencyparser(sent, size = 3) 
# show plot
dplot
```

Dependency parsing proves invaluable for a range of applications, including analyzing the relationships within sentences and shedding light on the roles of different elements. For instance, it helps distinguish between the agent and the patient in actions like crimes or other activities. This parsing technique enables a deeper understanding of the underlying grammatical and semantic structure of sentences, making it a valuable tool for linguistic analysis, information extraction, and natural language understanding.


# Citation & Session Info {-}

Schweinberger, Martin. 2023. *Practical Overview of Selected Text Analytics Methods*. Brisbane: The Language Technology and Data Analysis Laboratory (LADAL). url: https://ladal.edu.au/textanalysis.html (Version 2023.05.31).

```
@manual{schweinberger2023ta,
  author = {Schweinberger, Martin},
  title = {Practical Overview of Selected Text Analytics Methods},
  note = {https://ladal.edu.au/textanalysis.html},
  year = {2023},
  organization = {The Language Technology and Data Analysis Laboratory (LADAL)},
  address = {Brisbane},
  edition = {2023.05.31}
}
```



```{r fin}
sessionInfo()
```

[Back to top](#introduction)

[Back to HOME](https://ladal.edu.au)


# References{-}

