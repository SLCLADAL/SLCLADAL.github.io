{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "![An interactive LADAL notebook](https://slcladal.github.io/images/uq1.jpg)\n",
                "\n",
                "# Spell checking text data with R\n",
                "\n",
                "\n",
                "This tutorial is the interactive Jupyter notebook accompanying the [*Language Technology and Data Analysis Laboratory* (LADAL) tutorial *Spell checking text data with R*](https://ladal.edu.au/spellcheck.html). The tutorial provides more details and background information while this interactive notebook focuses strictly on practical aspects.\n",
                "\n",
                "***\n",
                "\n",
                "\n",
                "**Preparation and session set up**\n",
                "\n",
                "We set up our session by activating the packages we need for this tutorial.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# activate packages\n",
                "library(quanteda)\n",
                "library(dplyr)\n",
                "library(stringr)\n",
                "library(hunspell)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Once you have initiated the session by executing the code shown above, you are good to go.\n",
                "\n",
                "If you are using this notebook on your own computer and you have not already installed the R packages listed above, you need to install them. You can install them by replacing the `library` command with `install.packages` and putting the name of the package into quotation marks like this: `install.packages(\"quanteda\")`. Then, you simply run this command and R will install the package you specified.\n",
                "\n",
                "***\n",
                "\n",
                "## Using your own data\n",
                "\n",
                "While the tutorial uses data from the LADAL website, you can also use your own data. You can see below what you need to do to upload and use your own data.\n",
                "\n",
                "The code chunk below allows you to upload two files from your own computer. To be able to load your own data, you need to click on the folder symbol to the left of the screen:\n",
                "\n",
                "![Binder Folder Symbol](https://slcladal.github.io/images/binderfolder.JPG)\n",
                "\n",
                "\n",
                "Then on the upload symbol.\n",
                "\n",
                "![Binder Upload Symbol](https://slcladal.github.io/images/binderupload.JPG)\n",
                "\n",
                "Next, upload the files you want to analyze and then the respective files names in the file argument of the scan function. When you then execute the code (like to code chunk below, you will upload your own data.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "mytext1 <- scan(file = \"linguistics01.txt\",\n",
                "            what = \"char\", \n",
                "            sep = \"\", \n",
                "            quote = \"\", \n",
                "            quiet = T, \n",
                "            skipNul = T) %>%\n",
                "            paste0(collapse = \" \")\n",
                "mytext2 <- scan(file = \"linguistics02.txt\",\n",
                "            what = \"char\", \n",
                "            sep = \"\", \n",
                "            quote = \"\", \n",
                "            quiet = T, \n",
                "            skipNul = T) %>%\n",
                "            paste0(collapse = \" \")\n",
                "# inspect\n",
                "mytext1; mytext2\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Keep in mind though that you need to adapt the names of the texts in the code chunks below so that the code below work on your own texts!**\n",
                "\n",
                "***\n",
                "\n",
                "\n",
                "## Checking individual words \n",
                "\n",
                "We start by checking a vector of individual words.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "words <- c(\"analyze\", \"langauge\", \"data\")\n",
                "correct <- hunspell_check(words)\n",
                "print(correct)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The output shows that the second word was not found in the dictionary, i.e., it is identified as being incorrect. Next, we can ask for suggestions, i.e. the correct form of the word.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "hunspell_suggest(words[!correct])\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "In this case, there are two words that are deemed as potential alternatives.\n",
                "\n",
                "## Checking documents \n",
                "\n",
                "Since we rarely want to check individual words, we will now focus on spell checking full texts rather than individual vectors with words.\n",
                "\n",
                "First, we load a text (in this case an explanation of what grammer is that is taken from Wikipedia).\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# read in text\n",
                "exampletext  <- base::readRDS(url(\"https://slcladal.github.io/data/tx1.rda\", \"rb\"))\n",
                "# inspect\n",
                "exampletext\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We now use the `hunspell` function to find incorrect words (or, more precisely, words that are not in the default dictionary).\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "errors <- hunspell(exampletext)\n",
                "errors[[1]]\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We check what suggestions we get for these words.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "hunspell_suggest(errors[[1]])\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "In our case, the words are not errors but simply terms that do not occur in the dictionary so we will not replace them.\n",
                "\n",
                "## Stemming Words \n",
                "\n",
                "The `hunspell_stem` looks up words from the dictionary which match the root of the given word (sometimes multiple items are returned if there are multiple matches in the dictionary).\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "toks <- c(\"love\", \"loving\", \"lovingly\", \"loved\", \"lover\", \"lovely\")\n",
                "hunspell_stem(toks)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The `hunspell_stem` function can be very useful when trying to find the stems of words in a corpusto see, e.g., how many word types a text contains.\n",
                "\n",
                "\n",
                "## Working with texts \n",
                "\n",
                "It is quite common that we work with texts rather than individual  word vectors. As such, in the following, we will go through a workflow that resembles what one might use spell checking for in their research. \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "texttable <- quanteda::tokens(\"Noam said he loves to analyze langauge and collors.\") %>%\n",
                "  unlist() %>%\n",
                "  as.data.frame() %>%\n",
                "  dplyr::rename(words = 1) %>%\n",
                "  dplyr::mutate(id = 1:nrow(.),\n",
                "                error = hunspell::hunspell_check(words)) %>%\n",
                "  dplyr::relocate(id)\n",
                "# inspect\n",
                "texttable\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The output shows the original word and if it was identified as an errors (i.e., it did not occur in the dictionary). Next, we extract suggestions for the words that were identified as errors.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "texttable2 <- texttable %>%\n",
                "  # add suggestions\n",
                "  dplyr::mutate(suggestions = ifelse(error == F, \n",
                "                                     paste0(hunspell_suggest(words), \", \"), \n",
                "                                     \"\"),\n",
                "                # clean suggestions\n",
                "                suggestions = stringr::str_remove_all(suggestions, fixed(\"c(\")),\n",
                "                suggestions = stringr::str_remove_all(suggestions, fixed(\")\")),\n",
                "                suggestions = stringr::str_remove_all(suggestions, \", {0,1}$\")\n",
                "                )\n",
                "# inspect\n",
                "texttable2\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We now want to replace the errors with the correct words but aso retain words that are erroneously regraded as errors. \n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "texttable3 <- texttable2 %>%\n",
                "  # replace errors with the first suggestion\n",
                "  dplyr::mutate(corrected = dplyr::case_when(error == T ~ words,\n",
                "                                             error == F ~ stringr::str_remove_all(suggestions, '\\\\\", .*'),\n",
                "                                             T ~ words)) %>%\n",
                "  # clean the corrected words\n",
                "  dplyr::mutate(corrected = stringr::str_remove_all(corrected, '^\\\\\"')) %>%\n",
                "  # insert words where we do not want the suggestions but the original word\n",
                "  dplyr::mutate(corrected = dplyr::case_when(words == \"Noam\" ~ words,\n",
                "                                             T ~ corrected))\n",
                "# inspect\n",
                "texttable3\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We can now check how many errors there are in our text.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "texttable4 <- texttable3 %>%\n",
                "  dplyr::summarise(tokens = nrow(.),\n",
                "                   types = length(names(table(tolower(words)))),\n",
                "                   errors_n = sum(ifelse(corrected == words, 0, 1)),\n",
                "                   errors_percent = errors_n/tokens*100\n",
                "                   )\n",
                "# inspect\n",
                "texttable4\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Finally, we can put the corrected text back together. \n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "correctedtext <- paste0(texttable3$corrected, collapse = \" \") %>%\n",
                "  stringr::str_replace_all(\" \\\\.\", \".\")\n",
                "correctedtext\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Setting a Language \n",
                "\n",
                "By default, the `hunspell` package includes dictionaries for `en_US` and `en_GB` which means that you can easily switch from US American to British English spelling.\n",
                "\n",
                "The default `hunspell` uses the `en_US` dictionary as shown below.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "hunspell(\"At LADAL we like to analyse language and colours.\")\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "But we can easily switch to British English instead as shown below.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "hunspell(\"At LADAL we like to analyse language and colours.\", dict = 'en_GB')\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "knitr::include_graphics(\"https://slcladal.github.io/images/InstallLanguages.png\")\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "If you want to use another language you need to make sure that the dictionary is available in your system so that RStudio can access the dictionary. You can install dictionaries very easily in RStudio. Simply go to `Tools` > `Global options` > `Spelling` and then under `Main dictionary language` select `Install More Languages` from the drop-down menu. Once the additional languages are installed, their dictionaries are available to the `hunspell` package in RStudio.\n",
                "\n",
                "However, you may want to install dictionaries directly, e.g., into your working directory so that you can use the dictionary when working with text data either on your computer or in a cloud environment. In this case, you can go to the [wooorm dictionary GitHub repository](https://github.com/wooorm/dictionaries), which has dict and aff files (i.e., the files that are needed to create a dictionary) for many different languages, and install the dict and aff files manually.  In our case, I downloaded the dict and aff files from the German dictionary, stored them together with the other dictionary files in the `hunspell` package library, and renamed the files as `de.dict` and `de.aff`.\n",
                "\n",
                "\n",
                "If you then want to use the dictionary, you simply specify the `dict` argument as shown below for a German sentence.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "hunspell(\"Im LADAL analysieren wir Sprachen und Farben.\", dict = 'de')\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "This is the end of this short tutorial on spell checking with R. If you want to go deeper, please check out the documentation site of the `hunspell` package)[https://cran.r-project.org/web/packages/hunspell/vignettes/intro.html] [@hunspell].  \n",
                "\n",
                "\n",
                "***\n",
                "\n",
                "[Back to LADAL](https://ladal.edu.au/spellcheck.html)\n",
                "\n",
                "***\n"
            ]
        }
    ],
    "metadata": {
        "anaconda-cloud": "",
        "kernelspec": {
            "display_name": "R",
            "langauge": "R",
            "name": "ir"
        },
        "language_info": {
            "codemirror_mode": "r",
            "file_extension": ".r",
            "mimetype": "text/x-r-source",
            "name": "R",
            "pygments_lexer": "r",
            "version": "3.4.1"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}
