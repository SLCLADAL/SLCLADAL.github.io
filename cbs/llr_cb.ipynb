{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "![An interactive LADAL notebook](https://slcladal.github.io/images/uq1.jpg)\n",
                "\n",
                "# Analyzing learner language using R\n",
                "\n",
                "\n",
                "This tutorial is the interactive Jupyter notebook accompanying the [*Language Technology and Data Analysis Laboratory* (LADAL) tutorial *Analyzing learner language using R*](https://ladal.edu.au/llr.html). The tutorial provides more details and background information while this interactive notebook focuses strictly on practical aspects.\n",
                "\n",
                "***\n",
                "\n",
                "\n",
                "**Preparation and session set up**\n",
                "\n",
                "We set up our session by activating the packages we need for this tutorial.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# activate packages\n",
                "library(quanteda)\n",
                "library(dplyr)\n",
                "library(koRpus)\n",
                "library(tidyr)\n",
                "library(stringr)\n",
                "library(hunspell)\n",
                "library(udpipe)\n",
                "library(wordcloud2)\n",
                "library(tokenizers)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Once you have initiated the session by executing the code shown above, you are good to go.\n",
                "\n",
                "If you are using this notebook on your own computer and you have not already installed the R packages listed above, you need to install them. You can install them by replacing the `library` command with `install.packages` and putting the name of the package into quotation marks like this: `install.packages(\"quanteda\")`. Then, you simply run this command and R will install the package you specified.\n",
                "\n",
                "## Using your own data\n",
                "\n",
                "While the tutorial uses data from the LADAL website, you can also use your own data. You can see below what you need to do to upload and use your own data.\n",
                "\n",
                "The code chunk below allows you to upload two files from your own computer. To be able to load your own data, you need to click on the folder symbol to the left of the screen:\n",
                "\n",
                "![Binder Folder Symbol](https://slcladal.github.io/images/binderfolder.JPG)\n",
                "\n",
                "\n",
                "Then, when the menu has unfolded, click on the smaller folder symbol (encircled in red in the picture below).\n",
                "\n",
                "![Small Binder Folder Symbol](https://slcladal.github.io/images/upload2.png)\n",
                "\n",
                "Now, you are in the main menu and can click on the 'MyData' folder.\n",
                "\n",
                "![MyData Folder Symbol](https://slcladal.github.io/images/upload3.png)\n",
                "\n",
                "Now, that you are in the MyData folder, you can click on the upload symbol.\n",
                "\n",
                "![Binder Upload Symbol](https://slcladal.github.io/images/binderupload.JPG)\n",
                "\n",
                "Select and upload the files you want to analyze (**IMPORTANT: here, we assume that you upload some form of text data - not tabular data! You can upload only txt and docx files!**). When you then execute the code chunk below, you will upload your own data and you can then use it in this notebook.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "myfiles <- list.files(here::here(\"MyData\"), # path to the corpus data\n",
                "                          # full paths - not just the names of the files\n",
                "                          full.names = T) \n",
                "# load colt files\n",
                "mytext <- sapply(myfiles, function(x){\n",
                "  x <- scan(x, \n",
                "            what = \"char\", \n",
                "            sep = \"\", \n",
                "            quote = \"\", \n",
                "            quiet = T, \n",
                "            skipNul = T)\n",
                "  x <- paste0(x, sep = \" \", collapse = \" \")\n",
                "  x <- stringr::str_squish(x)\n",
                "})\n",
                "# inspect\n",
                "str(mytext)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Keep in mind though that you need to adapt the names of the texts in the code chunks below so that the code below work on your own texts!**\n",
                "\n",
                "***\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# load packages\n",
                "library(tidyverse)\n",
                "library(tm)\n",
                "library(tidytext)\n",
                "library(tidyr)\n",
                "library(NLP)\n",
                "library(openNLP)\n",
                "library(quanteda)\n",
                "library(quanteda.textstats)\n",
                "library(quanteda.textplots)\n",
                "library(koRpus)\n",
                "library(koRpus.lang.en)\n",
                "library(stringi)\n",
                "library(hunspell)\n",
                "library(wordcloud2)\n",
                "library(pacman)\n",
                "pacman::p_load_gh(\"trinker/entity\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Once you have installed R and RStudio and once you have also initiated the session by executing the code shown above, you are good to go.\n",
                "\n",
                "**Loading data**\n",
                "\n",
                "We use 7 essays written by learners from the [*International Corpus of Learner English* (ICLE)](https://uclouvain.be/en/research-institutes/ilc/cecl/icle.html) and two files containing a-level essays written by L1-English British students from [*The Louvain Corpus of Native English Essays* (LOCNESS)](https://uclouvain.be/en/research-institutes/ilc/cecl/locness.html) which was compiled by the *Centre for English Corpus Linguistics* (CECL), UniversitÃ© catholique de Louvain, Belgium. The code chunk below loads the data from the LADAL repository on GitHub into R.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# load essays from l1 speakers\n",
                "ns1 <- base::readRDS(url(\"https://slcladal.github.io/data/LCorpus/ns1.rda\", \"rb\"))\n",
                "ns2 <- base::readRDS(url(\"https://slcladal.github.io/data/LCorpus/ns2.rda\", \"rb\"))\n",
                "# load essays from l2 speakers\n",
                "es <- base::readRDS(url(\"https://slcladal.github.io/data/LCorpus/es.rda\", \"rb\"))\n",
                "de <- base::readRDS(url(\"https://slcladal.github.io/data/LCorpus/de.rda\", \"rb\"))\n",
                "fr <- base::readRDS(url(\"https://slcladal.github.io/data/LCorpus/fr.rda\", \"rb\"))\n",
                "it <- base::readRDS(url(\"https://slcladal.github.io/data/LCorpus/it.rda\", \"rb\"))\n",
                "pl <- base::readRDS(url(\"https://slcladal.github.io/data/LCorpus/pl.rda\", \"rb\"))\n",
                "ru <- base::readRDS(url(\"https://slcladal.github.io/data/LCorpus/ru.rda\", \"rb\"))\n",
                "# inspect\n",
                "ru %>%\n",
                "  # remove header\n",
                "  stringr::str_remove(., \"<[A-Z]{4,4}.*\") %>%\n",
                "  # remove empty elements\n",
                "  dplyr::na_if(\"\") %>%\n",
                "  na.omit %>%\n",
                "  #show first 3 elements\n",
                "  head(3)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The data inspection shows the first 3 text elements from the essay written a Russian learner of English to provide an idea of what the data look like. \n",
                "\n",
                "Now that we have loaded some data, we can go ahead and extract information from the texts and process the data to analyze differences between L1 speakers and learners of English.\n",
                "\n",
                "# Concordancing\n",
                "\n",
                "Concordancing refers to the extraction of words or phrases from a given text or texts [@lindquist2009corpus]. Commonly, concordances are displayed in the form of key-word in contexts (KWIC) where the search term is shown with some preceding and following context. Thus, such displays are referred to as key word in context concordances. A more elaborate tutorial on how to perform concordancing with R is available [here](https://slcladal.github.io/kwics.html).\n",
                "\n",
                "\n",
                "Concordancing is helpful for seeing how a given term or phrased is used in the data, for inspecting how often a given word occurs in a text or a collection of texts, for extracting examples, and it also represents a basic procedure, and often the first step, in more sophisticated analyses. \n",
                "\n",
                "We begin by creating KWIC displays of the term *problem* as shown below. To extract the kwic concordances, we use the `kwic` function from the `quanteda` package [cf. @benoit2018quanteda]. \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# combine data from learners\n",
                "learner <- c(de, es, fr, it, pl, ru)\n",
                "# extract kwic for term \"problem\" in learner data\n",
                "kwic <- quanteda::kwic(learner,               # the data in which to search\n",
                "                       pattern = \"problem.*\", # the pattern to look for\n",
                "                       valuetype = \"regex\",   # look for exact matches or patterns\n",
                "                       window = 10) %>%       # how much context to display (in elements) \n",
                "  # convert to table (called data.frame in R)\n",
                "  as.data.frame() %>%\n",
                "  # remove superfluous columns\n",
                "  dplyr::select(-to, -from, -pattern)\n",
                "# inspect\n",
                "head(kwic)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The output shows that the term *problem* occurs six times in the learner data.\n",
                "\n",
                "We can also arrange the output according to what comes before or after the search term as shown below.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# take kwic\n",
                "kwic %>%\n",
                "  # arrange kwic alphabetically by what comes after the key term\n",
                "  dplyr::arrange(post)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# take quick\n",
                "kwic %>%\n",
                "  # reverse the preceding context\n",
                "  dplyr::mutate(prerev = stringi::stri_reverse(pre)) %>%\n",
                "  # arrange kwic alphabetically by reversed preceding context\n",
                "  dplyr::arrange(prerev) %>%\n",
                "  # remove column with reversed preceding context\n",
                "  dplyr::select(-prerev)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We can also combine concordancing with visualizations. For instance, use the `textplot_xray` function from the `quanteda.textplots` package to visualize where in some texts the term *people* and the term *imagination*  occurs.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# create kwics for people and imagination\n",
                "kwic_people <- quanteda::kwic(learner, pattern = c(\"people\", \"imagination\"))\n",
                "# generate x-ray plot\n",
                "quanteda.textplots::textplot_xray(kwic_people)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We can also search for phrases rather than individual words. To do this, we need to use the `phrase` function in the `pattern` argument as shown below. In the code chunk below, we look for any combination of the word *very* and any following word. It we would wish, we could of course also  sort (or order) the concordances as we have done above.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# generate kwic for phrases staring with very\n",
                "kwic <- quanteda::kwic(learner,                              # data\n",
                "                       pattern = phrase(\"^very [a-z]{1,}\"),  # search pattern\n",
                "                       valuetype = \"regex\") %>%              # type of pattern\n",
                "  # convert into a data frame\n",
                "  as.data.frame()\n",
                "# inspect data\n",
                "head(kwic)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Frequency lists\n",
                "\n",
                "A useful procedure when dealing with texts is to extract frequency information. To exemplify how to extract frequency lists from texts, we will do this here using the L1 data.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "ftb <- c(ns1, ns2) %>%\n",
                "  # remove punctuation\n",
                "  stringr::str_replace_all(., \"\\\\W\", \" \") %>%\n",
                "  # remove superfluous white spaces\n",
                "  stringr::str_squish() %>%\n",
                "  # convert to lower case\n",
                "  tolower() %>%\n",
                "  # split into words\n",
                "  stringr::str_split(\" \") %>%\n",
                "  # unlist\n",
                "  unlist() %>%\n",
                "  # convert into table\n",
                "  as.data.frame() %>%\n",
                "  # rename column\n",
                "  dplyr::rename(word = 1) %>%\n",
                "  # remove empty rows\n",
                "  dplyr::filter(word != \"\") %>%\n",
                "  # count words\n",
                "  dplyr::group_by(word) %>%\n",
                "  dplyr::summarise(freq = n()) %>%\n",
                "  # order by freq\n",
                "  dplyr::arrange(-freq)\n",
                "# inspect\n",
                "head(ftb)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We can easily remove stop words (words without lexical content) using the `anti_join` function as shown below.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "ftb_wosw <- ftb %>%\n",
                "  # remove stop words\n",
                "  dplyr::anti_join(stop_words)\n",
                "# inspect\n",
                "head(ftb_wosw)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We can then visualize the results as a bar chart as shown below.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "ftb_wosw %>%\n",
                "  # take 20 most frequent terms\n",
                "  head(20) %>%\n",
                "  # generate a plot\n",
                "  ggplot(aes(x = reorder(word, -freq), y = freq, label = freq)) +\n",
                "  # define type of plot\n",
                "  geom_bar(stat = \"identity\") +\n",
                "  # add labels\n",
                "  geom_text(vjust=1.6, color = \"white\") +\n",
                "  # display in black-and-white theme\n",
                "  theme_bw() +\n",
                "  # adapt x-axis tick labels\n",
                "  theme(axis.text.x = element_text(size=8, angle=90)) +\n",
                "  # adapt axes labels\n",
                "  labs(y = \"Frequnecy\", x = \"Word\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Or we can visualize the data as a word cloud (see below).\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# create wordcloud\n",
                "wordcloud2(ftb_wosw[1:100,],    # define data to use\n",
                "           # define shape\n",
                "           shape = \"diamond\",\n",
                "           # define colors\n",
                "           color = scales::viridis_pal()(8))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Splitting texts into sentences\n",
                "\n",
                "It can be every useful to split texts into individual sentences. This can be done, e.g., to extract the average sentence length or simply to inspect or annotate individual sentences. To split a text into sentences, we clean the data by removing file identifiers and html tags as well as quotation marks within sentences. As we are dealing with several texts, we write a function that performs this task and that we can then apply to the individual texts.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "cleanText <- function(x,...){\n",
                "  require(tokenizers)\n",
                "  # paste text together\n",
                "  x <- paste0(x)\n",
                "  # remove file identifiers\n",
                "  x <- stringr::str_remove_all(x, \"<.*?>\")\n",
                "  # remove quotation marks\n",
                "  x <- stringr::str_remove_all(x, fixed(\"\\\"\"))\n",
                "  # remove empty elements\n",
                "  x <- x[!x==\"\"]\n",
                "  # split text into sentences\n",
                "  x <- tokenize_sentences(x)\n",
                "  x <- unlist(x)\n",
                "}\n",
                "# clean texts\n",
                "ns1_sen <- cleanText(ns1)\n",
                "ns2_sen <- cleanText(ns2)\n",
                "de_sen <- cleanText(de)\n",
                "es_sen <- cleanText(es)\n",
                "fr_sen <- cleanText(fr)\n",
                "it_sen <- cleanText(it)\n",
                "pl_sen <- cleanText(pl)\n",
                "ru_sen <- cleanText(ru)\n",
                "# inspect data\n",
                "head(ru_sen)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now that we have split the texts into individual sentences, we can easily extract and visualize the average sentence lengths of L1 speakers and learners of English.\n",
                "\n",
                "# Sentence length\n",
                "\n",
                "The most basic complexity measure is average sentence length. In the following, we will extract the average sentence length for L1-speakers and learners of English with different language backgrounds.\n",
                "\n",
                "We can use the `count_words` function from the `tokenizers` package to count the words in each sentence. We apply the function to all texts and generate a table (a data frame) of the results and add the L1 of the speaker who produced the sentence.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# extract sentences lengths\n",
                "ns1_sl <- tokenizers::count_words(ns1_sen)\n",
                "ns2_sl <- tokenizers::count_words(ns2_sen)\n",
                "de_sl <- tokenizers::count_words(de_sen)\n",
                "es_sl <- tokenizers::count_words(es_sen)\n",
                "fr_sl <- tokenizers::count_words(fr_sen)\n",
                "it_sl <- tokenizers::count_words(it_sen)\n",
                "pl_sl <- tokenizers::count_words(pl_sen)\n",
                "ru_sl <- tokenizers::count_words(ru_sen)\n",
                "# create a data frame from the results\n",
                "sl_df <- data.frame(c(ns1_sl, ns2_sl, de_sl, es_sl, fr_sl, it_sl, pl_sl, ru_sl)) %>%\n",
                "  dplyr::rename(sentenceLength = 1) %>%\n",
                "  dplyr::mutate(l1 = c(rep(\"en\", length(ns1_sl)),\n",
                "                       rep(\"en\", length(ns2_sl)),\n",
                "                       rep(\"de\", length(de_sl)),\n",
                "                       rep(\"es\", length(es_sl)),\n",
                "                       rep(\"fr\", length(fr_sl)),\n",
                "                       rep(\"it\", length(it_sl)),\n",
                "                       rep(\"pl\", length(pl_sl)),\n",
                "                       rep(\"ru\", length(ru_sl))))\n",
                "# inspect data\n",
                "head(sl_df)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now, we can use the resulting table to create a box plot showing the results.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "sl_df %>%\n",
                "  ggplot(aes(x = reorder(l1, -sentenceLength, mean), y = sentenceLength, fill = l1)) +\n",
                "  geom_boxplot() +\n",
                "  # adapt y-axis labels\n",
                "  labs(y = \"Sentence lenghts\") +\n",
                "  # adapt tick labels\n",
                "  scale_x_discrete(\"L1 of learners\", \n",
                "                   breaks = names(table(sl_df$l1)), \n",
                "                   labels = c(\"en\" = \"English\",\n",
                "                              \"de\" = \"German\",\n",
                "                              \"es\" = \"Spanish\",\n",
                "                              \"fr\" = \"French\",\n",
                "                              \"it\" = \"Italian\",\n",
                "                              \"pl\" = \"Polish\",\n",
                "                              \"ru\" = \"Russian\")) +\n",
                "  theme_bw() +\n",
                "  theme(legend.position = \"none\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Extracting N-grams\n",
                "\n",
                "In a next step, we extract n-grams using the `tokens_ngrams` function from the `quanteda` package. In a first step, we take the sentence data, convert it to lower case and remove punctuation. Then we apply the `tokens_ngrams` function to extract the n-grams (in this case 2-grams).\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "ns1_tok <- ns1_sen %>%\n",
                "  tolower() %>%\n",
                "  quanteda::tokens(remove_punct = TRUE)\n",
                "# extract n-grams\n",
                "ns1_2gram <- quanteda::tokens_ngrams(ns1_tok, n = 2)\n",
                "# inspect\n",
                "head(ns1_2gram[[2]], 10)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We can also extract tri-grams easily by changing the `n` argument in the `tokens_ngrams` function.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# extract n-grams\n",
                "ns1_3gram <- quanteda::tokens_ngrams(ns1_tok, n = 3)\n",
                "# inspect\n",
                "head(ns1_3gram[[2]])\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We now apply the same procedure to all texts as shown below.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "ns1_tok <- ns1_sen %>% tolower() %>% quanteda::tokens(remove_punct = TRUE)\n",
                "ns2_tok <- ns2_sen %>% tolower() %>% quanteda::tokens(remove_punct = TRUE)\n",
                "de_tok <- de_sen %>% tolower() %>% quanteda::tokens(remove_punct = TRUE)\n",
                "es_tok <- es_sen %>% tolower() %>% quanteda::tokens(remove_punct = TRUE)\n",
                "fr_tok <- fr_sen %>% tolower() %>% quanteda::tokens(remove_punct = TRUE)\n",
                "it_tok <- it_sen %>% tolower() %>% quanteda::tokens(remove_punct = TRUE)\n",
                "pl_tok <- pl_sen %>% tolower() %>% quanteda::tokens(remove_punct = TRUE)\n",
                "ru_tok <- ru_sen %>% tolower() %>% quanteda::tokens(remove_punct = TRUE)\n",
                "# extract n-grams\n",
                "ns1_2gram <- as.vector(unlist(quanteda::tokens_ngrams(ns1_tok, n = 2)))\n",
                "ns2_2gram <- as.vector(unlist(quanteda::tokens_ngrams(ns2_tok, n = 2)))\n",
                "de_2gram <- as.vector(unlist(quanteda::tokens_ngrams(de_tok, n = 2)))\n",
                "es_2gram <- as.vector(unlist(quanteda::tokens_ngrams(es_tok, n = 2)))\n",
                "fr_2gram <- as.vector(unlist(quanteda::tokens_ngrams(fr_tok, n = 2)))\n",
                "it_2gram <- as.vector(unlist(quanteda::tokens_ngrams(it_tok, n = 2)))\n",
                "pl_2gram <- as.vector(unlist(quanteda::tokens_ngrams(pl_tok, n = 2)))\n",
                "ru_2gram <- as.vector(unlist(quanteda::tokens_ngrams(ru_tok, n = 2)))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Next, we generate a table with the ngrams and the L1 background of the speaker that produced the bi-grams.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "ngram_df <- c(ns1_2gram, ns2_2gram, de_2gram, es_2gram, \n",
                "              fr_2gram, it_2gram, pl_2gram, ru_2gram) %>%\n",
                "  as.data.frame() %>%\n",
                "  dplyr::rename(ngram = 1) %>%\n",
                "  dplyr::mutate(l1 = c(rep(\"en\", length(ns1_2gram)),\n",
                "                       rep(\"en\", length(ns2_2gram)),\n",
                "                       rep(\"de\", length(de_2gram)),\n",
                "                       rep(\"es\", length(es_2gram)),\n",
                "                       rep(\"fr\", length(fr_2gram)),\n",
                "                       rep(\"it\", length(it_2gram)),\n",
                "                       rep(\"pl\", length(pl_2gram)),\n",
                "                       rep(\"ru\", length(ru_2gram))),\n",
                "                learner = ifelse(l1 == \"en\", \"no\", \"yes\"))\n",
                "# inspect\n",
                "head(ngram_df)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now, we process the table further to add frequency information, i.e., how often a given n-gram occurs in each the language of speakers with distinct L1 backgrounds.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "ngram_fdf <- ngram_df %>%\n",
                "  dplyr::group_by(ngram, learner) %>%\n",
                "  dplyr::summarise(freq = n()) %>%\n",
                "  dplyr::arrange(-freq)\n",
                "# inspect\n",
                "head(ngram_fdf)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "As the word counts of the texts are quite different, we normalize the frequencies to per-1,000-word frequencies which are comparable across texts of different lengths.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "ngram_nfdf <- ngram_fdf %>%\n",
                "  dplyr::group_by(ngram) %>%\n",
                "  dplyr::mutate(total_ngram = sum(freq)) %>%\n",
                "  dplyr::arrange(-total_ngram) %>%\n",
                "  # total by learner\n",
                "  dplyr::group_by(learner) %>%\n",
                "  dplyr::mutate(total_learner = sum(freq),\n",
                "                rfreq = freq/total_learner*1000)\n",
                "# inspect\n",
                "head(ngram_nfdf, 10)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We now reformat the table so that we have relative frequencies for both learners and L1 speakers even if a particular n-gram does not occur in the text produced by either a learner or a L1 speaker.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "ngram_rel <- ngram_nfdf %>%\n",
                "  dplyr::select(ngram, learner, rfreq, total_ngram) %>%\n",
                "  tidyr::spread(learner, rfreq) %>%\n",
                "  dplyr::mutate(no = ifelse(is.na(no), 0, no),\n",
                "                yes = ifelse(is.na(yes), 0, yes)) %>%\n",
                "  tidyr::gather(learner, rfreq, no:yes) %>%\n",
                "  dplyr::arrange(-total_ngram)\n",
                "# inspect\n",
                "head(ngram_rel)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Finally, we visualize the most frequent n-grams in the data in a bar chart.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "ngram_rel %>%\n",
                "  head(20) %>%\n",
                "  ggplot(aes(y = rfreq, x = reorder(ngram, -total_ngram), group = learner, fill = learner)) +\n",
                "  geom_bar(stat = \"identity\", position = position_dodge()) +\n",
                "  theme_bw() +\n",
                "  theme(axis.text.x = element_text(size=8, angle=90),\n",
                "        legend.position = \"top\") +\n",
                "  labs(y = \"Relative frequnecy\\n(per 1,000 words)\", x = \"n-gram\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We can, of course also investigate only specific n-grams, e.g., n-grams containing a specific word such as *public* (below, we only show the first 6 n-grams containing *public* by using the `head` function).\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "ngram_rel %>%\n",
                "  dplyr::filter(stringr::str_detect(ngram, \"public\")) %>%\n",
                "  head()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We can also specify the order by adding the underscore as shown below.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "ngram_rel %>%\n",
                "  dplyr::filter(stringr::str_detect(ngram, \"public_\")) %>%\n",
                "  head()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Differences in ngram use\n",
                "\n",
                "Next, we will set out to identify differences in n-gram frequencies between learners and L1 speakers. In a first step, we transform the table so that we have separate columns for learners and L1-speakers. In addition, we also add columns containing all the information we need to perform Fisher's exact test to check if learners use certain n-grams significantly  more or less frequently compared to L1-speakers. \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "sdif_ngram <- ngram_fdf %>%\n",
                "  tidyr::spread(learner, freq) %>%\n",
                "  dplyr::mutate(no = ifelse(is.na(no), 0, no),\n",
                "                yes = ifelse(is.na(yes), 0, yes)) %>%\n",
                "  dplyr::rename(l1speaker = no, \n",
                "                learner = yes) %>%\n",
                "  dplyr::mutate(total_ngram = l1speaker+learner) %>%\n",
                "  dplyr::ungroup() %>%\n",
                "  dplyr::mutate(total_learner = sum(learner),\n",
                "              total_l1 = sum(l1speaker)) %>%\n",
                "  dplyr::mutate(a = l1speaker,\n",
                "                b = learner) %>%\n",
                "  dplyr::mutate(c = total_l1-a,\n",
                "                d = total_learner-b)\n",
                "# inspect\n",
                "head(sdif_ngram)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "On this re-arranged data set, we can now apply the Fisher's exact tests. As we are performing many different tests, we need to correct for multiple comparisons. To this end, we create a column which holds the Bonferroni corrected critical value (\\alpha .05). If a p-value is lower than the corrected critical value, then the learners and L1-speakers differ significantly in their use of that n-gram.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "sdif_ngram <- sdif_ngram  %>%\n",
                "  # perform fishers exact test and extract estimate and p\n",
                "  dplyr::rowwise() %>%\n",
                "  dplyr::mutate(fisher_p = fisher.test(matrix(c(a,c,b,d), nrow= 2))$p.value,\n",
                "                oddsratio = fisher.test(matrix(c(a,c,b,d), nrow= 2))$estimate,\n",
                "                # calculate bonferroni correction\n",
                "                crit = .05/nrow(.),\n",
                "                sig_corr = ifelse(fisher_p < crit, \"p<.05\", \"n.s.\")) %>%\n",
                "  dplyr::arrange(fisher_p) %>%\n",
                "  dplyr::select(-total_ngram, -total_learner, -total_l1, -a, -b, -c, -d, -crit)\n",
                "# inspect\n",
                "head(sdif_ngram)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "In our case, there are no n-grams that differ significantly in their use by learners and L1-speakers once we have corrected for repeated testing as indicated by the *n.s.* (not significant) in the column called *sig_corr*.\n",
                "\n",
                "\n",
                "# Finding collocations\n",
                "\n",
                "There are various techniques for identifying collocations. To identify collocations without having a pre-defined target term, we can use the `textstat_collocations` function from the `quanteda.textstats` package [cf. @benoit2021package].\n",
                "\n",
                "However, before we can apply that function and start identifying collocations, we need to process the data to which we want to apply this function. In the present case, we will apply that function to the sentences in the L1 data which we extract in the code chunk below.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "ns_sen <- c(ns1_sen, ns2_sen) %>%\n",
                "  tolower()\n",
                "# inspect data\n",
                "head(ns_sen)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "From the output shown above, we also see that splitting texts did not work perfectly as it produces some unwarranted artifacts like the \"sentences\" that consist of headings (e.g., *transport 01*). Fortunately, these errors do not really matter in the case of our example.\n",
                "\n",
                "Now that we have the L1 data split into sentences, we can tokenize these sentences and apply the `textstat_collocations` function which identifies collocations.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# create a token object\n",
                "ns_tokens <- quanteda::tokens(ns_sen, remove_punct = TRUE)# %>%\n",
                "#  tokens_remove(stopwords(\"english\"))\n",
                "# extract collocations\n",
                "ns_coll <- quanteda.textstats::textstat_collocations(ns_tokens, size = 2, min_count = 20)\n",
                "# inspect data\n",
                "head(ns_coll)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The resulting table shows collocations in L1 data descending by collocation strength.\n",
                "\n",
                "## Visualizing collocation networks\n",
                "\n",
                "Network graphs are a very useful and flexible tool for visualizing relationships between elements such as words, personas, or authors. This section shows how to generate a network graph for collocations of the term *transport* using the `quanteda` package.\n",
                "\n",
                "In a first step, we generate a document-feature matrix based on the sentences in the L1 data. A document-feature matrix shows how often elements (here these elements are the words that occur in the L1 data) occur in a selection of documents (here these documents are the sentences in the L1 data).\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# create document-feature matrix\n",
                "ns_dfm <- ns_sen %>% \n",
                "  quanteda::dfm(remove = stopwords('english'), remove_punct = TRUE)\n",
                "# inspect data\n",
                "ns_dfm[1:6, 1:6]\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "As we want to generate a network graph of words that collocate with the term *organism*, we use the `calculateCoocStatistics` function to determine which words most strongly collocate with our target term (*organism*).  \n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# load function for co-occurrence calculation\n",
                "source(\"https://slcladal.github.io/rscripts/calculateCoocStatistics.R\")\n",
                "# define term\n",
                "coocTerm <- \"transport\"\n",
                "# calculate co-occurrence statistics\n",
                "coocs <- calculateCoocStatistics(coocTerm, ns_dfm, measure=\"LOGLIK\")\n",
                "# inspect results\n",
                "coocs[1:10]\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We now reduce the document-feature matrix to contain only the top 20 collocates of *transport* (plus our target word *transport*).\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "redux_dfm <- dfm_select(ns_dfm, \n",
                "                        pattern = c(names(coocs)[1:10], \"transport\"))\n",
                "# inspect data\n",
                "redux_dfm[1:6, 1:6]\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now, we can transform the document-feature matrix into a feature-co-occurrence matrix as shown below. A feature-co-occurrence matrix shows how often each element in that matrix co-occurs with every other element in that matrix.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "tag_fcm <- fcm(redux_dfm)\n",
                "# inspect data\n",
                "tag_fcm[1:6, 1:6]\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Using the feature-co-occurrence matrix, we can generate the network graph which shows the terms that collocate with the target term *transport* with the edges representing the co-occurrence frequency. To generate this network graph, we use the `textplot_network` function from the `quanteda.textplots` package.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# generate network graph\n",
                "quanteda.textplots::textplot_network(tag_fcm, \n",
                "                                     min_freq = 1, \n",
                "                                     edge_alpha = 0.3, \n",
                "                                     edge_size = 5,\n",
                "                                     edge_color = \"gray80\",\n",
                "                                     vertex_labelsize = log(rowSums(tag_fcm)*15))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Part-of-speech tagging\n",
                "\n",
                "Part-of-speech tagging is a very useful procedure for many analyses. Here, we automatically identify parts of speech (word classes) in the text which, for a well-studied language like English, is approximately 95% accurate.\n",
                "\n",
                "Here, we use the `udpipe` package to pos-tag text. We test this by pos-tagging a simple sentence to see if the function does what we want it to and to check the output format.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# generate test text\n",
                "text <- \"It is now a very wide spread opinion, that in the modern world there is no place for dreaming and imagination.\"\n",
                "# download language model (for english) \n",
                "m_eng   <- udpipe::udpipe_download_model(language = \"english-ewt\")\n",
                "m_eng <- udpipe_load_model(file = here::here(\"english-ewt-ud-2.5-191206.udpipe\"))\n",
                "# pos-tag  text\n",
                "tagged_text <- udpipe::udpipe_annotate(m_eng, x = text) %>%\n",
                "  as.data.frame() %>%\n",
                "  dplyr::select(-sentence) \n",
                "# collapse into text\n",
                "tagged_text <- paste0(tagged_text$token, \"/\", tagged_text$xpos, collapse = \" \")\n",
                "# inspect tagged text\n",
                "tagged_text\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The tags are not always transparent, and this is very much the case for the word class we will be looking at - the tag for an adjective is `/JJ`!\n",
                "\n",
                "The next step, we write a function that will clean our texts by removing tags and quotation marks as well as superfluous white spaces. In addition, we also pos-tag the texts.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "comText <- function(x,...){\n",
                "  # paste text together\n",
                "  x <- paste0(x)\n",
                "  # remove file identifiers\n",
                "  x <- stringr::str_remove_all(x, \"<.*?>\")\n",
                "  # remove quotation marks\n",
                "  x <- stringr::str_remove_all(x, fixed(\"\\\"\"))\n",
                "  # remove superfluous white spaces\n",
                "  x <- stringr::str_squish(x)\n",
                "  # remove empty elements\n",
                "  x <- x[!x==\"\"]\n",
                "  # postag text\n",
                "  x <- udpipe::udpipe_annotate(m_eng, x) %>%\n",
                "  as.data.frame() %>%\n",
                "  dplyr::select(-sentence) \n",
                "  x <- paste0(x$token, \"/\", x$xpos, collapse = \" \")\n",
                "}\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now we apply the text cleaning function to the texts.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# combine texts\n",
                "ns1_pos <- comText(ns1_sen)\n",
                "ns2_pos <- comText(ns2_sen)\n",
                "de_pos <- comText(de_sen)\n",
                "es_pos <- comText(es_sen)\n",
                "fr_pos <- comText(fr_sen)\n",
                "it_pos <- comText(it_sen)\n",
                "pl_pos <- comText(pl_sen)\n",
                "ru_pos <- comText(ru_sen)\n",
                "# inspect\n",
                "ns1_pos\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We end up with pos-tagged texts where the pos-tags are added to each word (or symbol).\n",
                "\n",
                "In the following section, we will use these pos-tags to identify potential differences between learners and L1-speakers of English.\n",
                "\n",
                "# Differences in pos-sequences\n",
                "\n",
                "To analyze differences in part-of-speech sequences between L1-speakers and learners of English,, we write a function that extracts pos-tag bigrams from the tagged texts. \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# tokenize and extract pos tags\n",
                "posngram <- function(x,...){\n",
                "  x <- x %>%\n",
                "  stringr::str_remove_all(\"\\\\w*/\") %>%\n",
                "  quanteda::tokens(remove_punct = TRUE)  %>%\n",
                "    quanteda::tokens_ngrams(n = 2) %>%\n",
                "    stringr::str_remove_all(\"-\")\n",
                "  return(x)\n",
                "}\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We now apply the function to the pos-tagged texts.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# apply pos-tag function to data\n",
                "ns1_posng <- as.vector(unlist(posngram(ns1_pos)))\n",
                "ns2_posng <- as.vector(unlist(posngram(ns2_pos)))\n",
                "de_posng <- as.vector(unlist(posngram(de_pos)))\n",
                "es_posng <- as.vector(unlist(posngram(es_pos)))\n",
                "fr_posng <- as.vector(unlist(posngram(fr_pos)))\n",
                "it_posng <- as.vector(unlist(posngram(it_pos)))\n",
                "pl_posng <- as.vector(unlist(posngram(pl_pos)))\n",
                "ru_posng <- as.vector(unlist(posngram(ru_pos)))\n",
                "# inspect\n",
                "head(ns1_posng)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "In a next step, we tabulate the results and add a column telling us about the L1 background of the speakers who have produced the texts.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "posngram_df <- c(ns1_posng, ns2_posng, de_posng, es_posng, fr_posng, \n",
                "                 it_posng, pl_posng, ru_posng) %>%\n",
                "  as.data.frame() %>%\n",
                "  # rename column\n",
                "  dplyr::rename(ngram = 1) %>%\n",
                "  # add l1\n",
                "  dplyr::mutate(l1 = c(rep(\"en\", length(ns1_posng)),\n",
                "                       rep(\"en\", length(ns2_posng)),\n",
                "                       rep(\"de\", length(de_posng)),\n",
                "                       rep(\"es\", length(es_posng)),\n",
                "                       rep(\"fr\", length(fr_posng)),\n",
                "                       rep(\"it\", length(it_posng)),\n",
                "                       rep(\"pl\", length(pl_posng)),\n",
                "                       rep(\"ru\", length(ru_posng))),\n",
                "                # add learner column\n",
                "                learner = ifelse(l1 == \"en\", \"no\", \"yes\")) %>%\n",
                "  # extract frequencies of ngrams\n",
                "  dplyr::group_by(ngram, learner) %>%\n",
                "  dplyr::summarise(freq = n()) %>%\n",
                "  dplyr::arrange(-freq)\n",
                "# inspect\n",
                "head(posngram_df)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Next, we transform the table and add all the information that we need to perform the Fisher's exact tests that we will use to determine if there are significant differences between L1 speakers and learners of English regarding their use of pos-sequences.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "posngram_df2 <- posngram_df %>%\n",
                "  tidyr::spread(learner, freq) %>%\n",
                "  dplyr::mutate(no = ifelse(is.na(no), 0, no),\n",
                "                yes = ifelse(is.na(yes), 0, yes)) %>%\n",
                "  dplyr::rename(l1speaker = no, \n",
                "                learner = yes) %>%\n",
                "  dplyr::mutate(total_ngram = l1speaker+learner) %>%\n",
                "  dplyr::ungroup() %>%\n",
                "  dplyr::mutate(total_learner = sum(learner),\n",
                "              total_l1 = sum(l1speaker)) %>%\n",
                "  dplyr::mutate(a = l1speaker,\n",
                "                b = learner) %>%\n",
                "  dplyr::mutate(c = total_l1-a,\n",
                "                d = total_learner-b)\n",
                "# inspect\n",
                "head(posngram_df2)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "On this re-arranged data set, we can now apply the Fisher's exact tests. As we are performing many different tests, we need to correct for multiple comparisons. To this end, we create a column which holds the Bonferroni corrected critical value (\\alpha .05). If a p-value is lower than the corrected critical value, then the learners and L1-speakers differ significantly in their use of that n-gram.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "sdif_posngram <- posngram_df2  %>%\n",
                "  # perform fishers exact test and extract estimate and p\n",
                "  dplyr::rowwise() %>%\n",
                "  dplyr::mutate(fisher_p = fisher.test(matrix(c(a,c,b,d), nrow= 2))$p.value,\n",
                "                oddsratio = fisher.test(matrix(c(a,c,b,d), nrow= 2))$estimate,\n",
                "                # calculate bonferroni correction\n",
                "                crit = .05/nrow(.),\n",
                "                sig_corr = ifelse(fisher_p < crit, \"p<.05\", \"n.s.\")) %>%\n",
                "  dplyr::arrange(fisher_p) %>%\n",
                "  dplyr::select(-total_ngram, -a, -b, -c, -d, -crit)\n",
                "# inspect\n",
                "head(sdif_posngram)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We can now check and compare the use of the the pos-tagged sequences that differ significantly between learners and L1 speakers of English using simple concordancing. We begin by checking the use in the L1-data.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# combine l1 data\n",
                "l1_pos <- c(ns1_pos, ns2_pos)\n",
                "# combine l2 data\n",
                "l2_pos <- c(de_pos, es_pos, fr_pos, it_pos, pl_pos, ru_pos)\n",
                "# extract PRP_VBZ\n",
                "PRP_VBZ_l1 <-quanteda::kwic(quanteda::tokens(l1_pos), \n",
                "                            pattern = phrase(\"\\\\w* / PRP \\\\w* / VBZ\"), \n",
                "                            valuetype = \"regex\",\n",
                "                            window = 10) %>%\n",
                "  as.data.frame() %>%\n",
                "  # remove superfluous columns\n",
                "  dplyr::select(-from, -to, -docname, -pattern)\n",
                "# inspect results\n",
                "head(PRP_VBZ_l1)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We now turn to the learner data and also extract concordances for the same pos-sequence.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# extract PRP_VBZ\n",
                "PRP_VBZ_l2 <-quanteda::kwic(quanteda::tokens(l2_pos), \n",
                "                            pattern = phrase(\"\\\\w* / PRP \\\\w* / VBZ\"), \n",
                "                            valuetype = \"regex\", \n",
                "                            window = 10) %>%\n",
                "  as.data.frame() %>%\n",
                "  # remove superfluous columns\n",
                "  dplyr::select(-from, -to, -docname, -pattern)\n",
                "# inspect results\n",
                "head(PRP_VBZ_l2)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Lexical diversity\n",
                "\n",
                "Another common measure used to asses the development of language learns is vocabulary size. Vocabulary size can be assessed with various measures that represent lexical diversity. In the present case, we will extract\n",
                "\n",
                "* `TTR`: *type-token ratio*\n",
                "* `C`: Herdan's C (see @tweedie1988lexdiv; sometimes referred to as LogTTR)\n",
                "* `R`: Guiraud's Root TTR (see @tweedie1988lexdiv)\n",
                "* `CTTR`: Carroll's Corrected TTR \n",
                "* `U`: Dugast's Uber Index (see @tweedie1988lexdiv)\n",
                "* `S`: Summer's index\n",
                "* `Maas`: Maas' indices\n",
                "\n",
                "The formulas showing how the lexical diversity measures are calculated as well as additional information about the lexical diversity measures can be found [here](https://quanteda.io/reference/textstat_lexdiv.html).\n",
                "\n",
                "While we will extract all of these scores, we will only visualize Carroll's Corrected TTR to keep things simple. \n",
                "\n",
                "\\begin{equation}\n",
                "  CTTR =  \\frac{N_{Types}}{\\sqrt{2 N_{Tokens}}}\n",
                "\\end{equation}\n",
                "\n",
                "However, before we extract the lexical diversity measures, we split the data into individual essays.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "cleanEss <- function(x){\n",
                "  x %>%\n",
                "  paste0(collapse = \" \") %>%\n",
                "  stringr::str_split(\"Transport [0-9]{1,2}\") %>%\n",
                "  unlist() %>%\n",
                "  stringr::str_squish() %>%\n",
                "  .[. != \"\"]\n",
                "}\n",
                "# apply function\n",
                "ns1_ess <- cleanEss(ns1)\n",
                "ns2_ess <- cleanEss(ns2)\n",
                "de_ess <- cleanEss(de)\n",
                "es_ess <- cleanEss(es)\n",
                "fr_ess <- cleanEss(fr)\n",
                "it_ess <- cleanEss(it)\n",
                "pl_ess <- cleanEss(pl)\n",
                "ru_ess <- cleanEss(ru)\n",
                "# inspect\n",
                "head(ns1_ess, 1)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "In a next step, we can apply the `lex.div` function from the `koRpus` package which calculates the different lexical diversity measures for us.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# extract lex. div. measures\n",
                "ns1_lds <- lapply(ns1_ess, function(x){\n",
                "  x <- koRpus::lex.div(x, force.lang = 'en', # define language \n",
                "                       segment = 20,      # define segment width\n",
                "                       window = 20,       # define window width\n",
                "                       quiet = T,\n",
                "                       # define lex div measures\n",
                "                       measure=c(\"TTR\", \"C\", \"R\", \"CTTR\", \"U\", \"Maas\"),\n",
                "                       char=c(\"TTR\", \"C\", \"R\", \"CTTR\",\"U\", \"Maas\"))\n",
                "})\n",
                "# inspect\n",
                "ns1_lds[1]\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We now go ahead and extract the lexical diversity scores for the other essays.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "lexDiv <- function(x){\n",
                "  lapply(x, function(y){\n",
                "    koRpus::lex.div(y, force.lang = 'en',  segment = 20, window = 20,  \n",
                "                    quiet = T, measure=c(\"TTR\", \"C\", \"R\", \"CTTR\", \"U\", \"Maas\"),\n",
                "                    char=c(\"TTR\", \"C\", \"R\", \"CTTR\",\"U\", \"Maas\"))\n",
                "  })\n",
                "}\n",
                "\n",
                "# extract lex. div. measures\n",
                "ns2_lds <- lexDiv(ns2_ess)\n",
                "de_lds <- lexDiv(de_ess)\n",
                "es_lds <- lexDiv(es_ess)\n",
                "fr_lds <- lexDiv(fr_ess)\n",
                "it_lds <- lexDiv(it_ess)\n",
                "pl_lds <- lexDiv(pl_ess)\n",
                "ru_lds <- lexDiv(ru_ess)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "In a next step, we extract the CTTR values from L1-speakers and learners and put the results into a table.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "cttr <- data.frame(c(as.vector(sapply(ns1_lds, '[', \"CTTR\")), \n",
                "                     as.vector(sapply(ns2_lds, '[', \"CTTR\")), \n",
                "                     as.vector(sapply(de_lds, '[', \"CTTR\")), \n",
                "                     as.vector(sapply(es_lds, '[', \"CTTR\")),\n",
                "                     as.vector(sapply(fr_lds, '[', \"CTTR\")), \n",
                "                     as.vector(sapply(it_lds, '[', \"CTTR\")), \n",
                "                     as.vector(sapply(pl_lds, '[', \"CTTR\")), \n",
                "                     as.vector(sapply(ru_lds, '[', \"CTTR\"))),\n",
                "          c(rep(\"en\", length(as.vector(sapply(ns1_lds, '[', \"CTTR\")))),\n",
                "            rep(\"en\", length(as.vector(sapply(ns2_lds, '[', \"CTTR\")))),\n",
                "            rep(\"de\", length(as.vector(sapply(de_lds, '[', \"CTTR\")))),\n",
                "            rep(\"es\", length(as.vector(sapply(es_lds, '[', \"CTTR\")))),\n",
                "            rep(\"fr\", length(as.vector(sapply(fr_lds, '[', \"CTTR\")))),\n",
                "            rep(\"it\", length(as.vector(sapply(it_lds, '[', \"CTTR\")))),\n",
                "            rep(\"pl\", length(as.vector(sapply(pl_lds, '[', \"CTTR\")))),\n",
                "            rep(\"ru\", length(as.vector(sapply(ru_lds, '[', \"CTTR\")))))) %>%\n",
                "  dplyr::rename(CTTR = 1,\n",
                "                l1 = 2)\n",
                "# inspect\n",
                "head(cttr)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We can now visualize the information in the table in the form of a dot plot to inspect potential differences with respect to the L1-background of speakers.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "cttr %>%\n",
                "  dplyr::group_by(l1) %>%\n",
                "  dplyr::summarise(CTTR = mean(CTTR)) %>%\n",
                "  ggplot(aes(x = reorder(l1, CTTR, mean), y = CTTR)) +\n",
                "  geom_point() +\n",
                "  # adapt y-axis labels\n",
                "  labs(y = \"Lexical diversity (CTTR)\") +\n",
                "  # adapt tick labels\n",
                "  scale_x_discrete(\"L1 of learners\", \n",
                "                   breaks = names(table(cttr$l1)), \n",
                "                   labels = c(\"en\" = \"English\",\n",
                "                              \"de\" = \"German\",\n",
                "                              \"es\" = \"Spanish\",\n",
                "                              \"fr\" = \"French\",\n",
                "                              \"it\" = \"Italian\",\n",
                "                              \"pl\" = \"Polish\",\n",
                "                              \"ru\" = \"Russian\")) +\n",
                "  theme_bw() +\n",
                "  coord_cartesian(ylim = c(0, 15)) +\n",
                "  theme(legend.position = \"none\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Readability\n",
                "\n",
                "Another measure to assess text quality or text complexity is *readability*. As with lexical diversity scores, the `textstat_readability` function from the `quanteda.textstats` package provides a multitude of different measures (see [here](https://quanteda.io/reference/textstat_readability.html) for the entire list of readability scores that can be extracted). In the following, we will focus on Flesch's Reading Ease Score exclusively [cf. @flesch1948new] (see below; ALS = average sentence length).\n",
                "\n",
                "\\begin{equation}\n",
                "  Flesch =  206.835â(1.015 ASL)â(84.6 \\frac{N_{Syllables}}{N_{Words}})\n",
                "\\end{equation}\n",
                "\n",
                "In a first step, we extract the Flesch scores by applying the `textstat_readability` to the essays.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "ns1_read <- quanteda.textstats::textstat_readability(ns1_ess)\n",
                "ns2_read <- quanteda.textstats::textstat_readability(ns2_ess)\n",
                "de_read <- quanteda.textstats::textstat_readability(de_ess)\n",
                "es_read <- quanteda.textstats::textstat_readability(es_ess)\n",
                "fr_read <- quanteda.textstats::textstat_readability(fr_ess)\n",
                "it_read <- quanteda.textstats::textstat_readability(it_ess)\n",
                "pl_read <- quanteda.textstats::textstat_readability(pl_ess)\n",
                "ru_read <- quanteda.textstats::textstat_readability(ru_ess)\n",
                "# inspect\n",
                "ns1_read\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now, we generate a table with the results and the L1 of the speaker that produced the essay.\n",
                " \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "l1 <- c(rep(\"en\", nrow(ns1_read)), rep(\"en\", nrow(ns2_read)),\n",
                "        \"de\", \"es\", \"fr\", \"it\", \"pl\", \"ru\")\n",
                "read_l1 <- base::rbind(ns1_read, ns2_read, de_read, es_read, \n",
                "                    fr_read, it_read, pl_read, ru_read)\n",
                "read_l1 <- cbind(read_l1, l1) %>%\n",
                "  as.data.frame() %>%\n",
                "  dplyr::mutate(l1 = factor(l1, level = c(\"en\", \"de\", \"es\", \"fr\", \"it\", \"pl\", \"ru\"))) %>%\n",
                "  dplyr::group_by(l1) %>%\n",
                "  dplyr::summarise(Flesch = mean(Flesch))\n",
                "# inspect\n",
                "read_l1\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "As before, we can visualize the results to check for potential differences between L1-speakers and learners of English. In this case, we use bar charts to visualize the results.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "read_l1 %>%\n",
                "  ggplot(aes(x = l1, y = Flesch, label = round(Flesch, 1))) +\n",
                "  geom_bar(stat = \"identity\") +\n",
                "  geom_text(vjust=1.6, color = \"white\")+\n",
                "  # adapt tick labels\n",
                "  scale_x_discrete(\"L1 of learners\", \n",
                "                   breaks = names(table(read_l1$l1)), \n",
                "                   labels = c(\"en\" = \"English\",\n",
                "                              \"de\" = \"German\",\n",
                "                              \"es\" = \"Spanish\",\n",
                "                              \"fr\" = \"French\",\n",
                "                              \"it\" = \"Italian\",\n",
                "                              \"pl\" = \"Polish\",\n",
                "                              \"ru\" = \"Russian\")) +\n",
                "  theme_bw() +\n",
                "  coord_cartesian(ylim = c(0, 75)) +\n",
                "  theme(legend.position = \"none\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                " \n",
                "# Spelling errors\n",
                "\n",
                "We can also determine the number of spelling errors in L1 and learner texts by checking if words in a given text occur in a dictionary or not. To do this, we can use the `hunspell` function from the `hunspell` package. We can choose between different dictionaries (use `list_dictionaries()` to see which dictionaries are available) and we can specify words to ignore via the `ignore` argument.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# list words that are not in dict\n",
                "hunspell(ns1_ess, \n",
                "         format = c(\"text\"),\n",
                "         dict = dictionary(\"en_GB\"),\n",
                "         ignore = en_stats) \n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We can check how many spelling mistakes and words are in a text as shown below.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "ns1_nerr <- hunspell(ns1_ess, dict = dictionary(\"en_GB\")) %>%\n",
                "  unlist() %>%\n",
                "  length()\n",
                "ns1_nw <- sum(tokenizers::count_words(ns1_ess))\n",
                "# inspect\n",
                "ns1_nerr; ns1_nw\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "To check if L1 speakers and learners differ regrading the likelihood of making spelling errors, we apply the `hunspell` function to all texts and also extract the number of words for each text.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ns1\n",
                "ns1_nerr <- hunspell(ns1_ess, dict = dictionary(\"en_GB\")) %>%  unlist() %>% length()\n",
                "ns1_nw <- sum(tokenizers::count_words(ns1_ess))\n",
                "# ns2\n",
                "ns2_nerr <- hunspell(ns2_ess, dict = dictionary(\"en_GB\")) %>%  unlist() %>% length()\n",
                "ns2_nw <- sum(tokenizers::count_words(ns2_ess))\n",
                "# de\n",
                "de_nerr <- hunspell(de_ess, dict = dictionary(\"en_GB\")) %>%  unlist() %>% length()\n",
                "de_nw <- sum(tokenizers::count_words(de_ess))\n",
                "# es\n",
                "es_nerr <- hunspell(es_ess, dict = dictionary(\"en_GB\")) %>%  unlist() %>% length()\n",
                "es_nw <- sum(tokenizers::count_words(es_ess))\n",
                "# fr\n",
                "fr_nerr <- hunspell(fr_ess, dict = dictionary(\"en_GB\")) %>%  unlist() %>% length()\n",
                "fr_nw <- sum(tokenizers::count_words(fr_ess))\n",
                "# it\n",
                "it_nerr <- hunspell(it_ess, dict = dictionary(\"en_GB\")) %>%  unlist() %>% length()\n",
                "it_nw <- sum(tokenizers::count_words(it_ess))\n",
                "# pl\n",
                "pl_nerr <- hunspell(pl_ess, dict = dictionary(\"en_GB\")) %>%  unlist() %>% length()\n",
                "pl_nw <- sum(tokenizers::count_words(pl_ess))\n",
                "# ru\n",
                "ru_nerr <- hunspell(ru_ess, dict = dictionary(\"en_GB\")) %>%  unlist() %>% length()\n",
                "ru_nw <- sum(tokenizers::count_words(ru_ess))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now, we generate a table from the results.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "err_tb <- c(ns1_nerr, ns2_nerr, de_nerr, es_nerr, fr_nerr, it_nerr, pl_nerr, ru_nerr) %>%\n",
                "  as.data.frame() %>%\n",
                "  # rename column\n",
                "  dplyr::rename(errors = 1) %>%\n",
                "  # add n of words\n",
                "  dplyr::mutate(words = c(ns1_nw, ns2_nw, de_nw, es_nw, fr_nw, it_nw, pl_nw, ru_nw)) %>%\n",
                "  # add l1\n",
                "  dplyr::mutate(l1 = c(\"en\", \"en\", \"de\", \"es\", \"fr\", \"it\", \"pl\", \"ru\")) %>%\n",
                "  # calculate rel freq\n",
                "  dplyr::mutate(freq = round(errors/words*1000, 1)) %>%\n",
                "  # summarise\n",
                "  dplyr::group_by(l1) %>%\n",
                "  dplyr::summarise(freq = mean(freq))\n",
                "# inspect\n",
                "head(err_tb)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We can now visualize the results.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "err_tb %>%\n",
                "  ggplot(aes(x = reorder(l1, -freq), y = freq, label = freq)) +\n",
                "  geom_bar(stat = \"identity\") +\n",
                "  geom_text(vjust=1.6, color = \"white\") +\n",
                "  # adapt tick labels\n",
                "  scale_x_discrete(\"L1 of learners\", \n",
                "                   breaks = names(table(read_l1$l1)), \n",
                "                   labels = c(\"en\" = \"English\",\n",
                "                              \"de\" = \"German\",\n",
                "                              \"es\" = \"Spanish\",\n",
                "                              \"fr\" = \"French\",\n",
                "                              \"it\" = \"Italian\",\n",
                "                              \"pl\" = \"Polish\",\n",
                "                              \"ru\" = \"Russian\")) +\n",
                "  labs(y = \"Relative frequency\\n(per 1,000 words)\") +\n",
                "  theme_bw() +\n",
                "  coord_cartesian(ylim = c(0, 40)) +\n",
                "  theme(legend.position = \"none\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "***\n",
                "\n",
                "[Back to LADAL](https://ladal.edu.au/llr.html)\n",
                "\n",
                "***\n"
            ]
        }
    ],
    "metadata": {
        "anaconda-cloud": "",
        "kernelspec": {
            "display_name": "R",
            "langauge": "R",
            "name": "ir"
        },
        "language_info": {
            "codemirror_mode": "r",
            "file_extension": ".r",
            "mimetype": "text/x-r-source",
            "name": "R",
            "pygments_lexer": "r",
            "version": "3.4.1"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}
