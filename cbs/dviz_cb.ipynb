{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "![An interactive LADAL notebook.](https://slcladal.github.io/images/uq1.jpg)\n",
                "\n",
                "***\n",
                "\n",
                "Please copy this Jupyter notebook so that you are able to edit it.\n",
                "\n",
                "Simply go to: File > Save a copy in Drive.\n",
                "\n",
                "If you want to run this notebook on your own computer, you need to do 2 things:\n",
                "\n",
                "1. Make sure that you have R and RStudio installed.\n",
                "\n",
                "2. You need to download the [bibliography file](https://slcladal.github.io/bibliography.bib) and store it in the same folder where you store the Rmd file.\n",
                "\n",
                "Once you have done that, you are good to go.\n",
                "\n",
                "***\n",
                "\n",
                "# Data Visualization with R\n",
                "\n",
                "This tutorial introduces different types of data visualization. The entire R Notebook for the tutorial can be downloaded [here](https://slcladal.github.io/dviz.Rmd).  A more in-depth and highly recommendable resource for data visualization in R is Wickham (2016). A more general introduction to data visualization - which is still highly recommendable is Healy (2018).\n",
                "\n",
                "This tutorial is aimed at beginners and intermediate users of R with the aim of showcasing how to visualize data using R. The aim is not to provide a fully-fledged analysis but rather to show and exemplify selected useful methods for data visualizations.\n",
                "\n",
                "**Graphics philosophies**\n",
                "\n",
                "Before we start, a few words on different frameworks for creating graphics in R are in order. There are three main frameworks in which to create graphics in R. The *basic* framework, the *lattice* framework, and the *ggplot* or *tidyverse* framework. These frameworks reflect the changing nature of R as a programming language (or as a programming environment). The so-called *base R* consists of about 30 packages that are always loaded automatically when you open R - it is, so to say - the default version of using R when nothing else is loaded. The *base R* framework is the oldest way to generate visualizations in R that was used when other packages did not exists yet. However, *base R* can and is still used to create visualizations although most visualizations are now generated using the *ggplot* or *tidyverse* framework. \n",
                "\n",
                "**The ggplot framework**\n",
                "\n",
                "The *ggplot* environment was written by [Hadley Wickham](http://hadley.nz/) and it implements a philosophy of graphic design described in builds on *The Grammar of Graphics* by Leland Wilkinson (Wilkinson 2012). Thus, ggplots typically start with the function call (`ggplot`) followed by the specification of the data, then the aesthetics (`aes`), and then a specification of the type of plot that is created (`geom_line` for line graphs, `geom_box` for box plots, `geom_bar` for bar graphs, `geom_text` for text, etc.). In addition, ggplot allows to specify all elements that the graph consists of (e.g. the theme and axes).\n",
                "\n",
                "**Preparation and session set up**\n",
                "\n",
                "This tutorial is based on R. If you have not installed R or are new to it, you will find an introduction to and more information how to use R [here](https://slcladal.github.io/intror.html). For this tutorials, we need to install certain *packages* from an R *library* so that the scripts shown below are executed without errors. Before turning to the code below, please install the packages by running the code below this paragraph. If you have already installed the packages mentioned below, then you can skip ahead and ignore this section. To install the necessary packages, simply run the following code - it may take some time (between 1 and 5 minutes to install all of the libraries so you do not need to worry if it takes some time).\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# install packages\n",
                "install.packages(\"lattice\")\n",
                "install.packages(\"tidyverse\")\n",
                "install.packages(\"likert\")\n",
                "install.packages(\"scales\")\n",
                "install.packages(\"vcd\")\n",
                "install.packages(\"tm\")\n",
                "install.packages(\"ggridges\")\n",
                "install.packages(\"SnowballC\")\n",
                "install.packages(\"tidyr\")\n",
                "install.packages(\"wordcloud\")\n",
                "install.packages(\"flextable\")\n",
                "install.packages(\"hexbin\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now that we have installed the packages, we activate them as shown below.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# set options\n",
                "options(stringsAsFactors = F)          # no automatic data transformation\n",
                "options(\"scipen\" = 100, \"digits\" = 12) # suppress math annotation\n",
                "options(max.print=1000)\n",
                "# activate packages\n",
                "library(lattice)\n",
                "library(tidyverse)\n",
                "library(likert)\n",
                "library(scales)\n",
                "library(vcd)\n",
                "library(tm)\n",
                "library(ggridges)\n",
                "library(SnowballC)\n",
                "library(tidyr)\n",
                "library(wordcloud)\n",
                "library(flextable)\n",
                "library(hexbin)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Once you have installed R and RStudio and once you have also initiated the session by executing the code shown above, you are good to go.\n",
                "\n",
                "# Getting started\n",
                "\n",
                "Before turning to the graphs, we load the data that we will display. The data set is called `lmmdata` but we will change the name to `pdat` for this tutorial. The data set is based on the [*Penn Parsed Corpora of Historical English*](https://www.ling.upenn.edu/hist-corpora/) (PPC) and it contains the date when a text was written (`Date`), the genre of the text (`Genre`), the name of the text (`Text`), the relative frequency of prepositions in the text (`Prepositions`), and the region in which the text was written (`Region`). We also add two more variables to the data called `GenreRedux` and `DateRedux`. `GenreRedux` collapses the existing genres into five main categories (*Conversational*, *Religious*, *Legal*, *Fiction*, and *NonFiction*) while `DateRedux` collapses the dates when the texts were composed into five main periods (1150-1499, 1500-1599, 1600-1699, 1700-1799, and 1800-1913). We also factorize non-numeric variables. \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# load data\n",
                "pdat  <- base::readRDS(url(\"https://slcladal.github.io/data/pvd.rda\", \"rb\"))\n",
                "# inspect\n",
                "head(pdat, 15)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The code chunk below allows you to upload two files from your own computer. To be able to load your own data, you need to click on the folder symbol to the left of the screen:\n",
                "\n",
                "![Colab Folder Symbol](https://slcladal.github.io/images/ColabFolder.png)\n",
                "\n",
                "Then on the upload symbol. \n",
                "\n",
                "![Colab Upload Symbol](https://slcladal.github.io/images/ColabUpload.png)\n",
                "\n",
                "Next, upload the files you want to analyze and then the respective files names in the `file` argument of the `scan` function. When you then execute the code (like to code chunk below, you will upload your own data.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "mytext1 <- scan(file = \"BGSU1001.txt\",\n",
                "            what = \"char\", \n",
                "            sep = \"\", \n",
                "            quote = \"\", \n",
                "            quiet = T, \n",
                "            skipNul = T) %>%\n",
                "            paste0(collapse = \" \")\n",
                "# inspect\n",
                "mytext1\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "To apply the code and functions below to your own data, you will need to modify the code chunks and replace the data we use here with your own data object. \n",
                "\n",
                "\n",
                "In addition, we will create a vector with colors that we will be using throughout this tutorial. This is not really necessary but it shares us from having to specify colors every time when we do not want to use the default colors that R provides. In this case, we will specify five colors but this palette could be extended. You can also check out the colors that are available in R  [here](http://www.stat.columbia.edu/~tzheng/files/Rcolor.pdf) and the palettes or sets of colors  [here](https://www.datanovia.com/en/blog/top-r-color-palettes-to-know-for-great-data-visualization/).\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "clrs5 <- c(\"indianred4\", \"gray30\", \"darkblue\", \"orange\", \"gray80\")\n",
                "clrs3 <- c(\"indianred4\", \"gray30\", \"darkblue\")\n",
                "clrs2 <- c(\"orange\", \"gray80\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We will now turn to creating the graphs.\n",
                "\n",
                "# Dot and Scatter Plots\n",
                "\n",
                "The first, and simplest graph, is a so-called *scatter* or *dot plot*. Scatter plots are used when the graph is set up to display the relationship between two numeric variables. We will start off with creating a scatter plot in *base*, then in *lattice* and finally in the *ggplot* environment.\n",
                "\n",
                "## Scatter Plots in base \n",
                "\n",
                "The most fundamental function to create plots in the *base* environment is to use the general \"plot\" function. Here, we use that function to create a simple scatter plot.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# create simple scatter plot\n",
                "plot(Prepositions ~ Date,                 # plot Prepositions by Date\n",
                "     type = \"p\",                          # plot type p (points) \n",
                "     data = pdat,                     # data from data set pdat  \n",
                "     ylab = \"Prepositions (Frequency)\",   # add y-axis label \n",
                "     xlab = \"Date (year of composition)\", # add x-axis label \n",
                "     main = \"plot type 'p' (points)\"      # add title \n",
                "     )                                    # end drawing plot\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Let us go over the command. The first part of the call is `plot` which is the function for plotting data in base R. In the round brackets are the arguments in which we specify what the plot should look like. The `Prepositions ~ Date` part tells R which variables should be displayed and the `type = \"p\"` part tells R which type of plot we want (`p` stands for points, `l` for lines, `b` for both lines and points). The part `data = pdat` tells R which data set to take the data from, and `ylab = \"Prepositions (Frequency)\"` and `xlab = \"Date (year of composition)\"` informs R about the axes' labels. The part `main = \"plot type 'p' (points)\"` informs R about what we want as the main title of the plot. \n",
                "\n",
                "In a next step, we will change the title, add two regression lines to the scatterplot (in the first case a linear and in the second case a smoothed regression line) and we will change the points as well as the color of the points.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# create simple scatter plot with regression lines (ablines)\n",
                "plot(Prepositions ~ Date,                 # plot Prepositions by Date\n",
                "     type = \"p\",                          # plot type p (points) \n",
                "     data = pdat,                     # data from data set iris  \n",
                "     ylab = \"Prepositions (Frequency)\",   # add y-axis label \n",
                "     xlab = \"Date (year of composition)\", # add x-axis label \n",
                "     main = \"Scatterplot\",                # add title \n",
                "     pch = 20,                            # use point symbol 20 (filled circles)\n",
                "     col = \"lightgrey\"                    # define symbol colour as light grey\n",
                "     )                                    # end drawing plot\n",
                "abline(                                   # add regression line (y~x) \n",
                "  lm(pdat$Prepositions ~ pdat$Date),      # draw regression line of linear model (lm) \n",
                "  col=\"red\"                               # define line colour as red\n",
                "  )                                       # end drawing line             \n",
                "lines(                                    # add line (x,y)\n",
                "  lowess(pdat$Prepositions ~ pdat$Date),  # draw smoothed lowess line (x,y) \n",
                "  col=\"blue\"                              # define line colour as blue\n",
                "  )                                       # end drawing line\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The only things that are different in the main call are the `pch` argument with has changed the points into filled dots (this is what the 20 stands for) and the `col` argument which we have specified as `lightgrey`. The regression lines are added using the `abline` and the `lines` argument.\n",
                "\n",
                "\n",
                "## Scatter Plots in lattice\n",
                "\n",
                "We now turn to data visualization in `lattice`. As the `lattice` package is already loaded, we can create a first simple scatter plot using the `xyplot` function form the `lattice` package. The scatter plot shows the relative frequency of prepositions by year of composition.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# create simple scatter plot\n",
                "xyplot(Prepositions ~ Date,  \n",
                "       # add y-axis label \n",
                "       ylab = \"Prepositions (Frequency)\",   \n",
                "       # add x-axis label\n",
                "       xlab = \"Date (year of composition)\", \n",
                "       data = pdat)                                    \n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Since the `lattice` package was created to plot multiple relationships with a single call, we will now make use of that feature and plot multiple relationships at once. In addition, we will add a grid to the plot to improve comparability of data points within the graph. Thus, the scatter plot shows the relative frequency of prepositions by year of composition and genre.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# create scatter plots by species\n",
                "xyplot(Prepositions ~ Date | Genre,     \n",
                "       # add y-axis label\n",
                "       ylab = \"Prepositions (Frequency)\", \n",
                "       # add y-axis label\n",
                "       xlab = \"Date (year of composition)\", \n",
                "       # add grids to panels\n",
                "       grid = TRUE,\n",
                "       data = pdat\n",
                "       )    \n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The only new code in the chunk above is the \"| Genre\" part. This part means that the relationship between Prepositions and Date should be displayed by Genre So, the |-symbol can be translated into \"by\". The splitting of the plot into different panels for Genre is then done automatically. \n",
                "\n",
                "Like in base, we can modify lattice-plots and specify, e.g. the symbols that are plotted or their color. \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# create scatter plots by species\n",
                "xyplot(Prepositions ~ Date | Genre,\n",
                "       ylab = \"Prepositions (Frequency)\",  \n",
                "       xlab = \"Date (year of composition)\", \n",
                "       grid = TRUE,   \n",
                "       # symbol type (20 = filled dots)\n",
                "       pch = 20,            \n",
                "       # color of symbols\n",
                "       col = \"black\",\n",
                "       data = pdat\n",
                "       ) \n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Next, we will use the `ggplot2` package to create a scatter plot. \n",
                "\n",
                "## Scatter Plots in ggplot2\n",
                "\n",
                "We now turn to data visualization using *ggplot*. As the *ggplot2* package is already loaded, we create a very basic scatterplot in *ggplot2* using the `geom_point` function to show the advantages of creating visualizations in this environment.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# create simple scatter plot\n",
                "# use data set \"pdat\"\n",
                "ggplot(pdat,  \n",
                "       # define axes\n",
                "       aes(x= Date,        \n",
                "           y= Prepositions)) + \n",
                "  # define plot type\n",
                "  geom_point()                  \n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Let's go over the code above. The function call for plotting in \"ggplot2\" is simply \"ggplot\". This function takes the data set as its first argument and then requires aesthetics. The aesthetics are defined within the \"ggplot\" function as the arguments of \"aes\". The \"aes\" function takes the axes as the arguments (in the current case). Then, we need to define the type of plot that we want. As we want a scatter plot with points, we add the \"geom_point()\" function without any arguments (as we do not want to specify the size, colour, and shape of the points just yet).\n",
                "\n",
                "The advantage of \"ggplot2\" is that is really easy to modify the plot by adding new layers and to change the basic outlook by modifying the theme which is what we will do in the code below.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "ggplot(pdat,    \n",
                "       # define axes\n",
                "       aes(x=Date,             \n",
                "           y= Prepositions, \n",
                "           # define to color by Species\n",
                "           color = GenreRedux)) + \n",
                "  # define plot type\n",
                "  geom_point() +   \n",
                "  # define theme  as black and white (bw)\n",
                "  theme_bw()                   \n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The white background is created by specifying the theme as a black and white theme (`theme_bw()`) while the color of the dots is changed by specifying that the color should be applied by Species (`color = GenreRedux`). Then, the colors to be used are defined in the function `scale_color_manual`. \n",
                "\n",
                "We can now specify the symbols in the scatter plot.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# create scatter plot colored by genre\n",
                "ggplot(pdat, aes(Date, Prepositions, color = GenreRedux, shape = GenreRedux)) +\n",
                "  geom_point() +\n",
                "  guides(shape=guide_legend(override.aes=list(fill=NA))) +\n",
                "  scale_shape_manual(name = \"Genre\", \n",
                "                     breaks = names(table(pdat$GenreRedux)), \n",
                "                     values = 1:5) +\n",
                "  scale_color_manual(name = \"Genre\", \n",
                "                     breaks = names(table(pdat$GenreRedux)), \n",
                "                     values = clrs5) +\n",
                "  theme_bw() +\n",
                "  theme(legend.position=\"top\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Extensions of dot plots\n",
                "\n",
                "In addition, we can add regression lines with error bars by Species and, if we want to show separate windows for the plots, we can use the \"facet_grid\" or \"facet_wrap\" function and define by which variable we want to create different panels.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# create scatter plot colored by genre in different panels\n",
                "ggplot(pdat, aes(Date, Prepositions,  color = Genre)) +\n",
                "  facet_wrap(vars(Genre), ncol = 4) +\n",
                "  geom_point() + \n",
                "  geom_smooth(method = \"lm\", se = F) +\n",
                "  theme_bw() +\n",
                "  theme(legend.title = element_blank(), \n",
                "        axis.text.x = element_text(size=8, angle=90))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "If we only want to show the lines, we simply drop the \"geom_point\" function.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# create scatter plot colored by genre in different panels\n",
                "ggplot(pdat, aes(x=Date, y= Prepositions,  color = Genre)) +\n",
                "  facet_wrap(vars(Genre), ncol = 4) +\n",
                "  geom_smooth(method = \"lm\", se = F) +\n",
                "  theme_bw() +\n",
                "  theme(legend.title = element_blank(), \n",
                "        axis.text.x = element_text(size=8, angle=90))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Another option is to plot density layers instead of plotting the data points.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# create scatter density plot\n",
                "ggplot(pdat, aes(x=Date, y= Prepositions,  color = GenreRedux)) +\n",
                "    facet_wrap(vars(GenreRedux), ncol = 5) +\n",
                "  theme_bw() +                  \n",
                "  geom_density_2d() +\n",
                "  theme(legend.position = \"top\",\n",
                "        legend.title = element_blank(), \n",
                "        axis.text.x = element_text(size=8, angle=90))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Although these are not scatterplots, plots with dot-symbols are very flexible and can be extended to show properties of the distribution of values. One way to create such a plot is to plot means as dot-symbols and add error bars to provide information about the underlying distribution. The plot below illustrates such a plot and additionally shows how plots can be further customized.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# scatter plot with error bars\n",
                "ggplot(pdat, aes(x=reorder(Genre, Prepositions, mean), y= Prepositions,  group = Genre)) +                 \n",
                "  stat_summary(fun = mean, geom = \"point\", aes(group= Genre)) +          \n",
                "  stat_summary(fun.data = mean_cl_boot,       \n",
                "               # add error bars\n",
                "               geom = \"errorbar\", width = 0.2) + \n",
                "  # def. y-axis range\n",
                "  coord_cartesian(ylim = c(100, 200)) +              \n",
                "  # def. font size\n",
                "  theme_bw(base_size = 15) +         \n",
                "  # def. x- and y-axis\n",
                "  theme(axis.text.x = element_text(size=10, angle = 90),  \n",
                "        axis.text.y = element_text(size=10, face=\"plain\")) + \n",
                "  # def. axes labels\n",
                "  labs(x = \"Genre\", y = \"Prepositions (Frequency)\") +     \n",
                "  # def. to col.\n",
                "  scale_color_manual(guide = FALSE)          \n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Balloon plots are an extension of scatter plots that are typically used to display data that represents\n",
                "\n",
                "* two categorical variables\n",
                "\n",
                "* one numeric variable.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ballon plot\n",
                "pdat %>%\n",
                "  dplyr::mutate(DateRedux = factor(DateRedux)) %>%\n",
                "  dplyr::group_by(DateRedux, GenreRedux) %>%\n",
                "  dplyr::summarise(Prepositions = mean(Prepositions)) %>%\n",
                "  ggplot(aes(DateRedux, 100, \n",
                "             size = Prepositions,\n",
                "             fill = GenreRedux)) +\n",
                "  facet_grid(vars(GenreRedux)) +\n",
                "  geom_point(shape = 21) +\n",
                "  scale_size_area(max_size = 15) +\n",
                "  coord_cartesian(ylim = c(50, 150)) +\n",
                "  theme_bw() +\n",
                "  theme(axis.title.y=element_blank(),\n",
                "        axis.text.y=element_blank(),\n",
                "        axis.ticks.y=element_blank()) +\n",
                "  scale_fill_discrete(guide = \"none\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Density Plots\n",
                "\n",
                "Another way to visualize the distribution of the data with respect to numeric variables are density plots or Kernel Density Plots. Density plots smooth the data using so-called kernel smoothing to even out the distribution of frequencies along the lines of a numeric or interval variable. The peaks of density plots help display where values are concentrated over the interval. To show the relationship between the variable and the density plot, we will first create a scatter plot and then create a density plot of the variable displayed on the x-axis of the scatter plot.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# create dot plot\n",
                "ggplot(pdat, aes(x = Date, y = Prepositions, color=Region)) +  \n",
                "  geom_point() +  \n",
                "  scale_color_manual(values = clrs2) + \n",
                "  theme(legend.position=c(0,1), legend.justification=c(0,1)) \n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We will now create a marginal density plot of Date (x-axis) to show when texts from the north and south were particularly common.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# create dot plot\n",
                "ggplot(pdat, aes(Date, fill=Region)) +  \n",
                "  geom_density(alpha=.5) +  \n",
                "  scale_fill_manual(values = clrs2) + \n",
                "  theme(legend.position=c(0,1), legend.justification=c(0,1)) \n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The density plot shows that the texts differ substantially with respect to where they were written as the distribution of texts written in southern Britain continues way into the 19^th^ century while we only have texts written in north until about 1800. \n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# create dot plot\n",
                "ggplot(pdat, aes(Date, Prepositions)) +  \n",
                "  geom_density2d_filled()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "An altrenative method for displaying densities is by using a hex plot as shown below. Hex plots divide the plotting area into hexagons and display density as hue of the hexagons.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# create dot plot\n",
                "pdat %>%\n",
                "  ggplot(aes(x = Date, y = Prepositions)) +  \n",
                "  geom_hex()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We are now in a position to start creating line graphs with *ggplot*.\n",
                "\n",
                "# Line Graphs\n",
                "\n",
                "Line graphs are used when we have numeric values that are linked (in one way or another) because they come from the same speaker or genre as in our case). \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "pdat %>%\n",
                "  dplyr::group_by(DateRedux, GenreRedux) %>%\n",
                "  dplyr::summarise(Frequency = mean(Prepositions)) %>%\n",
                "  ggplot(aes(x=DateRedux, y= Frequency, group= GenreRedux, color = GenreRedux)) +\n",
                "  # add geom layer with lines\n",
                "  geom_line()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Smoothed line graphs\n",
                "\n",
                "Another very useful function when creating line graphs with \"ggplot\" is \"geom_smooth\" which *smoothes* the lines to be drawn. \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "ggplot(pdat, aes(x=DateRedux, y= Prepositions, group= GenreRedux, color = GenreRedux)) +\n",
                "  # add geom layer with lines\n",
                "  geom_smooth()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "As this smoothed line graph is extremely useful, we will customize it to show how to modify your graph.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# define aesthetics\n",
                "ggplot(pdat, aes(x=Date, y= Prepositions,  color = GenreRedux, linetype = GenreRedux)) +\n",
                "  # add geom layer with lines\n",
                "  geom_smooth(se = F) +  \n",
                "  # legend without background color\n",
                "  guides(color=guide_legend(override.aes=list(fill=NA))) +  \n",
                "  # def. legend position\n",
                "  theme(legend.position=\"top\") +  \n",
                "  # def. linetype\n",
                "  scale_linetype_manual(values=c(\"twodash\", \"dashed\", \"dotdash\", \"dotted\", \"solid\"), \n",
                "                        # def. legend header\n",
                "                        name=c(\"Genre\"),\n",
                "                        # def. linetypes\n",
                "                        breaks = names(table(pdat$GenreRedux)),\n",
                "                        # def. labels\n",
                "                        labels = names(table(pdat$GenreRedux))) + \n",
                "  # def. col.\n",
                "  scale_colour_manual(values=clrs5,\n",
                "                      # define legend header\n",
                "                      name=c(\"Genre\"),\n",
                "                      # define elements\n",
                "                      breaks=names(table(pdat$GenreRedux)),  \n",
                "                      # define labels\n",
                "                      labels = names(table(pdat$GenreRedux))) +\n",
                "  # add x-axis label\n",
                "  labs(x = \"Year\") +      \n",
                "  # customize x-axis tick positions\n",
                "  scale_x_continuous(breaks=seq(1100, 1900, 100), \n",
                "                     # add labels to x-axis tick pos.\n",
                "                     labels=seq(1100, 1900, 100)) +\n",
                "  # add y-axis label\n",
                "  scale_y_continuous(name=\"Relative frequency \\n(per 1,000 words)\",  \n",
                "                     # customize tick y-axis\n",
                "                     limits=c(100, 200)) + \n",
                "  # define theme  as black and white\n",
                "  theme_bw(base_size = 10)  \n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Although the code for the customized smoothed line graph is much longer and requires addition specifications, it is a very nice way to portrait the development over time.\n",
                "\n",
                "## Ribbon plots\n",
                "\n",
                "Ribbon plots show an area, typically between minimum and maximum values. In addition, ribbon plots commonly also show the mean as depicted below.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# create dot plot\n",
                "pdat %>%\n",
                "  dplyr::mutate(DateRedux = as.numeric(DateRedux)) %>%\n",
                "  dplyr::group_by(DateRedux) %>%\n",
                "  dplyr::summarise(Mean = mean(Prepositions),\n",
                "                   Min = min(Prepositions),\n",
                "                   Max = max(Prepositions)) %>%\n",
                "  ggplot(aes(x = DateRedux, y = Mean)) +  \n",
                "  geom_ribbon(aes(ymin = Min, ymax = Max), fill = \"gray80\") +\n",
                "  geom_line() +\n",
                "  scale_x_continuous(labels = names(table(pdat$DateRedux)))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Line graphs for Likert data\n",
                "\n",
                "A special case of line graphs is used when dealing with Likert-scaled variables. In such cases, the line graph displays the density of cumulative frequencies of responses. The difference between the cumulative frequencies of responses displays differences in preferences. We will only focus on how to create such graphs using the \"ggplot\" environment here as it has an inbuild function (\"ecdf\") which is designed to handle such data.\n",
                "\n",
                "In a first step, we create a data set which consists of a Likert-scaled variable. The fictitious data created here consists of rating of students from three courses about how satisfied they were with their language-learning course. The response to the Likert item is numeric so that \"strongly disagree/very dissatisfied\" would get the lowest and \"strongly agree/very satisfied\" the highest numeric value. \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "ldat <- base::readRDS(url(\"https://slcladal.github.io/data/lid.rda\", \"rb\"))\n",
                "# inspect\n",
                "head(ldat, 15)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now that we have data resembling a Likert-scaled item from a questionnaire, we will display the data in a cumulative line graph.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# create cumulative density plot\n",
                "ggplot(ldat,aes(x = Satisfaction, color = Course)) + \n",
                "  geom_step(aes(y = ..y..), stat = \"ecdf\") +\n",
                "  labs(y = \"Cumulative Density\") + \n",
                "  scale_x_discrete(limits = 1:5, breaks = 1:5,\n",
                "        labels=c(\"very dissatisfied\", \"dissatisfied\", \"neutral\", \"satisfied\", \"very satisfied\")) + \n",
                "  scale_colour_manual(values = clrs3)  \n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The satisfaction of the German course was the lowest as the red line shows the highest density (frequency of responses) of \"very dissatisfied\" and \"dissatisfied\" ratings. The students in our fictitious data set were most satisfied with the Chinese course as the blue line is the lowest for \"very dissatisfied\" and \"dissatisfied\" ratings while the difference between the courses shrinks for \"satisfied\" and \"very satisfied\". The Japanese language course is in-between the German and the Chinese course.  \n",
                "\n",
                "# Pie charts\n",
                "\n",
                "Most commonly, the data for visualization comes from tables of absolute frequencies associated with a categorical or nominal variable. The default way to visualize such frequency tables are pie charts and bar plots. \n",
                "\n",
                "In a first step, we modify the original data to get counts and percentages. The data represents the number of documents per time period and the percentage of those documents across all time periods.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# create bar plot data\n",
                "bdat <- pdat %>%\n",
                "  dplyr::mutate(DateRedux = factor(DateRedux)) %>%\n",
                "  group_by(DateRedux) %>%\n",
                "  dplyr::summarise(Frequency = n()) %>%\n",
                "  dplyr::mutate(Percent = round(Frequency/sum(Frequency)*100, 1))\n",
                "# inspect\n",
                "head(bdat, 15)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Before creating bar plots, we will briefly turn to pie charts because pie charts are very common despite suffering from certain shortcomings. Consider the following example which highlights some of the issues that arise when using pie charts. In base R, we cerate pie charts using the `pie` function as shown below.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "pie(bdat$Percent,\n",
                "    col = clrs5,\n",
                "    labels = bdat$Percent)\n",
                "legend(\"topright\", names(table(bdat$DateRedux)), \n",
                "       fill = clrs5)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "In ggplot, we create pie charts by using the `geom_bar` and then define `coord_polar(\"y\", start=0). In contrast to base R, the labeling is not as easy as in base R. We will thus start with a pie chart without labels and then add the labels in a next step.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "ggplot(bdat,  aes(\"\", Percent, fill = DateRedux)) + \n",
                "  geom_bar(stat=\"identity\", width=1, color = \"white\") +\n",
                "  coord_polar(\"y\", start=0) +\n",
                "  scale_fill_manual(values = clrs5) +\n",
                "  theme_void()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "If the slices of the pie chart are not labelled, it is difficult to see which slices are smaller or bigger compared to other slices. This problem can easily be avoided when using a bar plot instead.\n",
                "\n",
                "The labelling of pie charts is, however, somewhat tedious as the positioning is tricky. Below is an example for adding labels without specification.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# create pie chart\n",
                "ggplot(bdat,  aes(\"\", Percent, fill = DateRedux)) + \n",
                "  geom_bar(stat=\"identity\", width=1, color = \"white\") +\n",
                "  coord_polar(\"y\", start=0) +\n",
                "  scale_fill_manual(values = clrs5) +\n",
                "  theme_void() +\n",
                "  geom_text(aes(y = Percent, label = Percent), color = \"white\", size=6)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "To place the labels where they make sense, we will add another variable to the data called \"Position\".\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "piedata <- bdat %>%\n",
                "  dplyr::arrange(desc(DateRedux)) %>%\n",
                "  dplyr::mutate(Position = cumsum(Percent)- 0.5*Percent)\n",
                "# inspect\n",
                "head(piedata, 15)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now that we have specified the position, we can include it into the pie chart.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# create pie chart\n",
                "ggplot(piedata,  aes(\"\", Percent, fill = DateRedux)) + \n",
                "  geom_bar(stat=\"identity\", width=1, color = \"white\") +\n",
                "  coord_polar(\"y\", start=0) +\n",
                "  scale_fill_manual(values = clrs5) +\n",
                "  theme_void() +\n",
                "  geom_text(aes(y = Position, label = Percent), color = \"white\", size=6)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Histograms\n",
                "\n",
                "Histograms summarize numeric variables by showing their distribution across bins. \n",
                "\n",
                "## Histograms in base R\n",
                "\n",
                "To create histograms in base R, we simply use the `hist` function and add the variable that we want to summarize as its argument.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "hist(pdat$Prepositions,\n",
                "     xlab = \"Prepositions (per 1,000 words)\",\n",
                "     main = \"\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Histograms in ggplot\n",
                "\n",
                "Using `ggplot`, we specify the variable we want to summarize in the aesthetics and use the `geom_histogram` function to generate a histogram.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "ggplot(pdat, aes(Prepositions)) +\n",
                "  geom_histogram()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We can simply add information about a second variable by specifying this variable as the basis for the coloring of the bars (which we do by specify the `fill` argument). \n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "ggplot(pdat, aes(Prepositions, fill = Region)) +\n",
                "  geom_histogram()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Bar plots\n",
                "\n",
                "Like pie charts, bar plot display frequency information across categorical variable levels. \n",
                "\n",
                "## Bar plots in base R\n",
                "\n",
                "In base R, we use the `barplot` function to create barplots. The `barplot` function is very flexible but takes a table with frequency counts as its main argument. We can also specify axes labels, a title, the color of the bras, and the axes limits.  We specify text, grids, and boxes separately after the `barplot` function call.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# create simple scatter plot\n",
                "barplot(table(pdat$DateRedux),        # plot Texts by DateRedux\n",
                "     ylab = \"Texts (Frequency)\",          # add y-axis label \n",
                "     xlab = \"Period of composition\",      # add x-axis label \n",
                "     main = \"bar plot in base R\",         # add title\n",
                "     col = clrs5,                         # add colors\n",
                "     ylim = c(0, 250)                     # define y-axis limits\n",
                "     )                                    # end drawing plot\n",
                "grid()                                    # add grid\n",
                "text(seq(0.7, 5.5, 1.2),                  # add label positions (x-axis)\n",
                "     table(pdat$DateRedux)+10,        # add label positions (y-axis)\n",
                "     table(pdat$DateRedux))           # add labels\n",
                "box()                                     # add box\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "To create grouped bar plots, we tabulate the variables that we are interested in. In the this example, we group by Region as shown below. \n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# create simple scatter plot\n",
                "barplot(table(pdat$DateRedux, pdat$Region), # plot Texts by DateRedux\n",
                "        beside = T,                          # bars beside each other\n",
                "        ylab = \"Texts (Frequency)\",          # add y-axis label \n",
                "        xlab = \"Period of composition\",      # add x-axis label\n",
                "        main = \"grouped bar plot in base R\", # add title\n",
                "        col = clrs5,                         # add colors\n",
                "        ylim = c(0, 250)                     # define y-axis limits\n",
                "        )                                    # end drawing plot\n",
                "grid()                                       # add grid\n",
                "text(c(seq(1.5, 5.5, 1.0), seq(7.5, 11.5, 1.0)),    # add label positions (x-axis)\n",
                "     table(pdat$DateRedux, pdat$Region)+10, # add label positions (y-axis)\n",
                "     table(pdat$DateRedux, pdat$Region))    # add labels\n",
                "legend(\"topleft\", names(table(pdat$DateRedux)), # add legend\n",
                "       fill = clrs5)                        # add colors\n",
                "box()                                       # add box\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "To transpose the plot, i.e. showing the Frequencies on the x- rather than the y-axis, we set the argument `horiz` to `TRUE`.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# create simple scatter plot\n",
                "barplot(table(pdat$DateRedux),        # plot Texts by DateRedux\n",
                "     ylab = \"Texts (Frequency)\",          # add y-axis label \n",
                "     xlab = \"Period of composition\",      # add x-axis label\n",
                "     col = clrs5,                         # add colors\n",
                "     horiz = T,                           # horizontal bars\n",
                "     xlim = c(0, 250),                    # define x-axis limits\n",
                "     las = 2,                             # change direction of axis labels\n",
                "     cex.names = .5)                      # reduce font of axis labels\n",
                "box()                                     # add box\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Bar plots in ggplot\n",
                "\n",
                "The creation of barplots in ggplot works just like other types of visualizations in this framework. We first define the data and the aesthetics and then use the `geom_bar` to create a barplot.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# bar plot\n",
                "ggplot(bdat, aes(DateRedux, Percent, fill = DateRedux)) +\n",
                "  geom_bar(stat=\"identity\") +          # determine type of plot\n",
                "  theme_bw() +                         # use black & white theme\n",
                "  # add and define text\n",
                "  geom_text(aes(y = Percent-5, label = Percent), color = \"white\", size=3) + \n",
                "  # add colors\n",
                "  scale_fill_manual(values = clrs5) +\n",
                "  # supress legend\n",
                "  theme(legend.position=\"none\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Compared with the pie chart, it is much easier to grasp the relative size and order of the percentage values which shows that pie charts are unfit to show relationships between elements in a graph and, as a general rule of thumb, should be avoided.\n",
                "\n",
                "Bar plot can be grouped to add another layer of information which is particularly useful when dealing with frequency counts across multiple categorical variables. To create grouped bar plots, we plot `Region` while including `DateRedux` as the `fill` argument. Also, we use the command `position=position_dodge()`.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# bar plot\n",
                "ggplot(pdat, aes(Region, fill = DateRedux)) + \n",
                "  geom_bar(position = position_dodge(), stat = \"count\") +  \n",
                "  theme_bw() +\n",
                "  scale_fill_manual(values = clrs5)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "If we leave out the `position=position_dodge()` argument, we get a stacked bar plot as shown below.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# bar plot\n",
                "ggplot(pdat, aes(DateRedux, fill = GenreRedux)) + \n",
                "  geom_bar(stat=\"count\") +  \n",
                "  theme_bw() +\n",
                "  scale_fill_manual(values = clrs5)    \n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "One issue to consider when using stacked bar plots is the number of variable levels: when dealing with many variable levels, stacked bar plots tend to become rather confusing. This can be solved by either collapsing infrequent variable levels or choose a colour palette that reflects some other inherent piece of information such as *formality* (e.g. blue) versus *informality* (e.g. red).\n",
                "\n",
                "Stacked bar plots can also be normalized so that changes in percentages become visible. This is done by exchanging `position=position_dodge()` with `position=\"fill\"`. \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# bar plot\n",
                "ggplot(pdat, aes(DateRedux, fill = GenreRedux)) + \n",
                "  geom_bar(stat=\"count\", position=\"fill\") +  \n",
                "  theme_bw() +\n",
                "  scale_fill_manual(values = clrs5) +\n",
                "  labs(y = \"Probability\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Bar plots for Likert data\n",
                "\n",
                "Bar plots are particularly useful when visualizing data obtained through Likert items. As this is a very common issue that empirical researchers face. There are two basic ways to display Likert items using bar plots: grouped bar plots and more elaborate scaled bar plots.\n",
                "\n",
                "Although we have seen above how to create grouped bar plots, we will repeat it here with the language course example used above when we used cumulative density line graphs to visualise how to display Likert data.  \n",
                "\n",
                "In a first step, we recreate the data set which we have used above. The data set consists of a Likert-scaled variable (Satisfaction) which represents rating of students from three courses about how satisfied they were with their language-learning course. The response to the Likert item is numeric so that \"strongly disagree/very dissatisfied\" would get the lowest and \"strongly agree/very satisfied\" the highest numeric value. \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# create likert data\n",
                "nlik <- ldat %>%\n",
                "  dplyr::group_by(Course, Satisfaction) %>%\n",
                "  dplyr::summarize(Frequency = n())\n",
                "# inspect data\n",
                "head(nlik)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now that we have data resembling a Likert-scaled item from a questionnaire, we will display the data in a cumulative line graph.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# create grouped bar plot\n",
                "ggplot(nlik, aes(Satisfaction, Frequency,  fill = Course)) +\n",
                "  geom_bar(stat=\"identity\", position=position_dodge()) +\n",
                "  # define colors\n",
                "  scale_fill_manual(values=clrs5) + \n",
                "  # add text and define colour\n",
                "  geom_text(aes(label=Frequency), vjust=1.6, color=\"white\", \n",
                "            # define text position and size\n",
                "            position = position_dodge(0.9),  size=3.5) +     \n",
                "    scale_x_discrete(limits=c(\"1\",\"2\",\"3\",\"4\",\"5\"), breaks=c(1,2,3,4,5),\n",
                "        labels=c(\"very dissatisfied\", \"dissatisfied\",  \"neutral\", \"satisfied\", \n",
                "                 \"very satisfied\")) + \n",
                "  theme_bw()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Another and very interesting way to display such data is by using the Likert package. In a first step, we need to activate the package, clean the data, and extract a subset for the data visualization example.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "sdat <- base::readRDS(url(\"https://slcladal.github.io/data/sdd.rda\", \"rb\"))\n",
                "# inspect\n",
                "head(sdat, 15)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "As you can see, we need to clean and adapt the column names. To do this, we will \n",
                "\n",
                "* add an identifier which shows which question we are dealing with (e.g. Q 1: question text)\n",
                "* remove the dots between words with spaces\n",
                "* add a question mark at the end of questions\n",
                "* remove superfluous white speaces\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# clean column names\n",
                "colnames(sdat)[3:ncol(sdat)] <- paste0(\"Q \", str_pad(1:10, 2, \"left\", \"0\"), \": \", colnames(sdat)[3:ncol(sdat)]) %>%\n",
                "  stringr::str_replace_all(\"\\\\.\", \" \") %>%\n",
                "  stringr::str_squish() %>%\n",
                "  stringr::str_replace_all(\"$\", \"?\")\n",
                "# inspect column names\n",
                "colnames(sdat)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now, that we have nice column names, we will replace the numeric values (1 to 5) with labels ranging from *disagree* to *agree* and convert our data into a data frame.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "lbs <- c(\"disagree\", \"somewhat disagree\", \"neither agree nor disagree\",  \"somewhat agree\", \"agree\")\n",
                "survey <- sdat %>%\n",
                "  dplyr::mutate_if(is.character, factor) %>%\n",
                "  dplyr::mutate_if(is.numeric, factor, levels = 1:5, labels = lbs) %>%\n",
                "  drop_na() %>%\n",
                "  as.data.frame()\n",
                "# inspect\n",
                "head(survey, 15)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now, we can use the `plot` and the `likert` function to visualize the survey data.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# load package\n",
                "library(likert)\n",
                "# generate plot\n",
                "plot(likert(survey[,3:12]), ordered = F, wrap= 60)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "To save this plot, you can use the `save_plot` function from the `cowplot` package as shown below.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "survey_p1 <- plot(likert(survey[,3:12]), ordered = F, wrap= 60)\n",
                "# save plot\n",
                "cowplot::save_plot(here(\"images\", \"stu_p1.png\"), # where to save the plot\n",
                "                   survey_p1,        # object to plot\n",
                "                   base_asp = 1.5,  # ratio of space fro questions vs space for plot\n",
                "                   base_height = 8) # size! higher for smaller font size\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "An additional and very helpful feature is that the `likert` package enables grouping the data as shown below. The display columns 3 to 8 and use column 1 for grouping.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# create plot\n",
                "plot(likert(survey[,3:8], grouping = survey[,1]))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Comparative bar plots with negative values\n",
                "\n",
                "Another frequent task is to evaluate the divergence of values from a reference, for instance when dealing with language learners where native speakers serve as a reference or target. To illustrate how such data can be visualized, we load the scales package as we want to create a bar plot in which we show the divergence of learners from native speakers regarding certain features and how that divergence changes over time. Then, we create an example data set which mirrors the format we expect for the actual data. \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# create a vector with values called Test1\n",
                "Test1 <- c(11.2, 13.5, 200, 185, 1.3, 3.5) \n",
                "# create a vector with values called Test2\n",
                "Test2 <- c(12.2, 14.7, 210, 175, 1.9, 3.0)   \n",
                "# create a vector with values called Test3\n",
                "Test3 <- c(13.2, 15.1, 177, 173, 2.4, 2.9)    \n",
                "# combine vectors in a data frame\n",
                "testdata <- data.frame(Test1, Test2, Test3)     \n",
                "# add rownames\n",
                "rownames(testdata) <- c(\"Feature1_Student\",     \n",
                "                        \"Feature1_Reference\", \n",
                "                        \"Feature2_Student\", \n",
                "                        \"Feature2_Reference\", \n",
                "                        \"Feature3_Student\", \n",
                "                        \"Feature3_Reference\")\n",
                "# inspect data\n",
                "testdata                                        \n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We can now determine how the learners deviate from the native speakers.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# determine divergence from reference\n",
                "# row 1 (student) minus row 2 (reference)\n",
                "FeatureA <- t(testdata[1,] - testdata[2,]) \n",
                "# row 3 (student) minus row 4 (reference)\n",
                "FeatureB <- t(testdata[3,] - testdata[4,])  \n",
                "# row 5 (student) minus row 6 (reference)\n",
                "FeatureC <- t(testdata[5,] - testdata[6,])  \n",
                "# create data frame\n",
                "plottable <- data.frame(rep(rownames(FeatureA), 3), \n",
                "                  c(FeatureA, FeatureB, FeatureC), \n",
                "                  c(rep(\"FeatureA\", 3), \n",
                "                    rep(\"FeatureB\", 3), \n",
                "                    rep(\"FeatureC\", 3)))\n",
                "# def. col. names\n",
                "colnames(plottable) <- c(\"Test\", \"Value\", \"Feature\")\n",
                "# inspect data\n",
                "plottable                                         \n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Finally, we graphically display the divergence using a bar plot.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# create plot\n",
                "ggplot(plottable, \n",
                "       aes(Test, Value)) + # def. x/y-axes\n",
                "  # separate plots for each feature\n",
                "  facet_grid(vars(Feature), scales = \"free_y\") +\n",
                "  # create bars\n",
                "  geom_bar(stat = \"identity\", aes(fill = Test)) +  \n",
                "  # black and white theme\n",
                "  theme_bw() +\n",
                "  # supress legend   \n",
                "  guides(fill=FALSE) + \n",
                "  # def. colours   \n",
                "  geom_bar(stat=\"identity\", fill=rep(clrs5[1:3], 3)) + \n",
                "  # axes titles\n",
                "  labs(x = \"\", y = \"Score\")                                               \n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Ridge Plots\n",
                "\n",
                "A very nice option to display frequency information about levels of a categorical variable are ridge plots. To generate ridge plots, we can use the `ggridges` package written by [Claus Wilke](https://github.com/clauswilke).\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# load package\n",
                "library(ggridges)\n",
                "# create ridge plot\n",
                "pdat %>%\n",
                "  ggplot(aes(x = Prepositions, y = GenreRedux, fill = GenreRedux)) +\n",
                "  geom_density_ridges() +\n",
                "  theme_ridges() + \n",
                "  theme(legend.position = \"none\") + \n",
                "  labs(y = \"\", x = \"Density of the relative frequency of prepostions\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "You can easily replace the density displays by histograms which only requires to define the `stat` argument and the bin width.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# create ridge plot\n",
                "pdat %>%\n",
                "  ggplot(aes(x = Prepositions, y = GenreRedux, fill = GenreRedux)) +\n",
                "  geom_density_ridges(alpha=0.6, stat=\"binline\", bins=20) +\n",
                "  theme_ridges() + \n",
                "  theme(legend.position = \"none\") + \n",
                "  labs(y = \"\", x = \"Histograms of the relative frequency of prepostions\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Boxplots\n",
                "\n",
                "So far, we have plotted values but we have not plotted the underlying distributions. For instance, we have plotted mean values but not the variance within the distribution. One handy way to combine plotting general trends and their underlying distributions are boxplots.\n",
                "\n",
                "Boxplots, or Box-and-Whisker Plots, are exploratory graphics first created by John W. Tukey and they show the relationships between categorical and numeric variables. They are very useful because they not only provide measures of central tendency (the median which is the line in the middle of the box) but they also offer information about the distribution of the data. To elaborate, fifty percent of data points fall within the box while seventy-five percent of data points fall within the whiskers (the lines which look like extended error bars): the box thus encompasses the interquartile range between the first and third quartile. The whiskers show the minimum and maximum values in the data and only outliers (data points that lie 1.5 times the interquartile range or more above the third quartile or 1.5 times the interquartile range or more below the first quartile. If the whiskers differ in length, then this means that the data is asymmetrically distributed.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# create boxplot\n",
                "ggplot(pdat, aes(DateRedux, Prepositions, color = GenreRedux)) +                 \n",
                "  geom_boxplot(fill=clrs5, \n",
                "               color=\"black\") \n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Another interesting feature of boxplots is that they allow us to visually get an idea whether categories differ significantly. Because if add \"notch = T\" and the notches of the boxplots do not overlap, then this is a very strong indication that the categories actually differ significantly (see below). \n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# create boxplot\n",
                "ggplot(pdat, aes(DateRedux, Prepositions, color = GenreRedux)) +                 \n",
                "  geom_boxplot(outlier.colour=\"red\", \n",
                "               outlier.shape=2, \n",
                "               outlier.size=5, \n",
                "               notch=T, \n",
                "               fill=clrs5, \n",
                "               color=\"black\") \n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "library(EnvStats)\n",
                "# create boxplot\n",
                "ggplot(pdat, aes(DateRedux, Prepositions, fill = DateRedux, color = DateRedux)) +                 \n",
                "  geom_boxplot(varwidth = T, color = \"black\", alpha = .2) +\n",
                "  geom_jitter(alpha = .2, height = 0, width = .2) + \n",
                "  facet_grid(~Region) +\n",
                "  EnvStats::stat_n_text(y.pos = 65) +\n",
                "  theme(legend.position = \"none\") +\n",
                "  labs(x = \"\", y = \"Frequency (per 1,000 words)\") +\n",
                "  ggtitle(\"Use of prepositions in English texts across time and regions\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "library(ggstatsplot)\n",
                "# create boxplot\n",
                "ggstatsplot::ggbetweenstats(data = pdat,\n",
                "                            x = DateRedux,\n",
                "                            y = Prepositions,\n",
                "                            plottype = \"box\",\n",
                "                            type = \"p\",\n",
                "                            conf.level = 0.95)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Violin plots\n",
                "\n",
                "An alternative to boxplots which display the distribution within the data even more accurately are violin plots. \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "ggplot(pdat, aes(DateRedux, Prepositions, fill = DateRedux)) +  \n",
                "  geom_violin(trim = FALSE, alpha = .5) +  \n",
                "  scale_fill_manual(values = clrs5) +\n",
                "  theme_bw() +\n",
                "  theme(legend.position = \"none\")         \n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Word clouds\n",
                "\n",
                "Word clouds visualize word frequencies of either single corpus or different corpora. Although word clouds are rarely used in academic publications, they are a common way to display language data and the topics of texts - which may be thought of as their semantic content. To exemplify how to use word clouds, we are going to have a look at rally speeches of Hillary Clinton and Donald Trump that were given during their 2016 campaigns. In a first step, we load and process the data as the relevant packages are already loaded.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# load and process speeches by clinton\n",
                "clinton <- base::readRDS(url(\"https://slcladal.github.io/data/Clinton.rda\", \"rb\")) %>% paste0(collapse = \" \")\n",
                "# load and process speeches by trump\n",
                "trump <- base::readRDS(url(\"https://slcladal.github.io/data/Trump.rda\", \"rb\")) %>%  paste0(collapse = \" \")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "After loading the data, we need to clean it.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# clean texts\n",
                "docs <- Corpus(VectorSource(c(clinton, trump))) %>%\n",
                "  # clean text data\n",
                "  tm::tm_map(removePunctuation) %>%\n",
                "  tm::tm_map(removeNumbers) %>%\n",
                "  tm::tm_map(tolower)  %>%\n",
                "  tm::tm_map(removeWords, stopwords(\"english\")) %>%\n",
                "  tm::tm_map(stripWhitespace) %>%\n",
                "  tm::tm_map(PlainTextDocument)\n",
                "# create term document matrix\n",
                "tdm <- TermDocumentMatrix(docs) %>%\n",
                "  as.matrix()\n",
                "colnames(tdm) <- c(\"Clinton\",\"Trump\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Next, we normalize the absolute frequencies of the terms in the document by converting them into relative frequencies.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# calculate rel. freq.\n",
                "tdm[, 1] <- as.vector(unlist(sapply(tdm[, 1], function(x) round(x/colSums(tdm)[1]*1000, 0) )))\n",
                "# calculate rel. freq.\n",
                "tdm[, 2] <- as.vector(unlist(sapply(tdm[, 2], function(x) round(x/colSums(tdm)[2]*1000, 0) )))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "After processing the data, we can now create word clouds. However, there are different word clouds: \n",
                "\n",
                "* (Common) word clouds\n",
                "* Comparative clouds\n",
                "* Commonality clouds\n",
                "\n",
                "Common or simple word clouds simply show the frequency of word types while comparative word clouds show which word types are particularly overrepresented in one sub-corpus compared to another sub-corpus. Commonality word clouds show words that are shared and are thus particularly indistinctive for different sub-corpora.\n",
                "\n",
                "Let us first inspect a common word cloud of the corpus.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# load package\n",
                "library(wordcloud)\n",
                "# create word cloud\n",
                "wordcloud(docs, max.words = 100, \n",
                "          colors = brewer.pal(6, \"BrBG\"), \n",
                "          random.order = FALSE)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The common word cloud shows the frequencies of words regardless of who used them. In contrast, the comparative cloud shown below highlights words that differ most with respect to their frequencies in the sub-corpora under investigation.  \n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# create comparison cloud\n",
                "comparison.cloud(tdm, \n",
                "                 max.words = 100, \n",
                "                 random.order = FALSE, \n",
                "                 colors = c(\"blue\", \"red\"), \n",
                "                 title.bg.colors=\"white\",\n",
                "                 bg.color = \"black\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The opposite of comparative clouds are commonality clouds which highlight words that use with similar relative frequencies in the sub-corpora under investigation and that are therefore particularly indistinctive.  \n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# create commonality cloud\n",
                "commonality.cloud(tdm, \n",
                "                  max.words = 100, \n",
                "                  random.order = FALSE, \n",
                "          colors = brewer.pal(6, \"Spectral\"))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "At first, I thought that word clouds are simply a fancy but not very helpful way to inspect language data but I have to admit that word clouds really surprised me as they do appear to possess potential to provide an idea of what groups of people are talking about. The comparative word cloud shows that the Trump uses a lot of contractions (\"'re\", \"'ll\", etc.) and stresses concepts linked to the future (*going*) thereby stressing his vision of the US (*great*). In Contrast, Clinton did not use contractions but talked about *Americans*, *work*, the *economy*, and *women*.\n",
                "\n",
                "# Association plots\n",
                "\n",
                "Another plot type that is related to bar plots is the association plot. Association plots are similar to bar plots in that they display difference as bars above or below a line (as shown above). However, association plots show the difference between the observed and expected frequencies rather than differences as deviations from a reference. Therefore, they are often used when graphically representing tables with absolute frequencies. We use the already loaded *vcd* package to create association plots. \n",
                "\n",
                "We also modify the reduced pdat as association plots work on matrices rather than data frames or tibbles. In addition, we will drop more genres as to avoid overlap in the y-axis labels later on.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# reduce data\n",
                "assocdata <- pdat %>%\n",
                "  droplevels() %>%\n",
                "  dplyr::mutate(GenreRedux <- as.character(GenreRedux),\n",
                "                GenreRedux = dplyr::case_when(GenreRedux == \"Conversational\" ~ \"Conv.\",\n",
                "                                              GenreRedux == \"Religious\" ~ \"Relig.\",\n",
                "                                              TRUE ~ GenreRedux)) %>%\n",
                "  dplyr::group_by(GenreRedux, DateRedux) %>%\n",
                "  dplyr::summarise(Prepositions = round(mean(Prepositions), 0)) %>%\n",
                "  tidyr::spread(DateRedux, Prepositions)\n",
                "# create matrix \n",
                "assocmx <- as.matrix(assocdata[,2:6])\n",
                "attr(assocmx, \"dimnames\")[1] <- as.vector(assocdata[,1])\n",
                "# inspect\n",
                "head(assocmx, 15)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Association plots are created by using the `assoc` function which takes a table (or a similar format such as a matrix or a data frame) as their argument. In addition, we specify `shade` as `T` in order to color code the bars in the association plot and to add a legend.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# create association plot\n",
                "assoc(assocmx, shade=TRUE)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The bars above the line indicate that the observed frequency is higher than expected, bars under the line indicate frequencies that are lower than expected. Darker shades of blue and red coloring suggest that there are significant differences between the observed and the expected frequencies. In the present example, this means that the frequencies of prepositions differ significantly across genres and periods. *However(!) as shown in the table above, this result is an artifact because the first period does not contain any data points for conversational or legal texts!*\n",
                "\n",
                "# Mosaic plots\n",
                "\n",
                "Another plot which is useful to graphically depict the relationship of categorical variables is the mosaic plot. The size of the boxes in a mosaic plot indicate how frequent that subcategory is and the colors show whether or not the category differs from the value that is expected if given the overall distribution in the table. In addition, the hue of the color shows how great the difference between observed and expected is and thus indicates whether the respective subcategory deviates significantly from the expected frequency. Boxes that are gray suggest the absence of significant differences. \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# create a mosaic plot\n",
                "mosaic(assocmx, shade=T, legend=TRUE)  \n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                " \n",
                "According to the mosaic plot above, there are some potentially significant differences in the first and second period. This, however, is still likely to be caused by the absence of data points from conversational or legal texts in the first period. Also, the absence of boxes for these text types in the first period indicate that there is a potential problem - something that was not visible in the mosaic plot!\n",
                "\n",
                "# Heat maps\n",
                "\n",
                "Heat maps are similar to mosaic plots in that they display frequency information and use color-coding to indicate high and low values. Heat maps also work on matrices but they are much more powerful and versatile that mosaic plots. \n",
                "\n",
                "Heat maps are a very popular way to display frequency information and various packages have been written to create or customize heatmaps (for example the packages \"ComplexHeatmap\", \"dendextend\", \"d3heatmap\", \"pheatmap\") which means that many aspects of heatmaps can be modified. In this example, we will only use the most basic function to create a heat map.\n",
                "\n",
                "We again modify the data and create a matrix from the original pdat. In addition, we scale the frequencies. This is not necessary in the present case but when dealing with variables which differ in their mean and variance because they reflect different variables, scaling will normalize such variables and render their values comparable. \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# create data\n",
                "heatdata <- pdat %>%\n",
                "  dplyr::group_by(DateRedux, GenreRedux) %>%\n",
                "  dplyr::summarise(Prepositions = mean(Prepositions)) %>%\n",
                "  tidyr::spread(DateRedux, Prepositions)\n",
                "# create matrix \n",
                "heatmx <- as.matrix(heatdata[,2:5])\n",
                "attr(heatmx, \"dimnames\")[1] <- as.vector(heatdata[,1])\n",
                "heatmx <- scale(heatmx) %>%\n",
                "  round(., 2)\n",
                "# inspect\n",
                "head(heatmx, 15)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now that we have created a data matrix, we can cerate a simple heat map.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# create heat map\n",
                "heatmap(heatmx, scale = \"none\", cexCol = 1, cexRow = 1)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The dendrogram on the top shows that documents from 1600 and 1700 as well as documents from 1800 and 1900 are grouped together and thus are more similar with respect to their preposition frequencies. The dendrogram on the left indicates that we have two categories of documents: the genres to towards the bottom tend to have fewer prepositions (indicated by the light colours) while the documents to the top tend to have more prepositions (thus the darker hues). Legal texts (genre = Law) have notably higher rates of prepositions as is derivable from the dark red colour of such texts.  \n",
                "\n",
                "# Citation & Session Info \n",
                "\n",
                "Schweinberger, Martin. 2021. *Data Visualization with R*. Brisbane: The University of Queensland. url: https://slcladal.github.io/dviz.html.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "sessionInfo()\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# References \n",
                "\n",
                "Healy, Kieran. 2018. *Data Visualization: A Practical Introduction*. Princeton University Press.\n",
                "\n",
                "Wickham, Hadley. 2016. *ggplot2: Elegant Graphics for Data Analysis*. Springer.\n",
                "\n",
                "Wilkinson, Leland. 2012. The Grammar of Graphics. In James E. Gentle, Wolfgang Karl H, and Yuichi Mori (eds.), *Handbook of Computational Statistics. Concepts and Methods*, 375414. Springer.\n"
            ]
        }
    ],
    "metadata": {
        "anaconda-cloud": "",
        "kernelspec": {
            "display_name": "R",
            "langauge": "R",
            "name": "ir"
        },
        "language_info": {
            "codemirror_mode": "r",
            "file_extension": ".r",
            "mimetype": "text/x-r-source",
            "name": "R",
            "pygments_lexer": "r",
            "version": "3.4.1"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}
