{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "![An interactive LADAL notebook](https://slcladal.github.io/images/uq1.jpg)\n",
                "\n",
                "\n",
                "# Part-of-speech tagging and syntactic parsing with R\n",
                "\n",
                "\n",
                "This tutorial is the interactive Jupyter notebook accompanying the [*Language Technology and Data Analysis Laboratory* (LADAL) tutorial *POS-Tagging and Syntactic Parsing with R*](https://ladal.edu.au/postag.html). The tutorial provides more details and background information while this interactive notebook focuses strictly on practical aspects.\n",
                "\n",
                "\n",
                "**Preparation and session set up**\n",
                "\n",
                "We set up our session by activating the packages we need for this tutorial. \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# activate packages\n",
                "library(dplyr)\n",
                "library(stringr)\n",
                "library(udpipe) \n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Once you have initiated the session by executing the code shown above, you are good to go.\n",
                "\n",
                "If you are using this notebook on your own computer and you have not already installed the R packages listed above, you need to install them. You can install them by replacing the `library` command with `install.packages` and putting the name of the package into quotation marks like this: `install.packages(\"dplyr\")`. Then, you simply run this command and R will install the package you specified.\n",
                "\n",
                "\n",
                "# Part-Of-Speech Tagging\n",
                "\n",
                "Many analyses of language data require that we distinguish different parts of speech. In order to determine the word class of a certain word, we use a procedure which is called part-of-speech tagging (commonly referred to as pos-, pos-, or PoS-tagging). \n",
                "\n",
                "Parts-of-speech, or word categories, refer to the grammatical nature or category of a lexical item, e.g. in the sentence *Jane likes the girl* each lexical item can be classified according to whether it belongs to the group of determiners, verbs, nouns, etc.  \n",
                "\n",
                "When posâ€“tagged, the example sentence could look like the example below.\n",
                "\n",
                "1. Jane/NNP likes/VBZ the/DT girl/NN\n",
                "\n",
                "In the example above, `NNP` stands for proper noun (singular), `VBZ` stands for 3rd person singular present tense verb, `DT` for determiner, and `NN` for noun(singular or mass). The pos-tags used by the `openNLPpackage` are the [Penn English Treebank pos-tags](https://dpdearing.com/posts/2011/12/opennlp-part-of-speech-pos-tags-penn-english-treebank/). A more elaborate description of the tags can be found here which is summarized below:\n",
                "\n",
                "Tag | Description | Examples\n",
                "----|-------------|---------\n",
                "CC | Coordinating conjunction | and, or, but\n",
                "CD | Cardinal number | one, two, three\n",
                "DT | Determiner | a, the\n",
                "EX | Existential there | There/EX was a party in progress\n",
                "FW | Foreign word | persona/FW non/FW grata/FW\n",
                "IN | Preposition or subordinating con | uh, well, yes\n",
                "JJ | Adjective | good, bad, ugly\n",
                "JJR | Adjective, comparative | better, nicer\n",
                "JJS | Adjective, superlative | best, nicest\n",
                "LS | List item marker | a., b., 1., 2.\n",
                "MD | Modal | can, would, will\n",
                "NN | Noun, singular or mass | tree, chair\n",
                "NNS | Noun, plural | trees, chairs\n",
                "NNP | Proper noun, singular | John, Paul, CIA\n",
                "NNPS | Proper noun, plural | Johns, Pauls, CIAs\n",
                "PDT | Predeterminer | all/PDT this marble, many/PDT a soul\n",
                "POS | Possessive ending | John/NNP 's/POS, the parents/NNP '/POS distress\n",
                "PRP | Personal pronoun | I, you, he\n",
                "PRP\\$ | Possessive pronoun | mine, yours\n",
                "RB | Adverb | every, enough, not\n",
                "RBR | Adverb, comparative | later\n",
                "RBS | Adverb, superlative | latest\n",
                "RP | Particle | RP\n",
                "SYM | Symbol | CO2\n",
                "TO | to | to\n",
                "UH | Interjection | uhm, uh\n",
                "VB | Verb, base form | go, walk\n",
                "VBD | Verb, past tense | walked, saw\n",
                "VBG | Verb, gerund or present part. | walking, seeing\n",
                "VBN | Verb, past participle | walked, thought\n",
                "VBP | Verb, non-3rd person singular pr | walk, think\n",
                "VBZ | Verb, 3rd person singular present | walks, thinks\n",
                "WDT | Wh-determiner | which, that\n",
                "WP | Wh-pronoun | what, who, whom (wh-pronoun)\n",
                "WP\\$ | Possessive wh-pronoun | whose, who (wh-words)\n",
                "WRB | Wh-adverb | how, where, why (wh-adverb)\n",
                "\n",
                "There are several different R packages that assist with pos-tagging texts [see @kumar2016mastering]. In this tutorial, we will use the `udpipe` [@udpipe]. The `udpipe` package is really great as it is easy to use, covers a wide range of languages, is very flexible, and very accurate.  It is particularly handy because it addresses and remedies major shortcomings that previous methods for pos-tagging had, namely\n",
                "\n",
                "* it offers a wide range of language models (64 languages at this point)\n",
                "* it does not rely on external software (like, e.g., TreeTagger, that had to be installed separately and could be challenging when using different operating systems)\n",
                "* it is really easy to implement as one only need to install and load the `udpipe` package and download and activate the language model one is interested in\n",
                "* it allows to train and tune one's own models rather easily\n",
                "\n",
                "The available pre-trained language models in UDPipe are:\n",
                "\n",
                "Languages | Models \n",
                "----------|-------\n",
                "Afrikaans | afrikaans-afribooms\n",
                "Ancient Greek | ancient_greek-perseus, ancient_greek-proiel\n",
                "Arabic | arabic-padt\n",
                "Armenian | armenian-armtdp\n",
                "Basque | basque-bdt\n",
                "Belarusian | belarusian-hse\n",
                "Bulgarian | bulgarian-btb\n",
                "Buryat | buryat-bdt\n",
                "Catalan | catalan-ancora\n",
                "Chinese | chinese-gsd, chinese-gsdsimp, classical_chinese-kyoto\n",
                "Coptic | coptic-scriptorium\n",
                "Croatian | croatian-set\n",
                "Czech | czech-cac, czech-cltt, czech-fictree, czech-pdt\n",
                "Danish | danish-ddt\n",
                "Dutch | dutch-alpino, dutch-lassysmall\n",
                "English | english-ewt, english-gum, english-lines, english-partut\n",
                "Estonian | estonian-edt, estonian-ewt\n",
                "Finnish | finnish-ftb, finnish-tdt\n",
                "French | french-gsd, french-partut, french-sequoia, french-spoken\n",
                "Galician | galician-ctg, galician-treegal\n",
                "German | german-gsd, german-hdt\n",
                "Gothic | gothic-proiel\n",
                "Greek | greek-gdt\n",
                "Hebrew | hebrew-htb\n",
                "Hindi | hindi-hdtb\n",
                "Hungarian | hungarian-szeged\n",
                "Indonesian | indonesian-gsd\n",
                "Irish Gaelic | irish-idt\n",
                "Italian | italian-isdt, italian-partut, italian-postwita, italian-twittiro, italian-vit\n",
                "Japanese | japanese-gsd\n",
                "Kazakh | kazakh-ktb\n",
                "Korean | korean-gsd, korean-kaist\n",
                "Kurmanji | kurmanji-mg\n",
                "Latin | latin-ittb, latin-perseus, latin-proiel\n",
                "Latvian | latvian-lvtb\n",
                "Lithuanian | lithuanian-alksnis, lithuanian-hse\n",
                "Maltese | maltese-mudt\n",
                "Marathi | marathi-ufal\n",
                "North Sami | north_sami-giella\n",
                "Norwegian | norwegian-bokmaal, norwegian-nynorsk, norwegian-nynorsklia\n",
                "Old Church Slavonic | old_church_slavonic-proiel\n",
                "Old French | old_french-srcmf\n",
                "Old Russian | old_russian-torot\n",
                "Persian | persian-seraji\n",
                "Polish | polish-lfg, polish-pdb, polish-sz\n",
                "Portugese | portuguese-bosque, portuguese-br, portuguese-gsd\n",
                "Romanian | romanian-nonstandard, romanian-rrt\n",
                "Russian | russian-gsd, russian-syntagrus, russian-taiga\n",
                "Sanskrit | sanskrit-ufal\n",
                "Scottish Gaelic | scottish_gaelic-arcosg\n",
                "Serbian | serbian-set\n",
                "Slovak | slovak-snk\n",
                "Slovenian | slovenian-ssj, slovenian-sst\n",
                "Spanish | spanish-ancora, spanish-gsd\n",
                "Swedish | swedish-lines, swedish-talbanken\n",
                "Tamil | tamil-ttb\n",
                "Telugu | telugu-mtg\n",
                "Turkish | turkish-imst\n",
                "Ukrainian | ukrainian-iu\n",
                "Upper Sorbia | upper_sorbian-ufal\n",
                "Urdu | urdu-udtb\n",
                "Uyghur | uyghur-udt\n",
                "Vietnamese | vietnamese-vtb\n",
                "Wolof | wolof-wtb\n",
                "\n",
                "\n",
                " \n",
                "\n",
                "To download any of these models, we can use the `udpipe_download_model` function. For example, to download the `english-ewt` model, we would use the call: `m_eng\t<- udpipe::udpipe_download_model(language = \"english-ewt\")`. \n",
                "\n",
                "We start by loading  a text\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# load text\n",
                "text <- readLines(\"https://slcladal.github.io/data/testcorpus/linguistics06.txt\", skipNul = T)\n",
                "# clean data\n",
                "text <- text %>%\n",
                " str_squish() \n",
                "# inspect\n",
                "text\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Using your own data\n",
                "\n",
                "While the tutorial uses data from the LADAL website, you can also use your own data. You can see below what you need to do to upload and use your own data.\n",
                "\n",
                "The code chunk below allows you to upload two files from your own computer. To be able to load your own data, you need to click on the folder symbol to the left of the screen:\n",
                "\n",
                "![Binder Folder Symbol](https://slcladal.github.io/images/binderfolder.JPG)\n",
                "\n",
                "\n",
                "Then, when the menu has unfolded, click on the smaller folder symbol (encircled in red in the picture below).\n",
                "\n",
                "![Small Binder Folder Symbol](https://slcladal.github.io/images/upload2.png)\n",
                "\n",
                "\n",
                "Now, you are in the main menu and can click on the 'MyData' folder.\n",
                "\n",
                "![MyData Folder Symbol](https://slcladal.github.io/images/upload3.png)\n",
                "\n",
                "Now, that you are in the MyData folder, you can click on the upload symbol.\n",
                "\n",
                "![Binder Upload Symbol](https://slcladal.github.io/images/binderupload.JPG)\n",
                "\n",
                "Select and upload the files you want to analyze (**IMPORTANT: here, we assume that you upload some form of text data - not tabular data! You can upload only txt and docx files!**). When you then execute the code chunk below, you will upload your own data and you can then use it in this notebook.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "myfiles <- list.files(here::here(\"MyData\"), # path to the corpus data\n",
                "                          # full paths - not just the names of the files\n",
                "                          full.names = T) \n",
                "# load colt files\n",
                "mytext <- sapply(myfiles, function(x){\n",
                "  x <- scan(x, \n",
                "            what = \"char\", \n",
                "            sep = \"\", \n",
                "            quote = \"\", \n",
                "            quiet = T, \n",
                "            skipNul = T)\n",
                "  x <- paste0(x, sep = \" \", collapse = \" \")\n",
                "  x <- stringr::str_squish(x)\n",
                "})\n",
                "# inspect\n",
                "str(mytext)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Keep in mind though that you need to adapt the names of the texts in the code chunks below so that the code below work on your own texts!**\n",
                "\n",
                "***\n",
                "\n",
                "\n",
                "Now that we have a text that we can work with, we will download a pre-trained language model.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# download language model\n",
                "m_eng\t<- udpipe::udpipe_download_model(language = \"english-ewt\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "If you have downloaded a model once, you can also load the model directly. Here, we have downloaded it into the main directory so we can acll it directly with only the file name.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# load language model from your computer after you have downloaded it once\n",
                "m_eng <- udpipe_load_model(file = \"english-ewt-ud-2.5-191206.udpipe\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We can now use the model to annotate out text.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# tokenise, tag, dependency parsing\n",
                "text_anndf <- udpipe::udpipe_annotate(m_eng, x = text) %>%\n",
                "  as.data.frame() %>%\n",
                "  dplyr::select(-sentence)\n",
                "# inspect\n",
                "head(text_anndf, 10)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "It can be useful to extract only the words and their pos-tags and convert them back into a text format (rather than a tabular format). \n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "tagged_text <- paste(text_anndf$token, \"/\", text_anndf$xpos, collapse = \" \", sep = \"\")\n",
                "# inspect tagged text\n",
                "tagged_text\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# POS-Tagging non-English texts \n",
                "\n",
                "We can apply the same method for annotating, e.g. adding pos-tags, to other languages. For this, we could train our own model, or, we can use one of the many pre-trained language models that `udpipe` provides.\n",
                "\n",
                "Let us explore how to do this by using  example texts from different languages, here from German and Spanish (but we could also annotate texts from any of the wide variety of languages for which UDPipe provides pre-trained models.\n",
                "\n",
                "\n",
                "We begin by loading a German and a Dutch text.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# load texts\n",
                "gertext <- readLines(\"https://slcladal.github.io/data/german.txt\") \n",
                "duttext <- readLines(\"https://slcladal.github.io/data/dutch.txt\") \n",
                "# inspect texts\n",
                "gertext; duttext\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Next, we install the pre-trained language models.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# download language model\n",
                "m_ger\t<- udpipe::udpipe_download_model(language = \"german-gsd\")\n",
                "m_dut\t<- udpipe::udpipe_download_model(language = \"dutch-alpino\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Or we load them from our machine (if we have downloaded and saved them before).\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# load language model from your computer after you have downloaded it once\n",
                "m_ger\t<- udpipe::udpipe_load_model(file = \"german-gsd-ud-2.5-191206.udpipe\")\n",
                "m_dut\t<- udpipe::udpipe_load_model(file = \"dutch-alpino-ud-2.5-191206.udpipe\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now, pos-tag the German text.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# tokenise, tag, dependency parsing of german text\n",
                "ger_pos <- udpipe::udpipe_annotate(m_ger, x = gertext) %>%\n",
                "  as.data.frame() %>%\n",
                "  dplyr::summarise(postxt = paste(token, \"/\", xpos, collapse = \" \", sep = \"\")) %>%\n",
                "  dplyr::pull(unique(postxt))\n",
                "# inspect\n",
                "ger_pos\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "And finally, we also pos-tag the Dutch text.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# tokenise, tag, dependency parsing of german text\n",
                "nl_pos <- udpipe::udpipe_annotate(m_dut, x = duttext) %>%\n",
                "   as.data.frame() %>%\n",
                "  dplyr::summarise(postxt = paste(token, \"/\", xpos, collapse = \" \", sep = \"\")) %>%\n",
                "  dplyr::pull(unique(postxt))\n",
                "# inspect\n",
                "nl_pos\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Dependency Parsing Using UDPipe\n",
                "\n",
                "In addition to pos-tagging, we can also generate plots showing the syntactic dependencies of the different constituents of a sentence. For this, we generate an object that contains a sentence (in this case, the sentence *Linguistics is the scientific study of language*), and we then plot (or visualize) the dependencies using the `textplot_dependencyparser` function.  \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# parse text\n",
                "sent <- udpipe::udpipe_annotate(m_eng, x = \"Linguistics is the scientific study of language\") %>%\n",
                "  as.data.frame()\n",
                "# inspect\n",
                "head(sent)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We now generate the plot.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# generate dependency plot\n",
                "dplot <- textplot::textplot_dependencyparser(sent, size = 3) \n",
                "# show plot\n",
                "dplot\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "***\n",
                "\n",
                "[Back to LADAL](https://ladal.edu.au/postag.html)\n",
                "\n",
                "***\n"
            ]
        }
    ],
    "metadata": {
        "anaconda-cloud": "",
        "kernelspec": {
            "display_name": "R",
            "langauge": "R",
            "name": "ir"
        },
        "language_info": {
            "codemirror_mode": "r",
            "file_extension": ".r",
            "mimetype": "text/x-r-source",
            "name": "R",
            "pygments_lexer": "r",
            "version": "3.4.1"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}
