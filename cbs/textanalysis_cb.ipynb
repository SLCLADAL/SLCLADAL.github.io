{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "![An interactive LADAL notebook](https://slcladal.github.io/images/uq1.jpg)\n",
                "\n",
                "# Practical Overview of Selected Text Analytics Methods\n",
                "\n",
                "This tutorial is the interactive Jupyter notebook accompanying the [*Language Technology and Data Analysis Laboratory* (LADAL) tutorial *A Practical Overview of Selected Text Analytics Methods*](https://ladal.edu.au/textanalysis.html). \n",
                "\n",
                "\n",
                "\n",
                "***\n",
                "\n",
                "\n",
                "**Preparation and session set up**\n",
                "\n",
                "We set up our session by activating the packages we need for this tutorial.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# activate packages\n",
                "library(quanteda)           # Text analysis and processing\n",
                "library(dplyr)              # Data manipulation and transformation\n",
                "library(stringr)            # String manipulation and pattern matching\n",
                "library(tidytext)           # Tidy text data for analysis\n",
                "library(ggplot2)            # Data visualization\n",
                "library(quanteda.textplots) # Text visualization using ggplot2\n",
                "library(quanteda.textstats) # Text statistics and analysis\n",
                "library(udpipe)             # Tokenization and part-of-speech tagging\n",
                "library(ggplot2)            # Additional data visualization with ggplot2\n",
                "library(textplot)           # Text-based plots\n",
                "library(openxlsx)           # Reading and writing Excel files\n",
                "library(viridis)            # Color mapping for data visualization\n",
                "library(writexl)            # For saving data\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Loading and processing textual data\n",
                "\n",
                "For this tutorial, the default data represents the text of Lewis Caroll's  *Alice's Adventures in Wonderland* which we download from the [GitHub data repository of the *Language Technology and Data Analysis Laboratory* (LADAL)](https://slcladal.github.io/data). **See [here](https://ladal.edu.au/load.html) for a LADAL tutorial on how to load and save different types of data.**  \n",
                "\n",
                "## Using your own data\n",
                "\n",
                "<div class=\"warning\" style='padding:0.1em; background-color: rgba(215,209,204,.3); color:#51247a'>\n",
                "<span>\n",
                "<p style='margin-top:1em; text-align:center'>\n",
                "\n",
                "While the tutorial uses data from the LADAL website, you can also **use your own data**. To use your own data, click on the folder called `MyTexts` (it is in the menu to the left of the screen) and then simply drag and drop your txt-files into the folder. When you then execute the code chunk below, you will upload your own data and you can then use it in this notebook.<br>\n",
                "<br>\n",
                "You can upload <b>only txt-files<\/b> (simple unformatted files created in or saved by a text editor)! The notebook assumes that you upload some form of text data - not tabular data! <br>\n",
                "<br>\n",
                "<b>IMPORTANT: Be sure to to then replace `mytext` with `text` in the above code chunk and not execute the following code chunk which loads an example text from the LADAL repository so that you work with your and not the sample data!<\/b><br>\n",
                "<\/p>\n",
                "<p style='margin-left:1em;'>\n",
                "<\/p><\/span>\n",
                "<\/div>\n",
                "\n",
                "<br>\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "myfiles <- list.files(here::here(\"notebooks/MyTexts\"), # path to the corpus data\n",
                "                          # full paths - not just the names of the files\n",
                "                          full.names = T) \n",
                "# loop over the vector 'myfiles' that contains paths to the data\n",
                "mytext <- sapply(myfiles, function(x){\n",
                "\n",
                "  # read the content of each file using 'scan'\n",
                "  x <- scan(x, \n",
                "            what = \"char\",    # specify that the input is characters\n",
                "            sep = \"\",         # set separator to an empty string (read entire content)\n",
                "            quote = \"\",       # set quote to an empty string (no quoting)\n",
                "            quiet = T,        # suppress scan messages\n",
                "            skipNul = T)      # skip NUL bytes if encountered\n",
                "\n",
                "  # combine the character vector into a single string with spaces\n",
                "  x <- paste0(x, sep = \" \", collapse = \" \")\n",
                "\n",
                "  # remove extra whitespaces using 'str_squish' from the 'stringr' package\n",
                "  x <- stringr::str_squish(x)\n",
                "\n",
                "})\n",
                "\n",
                "# inspect the structure of the text object\n",
                "str(mytext)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Keep in mind though that you need to adapt the names of the texts in the code chunks below so that the code below work on your own texts!**\n",
                "\n",
                "***\n",
                "\n",
                "## Loading the example data\n",
                "\n",
                "We begin by loading the data which represents the text of Lewis Caroll's  *Alice's Adventures in Wonderland*. \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# load text\n",
                "text <- base::readRDS(url(\"https://slcladal.github.io/data/alice.rda\", \"rb\"))\n",
                "# inspect data\n",
                "head(text)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Splitting the data into chapter\n",
                "\n",
                "The example data still consists of short text snippets which is why we collapse these snippets and then split the collapsed data into chapters. \n",
                "\n",
                "<div class=\"warning\" style='padding:0.1em; background-color: rgba(215,209,204,.3); color:#51247a'>\n",
                "<span>\n",
                "<p style='margin-top:1em; text-align:center'>\n",
                "<b>IMPORTANT: If your data does not consist of chapters that have *CHAPTER* in the chapter text, then do not use this code chunk when working with your own data!<\/b><br>\n",
                "<\/p>\n",
                "<p style='margin-left:1em;'>\n",
                "<\/p><\/span>\n",
                "<\/div>\n",
                "\n",
                "<br>\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# combine and split into chapters\n",
                "text_chapters <- text %>%\n",
                "  # paste all texts together into one long text\n",
                "  paste0(collapse = \" \") %>%\n",
                "  # replace Chapter I to Chapter XVI with qwertz \n",
                "  stringr::str_replace_all(\"(CHAPTER [XVI]{1,7}\\\\.{0,1}) \", \"qwertz\\\\1\") %>%\n",
                "  # convert text to lower case\n",
                "  tolower() %>%  \n",
                "  # split the long text into chapters\n",
                "  stringr::str_split(\"qwertz\") %>%\n",
                "  # unlist the result (convert into simple vector)\n",
                "  unlist()\n",
                "# inspect data\n",
                "text_chapters %>%\n",
                "  substr(start=1, stop=500) %>%\n",
                "  as.data.frame() %>%\n",
                "  head()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Concordancing\n",
                "\n",
                "Once we have split the data into chapters, we perform the concordancing and extract the KWICs using the `kwic` function from the `quanteda` package. This function has the following arguments: \n",
                "\n",
                "+ `x`: a text or collection of texts. The text needs to be tokenised, i.e. split it into individual words, which is why we use the *text* in the `tokens()` function. \n",
                "+ `pattern`: a keyword defined by a search pattern  \n",
                "+ `window`: the size of the context window (how many word before and after)  \n",
                "+ `valuetype`: the type of pattern matching  \n",
                "  + \"glob\" for \"glob\"-style wildcard expressions;  \n",
                "  + \"regex\" for regular expressions; or  \n",
                "  + \"fixed\" for exact matching  \n",
                "+ `separator`: a character to separate words in the output  \n",
                "+ `case_insensitive`: logical; if TRUE, ignore case when matching a pattern or dictionary values\n",
                "\n",
                "<div class=\"warning\" style='padding:0.1em; background-color: rgba(215,209,204,.3); color:#51247a'>\n",
                "<span>\n",
                "<p style='margin-top:1em; text-align:center'>\n",
                "<b>You can easily change and adapt the concordance. For instance, you can search for a different word, like *speak*, by substituting *alice* with *speak* as the pattern. Additionally, if you wish to widen the context window, just replace the '5' with '10'. This adjustment will extend the context around the keyword by 5 additional words in both the preceding and following context. <\/b><br>\n",
                "<\/p>\n",
                "<p style='margin-left:1em;'>\n",
                "<\/p><\/span>\n",
                "<\/div>\n",
                "\n",
                "<br>\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "kwic1 <- kwic(\n",
                "  # tokenise and define text\n",
                "  tokens(text), \n",
                "  # define target word (this is called the \"search pattern\")\n",
                "  pattern = phrase(\"alice\"),\n",
                "  # 5 words before and after\n",
                "  window = 5,\n",
                "  # no regex\n",
                "  valuetype = \"fixed\",\n",
                "  # words separated by whitespace\n",
                "  separator = \" \",\n",
                "  # search should be case insensitive\n",
                "  case_insensitive = TRUE)\n",
                "\n",
                "# inspect resulting kwic\n",
                "kwic1 %>%\n",
                "  # convert into a data frame\n",
                "  as.data.frame() %>%\n",
                "  # show only first 10 results\n",
                "  head(10)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Exporting tables\n",
                "\n",
                "To export a concordance table as an MS Excel spreadsheet, we use `write_xlsx`. \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# save data for MyOutput folder\n",
                "write_xlsx(kwic1, here::here(\"notebooks/MyOutput/kwic1.xlsx\"))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<div class=\"warning\" style='padding:0.1em; background-color: rgba(215,209,204,.3); color:#51247a'>\n",
                "<span>\n",
                "<p style='margin-top:1em; text-align:center'>\n",
                "<b>You will find the generated MS Excel spreadsheet named *mykwic.xlsx* in the `MyOutput` folder (located on the left side of the screen).<\/b> <br><br>Simply double-click the `MyOutput` folder icon, then right-click on the *mykwic.xlsx* file, and choose Download from the dropdown menu to download the file. <br>\n",
                "<\/p>\n",
                "<p style='margin-left:1em;'>\n",
                "<\/p><\/span>\n",
                "<\/div>\n",
                "\n",
                "<br>\n",
                "\n",
                "## More on concordancing\n",
                "\n",
                "We can also use regular expressions in our search to extract not only *alice* but also more complex and even vague patterns. Vague means that only part of the pattern is specified. For instance, maybe only *walk* is specified and now we want all words containing this sequence including *walking*, *walker*, *walked*, and *walks*. To retrieve such vague patterns, we need to use so-called *regular expressions*. Also, when using a regular expression in the `pattern` argument, we need to specify the `valuetype` as `regex` (as shown below).\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# create kwic\n",
                "kwic2 <- quanteda::kwic(x = tokens(text_chapters), \n",
                "                          pattern = \"walk.*\",\n",
                "                          window = 5,\n",
                "                          valuetype = \"regex\") %>%\n",
                "  # convert into a data frame\n",
                "  as.data.frame() %>%\n",
                "  # remove superfluous columns\n",
                "  dplyr::select(-to, -from, -pattern)\n",
                "# inspect data\n",
                "kwic2 %>%\n",
                "  head()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Again, to export a concordance table as an MS Excel spreadsheet, we use `write_xlsx`. Be aware that we use the `here` function to  save the file in the current working directory.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# save data for MyOutput folder\n",
                "write_xlsx(kwic2, here::here(\"notebooks/MyOutput/kwic2.xlsx\"))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "When search for expressions that represent phrase and that consists out of several elements such as *poor alice*, we also need to specify that we are looking for a phrase in the pattern argument. \n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# create kwic\n",
                "kwic3 <- quanteda::kwic(x = tokens(text_chapters), \n",
                "                          pattern = quanteda::phrase(\"poor alice\"),\n",
                "                          window = 5) %>%\n",
                "  # convert into a data frame\n",
                "  as.data.frame() %>%\n",
                "  # remove superfluous columns\n",
                "  dplyr::select(-to, -from, -pattern)\n",
                "# inspect data\n",
                "kwic3 %>%\n",
                "  head()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Again, to export a concordance table as an MS Excel spreadsheet, we use `write_xlsx`. Be aware that we use the `here` function to  save the file in the current working directory.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# save data for MyOutput folder\n",
                "write_xlsx(kwic3, here::here(\"notebooks/MyOutput/kwic3.xlsx\"))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We could now continue and analyze how the phrase *poor alice* is used or perform similar analyses.\n",
                "\n",
                "## Word Frequency\n",
                "\n",
                "Almost all methods used in text analytics rely on frequency information. Thus, fending out out frequent words are in a text is a fundamental technique in text analytics. In fact, frequency information lies at the very core of Text Analysis. Such frequency information often comes in the form of word frequency lists, i.e. lists of word forms and their frequency in a given text or collection of texts.  \n",
                "\n",
                "As extracting word frequency lists is very important, we will now We will now extract a frequency list from a corpus.\n",
                "\n",
                "In a first step, we load a corpus, convert everything to lower case, remove non-word symbols (including punctuation), and split the corpus data into individual words.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# load and process corpus\n",
                "text_words <- text  %>%\n",
                "  # convert everything to lower case\n",
                "  tolower() %>%\n",
                "  # remove non-word characters\n",
                "  str_replace_all(\"[^[:alpha:][:space:]]*\", \"\")  %>%\n",
                "  tm::removePunctuation() %>%\n",
                "  stringr::str_squish() %>%\n",
                "  stringr::str_split(\" \") %>%\n",
                "  unlist()\n",
                "# inspect data\n",
                "text_words %>%\n",
                "  head()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now that we have a vector of words, we can easily create a table representing a word frequency list (as shown below).\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# create table\n",
                "wfreq <- text_words %>%\n",
                "  table() %>%\n",
                "  as.data.frame() %>%\n",
                "  arrange(desc(Freq)) %>%\n",
                "  dplyr::rename(word = 1,\n",
                "                frequency = 2)\n",
                "# inspect data\n",
                "wfreq %>%\n",
                "  head()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The most frequent words are all function words which are often not meaningful or useful for an analysis. Thus, we now remove these function words (also called *stopwords*) from the frequency list and inspect the list without stopwords.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# create table wo stopwords\n",
                "wfreq_wostop <- wfreq %>%\n",
                "  anti_join(stop_words, by = \"word\") %>%\n",
                "  dplyr::filter(word != \"\")\n",
                "# inspect data\n",
                "wfreq_wostop %>%\n",
                "  head() \n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Such word frequency lists can be visualized in various ways. The most common way to visualize word frequency lists is in the form of bargraphs.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "wfreq_wostop %>%\n",
                "  head(10) %>%\n",
                "  ggplot(aes(x = reorder(word, -frequency, mean), y = frequency)) +\n",
                "  geom_bar(stat = \"identity\") +\n",
                "  labs(title = \"10 most frequent non-stop words in \\n the example text \",\n",
                "       x = \"\",\n",
                "       y = \"Frequency\") +\n",
                "  theme(axis.text.x = element_text(angle = 90, hjust = 1))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "To save the visualization, you can simply right-click and select `Copy Output to Clipboard` (the top most option) and then paste the graph, e.g., into a Word document. Alternatively, you can export the image as described below.\n",
                "\n",
                "\n",
                "\n",
                "## Exporting images\n",
                "\n",
                "To export image as an png-file, we use `ggsave`. Be aware that we use the `here` function to save the file in the `MyOutput` folder.\n",
                "\n",
                "The `ggsave` function has the following main arguments:\n",
                "\n",
                "+ `filename`: File name to create on disk.    \n",
                "+ `device`: Device to use. Can either be a device function (e.g. png), or one of \"eps\", \"ps\", \"tex\" (pictex), \"pdf\", \"jpeg\", \"tiff\", \"png\", \"bmp\", \"svg\" or \"wmf\" (windows only). If NULL (default), the device is guessed based on the filename extension  \n",
                "+ `path`: Path of the directory to save plot to: path and filename are combined to create the fully qualified file name. Defaults to the working directory.  \n",
                "+ `width, height`: Plot size in units expressed by the units argument. If not supplied, uses the size of the current graphics device.  \n",
                "+ `units`: One of the following units in which the width and height arguments are expressed: \"in\", \"cm\", \"mm\" or \"px\".  \n",
                "+ `dpi`: Plot resolution. Also accepts a string input: \"retina\" (320), \"print\" (300), or \"screen\" (72). Applies only to raster output types.  \n",
                "+ `bg`: Background colour. If NULL, uses the plot.background fill value from the plot theme.  \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# save network graph for MyOutput folder\n",
                "ggsave(here::here(\"notebooks/MyOutput/image_01.png\"), bg = \"white\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<div class=\"warning\" style='padding:0.1em; background-color: rgba(215,209,204,.3); color:#51247a'>\n",
                "<span>\n",
                "<p style='margin-top:1em; text-align:center'>\n",
                "<b>You will find the image-file named *image_01.png* in the `MyOutput` folder (located on the left side of the screen).<\/b> <br><br>Simply double-click the `MyOutput` folder icon, then right-click on the *image_01.png* file, and choose Download from the dropdown menu to download the file. <br>\n",
                "<\/p>\n",
                "<p style='margin-left:1em;'>\n",
                "<\/p><\/span>\n",
                "<\/div>\n",
                "\n",
                "<br>\n",
                "\n",
                "\n",
                "## Wordclouds\n",
                "\n",
                "Alternatively, word frequency lists can be visualized, although less informative, as word clouds. \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# create wordcloud\n",
                "text %>%\n",
                "  quanteda::corpus() %>%\n",
                "  quanteda::tokens(remove_punct = TRUE) %>%\n",
                "  quanteda::tokens_remove(stopwords(\"english\")) %>%\n",
                "  quanteda::dfm() %>%\n",
                "  quanteda.textplots::textplot_wordcloud(max_words = 100, # show 100 most frequent words\n",
                "                                         # add nice colors and end function\n",
                "                                         color = viridis_pal(option = \"A\")(8)) \n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "As we did above, we use the `ggsave` function to save the image in the `MyOutput` folder.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# save network graph for MyOutput folder\n",
                "ggsave(here::here(\"notebooks/MyOutput/image_02.png\"))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<div class=\"warning\" style='padding:0.1em; background-color: rgba(215,209,204,.3); color:#51247a'>\n",
                "<span>\n",
                "<p style='margin-top:1em; text-align:center'>\n",
                "<b>You will find the image-file named *image_02.png* in the `MyOutput` folder (located on the left side of the screen).<\/b> <br><br>Simply double-click the `MyOutput` folder icon, then right-click on the *image_02.png* file, and choose Download from the dropdown menu to download the file. <br>\n",
                "<\/p>\n",
                "<p style='margin-left:1em;'>\n",
                "<\/p><\/span>\n",
                "<\/div>\n",
                "\n",
                "<br>\n",
                "\n",
                "Another variant of word clouds, so-called *comparison clouds*, Word lists can be used to determine differences between texts. For instance, we can load different texts and check whether they differ with respect to word frequencies. To show this, we load Herman Melville's *Moby Dick*, George Orwell's *1984*, and we also use  in the example text . \n",
                "\n",
                "In a first step, we load these texts and collapse them into single documents.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# load data\n",
                "darwin_sep <- base::readRDS(url(\"https://slcladal.github.io/data/darwin.rda\", \"rb\"))\n",
                "darwin <- darwin_sep %>%\n",
                "  paste0(collapse = \" \")\n",
                "orwell_sep <- base::readRDS(url(\"https://slcladal.github.io/data/orwell.rda\", \"rb\"))\n",
                "orwell <- orwell_sep %>%\n",
                "  paste0(collapse = \" \")\n",
                "melville_sep <- base::readRDS(url(\"https://slcladal.github.io/data/melville.rda\", \"rb\"))\n",
                "melville <- melville_sep %>%\n",
                "  paste0(collapse = \" \")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now, we generate a corpus object from these texts and create a variable with the author name.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "corp_dom <- quanteda::corpus(c(darwin, melville, orwell)) \n",
                "attr(corp_dom, \"docvars\")$Author = c(\"Darwin\", \"Melville\", \"Orwell\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now, we can remove so-called *stopwords* (non-lexical function words) and punctuation and generate the comparison cloud.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "corp_dom  %>%\n",
                "    quanteda::tokens(remove_punct = TRUE) %>%\n",
                "    quanteda::tokens_remove(stopwords(\"english\")) %>%\n",
                "    quanteda::dfm() %>%\n",
                "    quanteda::dfm_group(groups = corp_dom$Author) %>%\n",
                "    quanteda::dfm_trim(min_termfreq = 200, verbose = FALSE) %>%\n",
                "    quanteda.textplots::textplot_wordcloud(comparison = TRUE,\n",
                "                                           labeloffset = -0.05, \n",
                "                                           color = c(\"darkgray\", \"orange\", \"purple\"),\n",
                "                                           max_words = 150)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "As we did above, we use the `ggsave` function to save the image in the `MyOutput` folder.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# save network graph for MyOutput folder\n",
                "ggsave(here::here(\"notebooks/MyOutput/image_03.png\"))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<div class=\"warning\" style='padding:0.1em; background-color: rgba(215,209,204,.3); color:#51247a'>\n",
                "<span>\n",
                "<p style='margin-top:1em; text-align:center'>\n",
                "<b>You will find the image-file named *image_03.png* in the `MyOutput` folder (located on the left side of the screen).<\/b> <br><br>Simply double-click the `MyOutput` folder icon, then right-click on the *image_03.png* file, and choose Download from the dropdown menu to download the file. <br>\n",
                "<\/p>\n",
                "<p style='margin-left:1em;'>\n",
                "<\/p><\/span>\n",
                "<\/div>\n",
                "\n",
                "<br>\n",
                "\n",
                "## Frequency changes\n",
                "\n",
                "We can also investigate the use of the term *organism* across chapters in Darwin's *Origin*. In a first step, we extract the number of words in each chapter.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# extract number of words per chapter\n",
                "Words <- text_chapters %>%\n",
                "  stringr::str_split(\" \")  %>%\n",
                "  lengths()\n",
                "# inspect data\n",
                "Words\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Next, we extract the number of matches in each chapter.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# extract number of matches per chapter\n",
                "Matches <- text_chapters %>%\n",
                "  stringr::str_count(\"alice\")\n",
                "# inspect the number of matches per chapter\n",
                "Matches\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now, we extract the names of the chapters and create a table with the chapter names and the relative frequency of matches per 1,000 words.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# extract chapters\n",
                "Chapters <- paste0(\"chapter \", 0:(length(text_chapters)-1))\n",
                "Chapters\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# create table of results\n",
                "tb <- data.frame(Chapters, Matches, Words) %>%\n",
                "  dplyr::mutate(Frequency = round(Matches/Words*1000, 2))\n",
                "# inspect data\n",
                "tb %>%\n",
                "  as.data.frame() %>%\n",
                "  head() \n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We can now visualize the relative frequencies of our search word per chapter.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# create plot\n",
                "ggplot(tb, aes(x = Chapters, y = Frequency, group = 1)) + \n",
                "  geom_smooth(color = \"orange\") +\n",
                "  geom_line(color = \"purple\") +         \n",
                "  guides(color=guide_legend(override.aes=list(fill=NA))) +\n",
                "  theme_bw() +\n",
                "  theme(axis.text.x = element_text(angle = 45, hjust = 1))+\n",
                "  scale_y_continuous(name =\"Relative Frequency (per 1,000 words)\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "As we did above, we use the `ggsave` function to save the image in the `MyOutput` folder.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# save network graph for MyOutput folder\n",
                "ggsave(here::here(\"notebooks/MyOutput/image_04.png\"))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<div class=\"warning\" style='padding:0.1em; background-color: rgba(215,209,204,.3); color:#51247a'>\n",
                "<span>\n",
                "<p style='margin-top:1em; text-align:center'>\n",
                "<b>You will find the image-file named *image_04.png* in the `MyOutput` folder (located on the left side of the screen).<\/b> <br><br>Simply double-click the `MyOutput` folder icon, then right-click on the *image_04.png* file, and choose Download from the dropdown menu to download the file. <br>\n",
                "<\/p>\n",
                "<p style='margin-left:1em;'>\n",
                "<\/p><\/span>\n",
                "<\/div>\n",
                "\n",
                "<br>\n",
                "\n",
                "## Dispersion plots\n",
                "\n",
                "To show when in a text or in a collection of texts certain terms occur, we can use *dispersion plots*. The `quanteda` package offers a very easy-to-use function `textplot_xray` to generate dispersion plots.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# add chapter names\n",
                "names(text_chapters) <- Chapters\n",
                "# generate corpus from chapters\n",
                "text_corpus <- quanteda::corpus(text_chapters)\n",
                "# generate dispersion plots\n",
                "quanteda.textplots::textplot_xray(kwic(tokens(text_corpus), pattern = \"alice\"),\n",
                "                                  kwic(tokens(text_corpus), pattern = \"hatter\"),\n",
                "                                  sort = T) + \n",
                "  # add color\n",
                "  aes(color = keyword) + \n",
                "  # define color\n",
                "  scale_color_manual(values = c('orange', 'darkgrey')) +\n",
                "  # suppress legend\n",
                "  theme(legend.position = \"none\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "As we did above, we use the `ggsave` function to save the image in the `MyOutput` folder.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# save network graph for MyOutput folder\n",
                "ggsave(here::here(\"notebooks/MyOutput/image_05.png\"))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<div class=\"warning\" style='padding:0.1em; background-color: rgba(215,209,204,.3); color:#51247a'>\n",
                "<span>\n",
                "<p style='margin-top:1em; text-align:center'>\n",
                "<b>You will find the image-file named *image_05.png* in the `MyOutput` folder (located on the left side of the screen).<\/b> <br><br>Simply double-click the `MyOutput` folder icon, then right-click on the *image_05.png* file, and choose Download from the dropdown menu to download the file. <br>\n",
                "<\/p>\n",
                "<p style='margin-left:1em;'>\n",
                "<\/p><\/span>\n",
                "<\/div>\n",
                "\n",
                "<br>\n",
                "\n",
                "## Collocations\n",
                "\n",
                "Collocation refers to the co-occurrence of words. A typical example of a collocation is *Merry Christmas* because the words merry and Christmas occur together more frequently together than would be expected by chance, if words were just randomly stringed together.\n",
                "\n",
                "Collocations are fundamental in text analysis and many other research areas working with language data. Unfortunately, words that collocate do not have to be immediately adjacent but can also encompass several slots which makes it harder to retrieve of collocates.\n",
                "\n",
                "There are various techniques for identifying collocations. To identify collocations without having a pre-defined target term, we can use the `textstat_collocations` function from the `quanteda.textstats` package.\n",
                "\n",
                "However, before we can apply that function and start identifying collocations, we need to process the data to which we want to apply this function. In the present case, we will apply that function to the sentences  in the example text  which we extract in the code chunk below.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "text_sentences <- text %>%\n",
                "  tolower() %>%\n",
                "  paste0(collapse= \" \") %>%\n",
                "  stringr::str_split(fixed(\".\")) %>%\n",
                "  unlist() %>%\n",
                "  tm::removePunctuation() %>%\n",
                "  stringr::str_squish()\n",
                "# inspect data\n",
                "text_sentences %>%\n",
                "  head() \n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "From the output shown above, we also see that splitting texts simply by full stops is not optimal as it produces some unwarranted artifacts like the \"sentences\" that consist of single characters. Fortunately, these errors do not really matter in the case of our example.\n",
                "\n",
                "Now that we have split the example text  into sentences, we can tokenize these sentences and apply the `textstat_collocations` function which identifies collocations.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# create a token object\n",
                "text_tokens <- tokens(text_sentences, remove_punct = TRUE) %>%\n",
                "  tokens_remove(stopwords(\"english\"))\n",
                "# extract collocations\n",
                "text_coll <- textstat_collocations(text_tokens, size = 2, min_count = 20)\n",
                "# inspect data\n",
                "text_coll %>%\n",
                "  head() \n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The resulting table shows collocations  in the example text  descending by collocation strength.\n",
                "\n",
                "## Visualizing Collocation Networks\n",
                "\n",
                "Network graphs are a very useful and flexible tool for visualizing relationships between elements such as words, personas, or authors. This section shows how to generate a network graph for collocations of the term *alice* using the `quanteda` package.\n",
                "\n",
                "In a first step, we generate a document-feature matrix based on the sentences  in the example text. A document-feature matrix shows how often elements (here these elements are the words that occur in the *Origin*) occur in a selection of documents (here these documents are the sentences in the example text).\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# create document-feature matrix\n",
                "text_dfm <- tokens(text_sentences, remove_punct = TRUE) %>%\n",
                "  quanteda::dfm() %>%\n",
                "  quanteda::dfm_remove(stopwords('english')) %>%\n",
                "  quanteda::dfm_trim(min_termfreq = 10, verbose = FALSE)\n",
                "# inspect data\n",
                "text_dfm[1:6, 1:6]\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We now reduce the document-feature matrix to contain only the top 20 collocates of *alice* (plus our target word *alice*).\n",
                "\n",
                "To do this, we use the `calculateCoocStatistics` function to determine which words most strongly collocate with our target term (organism).\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# load function for co-occurrence calculation\n",
                "source(\"https://slcladal.github.io/rscripts/calculateCoocStatistics.R\")\n",
                "# define term\n",
                "coocTerm <- \"alice\"\n",
                "# calculate co-occurrence statistics\n",
                "coocs <- calculateCoocStatistics(coocTerm, text_dfm, measure=\"LOGLIK\")\n",
                "# inspect results\n",
                "coocs[1:20]\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "In addition to selecting only the 20 strongest collocates of *alice*, we transform the document-feature matrix into a feature-co-occurrence matrix as shown below. A feature-co-occurrence matrix shows how often each element in that matrix co-occurs with every other element in that matrix.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "redux_dfm <- dfm_select(text_dfm, \n",
                "                        pattern = c(names(coocs)[1:20], \"alice\"))\n",
                "tag_fcm <- fcm(redux_dfm)\n",
                "# inspect data\n",
                "tag_fcm[1:6, 1:6] \n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Using the feature-co-occurrence matrix, we can generate the network graph which shows the terms that collocate with the target term *alice* with the edges representing the co-occurrence frequency. To generate this network graph, we use the `textplot_network` function from the `quanteda.textplots` package.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# generate network graph\n",
                "textplot_network(tag_fcm, \n",
                "                 min_freq = 1, \n",
                "                 edge_alpha = 0.1, \n",
                "                 edge_size = 5,\n",
                "                 edge_color = \"purple\",\n",
                "                 vertex_labelsize = log(rowSums(tag_fcm))*2)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "As we did above, we use the `ggsave` function to save the image in the `MyOutput` folder.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# save network graph for MyOutput folder\n",
                "ggsave(here::here(\"notebooks/MyOutput/image_06.png\"), bg = \"white\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<div class=\"warning\" style='padding:0.1em; background-color: rgba(215,209,204,.3); color:#51247a'>\n",
                "<span>\n",
                "<p style='margin-top:1em; text-align:center'>\n",
                "<b>You will find the image-file named *image_06.png* in the `MyOutput` folder (located on the left side of the screen).<\/b> <br><br>Simply double-click the `MyOutput` folder icon, then right-click on the *image_06.png* file, and choose Download from the dropdown menu to download the file. <br>\n",
                "<\/p>\n",
                "<p style='margin-left:1em;'>\n",
                "<\/p><\/span>\n",
                "<\/div>\n",
                "\n",
                "<br>\n",
                "\n",
                "\n",
                "## Keyness\n",
                "\n",
                "Another common method that can be used for automated text summarization is keyword extraction. Keyword extraction builds on identifying words that are particularly associated  with a certain text. In other words, keyness analysis aims to identify words that are particularly indicative of the content of a certain text.\n",
                "\n",
                "Below, we identify key words for Charles Darwin's *Origin*, Herman Melville's *Moby Dick*, and George Orwell's *1984*. We start by creating a weighted document feature matrix from the corpus containing the three texts. \n",
                "\n",
                "In order to create a corpus, we use the text objects that consist out of many different elements rather than the objects which contained the collapsed texts that we used above. Thus, in a first step, we create a corpus of the texts.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "corp_dom <- quanteda::corpus(c(darwin_sep, orwell_sep, melville_sep)) \n",
                "attr(corp_dom, \"docvars\")$Author = c(rep(\"Darwin\", length(darwin_sep)), \n",
                "                                     rep(\"Orwell\", length(orwell_sep)),\n",
                "                                     rep(\"Melville\", length(melville_sep)))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Next, we generate the document feature matrix and we clean it by removing stopwords and selected other words. In addition, we group the documents feature matrix by author.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "dfm_authors <- corp_dom %>%\n",
                "  quanteda::tokens(remove_punct = TRUE) %>%\n",
                "  quanteda::tokens_remove(quanteda::stopwords(\"english\")) %>%\n",
                "  quanteda::tokens_remove(c(\"now\", \"one\", \"like\", \"may\", \"can\")) %>%\n",
                "  quanteda::dfm() %>%\n",
                "  quanteda::dfm_group(groups = Author) %>%\n",
                "  quanteda::dfm_weight(scheme = \"prop\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "In a next step, we use the `textstat_frequency` function from the `quanteda` package to extract the most frequent non-stopwords in the three texts.\n",
                " \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Calculate relative frequency by president\n",
                "freq_weight <- quanteda.textstats::textstat_frequency(dfm_authors, \n",
                "                                                      n = 10,\n",
                "                                                      groups = dfm_authors$Author)\n",
                "# inspect data\n",
                "freq_weight %>%\n",
                "  as.data.frame() %>%\n",
                "  head()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now, we can simply plot the most common words and most indicative non-stop words in the three texts.\n",
                " \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "ggplot(freq_weight, aes(nrow(freq_weight):1, frequency)) +\n",
                "     geom_point() +\n",
                "     facet_wrap(~ group, scales = \"free\") +\n",
                "     coord_flip() +\n",
                "     scale_x_continuous(breaks = nrow(freq_weight):1,\n",
                "                        labels = freq_weight$feature) +\n",
                "     labs(x = NULL, y = \"Relative frequency\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "As we did above, we use the `ggsave` function to save the image in the `MyOutput` folder.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# save network graph for MyOutput folder\n",
                "ggsave(here::here(\"notebooks/MyOutput/image_07.png\"), bg = \"white\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<div class=\"warning\" style='padding:0.1em; background-color: rgba(215,209,204,.3); color:#51247a'>\n",
                "<span>\n",
                "<p style='margin-top:1em; text-align:center'>\n",
                "<b>You will find the image-file named *image_07.png* in the `MyOutput` folder (located on the left side of the screen).<\/b> <br><br>Simply double-click the `MyOutput` folder icon, then right-click on the *image_07.png* file, and choose Download from the dropdown menu to download the file. <br>\n",
                "<\/p>\n",
                "<p style='margin-left:1em;'>\n",
                "<\/p><\/span>\n",
                "<\/div>\n",
                "\n",
                "<br>\n",
                "\n",
                "## Part-of-Speech tagging\n",
                "\n",
                "A very common procedure to add information to texts is to part-of-speech tag the data, which means to determine to what type of word a specific word belongs. Below, we will add pos-tags to a short English text.\n",
                "\n",
                "We start by loading  a text\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# load text\n",
                "text  <- base::readRDS(url(\"https://slcladal.github.io/data/orwell.rda\", \"rb\")) %>%\n",
                "  .[1:10] %>%\n",
                "  paste0(collapse = \" \")\n",
                "# inspect\n",
                "substr(text, 1, 200)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now that we have a text that we can work with, we will download a pre-trained language model.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# download language model\n",
                "m_eng <- udpipe::udpipe_download_model(language = \"english-ewt\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "If you have downloaded a model it will by default be stored in the main directory.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# load language model from your computer after you have downloaded it once\n",
                "m_eng <-udpipe_load_model(\"english-ewt-ud-2.5-191206.udpipe\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We can now use the model to annotate out text.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# tokenise, tag, dependency parsing\n",
                "text_anndf <- udpipe::udpipe_annotate(m_eng, x = text) %>%\n",
                "  as.data.frame() %>%\n",
                "  dplyr::select(-sentence)\n",
                "# inspect\n",
                "head(text_anndf, 10)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "It can be useful to extract only the words and their pos-tags and convert them back into a text format (rather than a tabular format). \n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "tagged_text <- paste(text_anndf$token, \"/\", text_anndf$xpos, collapse = \" \", sep = \"\")\n",
                "# inspect tagged text\n",
                "substr(tagged_text, 1, 200)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We could use the pos-tagged data to study differences in the distribution of word classes across different registers. or to find certain syntactic patterns in a collection of texts. \n",
                "\n",
                "## Names Entity Recognition \n",
                "\n",
                "Named Entity Recognition (NER) (also referred to as *named entity extraction* or simply as *entity extraction*) is a text analytic method which allows us to automatically identify or extract named entities from text(s) such as persons, locations, brands, etc. \n",
                "\n",
                "As such, NER is a process during which textual elements which have characteristics that are common to proper nouns (locations, people, organizations, etc.) rather than other parts of speech, e.g. non-sentence initial capitalization, are extracted from texts. Retrieving entities is common in automated summarization and in Topic Modeling. NER can be achieved by simple feature extraction (e.g. extract all non-sentence initial capitalized words) or with the help of training sets. Using training sets, i.e. texts that are annotated for entities and non-entities, achieves better results when dealing with unknown data and data with inconsistent capitalization.\n",
                "\n",
                "Here, we will make use of the results obtained form the pos-tagging and simply extract terms that have been tagged as \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# tokenise, tag, dependency parsing\n",
                "ner_df <- text_anndf %>%\n",
                "  dplyr::filter(upos == \"PROPN\") %>%\n",
                "  dplyr::select(token_id, token, lemma, upos, feats)\n",
                "# inspect\n",
                "head(ner_df)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The results would now have to be processed further and could be categorized into person, location, dates, entities, for example. However, this should already give you an idea and get you started.\n",
                "\n",
                "## Dependency Parsing Using UDPipe\n",
                "\n",
                "In addition to pos-tagging, we can also generate plots showing the syntactic dependencies of the different constituents of a sentence. For this, we generate an object that contains a sentence (in this case, the sentence *John gave Mary a kiss*), and we then plot (or visualize) the dependencies using the `textplot_dependencyparser` function.  \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# parse text\n",
                "sent <- udpipe::udpipe_annotate(m_eng, x = \"John gave Mary a kiss.\") %>%\n",
                "  as.data.frame()\n",
                "# inspect\n",
                "head(sent)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We now generate the plot.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# generate dependency plot\n",
                "dplot <- textplot::textplot_dependencyparser(sent, size = 3) \n",
                "# show plot\n",
                "dplot\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "As we did above, we use the `ggsave` function to save the image in the `MyOutput` folder.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# save network graph for MyOutput folder\n",
                "ggsave(here::here(\"notebooks/MyOutput/image_08.png\"), bg = \"white\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<div class=\"warning\" style='padding:0.1em; background-color: rgba(215,209,204,.3); color:#51247a'>\n",
                "<span>\n",
                "<p style='margin-top:1em; text-align:center'>\n",
                "<b>You will find the image-file named *image_08.png* in the `MyOutput` folder (located on the left side of the screen).<\/b> <br><br>Simply double-click the `MyOutput` folder icon, then right-click on the *image_08.png* file, and choose Download from the dropdown menu to download the file. <br>\n",
                "<\/p>\n",
                "<p style='margin-left:1em;'>\n",
                "<\/p><\/span>\n",
                "<\/div>\n",
                "\n",
                "<br>\n",
                "\n",
                "Dependency parsing cane be used, e.g. to study who is the agent versus who is the patient of certain actions such as crimes or other activities.\n",
                "\n",
                "[Back to LADAL](https://ladal.edu.au)\n"
            ]
        }
    ],
    "metadata": {
        "anaconda-cloud": "",
        "kernelspec": {
            "display_name": "R",
            "langauge": "R",
            "name": "ir"
        },
        "language_info": {
            "codemirror_mode": "r",
            "file_extension": ".r",
            "mimetype": "text/x-r-source",
            "name": "R",
            "pygments_lexer": "r",
            "version": "3.4.1"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}
