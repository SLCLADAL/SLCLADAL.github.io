{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<!--html_preserve-->\n",
                "<!-- Global site tag (gtag.js) - Google Analytics -->\n",
                "<script async src=\"https://www.googletagmanager.com/gtag/js?id=UA-130562131-1\"><\/script>\n",
                "<script>\n",
                "  window.dataLayer = window.dataLayer || [];\n",
                "  function gtag(){dataLayer.push(arguments);}\n",
                "  gtag('js', new Date());\n",
                "\n",
                "  gtag('config', 'UA-130562131-1');\n",
                "<\/script>\n",
                "<!--/html_preserve-->\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "knitr::include_graphics(\"https://slcladal.github.io/images/uq1.jpg\")\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Introduction{-}\n",
                "\n",
                "This tutorial introduces Text Analysis [see @bernard1998text; @kabanoff1997introduction; @popping2000computer], i.e. computer-based analysis of language data or the (semi-)automated extraction of information from text. While Text Analysis encompasses a wide range of methods, this tutorial will only serve as a very basic introduction that highlights selected, commonly used techniques. The entire code for the sections below can be downloaded [here](https://slcladal.github.io/textanalysis.Rmd). \n",
                "\n",
                "Since Text Analysis extracts and analyses information from language data, it can be considered a derivative of computational linguistics or an application of *Natural Language Processing* (NLP) to HASS research. As such, Text Analysis represents the application of computational methods in the humanities. The advantage of Text Analysis over manual or traditional techniques (close reading) lies in the fact that Text Analysis allows the extraction of information from large sets of textual data and in a replicable manner. Other terms that are more or less synonymous with Text Analysis are Text Mining, Text Analytics, and Distant Reading. In some cases, Text Analysis is considered more qualitative while Text Analytics is considered to be quantitative. This distinction is not taken up here as Text Analysis, while allowing for qualitative analysis, builds upon quantitative information, i.e. information about frequencies or conditional probabilities. \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "knitr::include_graphics(\"https://slcladal.github.io/images/GoogleNgram.png\")\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Distant Reading is a cover term for applications of Text Analysis that allow to investigate literary and cultural trends using text data. Distant Reading contrasts with close reading, i.e. reading texts in the traditional sense whereas Distant Reading refers to the analysis of large amounts of text. Text Analysis and distant reading are similar with respect to the methods that are used but different with respect to their outlook. The outlook of distant reading is to extract information from text without close reading, i.e. reading the document(s) itself but rather focusing on emerging patterns in the language that is used. \n",
                "\n",
                "Text Analysis or Distant Reading are rapidly growing in use and gaining popularity in the humanities because textual data is readily available and because computational methods can be applied to a huge variety of research questions. The attractiveness of computational text analysis is thus based on the availability of (large amounts of) digitally available texts and in their capability to provide insights that cannot be derived from close reading techniques. \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "knitr::include_graphics(\"https://slcladal.github.io/images/romeonet.png\")\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "While rapidly growing as a valid approach to analyzing textual data, Text Analysis is [critizised](https://www.chronicle.com/article/The-Digital-Humanities-Debacle/245986)  for lack of \"quantitative rigor and because its findings are either banal or, if interesting, not statistically robust (see [here](https://www.chronicle.com/article/The-Digital-Humanities-Debacle/245986). This criticism is correct in that most of the analysis that performed in *Computational Literary Studies* (CLS) are not yet as rigorous as analyses in fields that have a longer history of computational based, quantitative research, such as, for instance, corpus linguistics. However, the practices and methods used in CLS will be refined, adapted and show a rapid increase in quality if more research is devoted to these approaches. Also, Text Analysis simply offers an alternative way to analyze texts that is not in competition to traditional techniques but rather complements them.\n",
                "\n",
                "Given it relatively recent emergence, so far, most of the applications of Text Analysis are based upon a relatively limited number of key procedures or concepts (e.g. concordancing, word frequencies, annotation or tagging, parsing, collocation, text classification, Sentiment Analysis, Entity Extraction, Topic Modeling, etc.). In the following, we will explore these procedures and introduce some basic tools that help you perform the introduced tasks. \n",
                "\n",
                "**Text Analysis at UQ**\n",
                "\n",
                "The [UQ Library](https://www.library.uq.edu.au/) offers a very handy and attractive summary of [resources, concepts, and tools](https://guides.library.uq.edu.au/research-techniques/text-mining-analysis/introduction) that can be used by researchers interested in Text Analysis and Distant Reading. Also, the UQ library site offers short video introductions and addresses issues that are not discussed here such as [copyright issues](https://guides.library.uq.edu.au/research-techniques/text-mining-analysis/considerations), [data sources available at the UQ library](https://guides.library.uq.edu.au/research-techniques/text-mining-analysis/sources-of-text-data), as well as [social media](https://guides.library.uq.edu.au/research-techniques/text-mining-analysis/sources-of-text-data) and [web scaping](https://guides.library.uq.edu.au/research-techniques/text-mining-analysis/web-scraping).\n",
                "\n",
                "In contrast to the UQ library site, the focus of this introduction lies on the practical how-to of text analysis. this means that the following concentrates on how to perform analyses rather than discussing their underlying concepts or evaluating their scientific merits. \n",
                "\n",
                "**Tools versus Scripts**\n",
                "\n",
                "It is perfectly fine to use tools for the analyses exemplified below. However, the aim here is not primarily to show how to perform text analyses but how to perform text analyses in a way that complies with practices that guarantee sustainable, transparent, reproducible research. As R code can be readily shared and optimally contains all the data extraction, processing, visualization, and analysis steps, using scripts is preferable over using (commercial) software. \n",
                "\n",
                "In addition to being not as transparent and hindering reproduction of research, using tools can also lead to dependencies on third parties which does not arise when using open source software. \n",
                "\n",
                "Finally, the widespread use of R particularly among data scientists, engineers, and analysts reduces the risk of software errors as a very active community corrects flawed functions typically quite rapidly. \n",
                "\n",
                "**Preparation and session set up**\n",
                "\n",
                "\n",
                "This tutorial is based on R. If you have not installed R or are new to it, you will find an introduction to and more information how to use R [here](https://slcladal.github.io/intror.html). For this tutorials, we need to install certain *packages* from an R *library* so that the scripts shown below are executed without errors. Before turning to the code below, please install the packages by running the code below this paragraph. If you have already installed the packages mentioned below, then you can skip ahead and ignore this section. To install the necessary packages, simply run the following code - it may take some time (between 1 and 5 minutes to install all of the packages so you do not need to worry if it takes some time).\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# install packages\n",
                "install.packages(\"DT\")\n",
                "install.packages(\"knitr\")\n",
                "install.packages(\"kableExtra\")\n",
                "install.packages(\"quanteda\")\n",
                "install.packages(\"tidyverse\")\n",
                "install.packages(\"tm\")\n",
                "install.packages(\"tidytext\")\n",
                "install.packages(\"wordcloud2\")\n",
                "install.packages(\"scales\")\n",
                "install.packages(\"quanteda.textstats\")\n",
                "install.packages(\"quanteda.textplots\")\n",
                "install.packages(\"tidyr\")\n",
                "install.packages(\"cluster\")\n",
                "install.packages(\"class\")\n",
                "install.packages(\"NLP\")\n",
                "install.packages(\"openNLP\")\n",
                "install.packages(\"openNLPdata\")\n",
                "install.packages(\"pacman\")\n",
                "# install klippy for copy-to-clipboard button in code chunks\n",
                "remotes::install_github(\"rlesur/klippy\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now that we have installed the packages, we can activate them as shown below.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# set options\n",
                "options(stringsAsFactors = F)\n",
                "options(scipen = 999)\n",
                "options(max.print=1000)\n",
                "# load packages\n",
                "library(tidyverse)\n",
                "library(flextable)\n",
                "library(quanteda)\n",
                "library(tm)\n",
                "library(tidytext)\n",
                "library(wordcloud2)\n",
                "library(scales)\n",
                "library(quanteda.textstats)\n",
                "library(quanteda.textplots)\n",
                "library(tidyr)\n",
                "library(cluster)\n",
                "library(class)\n",
                "library(NLP)\n",
                "library(openNLP)\n",
                "library(openNLPdata)\n",
                "library(pacman)\n",
                "pacman::p_load_gh(\"trinker/entity\")\n",
                "# activate klippy for copy-to-clipboard button\n",
                "klippy::klippy()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Once you have installed R and RStudio and once you have also initiated the session by executing the code shown above, you are good to go.\n",
                "\n",
                "# Concordancing\n",
                "\n",
                "In Text Analysis, concordancing refers to the extraction of words from a given text or texts [@lindquist2009corpus]. Commonly, concordances are displayed in the form of key-word in contexts (KWIC) where the search term is shown with some preceding and following context. Thus, such displays are referred to as key word in context concordances. A more elaborate tutorial on how to perform concordancing with R is available [here](https://slcladal.github.io/kwics.html).\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "knitr::include_graphics(\"https://slcladal.github.io/images/AntConcConcordance.png\")\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Concordancing is helpful for seeing how the term is used in the data, for inspecting how often a given word occurs in a text or a collection of texts, for extracting examples, and it also represents a basic procedure and often the first step in more sophisticated analyses of language data. \n",
                "\n",
                "In the following, we will use R to create KWICs displays of the term *organism* and the phrase *natural selection* using Charles Darwin's *On the origin of species by means of natural selection*. \n",
                "\n",
                "We begin by loading the `quanteda` package that we will use for generating the KWICs and the `tidyverse` package that we use to process the data. In addition, we load the data which represents the text of Charles Darwin's *On the Origin of Species*. For the present tutorial, we load data that is available on the LADAL GitHUb repository. If you want to know how to load your own data, have a look at [this tutorial](https://slcladal.github.io/intror.html#Working_with_text). \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# load text\n",
                "darwin <- base::readRDS(url(\"https://slcladal.github.io/data/darwin.rda\", \"rb\"))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# inspect data\n",
                "darwin %>%\n",
                "  as.data.frame() %>%\n",
                "  head(10) %>%\n",
                "  flextable() %>%\n",
                "  flextable::set_table_properties(width = .75, layout = \"autofit\") %>%\n",
                "  flextable::theme_zebra() %>%\n",
                "  flextable::fontsize(size = 12) %>%\n",
                "  flextable::fontsize(size = 12, part = \"header\") %>%\n",
                "  flextable::align_text_col(align = \"center\") %>%\n",
                "  flextable::set_caption(caption = \"First 10 text elements of Charles Darwin's Origin\")  %>%\n",
                "  flextable::border_outer()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The data still consists of short text snippets which is why we collapse these snippets and then split the collapsed data into chapters. \n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# combine and split into chapters\n",
                "darwin_chapters <- darwin %>%\n",
                "  # paste all texts together into one long text\n",
                "  paste0(collapse = \" \") %>%\n",
                "  # replace Chapter I to Chapter XVI with qwertz \n",
                "  stringr::str_replace_all(\"(CHAPTER [XVI]{1,7}\\\\.{0,1}) \", \"qwertz\\\\1\") %>%\n",
                "  # convert text to lower case\n",
                "  tolower() %>%  \n",
                "  # split the long text into chapters\n",
                "  stringr::str_split(\"qwertz\") %>%\n",
                "  # unlist the result (convert into simple vector)\n",
                "  unlist()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# inspect data\n",
                "darwin_chapters %>%\n",
                "  substr(start=1, stop=500) %>%\n",
                "  as.data.frame() %>%\n",
                "  head(5) %>%\n",
                "  flextable() %>%\n",
                "  flextable::set_table_properties(width = .95, layout = \"autofit\") %>%\n",
                "  flextable::theme_zebra() %>%\n",
                "  flextable::fontsize(size = 12) %>%\n",
                "  flextable::fontsize(size = 12, part = \"header\") %>%\n",
                "  flextable::align_text_col(align = \"center\") %>%\n",
                "  flextable::set_caption(caption = \"First 500 characters of the first 5 chapters of Charles Darwin's Origin\")  %>%\n",
                "  flextable::border_outer()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Once we have split the data into chapters, we perform the concordancing and extract the KWICs. To create these kwics, we use the `kwic` function from the `quanteda` package. This function takes the data (x), the search pattern (pattern), and the window size as its main arguments. \n",
                "\n",
                "To start with, we generate kwics for the term *organism* as shown below. \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# create kwic\n",
                "kwic_o <- quanteda::kwic(x = darwin_chapters, # define text(s) \n",
                "                         # define pattern\n",
                "                          pattern = \"organism\",\n",
                "                         # define window size\n",
                "                          window = 5) %>%\n",
                "  # convert into a data frame\n",
                "  as.data.frame() %>%\n",
                "  # remove superfluous columns\n",
                "  dplyr::select(-to, -from, -pattern)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# inspect data\n",
                "kwic_o %>%\n",
                "  as.data.frame() %>%\n",
                "  head(10) %>%\n",
                "  flextable() %>%\n",
                "  flextable::set_table_properties(width = .95, layout = \"autofit\") %>%\n",
                "  flextable::theme_zebra() %>%\n",
                "  flextable::fontsize(size = 12) %>%\n",
                "  flextable::fontsize(size = 12, part = \"header\") %>%\n",
                "  flextable::align_text_col(align = \"center\") %>%\n",
                "  flextable::set_caption(caption = \"First 10 concordances of organism in Charles Darwin's Origin\")  %>%\n",
                "  flextable::border_outer()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We can also use regular expressions in our search to extract not only *organism* but also *organisms* and *organic*. When using a regular expression in the `pattern` argument, we need to specify the `valuetype` as `regex` (as shown below).\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# create kwic\n",
                "kwic_os <- quanteda::kwic(x = darwin_chapters, \n",
                "                          pattern = \"organi.*\",\n",
                "                          window = 5,\n",
                "                          valuetype = \"regex\") %>%\n",
                "  # convert into a data frame\n",
                "  as.data.frame() %>%\n",
                "  # remove superfluous columns\n",
                "  dplyr::select(-to, -from, -pattern)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# inspect data\n",
                "kwic_o %>%\n",
                "  as.data.frame() %>%\n",
                "  head(10) %>%\n",
                "  flextable() %>%\n",
                "  flextable::set_table_properties(width = .95, layout = \"autofit\") %>%\n",
                "  flextable::theme_zebra() %>%\n",
                "  flextable::fontsize(size = 12) %>%\n",
                "  flextable::fontsize(size = 12, part = \"header\") %>%\n",
                "  flextable::align_text_col(align = \"center\") %>%\n",
                "  flextable::set_caption(caption = \"First 10 cleaned Concordances of organism in Charles Darwin's Origin\")  %>%\n",
                "  flextable::border_outer()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "When search for expressions that represent phrase and that consists out of several elements such as *natural selection*, we also need to specify that we are looking for a phrase in the pattern argument. \n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# create kwic\n",
                "kwic_ns <- quanteda::kwic(x = darwin_chapters, \n",
                "                          pattern = quanteda::phrase(\"natural selection\"),\n",
                "                          window = 5) %>%\n",
                "  # convert into a data frame\n",
                "  as.data.frame() %>%\n",
                "  # remove superfluous columns\n",
                "  dplyr::select(-to, -from, -pattern)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# inspect data\n",
                "kwic_ns %>%\n",
                "  as.data.frame() %>%\n",
                "  head(10) %>%\n",
                "  flextable() %>%\n",
                "  flextable::set_table_properties(width = .95, layout = \"autofit\") %>%\n",
                "  flextable::theme_zebra() %>%\n",
                "  flextable::fontsize(size = 12) %>%\n",
                "  flextable::fontsize(size = 12, part = \"header\") %>%\n",
                "  flextable::align_text_col(align = \"center\") %>%\n",
                "  flextable::set_caption(caption = \"First 10 cleaned Concordances of the phrase natural selection in Charles Darwin's Origin\")  %>%\n",
                "  flextable::border_outer()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We could now continue and analyze how Darwin used the phrase *natural selection* or we could go about investigating how Darwin has used the term *organism*.\n",
                "\n",
                "# Word Frequency\n",
                "\n",
                "Almost all methods used in text analytics rely on frequency information. Thus, fending out out frequent words are in a text is a fundamental technique in text analytics. In fact, frequency information lies at the very core of Text Analysis. Such frequency information often comes in the form of word frequency lists, i.e. lists of word forms and their frequency in a given text or collection of texts.  \n",
                "\n",
                "As extracting word frequency lists is very important, we will now We will now extract a frequency list from a corpus.\n",
                "\n",
                "In a first step, we load a corpus, convert everything to lower case, remove non-word symbols (including punctuation), and split the corpus data into individual words.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# load and process corpus\n",
                "darwin_words <- darwin  %>%\n",
                "  # convert everything to lower case\n",
                "  tolower() %>%\n",
                "  # remove non-word characters\n",
                "  str_replace_all(\"[^[:alpha:][:space:]]*\", \"\")  %>%\n",
                "  tm::removePunctuation() %>%\n",
                "  stringr::str_squish() %>%\n",
                "  stringr::str_split(\" \") %>%\n",
                "  unlist()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# inspect data\n",
                "darwin_words %>%\n",
                "  as.data.frame() %>%\n",
                "  head(15) %>%\n",
                "  flextable() %>%\n",
                "  flextable::set_table_properties(width = .5, layout = \"autofit\") %>%\n",
                "  flextable::theme_zebra() %>%\n",
                "  flextable::fontsize(size = 12) %>%\n",
                "  flextable::fontsize(size = 12, part = \"header\") %>%\n",
                "  flextable::align_text_col(align = \"center\") %>%\n",
                "  flextable::set_caption(caption = \"First 15 words in Charles Darwin's Origin\")  %>%\n",
                "  flextable::border_outer()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now that we have a vector of words, we can easily create a table representing a word frequency list (as shown below).\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# create table\n",
                "wfreq <- darwin_words %>%\n",
                "  table() %>%\n",
                "  as.data.frame() %>%\n",
                "  arrange(desc(Freq)) %>%\n",
                "  dplyr::rename(word = 1,\n",
                "                frequency = 2)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# inspect data\n",
                "wfreq %>%\n",
                "  as.data.frame() %>%\n",
                "  head(15) %>%\n",
                "  flextable() %>%\n",
                "  flextable::set_table_properties(width = .5, layout = \"autofit\") %>%\n",
                "  flextable::theme_zebra() %>%\n",
                "  flextable::fontsize(size = 12) %>%\n",
                "  flextable::fontsize(size = 12, part = \"header\") %>%\n",
                "  flextable::align_text_col(align = \"center\") %>%\n",
                "  flextable::set_caption(caption = \"Top 15 words in Charles Darwin's Origin by frequency.\")  %>%\n",
                "  flextable::border_outer()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The most frequent words are all function words which are often not meaningful or useful for an analysis. Thus, we now remove these function words (also called *stopwords*) from the frequency list and inspect the list without stopwords.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# create table wo stopwords\n",
                "wfreq_wostop <- wfreq %>%\n",
                "  anti_join(stop_words, by = \"word\") %>%\n",
                "  dplyr::filter(word != \"\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# inspect data\n",
                "wfreq_wostop %>%\n",
                "  as.data.frame() %>%\n",
                "  head(15) %>%\n",
                "  flextable() %>%\n",
                "  flextable::set_table_properties(width = .5, layout = \"autofit\") %>%\n",
                "  flextable::theme_zebra() %>%\n",
                "  flextable::fontsize(size = 12) %>%\n",
                "  flextable::fontsize(size = 12, part = \"header\") %>%\n",
                "  flextable::align_text_col(align = \"center\") %>%\n",
                "  flextable::set_caption(caption = \"Top 15 lexical words in Charles Darwin's Origin by frequency.\")  %>%\n",
                "  flextable::border_outer()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Such word frequency lists can be visualized in various ways. The most common way to visualize word frequency lists is in the form of bargraphs.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "wfreq_wostop %>%\n",
                "  head(10) %>%\n",
                "  ggplot(aes(x = reorder(word, -frequency, mean), y = frequency)) +\n",
                "  geom_bar(stat = \"identity\") +\n",
                "  labs(title = \"10 most frequent non-stop words in \\nCharles Darwin's Origin of Species\",\n",
                "       x = \"\") +\n",
                "  theme(axis.text.x = element_text(angle = 45, size = 12, hjust = 1))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Wordclouds{-}\n",
                "\n",
                "Alternatively, word frequency lists can be visualized, although less informative, as word clouds. \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# create wordcloud\n",
                "wordcloud2(wfreq_wostop[1:100,],\n",
                "           shape = \"diamond\",\n",
                "           color = scales::viridis_pal()(8)\n",
                "          )\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Another variant of word clouds, so-called *comparison clouds*, Word lists can be used to determine differences between texts. For instance, we can load different texts and check whether they differ with respect to word frequencies. To show this, we load Herman Melville's *Moby Dick*, George Orwell's *1984*, and we also use Darwin's *Origin*. \n",
                "\n",
                "In a first step, we load these texts and collapse them into single documents.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# load data\n",
                "orwell_sep <- base::readRDS(url(\"https://slcladal.github.io/data/orwell.rda\", \"rb\"))\n",
                "orwell <- orwell_sep %>%\n",
                "  paste0(collapse = \" \")\n",
                "melville_sep <- base::readRDS(url(\"https://slcladal.github.io/data/melville.rda\", \"rb\"))\n",
                "melville <- melville_sep %>%\n",
                "  paste0(collapse = \" \")\n",
                "darwin_sep <- darwin\n",
                "darwin <- paste0(darwin_sep, collapse = \" \")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now, we generate a corpus object from these texts and create a variable with the author name.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "corp_dom <- quanteda::corpus(c(darwin, orwell, melville)) \n",
                "attr(corp_dom, \"docvars\")$Author = c(\"Darwin\", \"Orwell\", \"Melville\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now, we can remove so-called *stopwords* (non-lexical function words) and punctuation and generate the comparison cloud.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "corp_dom  %>%\n",
                "    quanteda::tokens(remove_punct = TRUE) %>%\n",
                "    quanteda::tokens_remove(stopwords(\"english\")) %>%\n",
                "    quanteda::dfm() %>%\n",
                "    quanteda::dfm_group(groups = corp_dom$Author) %>%\n",
                "    quanteda::dfm_trim(min_termfreq = 200, verbose = FALSE) %>%\n",
                "    quanteda.textplots::textplot_wordcloud(comparison = TRUE, \n",
                "                                 max_words = 100,\n",
                "                                 max_size = 6)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Frequency changes{-}\n",
                "\n",
                "We can also investigate the use of the term *organism* across chapters in Darwin's *Origin*. In a first step, we extract the number of words in each chapter.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# extract number of words per chapter\n",
                "Words <- darwin_chapters %>%\n",
                "  stringr::str_split(\" \")  %>%\n",
                "  lengths()\n",
                "# inspect data\n",
                "Words\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Next, we extract the number of matches in each chapter.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# extract number of matches per chapter\n",
                "Matches <- darwin_chapters %>%\n",
                "  stringr::str_count(\"organism[s]{0,1}\")\n",
                "# inspect the number of matches per chapter\n",
                "Matches\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now, we extract the names of the chapters and create a table with the chapter names and the relative frequency of matches per 1,000 words.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# extract chapters\n",
                "Chapters <- darwin_chapters %>%\n",
                "  stringr::str_replace_all(\"(chapter [xvi]{1,7})\\\\.{0,1} .*\", \"\\\\1\")\n",
                "Chapters <- dplyr::case_when(nchar(Chapters) > 50 ~ \"chapter 0\", TRUE ~ Chapters)\n",
                "Chapters\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# create table of results\n",
                "tb <- data.frame(Chapters, Matches, Words) %>%\n",
                "  dplyr::mutate(Frequency = round(Matches/Words*1000, 2))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# inspect data\n",
                "tb %>%\n",
                "  as.data.frame() %>%\n",
                "  head(15) %>%\n",
                "  flextable() %>%\n",
                "  flextable::set_table_properties(width = .5, layout = \"autofit\") %>%\n",
                "  flextable::theme_zebra() %>%\n",
                "  flextable::fontsize(size = 12) %>%\n",
                "  flextable::fontsize(size = 12, part = \"header\") %>%\n",
                "  flextable::align_text_col(align = \"center\") %>%\n",
                "  flextable::set_caption(caption = \"Words and their (relative) freqeuncy across in Charles Darwin's Origin by frequency.\")  %>%\n",
                "  flextable::border_outer()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We can now visualize the relative frequencies of our search word per chapter.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# create plot\n",
                "ggplot(tb, aes(x = Chapters, y = Frequency, group = 1)) + \n",
                "  geom_smooth(color = \"purple\") +\n",
                "  geom_line(color = \"darkgray\") +         \n",
                "  guides(color=guide_legend(override.aes=list(fill=NA))) +\n",
                "  theme_bw() +\n",
                "  theme(axis.text.x = element_text(angle = 45, hjust = 1))+\n",
                "  scale_y_continuous(name =\"Relative Frequency (per 1,000 words)\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Dispersion plots{-}\n",
                "\n",
                "To show when in a text or in a collection of texts certain terms occur, we can use *dispersion plots*. The `quanteda` package offers a very easy-to-use function `textplot_xray` to generate dispersion plots.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# add chapter names\n",
                "names(darwin_chapters) <- Chapters\n",
                "# generate corpus from chapters\n",
                "darwin_corpus <- quanteda::corpus(darwin_chapters)\n",
                "# generate dispersion plots\n",
                "quanteda.textplots::textplot_xray(kwic(darwin_corpus, pattern = \"organism\"),\n",
                "              kwic(darwin_corpus, pattern = \"selection\"),\n",
                "              sort = T)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Over- and underuse{-}\n",
                "\n",
                "Frequency information can also tell us something about the nature of a text. For instance, private dialogues will typically contain higher rates of second person pronouns compared with more format text types, such as, for instance, scripted monologues like speeches. For this reason, word frequency lists can be used in text classification and to determine the formality of texts.\n",
                "\n",
                "As an example, below you find the number of the second person pronouns *you* and *your* and the number of all words except for these second person pronouns in private dialogues compared with scripted monologues in the Irish component of the *International Corpus of English* (ICE). In addition, the tables shows  the percentage of second person pronouns in both text types to enable seeing  whether private dialogues contain more of these second person pronouns than scripted monologues (i.e. speeches).\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "numbers <- matrix(c(\"you, your\", \"6761\", \"659\", \"Other words\",\t\"259625\",\t\n",
                "                    \"105295\", \"Percent\",\t\"2.60\",\t\"0.63\"), byrow = T, nrow = 3)\n",
                "colnames(numbers) <- c(\"\", \"Private dialogues\", \"Scripted monologues\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# inspect data\n",
                "ndf <- numbers %>%\n",
                "  as.data.frame()\n",
                "colnames(ndf)[1] <- \".\"\n",
                "ndf %>%\n",
                "  flextable() %>%\n",
                "  flextable::set_table_properties(width = .5, layout = \"autofit\") %>%\n",
                "  flextable::theme_zebra() %>%\n",
                "  flextable::fontsize(size = 12) %>%\n",
                "  flextable::fontsize(size = 12, part = \"header\") %>%\n",
                "  flextable::align_text_col(align = \"center\") %>%\n",
                "  flextable::set_caption(caption = \"Use of 2nd person pronouns (and all other words) in ICE Ireland.\")  %>%\n",
                "  flextable::border_outer()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "This simple example shows that second person pronouns make up 2.6 percent of all words that are used in private dialogues while they only amount to 0.63 percent in scripted speeches. A handy way to present such differences visually are association and mosaic plots.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "d <- matrix(c(6761, 659, 259625, 105295), nrow = 2, byrow = T)\n",
                "colnames(d) <- c(\"D\", \"M\")\n",
                "rownames(d) <- c(\"you, your\", \"Other words\")\n",
                "assocplot(d)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Bars above the dashed line indicate relative overuse while bars below the line suggest relative under-use. Therefore, the association plot indicates under-use of *you* and *your* and overuse of *other words* in monologues while the opposite trends holds true for dialogues, i.e. overuse of *you* and *your* and under-use of *Other words*. \n",
                "\n",
                "# N-grams, Collocations, and  Keyness\n",
                "\n",
                "Collocation refers to the co-occurrence of words. A typical example of a collocation is *Merry Christmas* because the words merry and Christmas occur together more frequently together than would be expected by chance, if words were just randomly stringed together.  \n",
                "\n",
                "N-grams are related to collocates in that they represent words that occur together (bi-grams are two words that occur together, tri-grams three words and so on). Fortunately, creating N-gram lists is very easy. We will use the *Origin* to create a bi-gram list. We can simply take each word and combine it with the following word.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# create data frame\n",
                "darwin_bigrams <- data.frame(darwin_words[1:length(darwin_words)-1], \n",
                "                       darwin_words[2:length(darwin_words)]) %>%\n",
                "  dplyr::rename(Word1 = 1,\n",
                "                Word2 = 2) %>%\n",
                "  dplyr::mutate(Bigram = paste0(Word1, \" \", Word2)) %>%\n",
                "  dplyr::group_by(Bigram) %>%\n",
                "  dplyr::summarise(Frequency = n()) %>%\n",
                "  dplyr::arrange(-Frequency)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# inspect data\n",
                "darwin_bigrams %>%\n",
                "  as.data.frame() %>%\n",
                "  head(15) %>%\n",
                "  flextable() %>%\n",
                "  flextable::set_table_properties(width = .5, layout = \"autofit\") %>%\n",
                "  flextable::theme_zebra() %>%\n",
                "  flextable::fontsize(size = 12) %>%\n",
                "  flextable::fontsize(size = 12, part = \"header\") %>%\n",
                "  flextable::align_text_col(align = \"center\") %>%\n",
                "  flextable::set_caption(caption = \"Top 10 most frequent bigrams and their (relative) freqeuncy in Charles Darwin's Origin.\")  %>%\n",
                "  flextable::border_outer()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Both N-grams and collocations are not only an important concept in language teaching but they are also fundamental in Text Analysis and many other research areas working with language data. Unfortunately, words that collocate do not have to be immediately adjacent but can also encompass several slots which makes it harder to retrieve of collocates that are not adjacent- We will find out how to identify non-adjacent collocates in the next section.\n",
                "\n",
                "## Finding collocations{-}\n",
                "\n",
                "There are various techniques for identifying collocations. To identify collocations without having a pre-defined target term, we can use the `textstat_collocations` function from the `quanteda.textstats` package.\n",
                "\n",
                "However, before we can apply that function and start identifying collocations, we need to process the data to which we want to apply this function. In the present case, we will apply that function to the sentences in Charles Darwin's *Origin* which we extract in the code chunk below.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "darwin_sentences <- darwin %>%\n",
                "  tolower() %>%\n",
                "  paste0(collapse= \" \") %>%\n",
                "  stringr::str_split(fixed(\".\")) %>%\n",
                "  unlist() %>%\n",
                "  tm::removePunctuation() %>%\n",
                "  stringr::str_squish()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# inspect data\n",
                "darwin_sentences %>%\n",
                "  as.data.frame() %>%\n",
                "  head(15) %>%\n",
                "  flextable() %>%\n",
                "  flextable::set_table_properties(width = .95, layout = \"autofit\") %>%\n",
                "  flextable::theme_zebra() %>%\n",
                "  flextable::fontsize(size = 12) %>%\n",
                "  flextable::fontsize(size = 12, part = \"header\") %>%\n",
                "  flextable::align_text_col(align = \"center\") %>%\n",
                "  flextable::set_caption(caption = \"First 10 sentences in Charles Darwin's Origin.\")  %>%\n",
                "  flextable::border_outer()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "From the output shown above, we also see that splitting texts simply by full stops is not optimal as it produces some unwarranted artifacts like the \"sentences\" that consist of single characters (due to the name of the H.M.S. Beagle - the ship on which Darwin traveled when he explored the southern hemisphere). Fortunately, these errors do not really matter in the case of our example.\n",
                "\n",
                "Now that we have split Darwin's *Origin* into sentences, we can tokenize these sentences and apply the `textstat_collocations` function which identifies collocations.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# create a token object\n",
                "darwin_tokens <- tokens(darwin_sentences, remove_punct = TRUE) %>%\n",
                "  tokens_remove(stopwords(\"english\"))\n",
                "# extract collocations\n",
                "darwin_coll <- textstat_collocations(darwin_tokens, size = 2, min_count = 20)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# inspect data\n",
                "darwin_coll %>%\n",
                "  as.data.frame() %>%\n",
                "  head(15) %>%\n",
                "  flextable() %>%\n",
                "  flextable::set_table_properties(width = .5, layout = \"autofit\") %>%\n",
                "  flextable::theme_zebra() %>%\n",
                "  flextable::fontsize(size = 12) %>%\n",
                "  flextable::fontsize(size = 12, part = \"header\") %>%\n",
                "  flextable::align_text_col(align = \"center\") %>%\n",
                "  flextable::set_caption(caption = \"Top 10 collocations in Charles Darwin's Origin.\")  %>%\n",
                "  flextable::border_outer()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The resulting table shows collocations in Darwin's *Origin* descending by collocation strength.\n",
                "\n",
                "## Visualizing Collocation Networks{-}\n",
                "\n",
                "Network graphs are a very useful and flexible tool for visualizing relationships between elements such as words, personas, or authors. This section shows how to generate a network graph for collocations of the term *organism* using the `quanteda` package.\n",
                "\n",
                "In a first step, we generate a document-feature matrix based on the sentence sin Charles Darwin's *Origin*. A document-feature matrix shows how often elements (here these elements are the words that occur in the *Origin*) occur in a selection of documents (here these documents are the sentences in the *Origin*).\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# create document-feature matrix\n",
                "darwin_dfm <- darwin_sentences %>% \n",
                "    quanteda::dfm(remove = stopwords('english'), remove_punct = TRUE) %>%\n",
                "    quanteda::dfm_trim(min_termfreq = 10, verbose = FALSE)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# inspect data\n",
                "darwin_dfm[1:6, 1:6] %>%\n",
                "  as.data.frame() %>%\n",
                "  head(15) %>%\n",
                "  flextable() %>%\n",
                "  flextable::set_table_properties(width = .5, layout = \"autofit\") %>%\n",
                "  flextable::theme_zebra() %>%\n",
                "  flextable::fontsize(size = 12) %>%\n",
                "  flextable::fontsize(size = 12, part = \"header\") %>%\n",
                "  flextable::align_text_col(align = \"center\") %>%\n",
                "  flextable::set_caption(caption = \"First 6 rows and columns of the document-feature matrix.\")  %>%\n",
                "  flextable::border_outer()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "As we want to generate a network graph of words that collocate with the term *organism*, we use the `calculateCoocStatistics` function to determine which words most strongly collocate with our target term (*organism*).  \n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# load function for co-occurrence calculation\n",
                "source(\"https://slcladal.github.io/rscripts/calculateCoocStatistics.R\")\n",
                "# define term\n",
                "coocTerm <- \"organism\"\n",
                "# calculate co-occurrence statistics\n",
                "coocs <- calculateCoocStatistics(coocTerm, darwin_dfm, measure=\"LOGLIK\")\n",
                "# inspect results\n",
                "coocs[1:20]\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We now reduce the document-feature matrix to contain only the top 20 collocates of *organism* (plus our target word *organism*).\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "redux_dfm <- dfm_select(darwin_dfm, \n",
                "                        pattern = c(names(coocs)[1:20], \"organism\"))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# inspect data\n",
                "redux_dfm[1:6, 1:6] %>%\n",
                "  as.data.frame() %>%\n",
                "  head(15) %>%\n",
                "  flextable() %>%\n",
                "  flextable::set_table_properties(width = .5, layout = \"autofit\") %>%\n",
                "  flextable::theme_zebra() %>%\n",
                "  flextable::fontsize(size = 12) %>%\n",
                "  flextable::fontsize(size = 12, part = \"header\") %>%\n",
                "  flextable::align_text_col(align = \"center\") %>%\n",
                "  flextable::set_caption(caption = \"First 6 rows and columns of the reduced feature co-occurrence matrix.\")  %>%\n",
                "  flextable::border_outer()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now, we can transform the document-feature matrix into a feature-co-occurrence matrix as shown below. A feature-co-occurrence matrix shows how often each element in that matrix co-occurs with every other element in that matrix.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "tag_fcm <- fcm(redux_dfm)\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# inspect data\n",
                "tag_fcm[1:6, 1:6] %>%\n",
                "  as.data.frame() %>%\n",
                "  head(15) %>%\n",
                "  flextable() %>%\n",
                "  flextable::set_table_properties(width = .5, layout = \"autofit\") %>%\n",
                "  flextable::theme_zebra() %>%\n",
                "  flextable::fontsize(size = 12) %>%\n",
                "  flextable::fontsize(size = 12, part = \"header\") %>%\n",
                "  flextable::align_text_col(align = \"center\") %>%\n",
                "  flextable::set_caption(caption = \"First 6 rows and columns of the feature co-occurrence matrix.\")  %>%\n",
                "  flextable::border_outer()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Using the feature-co-occurrence matrix, we can generate the network graph which shows the terms that collocate with the target term *organism* with the edges representing the co-occurrence frequency. To generate this network graph, we use the `textplot_network` function from the `quanteda.textplots` package.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# generate network graph\n",
                "textplot_network(tag_fcm, \n",
                "                 min_freq = 1, \n",
                "                 edge_alpha = 0.1, \n",
                "                 edge_size = 5,\n",
                "                 edge_color = \"purple\",\n",
                "                 vertex_labelsize = log(rowSums(tag_fcm))*2)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Keyness{-}\n",
                "\n",
                "Another common method that can be used for automated text summarization is keyword extraction. Keyword extraction builds on identifying words that are particularly associated  with a certain text. In other words, keyness analysis aims to identify words that are particularly indicative of the content of a certain text.\n",
                "\n",
                "Below, we identify key words for Charles Darwin's *Origin*, Herman Melville's *Moby Dick*, and George Orwell's *1984*. We start by creating a weighted document feature matrix from the corpus containing the three texts. \n",
                "\n",
                "In order to create a corpus, we use the text objects that consist out of many different elements rather than the objects which contained the collapsed texts that we used above. Thus, in a first step, we create a corpus of the texts.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "corp_dom <- quanteda::corpus(c(darwin_sep, orwell_sep, melville_sep)) \n",
                "attr(corp_dom, \"docvars\")$Author = c(rep(\"Darwin\", length(darwin_sep)), \n",
                "                                     rep(\"Orwell\", length(orwell_sep)),\n",
                "                                     rep(\"Melville\", length(melville_sep)))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Next, we generate the document feature matrix and we clean it by removing stopwords and selected other words. In addition, we group the documents feature matrix by author.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "dfm_authors <- corp_dom %>%\n",
                "  quanteda::tokens(remove_punct = TRUE) %>%\n",
                "  quanteda::tokens_remove(quanteda::stopwords(\"english\")) %>%\n",
                "  quanteda::tokens_remove(c(\"now\", \"one\", \"like\", \"may\", \"can\")) %>%\n",
                "  quanteda::dfm() %>%\n",
                "  quanteda::dfm_group(groups = Author) %>%\n",
                "  quanteda::dfm_weight(scheme = \"prop\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "In a next step, we use the `textstat_frequency` function from the `quanteda` package to extract the most frequent non-stopwords in the three texts.\n",
                " \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Calculate relative frequency by president\n",
                "freq_weight <- quanteda.textstats::textstat_frequency(dfm_authors, \n",
                "                                                      n = 10,\n",
                "                                                      groups = dfm_authors$Author)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# inspect data\n",
                "freq_weight %>%\n",
                "  as.data.frame() %>%\n",
                "  head(15) %>%\n",
                "  flextable() %>%\n",
                "  flextable::set_table_properties(width = .5, layout = \"autofit\") %>%\n",
                "  flextable::theme_zebra() %>%\n",
                "  flextable::fontsize(size = 12) %>%\n",
                "  flextable::fontsize(size = 12, part = \"header\") %>%\n",
                "  flextable::align_text_col(align = \"center\") %>%\n",
                "  flextable::set_caption(caption = \"Most common words across three texts.\")  %>%\n",
                "  flextable::border_outer()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now, we can simply plot the most common words and most indicative non-stop words in the three texts.\n",
                " \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "ggplot(freq_weight, aes(nrow(freq_weight):1, frequency)) +\n",
                "     geom_point() +\n",
                "     facet_wrap(~ group, scales = \"free\") +\n",
                "     coord_flip() +\n",
                "     scale_x_continuous(breaks = nrow(freq_weight):1,\n",
                "                        labels = freq_weight$feature) +\n",
                "     labs(x = NULL, y = \"Relative frequency\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Text Classification\n",
                "\n",
                "Text classification refers to methods that allow to classify a given text to a predefined set of languages, genres, authors, or the like. Such classifications are typically based on the relative frequency of word classes, key words, phonemes, or other linguistic features such as average sentence length, words per line, etc.\n",
                "\n",
                "As with most other methods that are used in text analysis, text classification typically builds upon a training set that is already annotated with the required tags. Training sets and the features that are derived from these training sets can be created by oneself or one can use build in training sets that are provided in the respective software packages or tools. \n",
                "\n",
                "In the following, we will use the frequency of phonemes to classify a text. In a first step, we read in a German text, and split it into phonemes.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# read in German text\n",
                "German <- readLines(\"https://slcladal.github.io/data/phonemictext1.txt\") %>%\n",
                "  stringr::str_remove_all(\" \") %>%\n",
                "  stringr::str_split(\"\") %>%\n",
                "  unlist()\n",
                "# inspect data\n",
                "head(German, 20)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We now do the same for three other texts - an English and a Spanish text as well as one text in a language that we will determine using classification.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# read in texts\n",
                "English <- readLines(\"https://slcladal.github.io/data/phonemictext2.txt\")\n",
                "Spanish <- readLines(\"https://slcladal.github.io/data/phonemictext3.txt\")\n",
                "Unknown <- readLines(\"https://slcladal.github.io/data/phonemictext4.txt\")\n",
                "# clean, split texts into phonemes, unlist and convert them into vectors\n",
                "English <- as.vector(unlist(strsplit(gsub(\" \", \"\", English), \"\")))\n",
                "Spanish <- as.vector(unlist(strsplit(gsub(\" \", \"\", Spanish), \"\")))\n",
                "Unknown <- as.vector(unlist(strsplit(gsub(\" \", \"\", Unknown), \"\")))\n",
                "# inspect data\n",
                "head(English, 20)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We will now create a table that represents the phonemes and their frequencies in each of the 4 texts. In addition, we will add the language and simply the column names.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# create data tables\n",
                "German <- data.frame(names(table(German)), as.vector(table(German)))\n",
                "English <- data.frame(names(table(English)), as.vector(table(English)))\n",
                "Spanish <- data.frame(names(table(Spanish)), as.vector(table(Spanish)))\n",
                "Unknown <- data.frame(names(table(Unknown)), as.vector(table(Unknown)))\n",
                "# add column with language\n",
                "German$Language <- \"German\"\n",
                "English$Language <- \"English\"\n",
                "Spanish$Language <- \"Spanish\"\n",
                "Unknown$Language <- \"Unknown\"\n",
                "# simplify column names\n",
                "colnames(German)[1:2] <- c(\"Phoneme\", \"Frequency\")\n",
                "colnames(English)[1:2] <- c(\"Phoneme\", \"Frequency\")\n",
                "colnames(Spanish)[1:2] <- c(\"Phoneme\", \"Frequency\")\n",
                "colnames(Unknown)[1:2] <- c(\"Phoneme\", \"Frequency\")\n",
                "# combine all tables into a single table\n",
                "classdata <- rbind(German, English, Spanish, Unknown) \n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# inspect data\n",
                "classdata %>%\n",
                "  as.data.frame() %>%\n",
                "  head(10) %>%\n",
                "  flextable() %>%\n",
                "  flextable::set_table_properties(width = .5, layout = \"autofit\") %>%\n",
                "  flextable::theme_zebra() %>%\n",
                "  flextable::fontsize(size = 12) %>%\n",
                "  flextable::fontsize(size = 12, part = \"header\") %>%\n",
                "  flextable::align_text_col(align = \"center\") %>%\n",
                "  flextable::set_caption(caption = \"First 10 lines of the class data.\")  %>%\n",
                "  flextable::border_outer()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now, we group the data so that we see, how often each phoneme is used in each language.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# convert into wide format\n",
                "classdw <- classdata %>%\n",
                "  spread(Phoneme, Frequency) %>%\n",
                "  replace(is.na(.), 0)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# inspect data\n",
                "classdw[, 1:6] %>%\n",
                "  as.data.frame() %>%\n",
                "  head(10) %>%\n",
                "  flextable() %>%\n",
                "  flextable::set_table_properties(width = .5, layout = \"autofit\") %>%\n",
                "  flextable::theme_zebra() %>%\n",
                "  flextable::fontsize(size = 12) %>%\n",
                "  flextable::fontsize(size = 12, part = \"header\") %>%\n",
                "  flextable::align_text_col(align = \"center\") %>%\n",
                "  flextable::set_caption(caption = \"Overview of the class data in wide format.\")  %>%\n",
                "  flextable::border_outer()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now, we need to transform the data again, so that we have the frequency of each phoneme by language as the classifier will use \"Language\" as the dependent variable and the phoneme frequencies as predictors.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "numvar <- colnames(classdw)[2:length(colnames(classdw))]\n",
                "classdw[numvar] <- lapply(classdw[numvar], as.numeric)\n",
                "# function for normalizing numeric variables\n",
                "normalize <- function(x) { (x-min(x))/(max(x)-min(x))   }\n",
                "# apply normalization\n",
                " classdw[numvar] <- as.data.frame(lapply(classdw[numvar], normalize))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# inspect data\n",
                "classdw[, 1:6] %>%\n",
                "  as.data.frame() %>%\n",
                "  head(10) %>%\n",
                "  flextable() %>%\n",
                "  flextable::set_table_properties(width = .5, layout = \"autofit\") %>%\n",
                "  flextable::theme_zebra() %>%\n",
                "  flextable::fontsize(size = 12) %>%\n",
                "  flextable::fontsize(size = 12, part = \"header\") %>%\n",
                "  flextable::align_text_col(align = \"center\") %>%\n",
                "  flextable::set_caption(caption = \"Overview of the probabilities.\")  %>%\n",
                "  flextable::border_outer()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Before turning to the actual classification, we will use a cluster analysis to see which texts the unknown text is most similar with.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# remove language column\n",
                "textm <- classdw[,2:ncol(classdw)]\n",
                "# add languages as row names\n",
                "rownames(textm) <- classdw[,1]\n",
                "# create distance matrix\n",
                "distmtx <- dist(textm)\n",
                "# perform clustering\n",
                "clustertexts <- hclust(distmtx, method=\"ward.D\")  \n",
                "# visualize cluster result\n",
                "plot(clustertexts, hang = .25,main = \"\")           \n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "According to the cluster analysis, the unknown text clusters together with the English texts which suggests that the unknown text is likely to be English.\n",
                "\n",
                "Before we begin with the actual classification, we will split the data so that we have one data set without \"Unknown\" (this is our training set) and one data set with only \"Unknown\" (this is our test set).\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# create training set\n",
                "train <- classdw %>%\n",
                "  filter(Language != \"Unknown\")\n",
                "# create test set\n",
                "test <- classdw %>%\n",
                "  filter(Language == \"Unknown\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# inspect data\n",
                "classdw[, 1:6] %>%\n",
                "  as.data.frame() %>%\n",
                "  head(10) %>%\n",
                "  flextable() %>%\n",
                "  flextable::set_table_properties(width = .5, layout = \"autofit\") %>%\n",
                "  flextable::theme_zebra() %>%\n",
                "  flextable::fontsize(size = 12) %>%\n",
                "  flextable::fontsize(size = 12, part = \"header\") %>%\n",
                "  flextable::align_text_col(align = \"center\") %>%\n",
                "  flextable::set_caption(caption = \"Overview of the training set probabilities.\")  %>%\n",
                "  flextable::border_outer()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Finally, we can apply our classifier to our data. The classifier we use is a k-nearest neighbor classifier as the underlying function will classify an unknown element given its proximity to the clusters in the training set.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# set seed for reproducibility\n",
                "set.seed(12345)\n",
                "# apply k-nearest-neighbor (knn) classifier\n",
                "prediction <- class::knn(train[,2:ncol(train)], \n",
                "                         test[,2:ncol(test)], \n",
                "                         cl = train[, 1], \n",
                "                         k = 3)\n",
                "# inspect the result\n",
                "prediction\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Based on the frequencies of phonemes in the unknown text, the knn-classifier predicts that the unknown text is English. This is in fact true as the text is a subsection of the Wikipedia article for Aldous Huxley's *Brave New World*. The training texts were German, English, and Spanish translations of a subsection of Wikipedia's article for Hermann Hesse's *Steppenwolf*. \n",
                "\n",
                "# Entity Extraction\n",
                "\n",
                "Named Entity Recognition (NER) (also referred to as *named entity extraction* or simply as *entity extraction*) is a text analytic method which allows us to automatically identify or extract named entities from text(s) such as persons, locations, brands, etc. \n",
                "\n",
                "As such, NER is a process during which textual elements which have characteristics that are common to proper nouns (locations, people, organizations, etc.) rather than other parts of speech, e.g. non-sentence initial capitalization, are extracted from texts. Retrieving entities is common in automated summarization and in Topic Modeling. NER can be achieved by simple feature extraction (e.g. extract all non-sentence initial capitalized words) or with the help of training sets. Using training sets, i.e. texts that are annotated for entities and non-entities, achieves better results when dealing with unknown data and data with inconsistent capitalization.\n",
                "\n",
                "There are different options to extract entities from texts in R. Here, we will focus on two methods:\n",
                "\n",
                "a) entity extraction using `pacman` and `entity` packages provided by Tyler Rinker which builds on the `NLP` and `openNLP` packages; and\n",
                "\n",
                "b) entity extraction using the `NLP` and `openNLP` packages directly.\n",
                "\n",
                "## NER using pacman\n",
                "\n",
                "As we have already installed and activated the required packages during the session preparation, we can start right away by loading and cleaning a sample text - which is a summary of Aldous Huxley's *Brave New World*.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# load corpus data\n",
                "text <- readLines(\"https://slcladal.github.io/data/text4.txt\", skipNul = T) %>%\n",
                "  # clean data\n",
                "  stringr::str_squish() %>%\n",
                "  # remove empty elements\n",
                "  .[. != \"\"]\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# inspect data\n",
                "text %>%\n",
                "  as.data.frame() %>%\n",
                "  head(1) %>%\n",
                "  flextable() %>%\n",
                "  flextable::set_table_properties(width = .5, layout = \"autofit\") %>%\n",
                "  flextable::theme_zebra() %>%\n",
                "  flextable::fontsize(size = 12) %>%\n",
                "  flextable::fontsize(size = 12, part = \"header\") %>%\n",
                "  flextable::align_text_col(align = \"center\") %>%\n",
                "  flextable::set_caption(caption = \"First paragraph of the text element (a description of Aldous Huxley's Brave New World)\")  %>%\n",
                "  flextable::border_outer()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We can now extract the entities form that text using the `person_entity` from the `entity` package.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# extract person entities\n",
                "entity::person_entity(text)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We can also extract other types of entities as shown below - I have de-activated the code chunk, but you can run the code and check out the results.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# extract locations\n",
                "entity::location_entity(text)\n",
                "entity::organization_entity(text)\n",
                "entity::date_entity(text)\n",
                "entity::money_entity(text)\n",
                "entity::percent_entity(text)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## NER using NLP and openNLP{-}\n",
                "\n",
                "We can also perform NER using the `NLP` and `openNLP` packages directly (rather than the `pacman` and `entity` packages which are wrapper of the `NLP` and `openNLP` packages). This is a bit more complex, but it also allows for more flexibility, e.g. extarcting entities by sentence rather than by text.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# convert text into string\n",
                "text = as.String(text)\n",
                "# define annotators\n",
                "# sentence annotator\n",
                "sent_annot = openNLP::Maxent_Sent_Token_Annotator()\n",
                "# word annotator\n",
                "word_annot = openNLP::Maxent_Word_Token_Annotator()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "***\n",
                "\n",
                "<div class=\"warning\" style='padding:0.1em; background-color:#51247a; color:#f2f2f2'>\n",
                "<span>\n",
                "<p style='margin-top:1em; text-align:center'>\n",
                "<b>NOTE<\/b><br>I had an issue with the following lines of code because R asked me for a package (openNLPmodels.en) that I could not install for the R version I am currently running (4.1). You can find the language packages for English, Spanish, German, Swedish, Dutch, Italian, Danish, and Polish under: https://slcladal.github.io/packages/ - simply download and install them to the library of your current R version.<br><br>The original versions of these packages can be downloaded from https://datacube.wu.ac.at/src/contrib/.<\/p>\n",
                "<p style='margin-left:1em;'>\n",
                "<\/p><\/span>\n",
                "<\/div>\n",
                "\n",
                "***\n",
                "\n",
                "Once the `openNLPmodels.en` model is your R library, you can run the code chunk below.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# location annotator\n",
                "loc_annot = openNLP::Maxent_Entity_Annotator(kind = \"location\") \n",
                "# person annotator\n",
                "people_annot = openNLP::Maxent_Entity_Annotator(kind = \"person\") \n",
                "# apply annotations\n",
                "textanno = NLP::annotate(text, list(sent_annot, word_annot, \n",
                "                                        loc_annot, people_annot))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now that we have annotated the data, we can extract elements that have been identified as locations of persons.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# extract features\n",
                "k <- sapply(textanno$features, `[[`, \"kind\")\n",
                "# extract locations\n",
                "textlocations = names(table(text[textanno[k == \"location\"]]))\n",
                "# extract people\n",
                "textpeople = names(table(text[textanno[k == \"person\"]]))\n",
                "# inspect extract people\n",
                "textpeople\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The output shows that the extraction of people and locations from the example text was successful as we can see various places and persons in the output. \n",
                "\n",
                "# Part-of-Speech tagging\n",
                "\n",
                "A very common procedure to add information to texts is to part-of-speech tag the data, which means to determine to what type of word a specific word belongs. Below, we will add pos-tags to a short English text.\n",
                "\n",
                "We begin by writing a function which adds part of speech tags to an English text.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# function for pos-tagging\n",
                "tagPOS <-  function(x, ...) {\n",
                "  s <- as.String(x)\n",
                "  word_token_annotator <- Maxent_Word_Token_Annotator()\n",
                "  a2 <- Annotation(1L, \"sentence\", 1L, nchar(s))\n",
                "  a2 <- annotate(s, word_token_annotator, a2)\n",
                "  a3 <- annotate(s, Maxent_POS_Tag_Annotator(), a2)\n",
                "  a3w <- a3[a3$type == \"word\"]\n",
                "  POStags <- unlist(lapply(a3w$features, `[[`, \"POS\"))\n",
                "  POStagged <- paste(sprintf(\"%s/%s\", s[a3w], POStags), collapse = \" \")\n",
                "  list(POStagged = POStagged)\n",
                "  }\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We now apply this function to the text and then display the results.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# load text\n",
                "text <- readLines(\"https://slcladal.github.io/data/english.txt\")\n",
                "# pos tagging data\n",
                "textpos <- tagPOS(text)\n",
                "# inspect data\n",
                "textpos\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We could, for example, use pos-tagging to check differences in the distribution of word classes across different registers. or to find certain syntactic patterns in a collection of texts. \n",
                "\n",
                "# Citation & Session Info {-}\n",
                "\n",
                "Schweinberger, Martin. `r format(Sys.time(), '%Y')`. *Text Analysis and Distant Reading using R*. Brisbane: The University of Queensland. url: https://slcladal.github.io/textanalysis.html (Version `r format(Sys.time(), '%Y.%m.%d')`).\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "@manual{schweinberger`r format(Sys.time(), '%Y')`ta,\n",
                "  author = {Schweinberger, Martin},\n",
                "  title = {Text Analysis and Distant Reading using R},\n",
                "  note = {https://slcladal.github.io/textanalysis.html},\n",
                "  year = {`r format(Sys.time(), '%Y')`},\n",
                "  organization = \"The University of Queensland, Australia. School of Languages and Cultures},\n",
                "  address = {Brisbane},\n",
                "  edition = {`r format(Sys.time(), '%Y.%m.%d')`}\n",
                "}\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "sessionInfo()\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "***\n",
                "\n",
                "[Back to top](#introduction)\n",
                "\n",
                "[Back to HOME](https://slcladal.github.io/index.html)\n",
                "\n",
                "***\n",
                "\n",
                "\n",
                "# References{-}\n"
            ]
        }
    ],
    "metadata": {
        "anaconda-cloud": "",
        "kernelspec": {
            "display_name": "R",
            "langauge": "R",
            "name": "ir"
        },
        "language_info": {
            "codemirror_mode": "r",
            "file_extension": ".r",
            "mimetype": "text/x-r-source",
            "name": "R",
            "pygments_lexer": "r",
            "version": "3.4.1"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}
