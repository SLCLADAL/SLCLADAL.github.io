![An interactive LADAL notebook](https://slcladal.github.io/images/uq1.jpg)


This tutorial is the interactive Jupyter notebook accompanying the [*Language Technology and Data Analysis Laboratory* (LADAL) tutorial *String Processing in R*](https://ladal.edu.au/string.html). 


**Preparation and session set up**

We set up our session by activating the packages we need for this tutorial. 


```{r prep2, message=FALSE} 
# activate packages
library(stringr)
library(dplyr)
library(htmlwidgets)
```

Once you have initiated the session by executing the code shown above, you are good to go.

If you are using this notebook on your own computer and you have not already installed the R packages listed above, you need to install them. You can install them by replacing the `library` command with `install.packages` and putting the name of the package into quotation marks like this: `install.packages("stringr")`. Then, you simply run this command and R will install the package you specified.

# Loading data

Before we start with string processing, we will load some example texts on which we will perform the processing. 

```{r readdat1, echo=T, eval = T, message=FALSE, warning=FALSE}
# read in text
exampletext  <- base::readRDS(url("https://slcladal.github.io/data/tx1.rda", "rb"))
# inspect
exampletext
```

This first example text represents a paragraph about grammar.

```{r readdat2, echo=T, eval = T, message=FALSE, warning=FALSE}
# read in text
splitexampletext  <- base::readRDS(url("https://slcladal.github.io/data/tx2.rda", "rb"))
# inspect
splitexampletext
```


This second example text represents the same paragraph about grammar, but split into individual sentences.


```{r readdat3, echo=T, eval = T, message=FALSE, warning=FALSE}
additionaltext  <- base::readRDS(url("https://slcladal.github.io/data/tx3.rda", "rb"))
# inspect
additionaltext
```

The third example text represents a paragraph about Ferdinand de Saussure - the founder of modern linguistics. 

```{r readdat4, echo=T, eval = T, message=FALSE, warning=FALSE}
sentences  <- base::readRDS(url("https://slcladal.github.io/data/tx4.rda", "rb"))
# inspect
sentences
```

The third example text consist of 3 short plain sentences. 

In the following, we will perform various operations on the example texts.


## Using your own data

While the tutorial uses data from the LADAL website, you can also use your own data. You can see below what you need to do to upload and use your own data.

The code chunk below allows you to upload two files from your own computer. To be able to load your own data, you need to click on the folder symbol to the left of the screen:

![Binder Folder Symbol](https://slcladal.github.io/images/binderfolder.JPG)


Then, when the menu has unfolded, click on the smaller folder symbol (encircled in red in the picture below).

![Small Binder Folder Symbol](https://slcladal.github.io/images/upload2.png)

Now, you are in the main menu and can click on the 'MyData' folder.

![MyData Folder Symbol](https://slcladal.github.io/images/upload3.png)

Now, that you are in the MyData folder, you can click on the upload symbol.

![Binder Upload Symbol](https://slcladal.github.io/images/binderupload.JPG)

Select and upload the files you want to analyze (**IMPORTANT: here, we assume that you upload some form of text data - not tabular data! You can upload only txt and docx files!**). When you then execute the code chunk below, you will upload your own data and you can then use it in this notebook.

```{r}
myfiles <- list.files(here::here("MyData"), # path to the corpus data
                          # full paths - not just the names of the files
                          full.names = T) 
# load files
mytext <- sapply(myfiles, function(x){
  x <- scan(x,                # reads the file specified by 'x'
            what = "char",    # treat each character as a separate element
            sep = "",         # specify no separator between elements
            quote = "",       # do not consider quotes in the file
            quiet = T,        # suppress any output during the scanning process
            skipNul = T)      # skip any null values encountered during scanning
  # concatenate the elements into a single text, separating them by a space
  x <- paste0(x, sep = " ", collapse = " ")   
  x <- stringr::str_squish(x)  # remove redundant whitespace from the text
})

# inspect
str(mytext)    # display the structure of the 'mytext' object
```


**Keep in mind though that you need to adapt the names of the texts in the code chunks below so that the code below work on your own texts!**

***


# Basic String Processing 

Before turning to functions provided in the `stringr`, let us just briefly focus on some base functions that are extremely useful when working with texts.

A very useful function is, e.g. `tolower` which converts everything to lower case. 

```{r lwr, echo=T, eval = T, message=FALSE, warning=FALSE}
tolower(exampletext)
```

Conversely, `toupper` converts everything to upper case. 

```{r upr, echo=T, eval = T, message=FALSE, warning=FALSE}
toupper(exampletext)
```


The `stringr` package (see [here](https://raw.githubusercontent.com/rstudio/cheatsheets/main/stringr.pdf) is part of the so-called *tidyverse* - a collection of packages that allows to write R code in a readable manner - and it is the most widely used package for string processing in . The advantage of using `stringr` is that it makes string processing very easy. All `stringr` functions share a common structure:

`str_function(string, pattern)`

The two arguments in the structure of `stringr` functions are:  *string* which is the character string to be processed and a pattern which is either a simple sequence of characters, a regular expression, or a combination of both. Because the *string* comes first, the `stringr` functions are ideal for piping and thus use in tidyverse style R. 

All function names of `stringr` begin with str, then an underscore and then the name of the action to be performed.  For example, to replace the first occurrence of a pattern in a string, we should use `str_replace()`. In the following, we will use `stringr` functions to perform various operations on the example text. As we have already loaded the `tidyverse` package, we can start right away with using `stringr` functions as shown below.

Like `nchar` in `base`, `str_count` provides the number of characters of a text.

```{r stringr2, echo=T, eval = T, message=FALSE, warning=FALSE}
str_count(splitexampletext)
```

The function `str_detect` informs about whether a pattern is present in a text and outputs a logical vector with *TRUE* if the pattern occurs and *FALSE* if it does not.

```{r stringr3, echo=T, eval = T, message=FALSE, warning=FALSE}
str_detect(splitexampletext, "and")
```	

The function `str_extract` extracts the first occurrence of a pattern, if that pattern is present in a text.

```{r stringr4, echo=T, eval = T, message=FALSE, warning=FALSE}
str_extract(exampletext, "and")
```	

The function `str_extract_all` extracts all occurrences of a pattern, if that pattern is present in a text.

```{r stringr5, echo=T, eval = T, message=FALSE, warning=FALSE}
str_extract_all(exampletext, "and")
```	

The function `str_locate` provides the start and end position of the match of the pattern in a text.

```{r stringr6, echo=T, eval = T, message=FALSE, warning=FALSE}

str_locate(exampletext, "and") 
```

The function `str_locate_all` provides the start and end positions of the match of the pattern in a text and displays the result in matrix-form.

```{r stringr7, echo=T, eval = T, message=FALSE, warning=FALSE}
str_locate_all(exampletext, "and")
```

The function `str_match` extracts the first occurrence of the pattern in a text.


```{r stringr8, echo=T, eval = T, message=FALSE, warning=FALSE}
str_match(exampletext, "and") 

```

The function `str_match_all` extracts the all occurrences of the pattern from a text.

```{r stringr9, echo=T, eval = T, message=FALSE, warning=FALSE}
str_match_all(exampletext, "and")
```

The function `str_remove` removes the first occurrence of a pattern in a text.

```{r stringr10, echo=T, eval = T, message=FALSE, warning=FALSE}
str_remove(exampletext, "and") 
```

The function `str_remove_all` removes all occurrences of a pattern from a text.

```{r stringr11, echo=T, eval = T, message=FALSE, warning=FALSE}
str_remove_all(exampletext, "and")
```

The function `str_replace` replaces the first occurrence of a pattern with something else in a text.

```{r stringr12, echo=T, eval = T, message=FALSE, warning=FALSE}
str_replace(exampletext, "and", "AND")
```

The function `str_replace_all` replaces all occurrences of a pattern with something else in a text.

```{r stringr13, echo=T, eval = T, message=FALSE, warning=FALSE}
str_replace_all(exampletext, "and", "AND")
```

The function `str_starts` tests whether a given text begins with a certain pattern and outputs a logical vector.

```{r stringr14, echo=T, eval = T, message=FALSE, warning=FALSE}
str_starts(exampletext, "and") 
```

The function `str_ends` tests whether a text ends with a certain pattern and outputs a logical vector.

```{r stringr15, echo=T, eval = T, message=FALSE, warning=FALSE}
str_ends(exampletext, "and")
```

Like `strsplit`, the function `str_split` splits a text when a given pattern occurs. If no pattern is provided, then the text is split into individual symbols.

```{r stringr16, echo=T, eval = T, message=FALSE, warning=FALSE}
str_split(exampletext, "and") 
```

The function `str_split_fixed` splits a text when a given pattern occurs but only so often as is indicated by the argument `n`. So, even if the patter occur more often than `n`, `str_split_fixed` will only split the text `n` times.

```{r stringr17, echo=T, eval = T, message=FALSE, warning=FALSE}
str_split_fixed(exampletext, "and", n = 3)
```

The function `str_subset` extracts those subsets of a text that contain a certain pattern.  

```{r stringr18, echo=T, eval = T, message=FALSE, warning=FALSE}
str_subset(splitexampletext, "and") 
```

The function `str_which` provides a vector with the indices of the texts that contain a certain pattern. 

```{r stringr19, echo=T, eval = T, message=FALSE, warning=FALSE}
str_which(splitexampletext, "and")
```


The function `str_pad` adds white spaces to a text or vector of texts so that they reach a given number of symbols.

```{r stringr22, echo=T, eval = T, message=FALSE, warning=FALSE}
# create text with white spaces
text <- " this    is a    text   "
str_pad(text, width = 30)
```

The function `str_trim` removes white spaces from the beginning(s) and end(s) of a text or vector of texts.

```{r stringr23, echo=T, eval = T, message=FALSE, warning=FALSE}
str_trim(text) 
```

The function `str_squish` removes white spaces that occur within a text or vector of texts.

```{r stringr24, echo=T, eval = T, message=FALSE, warning=FALSE}
str_squish(text)
```

The function `str_wrap` removes white spaces  from the beginning(s) and end(s) of a text or vector of texts and also those white spaces that occur within a text or vector of texts.

```{r stringr25, echo=T, eval = T, message=FALSE, warning=FALSE}
str_wrap(text)
```

The function `str_order` provides a vector that represents the order of a vector of texts according to the lengths of texts in that vector.

```{r stringr26, echo=T, eval = T, message=FALSE, warning=FALSE}
str_order(splitexampletext)
```

The function `str_sort` orders of a vector of texts according to the lengths of texts in that vector.

```{r stringr27, echo=T, eval = T, message=FALSE, warning=FALSE}
str_sort(splitexampletext)
```

The function `str_to_upper` converts all symbols in a text or vector of texts to upper case.

```{r stringr28, echo=T, eval = T, message=FALSE, warning=FALSE}
str_to_upper(exampletext) 
```

The function `str_to_lower` converts all symbols in a text or vector of texts to lower case.

```{r stringr29, echo=T, eval = T, message=FALSE, warning=FALSE}
str_to_lower(exampletext) 
```

The function `str_c` combines texts into one text

```{r stringr30, echo=T, eval = T, message=FALSE, warning=FALSE}
str_c(exampletext, additionaltext)
```

The function `str_conv` converts a text into a certain type of encoding, e.g. into `UTF-8` or `Latin1`.

```{r stringr31, echo=T, eval = T, message=FALSE, warning=FALSE}
str_conv(exampletext, encoding = "UTF-8")
```

The function `str_dup` reduplicates a text or a vector of texts n times.

```{r stringr32, echo=T, eval = T, message=FALSE, warning=FALSE}
str_dup(exampletext, times=2)
```

The function `str_flatten` combines a vector of texts into one text. The argument `collapse` defines the symbol that occurs between the combined texts. If the argument `collapse` is left out, the texts will be combined without any symbol between the combined texts.


```{r stringr33, echo=T, eval = T, message=FALSE, warning=FALSE}
str_flatten(sentences, collapse = " ")
```

 If the argument `collapse` is left out, the texts will be combined without any symbol between the combined texts.

```{r stringr34, echo=T, eval = T, message=FALSE, warning=FALSE}
str_flatten(sentences)
```

The function `str_length` provides the length of texts in characters.

```{r stringr35, echo=T, eval = T, message=FALSE, warning=FALSE}
str_length(exampletext)
```

The function `str_replace_na` replaces NA in texts. It is important to note that NA, if it occurs within a string, is considered to be the literal string `NA`.

```{r stringr36, echo=T, eval = T, message=FALSE, warning=FALSE}
# create sentences with NA
sentencesna <- c("Some text", NA, "Some more text", "Some NA text")
# apply str_replace_na function
str_replace_na(sentencesna, replacement = "Something new")
```

The function `str_trunc` ends strings with ... after a certain number of characters.

```{r stringr37, echo=T, eval = T, message=FALSE, warning=FALSE}
str_trunc(sentences, width = 20)
```

The function `str_sub` extracts a string from a text from a start location to an end position (expressed as character positions).

```{r stringr38, echo=T, eval = T, message=FALSE, warning=FALSE}
str_sub(exampletext, 5, 25)
```

The function `word` extracts words from a text (expressed as word positions).


```{r stringr39, echo=T, eval = T, message=FALSE, warning=FALSE}
word(exampletext, 2:7)
```


The function `str_glue` combines strings and allows to input variables.

```{r stringr40, echo=T, eval = T, message=FALSE, warning=FALSE}
name <- "Fred"
age <- 50
anniversary <- as.Date("1991-10-12")
str_glue(
  "My name is {name}, ",
  "my age next year is {age + 1}, ",
  "and my anniversary is {format(anniversary, '%A, %B %d, %Y')}."
)
```

The function `str_glue_data` is particularly useful when it is used in data pipelines. The data set `mtcars` is a build in data set that is loaded automatically when starting R.

```{r stringr41, echo=T, eval = T, message=FALSE, warning=FALSE}

mtcars %>% 
  str_glue_data("{rownames(.)} has {hp} hp")
```



# Advanced String Processing 

Above, we have used functions and regular expressions to extract and find patters in textual data. Here, we will focus on common methods for cleaning text data that are applied before implementing certain methods.

We start by loading some additional packages, e.g., the `quanteda` (see [here](https://raw.githubusercontent.com/rstudio/cheatsheets/main/quanteda.pdf) for a cheat sheet for the `quanteda` package), the `tm`, and the `udpipe` package, which are extremely useful when dealing with more advanced text processing.


```{r atp1, message=F, warning=F}
library(quanteda)
library(tm)
library(udpipe) 
```

One common procedure is to split texts into sentences which we can do by using, e.g., the  `tokenize_sentence` function from the `quanteda` package. I also unlist the data to have a vector wot work with (rather than a list).


```{r atp3, message=F, warning=F}
et_sent <- quanteda::tokenize_sentence(exampletext) %>%
  unlist()
# inspect
et_sent
```

Another common procedure is to remove stop words, i.e., words that do not have semantic or referential meaning (like nouns such as *tree* or *cat*, or verbs like *sit* or *speak* or adjectives such as *green* or *loud*) but that indicate syntactic relations, roles, or features.(e.g., articles and pronouns). We can remove stopwords using, e.g., the  `removeWords` function from the `tm` package


```{r atp5, message=F, warning=F}
et_wostop <-  tm::removeWords(exampletext, tm::stopwords("english"))
# inspect
et_wostop
```

To remove the superfluous whote spaces, we can use, e.g., the  `stripWhitespace` function from the `tm` package.

```{r atp6, message=F, warning=F}
et_wows <-  tm::stripWhitespace(et_wostop)
# inspect
et_wows
```

It can also be useful to remove numbers. We can do this using, e.g., the  `removeNumbers` function from the `tm` package.


```{r atp7, message=F, warning=F}
et_wonum <-  tm::removeNumbers("This is the 1 and only sentence I will write in 2022.")
# inspect
et_wonum
```

We may also want to remove any type of punctuation using, e.g., the  `removePunctuation` function from the `tm` package.



```{r atp9, message=F, warning=F}
et_wopunct <-  tm::removePunctuation(exampletext)
# inspect
et_wopunct
```

We may also want to stem the words in a  document, i.e. removing the ends of words to be able to group together semantically related words such as *walk*, *walks*, *walking*, *walked* which would all be stemmed into *walk*. We can stem a text using, e.g., the  `stemDocument` function from the `tm` package.


```{r atp11, eval = F, message=F, warning=F}
et_stem <-  tm::stemDocument(exampletext, language = "en")
# inspect
et_stem
```

**Tokenization, lemmatization, pos-tagging, and dependency parsing**

A far better option than stemming is lemmatization as lemmatization is based on proper morphological information and vocabularies. For lemmatization, we can use the `udpipe` package which also tokenizes texts, adds part-of-speech tags, and provides information about dependency relations. 

Before we can tokenize, lemmatize, pos-tag and parse though, we need to download a pre-trained language model.

```{r atp12, eval = F, message=F, warning=F}
# download language model
m_eng  <- udpipe::udpipe_download_model(language = "english-ewt")
# activate language model
m_eng <- udpipe_load_model(file ="english-ewt-ud-2.5-191206.udpipe")
```


We can now use the model to annotate out text.

```{r atp17, message=F, warning=F}
# tokenise, tag, dependency parsing
text_anndf <- udpipe::udpipe_annotate(m_eng, x = exampletext) %>%
  as.data.frame() %>%
  dplyr::select(-sentence)
# inspect
head(text_anndf, 10)
```

We could, of course, perform many more manipulations of textual data but this should suffice to get you started.


***


[Back to LADAL](https://ladal.edu.au/string.html)

***



