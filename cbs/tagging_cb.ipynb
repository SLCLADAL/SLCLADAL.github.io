{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "![This is an interactive LADAL notebook.](https://slcladal.github.io/images/uq1.jpg)\n",
                "\n",
                "***\n",
                "\n",
                "Please copy this Jupyter notebook so that you are able to edit it.\n",
                "\n",
                "Simply go to: File > Save a copy in Drive.\n",
                "\n",
                "If you want to run this notebook on your own computer, you need to do 2 things:\n",
                "\n",
                "1. Make sure that you have R installed.\n",
                "\n",
                "2. You need to download the [bibliography file](https://slcladal.github.io/bibliography.bib) and store it in the same folder where you store the Rmd file.\n",
                "\n",
                "Once you have done that, you are good to go.\n",
                "\n",
                "***\n",
                "\n",
                "# POS-Tagging and Syntactic Parsing with R\n",
                "\n",
                "This tutorial introduces part-of-speech tagging and syntactic parsing using R. This tutorial is aimed at beginners and intermediate users of R with the aim of showcasing how to annotate textual data with part-of-speech (pos) tags and how to syntactically parse textual data  using R. The aim is not to provide a fully-fledged analysis but rather to show and exemplify selected useful methods associated with pos-tagging and syntactic parsing. Another highly recommendable tutorial on part-of-speech tagging in R with UDPipe is available [here](https://bnosac.github.io/udpipe/en/) and another tutorial on pos-tagging and syntactic parsing by Andreas Niekler and Gregor Wiedemann can be found [here](https://tm4ss.github.io/docs/Tutorial_8_NER_POS.html)  (see Wiedemann and Niekler 2017).\n",
                "\n",
                "# Part-Of-Speech Tagging\n",
                "\n",
                "Many analyses of language data require that we distinguish different parts of speech. In order to determine the word class of a certain word, we use a procedure which is called part-of-speech tagging (commonly referred to as pos-, pos-, or PoS-tagging). pos-tagging is a common procedure when working with natural language data. Despite being used quite frequently, it is a rather complex issue that requires the application of statistical methods that are quite advanced. In the following, we will explore different options for pos-tagging and syntactic parsing. \n",
                "\n",
                "Parts-of-speech, or word categories, refer to the grammatical nature or category of a lexical item, e.g. in the sentence *Jane likes the girl* each lexical item can be classified according to whether it belongs to the group of determiners, verbs, nouns, etc.  pos-tagging refers to a (computation) process in which information is added to existing text. This process is also called *annotation*. Annotation can be very different depending on the task at hand. The most common type of annotation when it comes to language data is part-of-speech tagging where the word class is determined for each word in a text and the word class is then added to the word as a tag. However, there are many different ways to tag or annotate texts. \n",
                "\n",
                "Pos–tagging assigns part-of-speech tags to character strings (these represent mostly words, of course, but also encompass punctuation marks and other elements). This means that pos–tagging is one specific type of annotation, i.e. adding information to data (either by directly adding information to the data itself or by storing information in e.g. a list which is linked to the data). It is important to note that annotation encompasses various types of information such as pauses, overlap, etc. pos–tagging is just one of these many ways in which corpus data can be *enriched*. Sentiment Analysis, for instance, also annotates texts or words with respect to its or their emotional value or polarity. \n",
                "\n",
                "Annotation is required in many machine-learning contexts because annotated texts are commonly used as training sets on which machine learning or deep learning models are trained that then predict, for unknown words or texts, what values they would most likely be assigned if the annotation were done manually. Also, it should be mentioned that  by many online services offer pos-tagging (e.g. [here](http://www.infogistics.com/posdemo.htm) or [here](https://linguakit.com/en/part-of-speech-tagging).\n",
                "\n",
                "When pos–tagged, the example sentence could look like the example below.\n",
                "\n",
                "1. Jane/NNP likes/VBZ the/DT girl/NN\n",
                "\n",
                "In the example above, `NNP` stands for proper noun (singular), `VBZ` stands for 3rd person singular present tense verb, `DT` for determiner, and `NN` for noun(singular or mass). The pos-tags used by the `openNLPpackage` are the [Penn English Treebank pos-tags](https://dpdearing.com/posts/2011/12/opennlp-part-of-speech-pos-tags-penn-english-treebank/). A more elaborate description of the tags can be found here which is summarised below:\n",
                "\n",
                "![Overview of Penn English Treebank part-of-speech tags.](https://slcladal.github.io/images/postagtb.png)\n",
                "\n",
                "Assigning these pos-tags to words appears to be rather straight forward. However, pos-tagging is quite complex and there are various ways by which a computer can be trained to assign pos-tags. For example, one could use orthographic or morphological information to devise rules such as. . .\n",
                "\n",
                "* If a word ends in *ment*, assign the pos-tag `NN` (for common noun)\n",
                "\n",
                "* If a word does not occur at the beginning of a sentence but is capitalized, assign the pos-tag `NNP` (for proper noun)\n",
                "\n",
                "Using such rules has the disadvantage that pos-tags can only be assigned to a relatively small number of words as most words will be ambiguous – think of the similarity of the English plural (-(e)s)  and the English 3rd person, present tense indicative morpheme (-(e)s), for instance, which are orthographically identical.Another option would be to use a dictionary in which each word is as-signed a certain pos-tag and a program could assign the pos-tag if the word occurs in a given text. This procedure has the disadvantage that most words belong to more than one word class and pos-tagging would thus have to rely on additional information.The problem of words that belong to more than one word class can partly be remedied by including contextual information such as. . \n",
                "\n",
                "* If the previous word is a determiner and the following word is a common noun, assign the pos-tag `JJ` (for a common adjective)\n",
                "\n",
                "\n",
                "This procedure works quite well but there are still better options.The best way to pos-tag a text is to create a manually annotated training set which resembles the language variety at hand. Based on the frequency of the association between a given word and the pos-tags it is assigned in the training data, it is possible to tag a word with the pos-tag that is most often assigned to the given word in the training data.All of the above methods can and should be optimized by combining them and additionally including pos–n–grams, i.e. determining a pos-tag of an unknown word based on which sequence of pos-tags is most similar to the sequence at hand and also most common in the training data.This introduction is extremely superficial and only intends to scratch some of the basic procedures that pos-tagging relies on. The interested reader is referred to introductions on machine learning and pos-tagging such as e.g.https://class.coursera.org/nlp/lecture/149.\n",
                "\n",
                "\n",
                "There are several different R packages that assist with pos-tagging texts (see Kumar and Paul 2016). In this tutorial, we will use the `udpipe` (Wijffels 2021) and the `openNLP`  packages  (Hornik 2019). Each of these has advantages and shortcomings and it is advantageous to try which result best matches one's needs. That said, the `udpipe` package is really great as it is easy to use, covers a wide range of languages, is very flexible, and very accurate.\n",
                "\n",
                "**Preparation and session set up**\n",
                "\n",
                "This tutorial is based on R. If you have not installed R or are new to it, you will find an introduction to and more information how to use R [here](https://slcladal.github.io/intror.html). For this tutorials, we need to install certain *packages* from an R *library* so that the scripts shown below are executed without errors. Before turning to the code below, please install the packages by running the code below this paragraph. If you have already installed the packages mentioned below, then you can skip ahead ignore this section. To install the necessary packages, simply run the following code - it may take some time (between 1 and 5 minutes to install all of the libraries so you do not need to worry if it takes some time).\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# install packages\n",
                "install.packages(\"tidyverse\")\n",
                "install.packages(\"igraph\")\n",
                "install.packages(\"tm\")\n",
                "install.packages(\"NLP\")\n",
                "install.packages(\"openNLP\")\n",
                "install.packages(\"openNLPdata\")\n",
                "install.packages(\"udpipe\")\n",
                "install.packages(\"textplot\") \n",
                "install.packages(\"ggraph\") \n",
                "install.packages(\"ggplot2\") \n",
                "install.packages(\"pacman\") \n",
                "# install phrasemachine\n",
                "phrasemachineurl <- \"https://cran.r-project.org/src/contrib/Archive/phrasemachine/phrasemachine_1.1.2.tar.gz\"\n",
                "install.packages(phrasemachineurl, repos=NULL, type=\"source\")\n",
                "# install parsent\n",
                "pacman::p_load_gh(c(\"trinker/textshape\", \"trinker/parsent\"))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now that we have installed the packages, we activate them as shown below.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# set options\n",
                "options(stringsAsFactors = F)         # no automatic data transformation\n",
                "options(\"scipen\" = 100, \"digits\" = 4) # suppress math annotation\n",
                "# load packages\n",
                "library(tidyverse)\n",
                "library(igraph)\n",
                "library(tm)\n",
                "library(NLP)\n",
                "library(openNLP)\n",
                "library(openNLPdata)\n",
                "library(udpipe)\n",
                "library(textplot) \n",
                "library(udpipe) \n",
                "library(ggraph) \n",
                "library(ggplot2) \n",
                "library(igraph)\n",
                "library(phrasemachine)\n",
                "# load function for pos-tagging objects in R\n",
                "source(\"https://slcladal.github.io/rscripts/POStagObject.r\") \n",
                "# syntax tree drawing function\n",
                "source(\"https://slcladal.github.io/rscripts/parsetgraph.R\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Once you have installed R and RStudio and initiated the session by executing the code shown above, you are good to go.\n",
                "\n",
                "# POS-Tagging with UDPipe\n",
                "\n",
                "UDPipe was developed at the Charles University in Prague and the `udpipe` R package is an extremely interesting and really fantastic package as it provides a very easy and handy way for language-agnostic tokenization, pos-tagging, lemmatization and dependency parsing of raw text in R. It is particularly handy because it addresses and remedies major shortcomings that previous methods for pos-tagging had, namely\n",
                "\n",
                "* it offers a wide range of language models (64 languages at this point)\n",
                "* it does not rely on external software (like, e.g., TreeTagger, that had to be installed separately and could be challenging when using different operating systems)\n",
                "* it is really easy to implement as one only need to install and load the `udpipe` package and download and activate the language model one is interested in\n",
                "* it allows to train and tune one's own models rather easily\n",
                "\n",
                "The available pre-trained language models in UDPipe are:\n",
                "\n",
                "* Afrikaans: afrikaans-afribooms   \n",
                "* Ancient Greek:   \n",
                "  + ancient_greek-perseus  \n",
                "  + ancient_greek-proiel   \n",
                "* Arabic: arabic-padt   \n",
                "* Armenian: armenian-armtdp   \n",
                "* Basque: basque-bdt   \n",
                "* Belarusian: belarusian-hse   \n",
                "* bulgarian-btb   \n",
                "* Buryat: buryat-bdt  \n",
                "* Catalan: catalan-ancora   \n",
                "* Chinese:   \n",
                "  + chinese-gsd  \n",
                "  + chinese-gsdsimp  \n",
                "  + classical_chinese-kyoto  \n",
                "* Coptic: coptic-scriptorium  \n",
                "* Croatian: croatian-set  \n",
                "* Czech  \n",
                "  + czech-cac  \n",
                "  + czech-cltt  \n",
                "  + czech-fictree  \n",
                "  + czech-pdt  \n",
                "* Danish: danish-ddt  \n",
                "* Dutch  \n",
                "  + dutch-alpino  \n",
                "  + dutch-lassysmall  \n",
                "* English  \n",
                "  + english-ewt  \n",
                "  + english-gum  \n",
                "  + english-lines  \n",
                "  + english-partut  \n",
                "* Estonian   \n",
                "  + estonian-edt  \n",
                "  + estonian-ewt  \n",
                "* Finnish  \n",
                "  + finnish-ftb  \n",
                "  + finnish-tdt  \n",
                "* French  \n",
                "  + french-gsd  \n",
                "  + french-partut  \n",
                "  + french-sequoia  \n",
                "  + french-spoken  \n",
                "* Galician  \n",
                "  + galician-ctg  \n",
                "  + galician-treegal  \n",
                "* German  \n",
                "  + german-gsd  \n",
                "  + german-hdt  \n",
                "* Gothic: gothic-proiel  \n",
                "* Greek: greek-gdt  \n",
                "* Hebrew: hebrew-htb  \n",
                "* Hindi: hindi-hdtb  \n",
                "* Hungarian: hungarian-szeged  \n",
                "* Indonesian: indonesian-gsd  \n",
                "* Irish Gaelic: irish-idt  \n",
                "* Italian  \n",
                "  + italian-isdt  \n",
                "  + italian-partut  \n",
                "  + italian-postwita  \n",
                "  + italian-twittiro  \n",
                "  + italian-vit  \n",
                "* Japanese: japanese-gsd  \n",
                "* Kazakh: kazakh-ktb  \n",
                "* Korean  \n",
                "  + korean-gsd  \n",
                "  + korean-kaist  \n",
                "* Kurmanji: kurmanji-mg  \n",
                "* Latin  \n",
                "  + latin-ittb  \n",
                "  + latin-perseus  \n",
                "  + latin-proiel  \n",
                "* Latvian: latvian-lvtb  \n",
                "* Lithuanian  \n",
                "  + lithuanian-alksnis  \n",
                "  + lithuanian-hse  \n",
                "* Maltese: maltese-mudt  \n",
                "* Marathi: marathi-ufal  \n",
                "* North Sami: north_sami-giella  \n",
                "* Norwegian  \n",
                "  + norwegian-bokmaal  \n",
                "  + norwegian-nynorsk  \n",
                "  + norwegian-nynorsklia  \n",
                "* old_church_slavonic-proiel  \n",
                "* Old French: old_french-srcmf  \n",
                "* Old Russian: old_russian-torot  \n",
                "* Persian: persian-seraji  \n",
                "* Polish  \n",
                "  + polish-lfg  \n",
                "  + polish-pdb  \n",
                "  + polish-sz  \n",
                "* Portugese  \n",
                "  + portuguese-bosque  \n",
                "  + portuguese-br  \n",
                "  + portuguese-gsd  \n",
                "* Romanian  \n",
                "  + romanian-nonstandard  \n",
                "  + romanian-rrt  \n",
                "* Russian  \n",
                "  + russian-gsd  \n",
                "  + russian-syntagrus  \n",
                "  + russian-taiga  \n",
                "* Sanskrit: sanskrit-ufal  \n",
                "* Scottish Gaelic: scottish_gaelic-arcosg  \n",
                "* Serbian: serbian-set  \n",
                "* Slovak: slovak-snk  \n",
                "* Slovenian  \n",
                "  + slovenian-ssj  \n",
                "  + slovenian-sst  \n",
                "* Spanish  \n",
                "  + spanish-ancora  \n",
                "  + spanish-gsd  \n",
                "* Swedish  \n",
                "  + swedish-lines  \n",
                "  + swedish-talbanken  \n",
                "* Tamil: tamil-ttb  \n",
                "* Telugu: telugu-mtg  \n",
                "* Turkish: turkish-imst  \n",
                "* Ukrainian: ukrainian-iu  \n",
                "* Upper Sorbia: upper_sorbian-ufal  \n",
                "* Urdu: urdu-udtb  \n",
                "* Uyghur: uyghur-udt  \n",
                "* Vietnamese: vietnamese-vtb   \n",
                "* Wolof: wolof-wtb   \n",
                "\n",
                "\n",
                "To download any of these models, we can use the `udpipe_download_model` function. For example, to download the `english-ewt` model, we would use the call: `m_eng\t<- udpipe::udpipe_download_model(language = \"english-ewt\")`. \n",
                "\n",
                "We start by loading  a text\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# load text\n",
                "text <- readLines(\"https://slcladal.github.io/data/testcorpus/linguistics06.txt\", skipNul = T)\n",
                "# clean data\n",
                "text <- text %>%\n",
                " str_squish() \n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "***\n",
                "\n",
                "You can also use you own data. The code chunk below shows you how to upload two files from your own computer **BUT** to be able to load your own data, you need to click on the folder symbol to the left of the screen:\n",
                "\n",
                "![Colab Folder Symbol](https://slcladal.github.io/images/ColabFolder.png)\n",
                "\n",
                "Then on the upload symbol. \n",
                "\n",
                "![Colab Upload Symbol](https://slcladal.github.io/images/ColabUpload.png)\n",
                "\n",
                "Next, upload the files you want to analyze and then the respective files names in the `file` argument of the `scan` function. When you then execute the code (like to code chunk below, you will upload your own data.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "mytext1 <- scan(file = \"linguistics01.txt\",\n",
                "            what = \"char\", \n",
                "            sep = \"\", \n",
                "            quote = \"\", \n",
                "            quiet = T, \n",
                "            skipNul = T) %>%\n",
                "            paste0(collapse = \" \")\n",
                "mytext2 <- scan(file = \"linguistics02.txt\",\n",
                "            what = \"char\", \n",
                "            sep = \"\", \n",
                "            quote = \"\", \n",
                "            quiet = T, \n",
                "            skipNul = T) %>%\n",
                "            paste0(collapse = \" \")\n",
                "# inspect\n",
                "mytext1; mytext2\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "To apply the code and functions below to your own data, you will need to modify the code chunks and replace the data we use here with your own data object. \n",
                "\n",
                "***\n",
                "\n",
                "\n",
                "Now that we have a text that we can work with, we will download a pre-trained language model.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# download language model\n",
                "m_eng\t<- udpipe::udpipe_download_model(language = \"english-ewt\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "If you have downloaded a model once, you can also load the model directly from the current Google Drive directory if you have downloaded it in the current session.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# load language model from your computer after you have downloaded it once\n",
                "#m_eng <- udpipe_load_model(file = \"english-ewt-ud-2.5-191206.udpipe\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We can now use the model to annotate out text.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# tokenise, tag, dependency parsing\n",
                "text_anndf <- udpipe::udpipe_annotate(m_eng, x = text) %>%\n",
                "  as.data.frame() %>%\n",
                "  dplyr::select(-sentence)\n",
                "# inspect\n",
                "head(text_anndf, 10)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "It can be useful to extract only the words and their pos-tags and convert them back into a text format (rather than a tabular format). \n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "tagged_text <- paste(text_anndf$token, \"/\", text_anndf$xpos, collapse = \" \", sep = \"\")\n",
                "# inspect tagged text\n",
                "tagged_text\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# POS-Tagging non-English texts \n",
                "\n",
                "We can apply the same method for annotating, e.g. adding pos-tags, to other languages. For this, we could train our own model, or, we can use one of the many pre-trained language models that `udpipe` provides.\n",
                "\n",
                "Let us explore how to do this by using  example texts from different languages, here from German and Spanish (but we could also annotate texts from any of the wide variety of languages for which UDPipe provides pre-trained models.\n",
                "\n",
                "\n",
                "We begin by loading a German and a Dutch text.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# load texts\n",
                "gertext <- readLines(\"https://slcladal.github.io/data/german.txt\") \n",
                "duttext <- readLines(\"https://slcladal.github.io/data/dutch.txt\") \n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Next, we install the pre-trained language models.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# download language model\n",
                "m_ger\t<- udpipe::udpipe_download_model(language = \"german-gsd\")\n",
                "m_dut\t<- udpipe::udpipe_download_model(language = \"dutch-alpino\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Or we load them from our the current Google Drive workspace (if we have downloaded and saved them before).\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# load language model from your computer after you have downloaded it once\n",
                "#m_ger\t<- udpipe::udpipe_load_model(file = \"german-gsd-ud-2.5-191206.udpipe\")\n",
                "#m_dut\t<- udpipe::udpipe_load_model(file = \"dutch-alpino-ud-2.5-191206.udpipe\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now, pos-tag the German text.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# tokenise, tag, dependency parsing of german text\n",
                "ger_pos <- udpipe::udpipe_annotate(m_ger, x = gertext) %>%\n",
                "  as.data.frame() %>%\n",
                "  dplyr::summarise(postxt = paste(token, \"/\", xpos, collapse = \" \", sep = \"\")) %>%\n",
                "  dplyr::pull(unique(postxt))\n",
                "# inspect\n",
                "ger_pos\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "And finally, we also pos-tag the Dutch text.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# tokenise, tag, dependency parsing of german text\n",
                "nl_pos <- udpipe::udpipe_annotate(m_dut, x = duttext) %>%\n",
                "   as.data.frame() %>%\n",
                "  dplyr::summarise(postxt = paste(token, \"/\", xpos, collapse = \" \", sep = \"\")) %>%\n",
                "  dplyr::pull(unique(postxt))\n",
                "# inspect\n",
                "nl_pos\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Dependency Parsing Using UDPipe\n",
                "\n",
                "In addition to pos-tagging, we can also generate plots showing the syntactic dependencies of the different constituents of a sentence. For this, we generate an object that contains a sentence (in this case, the sentence *Linguistics is the scientific study of language*), and we then plot (or visualize) the dependencies using the `textplot_dependencyparser` fucntion.  \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# parse text\n",
                "sent <- udpipe::udpipe_annotate(m_eng, x = \"Linguistics is the scientific study of language\") %>%\n",
                "  as.data.frame()\n",
                "# inspect\n",
                "head(sent)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We now generate the plot.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# generate dependency plot\n",
                "dplot <- textplot::textplot_dependencyparser(sent, size = 4) \n",
                "# show plot\n",
                "dplot\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Syntactic Parsing with openNLP\n",
                "\n",
                "Parsing refers to another type of annotation in which either structural information (as in the case of XML documents) or syntactic relations are added to text. As syntactic parsing is commonly more relevant in the language sciences, the following will focus only on syntactic parsing. syntactic parsing builds on pos-tagging and allows drawing syntactic trees or dependencies. Unfortunately, syntactic parsing still has relatively high error rates when dealing with language that is not very formal. However, syntactic parsing is very reliable when dealing with written language.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "text <- readLines(\"https://slcladal.github.io/data/english.txt\")\n",
                "# convert character to string\n",
                "s <- as.String(text)\n",
                "# define sentence and word token annotator\n",
                "sent_token_annotator <- openNLP::Maxent_Sent_Token_Annotator()\n",
                "word_token_annotator <- openNLP::Maxent_Word_Token_Annotator()\n",
                "# apply sentence and word annotator\n",
                "a2 <- NLP::annotate(s, list(sent_token_annotator, word_token_annotator))\n",
                "# define syntactic parsing annotator\n",
                "parse_annotator <- openNLP::Parse_Annotator()\n",
                "# apply parser\n",
                "p <- parse_annotator(s, a2)\n",
                "# extract parsed information\n",
                "ptexts <- sapply(p$features, '[[', \"parse\")\n",
                "ptexts\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# read into NLP Tree objects.\n",
                "ptrees <- lapply(ptexts, Tree_parse)\n",
                "# show frist tree\n",
                "ptrees[[1]]\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "These trees can, of course, also be shown visually, for instance, in the form of a syntax trees (or tree dendrogram). \n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# load function\n",
                "source(\"https://slcladal.github.io/rscripts/parsetgraph.R\")\n",
                "# generate syntax tree\n",
                "parse2graph(ptexts[1], leaf.color='red',\n",
                "            # to put sentence in title (not advisable for long sentences)\n",
                "            #title = stringr::str_squish(stringr::str_remove_all(ptexts[1], \"\\\\(\\\\,{0,1}[A-Z]{0,4}|\\\\)\")), \n",
                "            margin=-0.05,\n",
                "            vertex.color=NA,\n",
                "            vertex.frame.color=NA, \n",
                "            vertex.label.font=2,\n",
                "            vertex.label.cex=.75,  \n",
                "            asp=.8,\n",
                "            edge.width=.5, \n",
                "            edge.color='gray', \n",
                "            edge.arrow.size=0)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Syntax trees are very handy because the allow us to check how reliable the parser performed. \n",
                "\n",
                "We can use the `get_phrase_type_regex` function from the `parsent` package written by Tyler Rinker\n",
                "to extract phrases from the parsed tree.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "pacman::p_load_gh(c(\"trinker/textshape\", \"trinker/parsent\"))\n",
                "nps <- get_phrase_type_regex(ptexts[1], \"NP\") %>%\n",
                "  unlist()\n",
                "# inspect\n",
                "nps\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We can now extract the leaves from the text to get the parsed object.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "nps_text <- stringr::str_squish(stringr::str_remove_all(nps, \"\\\\(\\\\,{0,1}[A-Z]{0,4}|\\\\)\"))\n",
                "# inspect\n",
                "nps_text\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Unfortunately, we can only extract top level phrases (the NPs with the NPs are npt extracted separately).\n",
                "\n",
                "In order to extract all phrases, we can use the `phrasemachine` from the CRAN archive. \n",
                "\n",
                "We now load the  `phrasemachine` package and pos-tag the text(s) (we will simply re-use the English text we pos-tagged before.)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# pos tag text\n",
                "tagged_documents <- phrasemachine::POS_tag_documents(text)\n",
                "# inspect\n",
                "tagged_documents\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "In a next step, we can use the `extract_phrases` function to extract phrases.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#extract phrases\n",
                "phrases <- phrasemachine::extract_phrases(tagged_documents,\n",
                "                                          regex = \"(A|N)*N(PD*(A|N)*N)*\",\n",
                "                                          maximum_ngram_length = 8,\n",
                "                                          minimum_ngram_length = 1)\n",
                "# inspect\n",
                "phrases\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now, we have all noun phrases that occur in the English sample text.\n",
                "\n",
                "\n",
                "# Citation & Session Info \n",
                "\n",
                "Schweinberger, Martin. 2022. *POS-Tagging and Syntactic Parsing with R*. Brisbane: The University of Queensland. url: https://slcladal.github.io/tagging.html.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "sessionInfo()\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# References\n",
                "\n",
                "Hornik, Kurt. 2019. *OpenNLP: Apache Opennlp Tools Interface*. https://cran.r-project.org/web/packages/openNLP/index.html.\n",
                "\n",
                "Kumar, Ashish, and Avinash Paul. 2016. *Mastering Text Mining with R*. Packt Publishing Ltd.\n",
                "\n",
                "Wiedemann, Gregor, and Andreas Niekler. 2017. Hands-on: A Five Day Text Mining Course for Humanists and Social Scientists in R. In *Proceedings of the Workshop on Teaching NLP for Digital Humanities (Teach4DH2017)*, Berlin, Germany, September 12, 2017., 57–65. http://ceur-ws.org/Vol-1918/wiedemann.pdf.\n",
                "\n",
                "Wijffels, Jan. 2021. *Udpipe: Tokenization, Parts of Speech Tagging, Lemmatization and Dependency Parsing with the ’Udpipe’ ’Nlp’ Toolkit.* https://CRAN.R-project.org/package=udpipe.\n"
            ]
        }
    ],
    "metadata": {
        "anaconda-cloud": "",
        "kernelspec": {
            "display_name": "R",
            "langauge": "R",
            "name": "ir"
        },
        "language_info": {
            "codemirror_mode": "r",
            "file_extension": ".r",
            "mimetype": "text/x-r-source",
            "name": "R",
            "pygments_lexer": "r",
            "version": "3.4.1"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}
