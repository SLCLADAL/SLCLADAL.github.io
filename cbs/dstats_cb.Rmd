![An interactive LADAL notebook](https://slcladal.github.io/images/uq1.jpg)


This tutorial is the interactive Jupyter notebook accompanying the [*Language Technology and Data Analysis Laboratory* (LADAL) tutorial *Descriptive Statistics with R*](https://ladal.edu.au/dstats.html). The tutorial provides more details and background information while this interactive notebook focuses strictly on practical aspects.


**Preparation and session set up**

We set up our session by activating the packages we need for this tutorial. 

```{r prep2, message=FALSE, warning=FALSE, class.source='klippy'}
# activate packages
library(boot)
library(DescTools)
library(dplyr)
library(stringr)
library(ggplot2)
library(flextable)
library(ggpubr)
library(psych)
library(Rmisc)
```


Once you have initiated the session by executing the code shown above, you are good to go.

If you are using this notebook on your own computer and you have not already installed the R packages listed above, you need to install them. You can install them by replacing the `library` command with `install.packages` and putting the name of the package into quotation marks like this: `install.packages("dplyr")`. Then, you simply run this command and R will install the package you specified.




***

## Using your own data

While the tutorial uses data from the LADAL website, you can also use your own data. You can see below what you need to do to upload and use your own data.

The code chunk below allows you to upload two files from your own computer. To be able to load your own data, you need to click on the folder symbol to the left of the screen:

![Binder Folder Symbol](https://slcladal.github.io/images/binderfolder.JPG)


Then on the upload symbol.

![Binder Upload Symbol](https://slcladal.github.io/images/binderupload.JPG)

Next, upload the files you want to analyze and then the respective files names in the file argument of the scan function. When you then execute the code (like to code chunk below, you will upload your own data.

```{r eval = F}
mytable1 <- openxlsx::read.xlsx("testdata1.xlsx", sheet = 1)
# inspect
mytable1
```


**Keep in mind though that you need to adapt the names of the files in the code chunks below so that the code below work on your own data!**

***



# Measures of Centrality

In linguistics three measures of centrality or measures of central tendency are of particular relevance: the *mean*, the *median* and the *mode*. What measure is appropriate depends on the type of variable scaling, the distribution of the data, and what is the intended aim of the data summary.


## Mean

The mean is used when the data is numeric and normally distributed. The mean is calculated by applying the formula shown below.

\begin{equation}
  \bar{x}=\frac{1}{n} \sum_{i=1}^n x_i = \frac{x_{1}+x_{2}+ \dots + x_{n}}{n}
\end{equation}

To calculate the mean, sum up all values and divide by the number of values. In R, the *mean* is calculated as follows.

```{r ds06, message=FALSE, warning=FALSE, class.source='klippy'}
# create numeric vector
frequencies <- c(3, 40, 15, 87)
# calculate mean
mean(frequencies)
```

The mean is the most common way to summarize numeric variables and it is very easy and intuitive to understand. A disadvantage of the mean is that it is very strongly affected by outliers which is why the median is the preferable measure of centrality when dealing with data that is not normal or that contains outliers.


## Median

The median can be used for both numeric and ordinal variables. In contrast to the mean, it is more robust and not as easily affected by outliers. 

\begin{equation}
median_{x}=
\begin{cases}
x_{\frac{n+1}{2}} & n\text{ uneven} \\
\frac{1}{2}\bigl(x_{\frac{n}{2}}+x_{\frac{n+1}{2}}\bigr) & n\text{ even}
\end{cases}
\label{eq:median}
\end{equation}


In R, the *median* is calculated as shown below.

```{r ds11, message=FALSE, warning=FALSE}
# create a vector consisting out of ranks
ranks <- c(rep(1, 9), rep(2, 160), rep(3, 70), rep(4, 15), rep(5, 9), rep(6, 57))
# calculate median
median(ranks)
```

In our case, the median age is *19-25* because the 160^th^ speaker belongs to the 2^nd^ age group, i.e. the age group with speakers between 19 and 25 years old.  


## Mode

The mode is typically used when dealing with categorical variables and it reports which level of a factor or a categorical variable is the most frequent. 

In R the *mode* is calculated as shown below:

```{r ds16, message=FALSE, warning=FALSE}
# create a factor with the current residence of speakers
CurrentResidence <- c(rep("Belfast", 98),         # repeat "Belfast" 98 times
                      rep("Down", 20),            # repeat "Down" 20 times
                      rep("Dublin (city)", 110),  # repeat "Dublin (city)" 110 times
                      rep("Limerick", 13),        # repeat "Limerick" 13 times
                      rep("Tipperary", 19))       # repeat "Tipperary" 19 times
# calculate mode
names(which.max(table(CurrentResidence)))         # extract which level occurs most frequently
```

A word of warning is in order here as only the first(!) maximal value is provided by R even if several categories have the same frequency. 



# Measures of Variability

Measures of variability provide information about the distribution of values such as whether the data are distributed evenly and do not differ substantially or whether the data are rather heterogeneous and are distributed very unevenly. In the following, we will have a look at the *range*, *variance*, and the *standard deviation*. 

## Range

The range is the simplest measure of variability and reports the lowest and highest value of a distribution. That is, the range provides minimum and maximum of a vector to show the span of values within a distribution.

In R, the *range* is extracted as shown below.

```{r ds24, message=FALSE, warning=FALSE}
# create a numeric vector
Moscow <- c(-5, -12, 5, 12, 15, 18, 22, 23, 20, 16, 8, 1)
min(Moscow); max(Moscow) # extract range
```

The lowest temperature value for Moscow is -12 degrees Celsius and the highest value is 23 degrees Celsius. The range thus spans from -12 to 23. 

## Interquartile range (IQR)

The interquartile range (IQR) denotes the range that encompasses the central 50 percent of data points and thus  informs about how values are distributed. 

The easiest way to extract the IQR in R is to apply the `summary` function to a vector as shown below and then subtract the value of the  1^st^ quartile from the value of the 3^rd^ quartile.

```{r ds25, message=FALSE, warning=FALSE}
summary(Moscow) # extract IQR
```

The `summary` function reports that the minimum temperature is -12 degrees Celsius and that the maximum temperature is 23 degrees Celsius. Also, the lower 25 percent of the data fall within -12 and 4 degrees Celsius (from the minimum value to the 1^st^ quartile) and the upper 25 percent fall within 18.5 and 23 degrees Celsius (from the 3^rd^ quartile to the maximum value). The IQR range represents a range that encompasses the central 50% of the data and thus represents the value that can be calculated by subtracting the value of the 1^st^ from the value of the 3^rd^ quartile..

Thus, the IQR is 18.5 - 4 = 14.5 

.

## Variance

The variance is calculated according to the formula below. To calculate the variance, each value is subtracted from the mean and the result is squared. The squared values are then added and the resulting sum is divided by the number of values minus 1.


$s = \sigma^2 = \frac{1}{n-1} \sum_{i=1}^{n} (x_i - \bar{x})^{2}$


For our example, the variance of temperatures for Moscow is 123.6591 and 9.477273 for Hamburg.

In R, the *variance* is calculated as shown below.

```{r ds26, message=FALSE, warning=FALSE}
sd(Moscow)^2
```


## Standard deviation

The standard deviation (abbreviated with capital $sigma$ $\sigma$) is calculated according to first equation shown below or, alternatively, according to second equation shown below and it is the square root of the squared variance.

$\sigma = \sqrt{s} = \sqrt{\frac{1}{n-1} \sum_{i=1}^{n} (x_i - \bar{x})^2}$

$\sigma = \sqrt{\frac{ \sum_{i=1}^{n} (x_i - \bar{x})^2}{n-1}}$

For our example, the first equation shown above provides a standard deviation of 11.12 for Moscow and a standard deviation of 3.08 for Hamburg.

In R, the *standard deviation* is calculated as shown below.

```{r ds28, message=FALSE, warning=FALSE}
# calculate standard deviation
sd(Moscow) 
```

The standard deviation of temperature values of Moscow is 11.12.



## Standard Error

The standard error is a measure of variability and it reports the average distance from some parameters (most often from the mean). It is calculated as the standard deviation of the residuals of the parameter in question. To exemplify the standard error, we will have a look at reaction times which show how fast participants realized that a sequence of letters were either existing words or just a sequence of letters. 

```{r ds30, echo = F, message=FALSE, warning=FALSE}
set.seed(12345)
RT = c(rnorm(5, 400, 50), rnorm(5, 380, 50), rnorm(5, 450, 50), rnorm(5, 480, 50))
State = c(rep("Sober", 10), rep("Drunk", 10))
Gender = rep(c(rep("Male", 5), rep("Female", 5)), 2)
rts <- data.frame(RT, State, Gender) %>%
  dplyr::mutate(RT = round(RT, 3))
# show data
rts
```

The standard error of the mean is calculated using the equation below.

\begin{equation}
\sigma~{\bar{x}}~ =\frac{\sigma}{\sqrt{n}} 
\end{equation}

The standard error can be calculated manually (see below) by implementing the equation from above.

```{r ds31, message=FALSE, warning=FALSE}
sd(rts$RT, na.rm=TRUE) /  
   sqrt(length(rts$RT[!is.na(rts$RT)]))  
```

An easier way to extract standard errors is to use the `describe` function from the `psych` package (see below)

```{r ds32, message=FALSE, warning=FALSE}
# describe data
psych::describe(rts$RT, type=2)
```

# Confidence Intervals

Confidence intervals provide an estimation of in-between which values the reported value would lie in the population with a confidence of, e.g., 95 percent. 

\begin{equation}
  \bar{x} \mp z \frac{s}{\sqrt{n}}
\end{equation}

The z-value for 95% probability (a two-tailed) of a normal distribution is 1.96. To check this, we can use the `qnorm` function and extract the z-value for a probability of .975 - we do not use .95 because we want 2.5% of the lower tail (-1.96) and 2.5% of the higher tail (1.96). 

```{r zci, message=FALSE, warning=FALSE}
qnorm(0.975)   
```
This means that for a 95% confidence interval for normally distributed data, we can use the formula shown below.

\begin{equation}
  \bar{x} \mp 1.96 \frac{s}{\sqrt{n}}
\end{equation}

If we have a vector of values (e.g., 4,5,2,3,1,4,3,6,3,2,4,1), we can easily calculate the confidence intervals for the mean as follows.

```{r cim1, message=FALSE, warning=FALSE}
# calculate mean
m <- mean(c(4,5,2,3,1,4,3,6,3,2,4,1))
# calculate standard deviation
s <- sd(c(4,5,2,3,1,4,3,6,3,2,4,1))
# calculate n
n <- length(c(4,5,2,3,1,4,3,6,3,2,4,1))
# calculate lower and upper ci
lower <- m-1.96*(s/sqrt(n))
upper <- m+1.96*(s/sqrt(n))
# show lower ci, mean, and upper ci
lower; m; upper
```


There are several functions in R to extract confidence intervals. To show how you can do this for different types of elements, we will continue to work with the reaction times data.


## Confidence Intervals for Simple Vectors

Confidence intervals (CIs) give a range that’s likely to include a population value with a certain degree of confidence. As such, CIs tell us how likely it is to get a value within a certain range if we drew another sample from the same population. 

One easy method for extracting confidence intervals is to apply the `CI` function from the `Rmisc` package.

```{r ds36, message=FALSE, warning=FALSE}
# extract mean and confidence intervals
Rmisc::CI(rts$RT, ci=0.95)   
``` 


```{r ds34b, echo = F,  message=FALSE, warning=FALSE}
# extract mean and confidence intervals
tt <- Rmisc::CI(rts$RT, ci=0.95)  
```


The ´CI´ function provides the mean reaction time (`r tt[2]`) and the 95 percent confidence band. With 95 percent confidence, the mean reaction time will have a mean between `r round(tt[3], 2)`  and `r round(tt[1], 2)` milliseconds (ms).

Another way to extract the mean and its confidence intervals is by using  `t.test` function.  

```{r ds34,  message=FALSE, warning=FALSE}
# extract mean and confidence intervals
stats::t.test(rts$RT, conf.level=0.95)  
```


Another alternative to extract the man ans the confidence interval from a range of values is to use the `MeanCI` function from the `DescTools` package.

```{r ds37, message=FALSE, warning=FALSE}
# extract mean and confidence intervals
DescTools::MeanCI(rts$RT, conf.level=0.95)   
``` 

This method is particularly interesting because it uses bootstrapping or resampling the data. As such, it is an empirical method to extract the mean and the confidence intervals. The values will differ given how many samples are drawn and we can get very precise estimates using this method.


```{r ds38, message=FALSE, warning=FALSE}
# extract mean CIs
DescTools::MeanCI(rts$RT, method="boot", type="norm", R=1000)
``` 

Because this is a data-driven approach, the results will vary, depending on the characteristics of the resampled data. To illustrate, compare the values provided above to the values generated below.

```{r ds40,  message=FALSE, warning=FALSE}
# extract mean CIs
DescTools::MeanCI(rts$RT, method="boot", type="norm", R=1000)
```

Another method for extracting the mean and the confidence intervals from a range of values using bootstrapping is to use the `boot` function from the `boot` package.

```{r ds42, message=FALSE, warning=FALSE}
# function to extract values
BootFunction = function(x, index) {                        
                  return(c(mean(x[index]),
                           var(x[index]) / length(index)))
}
# apply function to data
Bootstrapped = boot(data=rts$RT,     
                    statistic=BootFunction,
                    R=1000)
# extract values
mean(Bootstrapped$t[,1])                                   
# alternative to extract values
boot.ci(Bootstrapped, conf=0.95)                           
```  


The advantage of using bootstrapping methods lies in the fact that the data is (frequently) not distributed normally which is not an issue for the bootstrapping and it will thus provide more reliable results as it does not rely on distributional assumptions about the data. 

## Confidence Intervals for Grouped Data

To extract the confidence intervals for grouped data, we can sue the `summarySE` function from the `Rmisc` package.

```{r ds41,  message=FALSE, warning=FALSE}
# apply summarySE function to data
Rmisc::summarySE(data=rts,   
                 # define variable representing frequencies
                 measurevar="RT", 
                 # define grouping variable
                 groupvars="Gender",
                 # extract standard deviation, standard error, and confidence intervals
                 conf.interval = 0.95)   
```  


## Confidence Intervals for Nominal Data

We now turn to confidence intervals for nominal data. When dealing with nominal data, confidence intervals can be determined with the `binom.test` function in the in-built `stats` package. Alternative methods are available via the `BinomCI` and `MultinomCI` functions from the `DescTools` package.  More advanced techniques for confidence intervals on nominal data are available via the `PropCIs` package.

```{r ds44, message=FALSE, warning=FALSE}
stats::binom.test(2, 20, 0.5,              # binom.test(x, n, p = 0.5, ...)
                  alternative="two.sided", # define sidedness
                  conf.level=0.95)         # define confidence level
```  

Another way to use the `BinomCI` function is shown below.
 
```{r ds48,  message=FALSE, warning=FALSE}
# extract CIs                  
BinomCI(2, 20,                        # apply BinomCI function
        conf.level = 0.95,            # define ci
        method = "modified wilson")   # define method for ci extraction
```  

## Confidence Intervals for Multinomial Data

We use the `MultinomCI` function to extract the confidence intervals form multinominal data.

```{r ds50, message=FALSE, warning=FALSE}
observed = c(35,74,22,69)       # define multinominal vector
MultinomCI(observed,            # apply MultinomCI function
           conf.level=0.95,     # define ci
           method="goodman")    # define method for ci extraction
```  


***

[Back to LADAL](https://ladal.edu.au/dstats.html)

***
