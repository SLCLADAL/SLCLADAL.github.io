{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "![An interactive LADAL notebook](https://slcladal.github.io/images/uq1.jpg)\n",
                "\n",
                "# Concordancing with R\n",
                "\n",
                "This tutorial is the interactive Jupyter notebook accompanying the [*Language Technology and Data Analysis Laboratory* (LADAL) tutorial **Concordancing with R**](https://ladal.edu.au/kwics.html). \n",
                "\n",
                "\n",
                "**Preparation and session set up**\n",
                "\n",
                "We start by activating the packages we need for this tutorial.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# set options\n",
                "options(warn=-1)  # do not show warnings or messages\n",
                "# activate packages\n",
                "library(quanteda) # for concordancing\n",
                "library(dplyr)    # for table processing\n",
                "library(stringr)  # for text processing\n",
                "library(writexl)  # for saving data\n",
                "library(here)     # for easy pathing\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<div class=\"warning\" style='padding:0.1em; background-color: rgba(251,184,0,.5); color:#51247a'>\n",
                "<span>\n",
                "<p style='margin-top:1em; text-align:center'>\n",
                "<b>If you are using this notebook on your own computer and you have not already installed the R packages listed above, you need to install them.<br> <a href=\n",
                "\"https://www.dataquest.io/blog/install-package-r/\">\n",
                "        <div class=\"text\">\n",
                "        <p style='margin-top:1em; text-align:center'>\n",
                "            Here is Dataquest post on how to install packages in R.\n",
                "            <\/p>\n",
                "        <\/div>\n",
                "    <\/a>\n",
                "    <\/b>\n",
                "<\/p>\n",
                "<\/span>\n",
                "<\/div>\n",
                "\n",
                "<br>\n",
                "\n",
                "## Using your own data\n",
                "\n",
                "While the tutorial uses data from the LADAL website, you can also use your own data. To use your own data, follow the instructions below.\n",
                "\n",
                "To be able to load your own data, you need to click on the folder symbol to the left of the screen:\n",
                "\n",
                "![Binder Folder Symbol](https://slcladal.github.io/images/binderfolder.JPG)\n",
                "\n",
                "\n",
                "Then, when the menu has unfolded, click on the smaller folder symbol (encircled in red in the picture below).\n",
                "\n",
                "![Small Binder Folder Symbol](https://slcladal.github.io/images/upload2.png)\n",
                "\n",
                "Now, you are in the main menu and can click on the 'MyData' folder.\n",
                "\n",
                "![MyData Folder Symbol](https://slcladal.github.io/images/upload3.png)\n",
                "\n",
                "Now, that you are in the MyData folder, you can click on the upload symbol.\n",
                "\n",
                "![Binder Upload Symbol](https://slcladal.github.io/images/binderupload.JPG)\n",
                "\n",
                "Select and upload the files you want to analyze. When you then execute the code chunk below, you will upload your own data and you can then use it in this notebook.\n",
                "\n",
                "<div class=\"warning\" style='padding:0.1em; background-color: rgba(251,184,0,.5); color:#51247a'>\n",
                "<span>\n",
                "<p style='margin-top:1em; text-align:center'>\n",
                "<b>IMPORTANT: here, we assume that you upload some form of text data - not tabular data! You can upload only txt and docx files!<\/b><br>\n",
                "<\/p>\n",
                "<p style='margin-left:1em;'>\n",
                "<\/p><\/span>\n",
                "<\/div>\n",
                "\n",
                "<br>\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "myfiles <- list.files(here::here(\"MyData\"), # path to the corpus data\n",
                "                          # full paths - not just the names of the files\n",
                "                          full.names = T) \n",
                "# load files\n",
                "mytext <- sapply(myfiles, function(x){\n",
                "  x <- scan(x, \n",
                "            what = \"char\", \n",
                "            sep = \"\", \n",
                "            quote = \"\", \n",
                "            quiet = T, \n",
                "            skipNul = T)\n",
                "  x <- paste0(x, sep = \" \", collapse = \" \")\n",
                "  x <- stringr::str_squish(x)\n",
                "})\n",
                "# inspect\n",
                "str(mytext)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<div class=\"warning\" style='padding:0.1em; background-color: rgba(251,184,0,.5); color:#51247a'>\n",
                "<span>\n",
                "<p style='margin-top:1em; text-align:center'>\n",
                "<b>If you are using your own data, do not execute the next code chunk and change `mytext` into `text` in the code chunk above.<\/b><br>\n",
                "<\/p>\n",
                "<p style='margin-left:1em;'>\n",
                "<\/p><\/span>\n",
                "<\/div>\n",
                "\n",
                "<br>\n",
                "\n",
                "\n",
                "If you do not use your own data, you can load the default data, Lewis Caroll's  *Alice's Adventures in Wonderland*, by executing the following code chunk.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "text <- base::readRDS(url(\"https://slcladal.github.io/data/alice.rda\", \"rb\"))\n",
                "# inspect first 6 text elements\n",
                "head(text)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The data consists of many separate text elements. Next, we combine the elements into a single text. Then we clean it by removing superfluous white spaces and then we split it into individual  words (this is called tokenising).\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "text <- text %>%\n",
                "  # collapse lines into a single  text\n",
                "  paste0(collapse = \" \") %>%\n",
                "  # remove superfluous white spaces\n",
                "  str_squish() %>%\n",
                "  # tokenize\n",
                "  tokens()\n",
                "# inspect\n",
                "head(text)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The text is now split into individual words. \n",
                "\n",
                "## Creating simple concordances\n",
                "\n",
                "Now we can extract concordances using the `kwic` function from the `quanteda` package. This function requires \n",
                "\n",
                "+ a text (`x`) \n",
                "+ a keyword defined by a search pattern (`pattern`) \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "mykwic <- kwic(\n",
                "  # define text\n",
                "  text, \n",
                "  # define target word (this is called the \"search pattern\")\n",
                "  pattern = \"alice\")\n",
                "# inspect\n",
                "mykwic %>%\n",
                "  as.data.frame() %>%\n",
                "  head()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "To extract the frequency of the search term (*alice*) we can use `nrow` or `length`.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "nrow(mykwic); length(mykwic$keyword)\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The results show that there are 386 instances of the search term (*alice*). \n",
                "\n",
                "We now  increase the context window size to 10 words/elements (the default is 5 word/elements).\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "mykwic_longer <- kwic(text, pattern = \"alice\", \n",
                "  # define context window size\n",
                "  window = 10)\n",
                "# inspect\n",
                "mykwic_longer %>%\n",
                "  as.data.frame() %>%\n",
                "  head()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Exporting concordances\n",
                "\n",
                "To export a concordance table as an MS Excel spreadsheet, we use `write_xlsx`. Be aware that we use the `here` function to  save the file in the current working directory.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "write_xlsx(mykwic, here::here(\"mykwic.xlsx\"))\n",
                "# check where the working directory is\n",
                "getwd()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Extracting more than single words\n",
                "\n",
                "To extract more than just one word, we specify that we are searching for a `phrase` (you can also include full sentences).\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "kwic_pooralice <- kwic(text, pattern = phrase(\"poor alice\"))\n",
                "# inspect\n",
                "kwic_pooralice %>%\n",
                "  as.data.frame() %>%\n",
                "  head()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Searches using regular expressions\n",
                "\n",
                "Regular expressions add flexibility by allowing us to search for abstract patterns rather than concrete words or phrases. A regular expression is a special sequence of characters that describe a pattern. For more information about regular expression in R [see this tutorial](https://ladal.edu.au/regex.html).\n",
                "\n",
                "\n",
                "To specifiy that we are using regular expressions, we set `valuetype` to `\"regex\"`. The search pattern `\"\\\\balic.*|\\\\bhatt.*\"` retrieves elements that contain `alic` and `hatt` followed by any characters and where the `a` in `alic` and the `h` in `hatt` are at a word boundary. The `|` is an operator (like `+`, `-`, or `*`) that stands for *or*.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# define search patterns\n",
                "patterns <- c(\"\\\\balic.*|\\\\bhatt.*\")\n",
                "kwic_regex <- kwic(text, patterns, \n",
                "  # define valuetype\n",
                "  valuetype = \"regex\")\n",
                "# inspect\n",
                "kwic_regex %>%\n",
                "  as.data.frame() %>%\n",
                "  head()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Piping concordances\n",
                "\n",
                "Quite often, we only want to retrieve patterns if they occur in a certain context. For instance, we might be interested in instances of *alice* but only if the preceding word is *poor*. Such conditional concordances can be retrieved by piping using  `%>%`  which can be translated as *and then*. We then extract concordances that contain *poor* using `filter`. Note the the `$` stands for the end of a string so that *poor$* means that *poor* is the last element preceding the keyword.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "kwic_pipe <- kwic(x = text, pattern = \"alice\") %>%\n",
                "  dplyr::filter(stringr::str_detect(pre, \"poor$|little$\"))\n",
                "# inspect\n",
                "kwic_pipe %>%\n",
                "  as.data.frame() %>%\n",
                "  head()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Arranging concordances and adding frequency information\n",
                "\n",
                "When inspecting concordances, it is useful to re-order the concordances so that they appeared with frequent collocates first (at the top). To reorder concordances, we use  `arrange` which takes the column by which we want to re-arrange as its argument. \n",
                "\n",
                "In the example below, we extract all instances of *alice* and then arrange the instances according to the alphabetical order of the first word in the `post` column.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "kwic_ordered <- kwic(x = text, pattern = \"alice\") %>%\n",
                "  dplyr::arrange(post)\n",
                "# inspect\n",
                "kwic_ordered %>%\n",
                "  as.data.frame() %>%\n",
                "  head() \n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "A more useful option may be to arrange concordances according to the frequency of co-occurring terms. In order to do this, we need to extract the co-occurring words and their frequency. We do this by using `mutate`, `group_by`, `n()` and`str_remove_all`. \n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "kwic_ordered_coll <- kwic(\n",
                "  # define text\n",
                "  x = text, \n",
                "  # define search pattern\n",
                "  pattern = \"alice\") %>%\n",
                "  # extract word following the keyword\n",
                "  dplyr::mutate(post_word = str_remove_all(post, \" .*\")) %>%\n",
                "  # group following words\n",
                "  dplyr::group_by(post_word) %>%\n",
                "  # extract frequencies of the following words\n",
                "  dplyr::mutate(post_word_freq = n()) %>%\n",
                "  # arrange/order by the frequency of the following word\n",
                "  dplyr::arrange(-post_word_freq)\n",
                "# inspect\n",
                "kwic_ordered_coll %>%\n",
                "  as.data.frame() %>%\n",
                "  head()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Ordering by subsequent elements\n",
                "\n",
                "We now extract the three words following the keyword (*alice*) and organize the concordances by the frequencies of the following words. \n",
                "\n",
                "We begin by creating a clean *post* column (that is all in)we convert post to lower case and remove punctuation).\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "mykwic %>%\n",
                "  # convert to data frame\n",
                "  as.data.frame() %>%\n",
                "  # create new CleanPost\n",
                "  dplyr::mutate(CleanPost = stringr::str_remove_all(post, \"[:punct:]\"),\n",
                "                CleanPost = stringr::str_squish(CleanPost),\n",
                "                CleanPost = tolower(CleanPost))-> mykwic_following\n",
                "# inspect\n",
                "head(mykwic_following)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Next, we extract the 1^st^, 2^nd^, and 3^rd^ words following the keyword.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "mykwic_following %>%\n",
                "  # extract first element after keyword\n",
                "  dplyr::mutate(FirstWord = stringr::str_remove_all(CleanPost, \" .*\")) %>%\n",
                "  # extract second element after keyword\n",
                "  dplyr::mutate(SecWord = stringr::str_remove(CleanPost, \".*? \"),\n",
                "                SecWord = stringr::str_remove_all(SecWord, \" .*\")) %>%\n",
                "  # extract third element after keyword\n",
                "  dplyr::mutate(ThirdWord = stringr::str_remove(CleanPost, \".*? \"),\n",
                "                ThirdWord = stringr::str_remove(ThirdWord, \".*? \"),\n",
                "                ThirdWord = stringr::str_remove_all(ThirdWord, \" .*\")) -> mykwic_following\n",
                "# inspect\n",
                "head(mykwic_following)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now, we calculate the frequencies of the subsequent words and order in descending order from the  1^st^ to the 3^rd^ word following the keyword.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "mykwic_following %>%\n",
                "  # calculate frequency of following words\n",
                "  # 1st word\n",
                "  dplyr::group_by(FirstWord) %>%\n",
                "  dplyr::mutate(FreqW1 = n()) %>%\n",
                "  # 2nd word\n",
                "  dplyr::group_by(SecWord) %>%\n",
                "  dplyr::mutate(FreqW2 = n()) %>%\n",
                "  # 3rd word\n",
                "  dplyr::group_by(ThirdWord) %>%\n",
                "  dplyr::mutate(FreqW3 = n()) %>%\n",
                "  # ungroup\n",
                "  dplyr::ungroup() %>%\n",
                "  # arrange by following words\n",
                "  dplyr::arrange(-FreqW1, -FreqW2, -FreqW3) -> mykwic_following\n",
                "# inspect results\n",
                "head(mykwic_following, 10)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The results now show the concordance arranged by the frequency of the words following the keyword.\n",
                "\n",
                "\n",
                "[Back to LADAL](https://ladal.edu.au/kwics.html)\n"
            ]
        }
    ],
    "metadata": {
        "anaconda-cloud": "",
        "kernelspec": {
            "display_name": "R",
            "langauge": "R",
            "name": "ir"
        },
        "language_info": {
            "codemirror_mode": "r",
            "file_extension": ".r",
            "mimetype": "text/x-r-source",
            "name": "R",
            "pygments_lexer": "r",
            "version": "3.4.1"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}
