{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "![An interactive LADAL notebook](https://slcladal.github.io/images/uq1.jpg)\n",
                "\n",
                "***\n",
                "\n",
                "Please copy this Jupyter notebook so that you are able to edit it.\n",
                "\n",
                "Simply go to: File > Save a copy in Drive.\n",
                "\n",
                "Once you have done that, you are good to go.\n",
                "\n",
                "***\n",
                "\n",
                "This tutorial is the interactive Jupyter notebook accompanying the [*Language Technology and Data Analysis Laboratory* (LADAL) tutorial *Concordancing with R*](https://ladal.edu.au/kwics.html). \n",
                "\n",
                "\n",
                "\n",
                "***\n",
                "\n",
                "\n",
                "**Preparation and session set up**\n",
                "\n",
                "We set up our session by activating the packages we need for this tutorial.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# activate packages\n",
                "library(quanteda)\n",
                "library(dplyr)\n",
                "library(stringr)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Once you have initiated the session by executing the code shown above, you are good to go.\n",
                "\n",
                "If you are using this notebook on your own computer and you have not already installed the R packages listed above, you need to install them. You can install them by replacing the `library` command with `install.packages` and putting the name of the package into quotation marks like this: `install.packages(\"quanteda\")`. Then, you simply run this command and R will install the package you specified.\n",
                "\n",
                "\n",
                "## Loading and processing textual data\n",
                "\n",
                "For this tutorial, the default data represents the text of Lewis Caroll's  *Alice's Adventures in Wonderland* which we download from the [GitHub data repository of the *Language Technology and Data Analysis Laboratory* (LADAL)](https://slcladal.github.io/data). \n",
                "\n",
                "***\n",
                "\n",
                "## Using your own data\n",
                "\n",
                "While the tutorial uses data from the LADAL website, you can also use your own data. You can see below what you need to do to upload and use your own data.\n",
                "\n",
                "The code chunk below allows you to upload two files from your own computer. To be able to load your own data, you need to click on the folder symbol to the left of the screen:\n",
                "\n",
                "![Binder Folder Symbol](https://slcladal.github.io/images/binderfolder.JPG)\n",
                "\n",
                "\n",
                "Then on the upload symbol.\n",
                "\n",
                "![Binder Upload Symbol](https://slcladal.github.io/images/binderupload.JPG)\n",
                "\n",
                "Next, upload the files you want to analyze and then the respective files names in the file argument of the scan function. When you then execute the code (like to code chunk below, you will upload your own data.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "mytext1 <- scan(file = \"linguistics01.txt\",\n",
                "            what = \"char\", \n",
                "            sep = \"\", \n",
                "            quote = \"\", \n",
                "            quiet = T, \n",
                "            skipNul = T) %>%\n",
                "            paste0(collapse = \" \")\n",
                "mytext2 <- scan(file = \"linguistics02.txt\",\n",
                "            what = \"char\", \n",
                "            sep = \"\", \n",
                "            quote = \"\", \n",
                "            quiet = T, \n",
                "            skipNul = T) %>%\n",
                "            paste0(collapse = \" \")\n",
                "# inspect\n",
                "mytext1; mytext2\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Keep in mind though that you need to adapt the names of the texts in the code chunks below so that the code below work on your own texts!**\n",
                "\n",
                "***\n",
                "\n",
                "If you do not use your own data, you can continue with the default data, Lewis Caroll's  *Alice's Adventures in Wonderland*, which we load by running the code below (but you have to have access to the internet to do so).\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "text <- base::readRDS(url(\"https://slcladal.github.io/data/alice.rda\", \"rb\"))\n",
                "# inspect\n",
                "head(text)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The inspection of the data shows that the data consists of a vector of individual strings which contain the example text. This means that the data requires formatting so that we can use it. Therefore, we collapse it into a single object (or text),  remove superfluous white spaces, and then tokenize the data (tokenizing means that we split it into individual  tokens or words.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "text <- text %>%\n",
                "  # collapse lines into a single  text\n",
                "  paste0(collapse = \" \") %>%\n",
                "  # remove superfluous white spaces\n",
                "  str_squish() %>%\n",
                "  # tokenize\n",
                "  tokens()\n",
                "# inspect\n",
                "head(text)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The result confirms that the entire text is now split into individual words. \n",
                "\n",
                "## Creating simple concordances\n",
                "\n",
                "Now that we have loaded the data, we can easily extract concordances using the `kwic` function from the `quanteda` package. The `kwic` function takes the text (`x`) and the search pattern (`pattern`) as it main arguments but it also allows the specification of the context window, i.e. how many words/elements are show to the left and right of the key word (we will go over this later on).\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "kwic_alice <- kwic(\n",
                "  # define text\n",
                "  text, \n",
                "  # define target word (this is called the \"search pattern\")\n",
                "  pattern = \"alice\")\n",
                "# inspect\n",
                "kwic_alice %>%\n",
                "  as.data.frame() %>%\n",
                "  head()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We can easily extract the frequency of the search term (*alice*) using the `nrow` or the `length` functions which provide the number of rows of a tables (`nrow`) or the length of a vector (`length`).\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "nrow(kwic_alice); length(kwic_alice$keyword)\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The results show that there are 386 instances of the search term (*alice*). To get a better understanding of the use of a word, it is often useful to extract more context. This is easily done by increasing size of the context window. To do this, we specify the `window` argument of the `kwic` function. In the example below, we set the context window size to 10 words/elements rather than using the default (which is 5 word/elements).\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "kwic_alice_longer <- kwic(text, pattern = \"alice\", \n",
                "  # define context window size\n",
                "  window = 10)\n",
                "# inspect\n",
                "kwic_alice_longer %>%\n",
                "  as.data.frame() %>%\n",
                "  head()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Extracting more than single words\n",
                "\n",
                "While extracting single words is very common, you may want to extract more than just one word. To extract phrases, all you need to so is to specify that the pattern you are looking for is a phrase, as shown below.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "kwic_pooralice <- kwic(text, pattern = phrase(\"poor alice\"))\n",
                "# inspect\n",
                "kwic_pooralice %>%\n",
                "  as.data.frame() %>%\n",
                "  head()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Of course you can extend this to longer sequences such as entire sentences. However, you may want to extract more or less concrete patterns rather than words or phrases. To search for patterns rather than words, you need to include regular expressions in your search pattern. \n",
                "\n",
                "\n",
                "\n",
                "## Searches using regular expressions\n",
                "\n",
                "Regular expressions allow you to search for abstract patterns rather than concrete words or phrases which provides you with an extreme flexibility in what you can retrieve. A regular expression (in short also called *regex* or *regexp*) is a special sequence of characters that stand for are that describe a pattern. For more information about regular expression in R [see this tutorial](https://ladal.edu.au/regex.html).\n",
                "\n",
                "\n",
                "To include regular expressions in your KWIC searches, you include them in your search pattern and set the argument `valuetype` to `\"regex\"`. The search pattern `\"\\\\balic.*|\\\\bhatt.*\"` retrieves elements that contain `alic` and `hatt` followed by any characters and where the `a` in `alic` and the `h` in `hatt` are at a word boundary, i.e. where they are the first letters of a word. Hence, our search would not retrieve words like *malice* or *shatter*. The `|` is an operator (like `+`, `-`, or `*`) that stands for *or*.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# define search patterns\n",
                "patterns <- c(\"\\\\balic.*|\\\\bhatt.*\")\n",
                "kwic_regex <- kwic(text, patterns, \n",
                "  # define valuetype\n",
                "  valuetype = \"regex\")\n",
                "# inspect\n",
                "kwic_regex %>%\n",
                "  as.data.frame() %>%\n",
                "  head()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Piping concordances\n",
                "\n",
                "Quite often, we only want to retrieve patterns if they occur in a certain context. For instance, we might be interested in instances of *alice* but only if the preceding word is *poor*. Such conditional concordances could be extracted using regular expressions but they are easier to retrieve by piping. Piping is done using the `%>%` function from the `dplyr` package and the piping sequence can be translated as *and then*. We can then filter those concordances that contain *poor* using the `filter` function from the `dplyr` package. Note the the `$` stands for the end of a string so that *poor$* means that *poor* is the last element in the string that is preceding the keyword.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "kwic_pipe <- kwic(x = text, pattern = \"alice\") %>%\n",
                "  dplyr::filter(stringr::str_detect(pre, \"poor$|little$\"))\n",
                "# inspect\n",
                "kwic_pipe %>%\n",
                "  as.data.frame() %>%\n",
                "  head()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Piping is a very useful helper function and it is very frequently used in R - not only in the context of text processing but in all data science related domains.\n",
                "\n",
                "## Arranging concordances and adding frequency information\n",
                "\n",
                "When inspecting concordances, it is useful to re-order the concordances so that they do not appear in the order that they appeared in the text or texts but by the context. To reorder concordances, we can use the `arrange` function from the `dplyr` package which takes the column according to which we want to re-arrange the data as it main argument. \n",
                "\n",
                "In the example below, we extract all instances of *alice* and then arrange the instances according to the content of the `post` column in alphabetical.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "kwic_ordered <- kwic(x = text, pattern = \"alice\") %>%\n",
                "  dplyr::arrange(post)\n",
                "# inspect\n",
                "kwic_ordered %>%\n",
                "  as.data.frame() %>%\n",
                "  head() \n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Arranging concordances according to alphabetical properties may, however, not be the most useful option. A more useful option may be to arrange concordances according to the frequency of co-occurring terms or collocates. In order to do this, we need to extract the co-occurring words and calculate their frequency. We can do this by combining the  `mutate`, `group_by`, `n()` functions from the `dplyr` package with the `str_remove_all` function from the `stringr` package. Then, we arrange the concordances by the frequency of the collocates in descending order (that is why we put a `-` in the arrange function). In order to do this, we need to \n",
                "\n",
                "1. create a new variable or column which represents the word that co-occurs with, or, as in the example below, immediately follows the search term. In the example below, we use the `mutate` function to create a new column called `post_word`. We then use the `str_remove_all` function to remove everything except for the word that immediately follows the search term (we simply remove everything and including a white space).\n",
                "\n",
                "2. group the data by the word that immediately follows the search term.\n",
                "\n",
                "3. create a new column called `post_word_freq` which represents the frequencies of all the words that immediately follow the search term.\n",
                "\n",
                "4. arrange the concordances by the frequency of the collocates in descending order.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "kwic_ordered_coll <- kwic(\n",
                "  # define text\n",
                "  x = text, \n",
                "  # define search pattern\n",
                "  pattern = \"alice\") %>%\n",
                "  # extract word following the keyword\n",
                "  dplyr::mutate(post_word = str_remove_all(post, \" .*\")) %>%\n",
                "  # group following words\n",
                "  dplyr::group_by(post_word) %>%\n",
                "  # extract frequencies of the following words\n",
                "  dplyr::mutate(post_word_freq = n()) %>%\n",
                "  # arrange/order by the frequency of the following word\n",
                "  dplyr::arrange(-post_word_freq)\n",
                "# inspect\n",
                "kwic_ordered_coll %>%\n",
                "  as.data.frame() %>%\n",
                "  head()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We add more columns according to which we could arrange the concordance following the same schema. For example, we could add another column that represented the frequency of words that immediately preceded the search term and then arrange according to this column.\n",
                "\n",
                "## Ordering by subsequent elements\n",
                "\n",
                "In this section, we will extract the three words following the keyword (*alice*) and organize the concordances by the frequencies of the following words. We begin by inspecting the first 6 lines of the concordance of *alice*.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "head(kwic_alice)\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Next, we take the concordances and create a clean post column that is all in lower case and that does not contain any punctuation.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "kwic_alice %>%\n",
                "  # convert to data frame\n",
                "  as.data.frame() %>%\n",
                "  # create new CleanPost\n",
                "  dplyr::mutate(CleanPost = stringr::str_remove_all(post, \"[:punct:]\"),\n",
                "                CleanPost = stringr::str_squish(CleanPost),\n",
                "                CleanPost = tolower(CleanPost))-> kwic_alice_following\n",
                "# inspect\n",
                "head(kwic_alice_following)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "In a next step, we extract the 1^st^, 2^nd^, and 3^rd^ words following the keyword.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "kwic_alice_following %>%\n",
                "  # extract first element after keyword\n",
                "  dplyr::mutate(FirstWord = stringr::str_remove_all(CleanPost, \" .*\")) %>%\n",
                "  # extract second element after keyword\n",
                "  dplyr::mutate(SecWord = stringr::str_remove(CleanPost, \".*? \"),\n",
                "                SecWord = stringr::str_remove_all(SecWord, \" .*\")) %>%\n",
                "  # extract third element after keyword\n",
                "  dplyr::mutate(ThirdWord = stringr::str_remove(CleanPost, \".*? \"),\n",
                "                ThirdWord = stringr::str_remove(ThirdWord, \".*? \"),\n",
                "                ThirdWord = stringr::str_remove_all(ThirdWord, \" .*\")) -> kwic_alice_following\n",
                "# inspect\n",
                "head(kwic_alice_following)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Next, we calculate the frequencies of the subsequent words and order in descending order from the  1^st^ to the 3^rd^ word following the keyword.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "kwic_alice_following %>%\n",
                "  # calculate frequency of following words\n",
                "  # 1st word\n",
                "  dplyr::group_by(FirstWord) %>%\n",
                "  dplyr::mutate(FreqW1 = n()) %>%\n",
                "  # 2nd word\n",
                "  dplyr::group_by(SecWord) %>%\n",
                "  dplyr::mutate(FreqW2 = n()) %>%\n",
                "  # 3rd word\n",
                "  dplyr::group_by(ThirdWord) %>%\n",
                "  dplyr::mutate(FreqW3 = n()) %>%\n",
                "  # ungroup\n",
                "  dplyr::ungroup() %>%\n",
                "  # arrange by following words\n",
                "  dplyr::arrange(-FreqW1, -FreqW2, -FreqW3) -> kwic_alice_following\n",
                "# inspect results\n",
                "head(kwic_alice_following, 10)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The results now show the concordance arranged by the frequency of the words following the keyword.\n",
                "\n",
                "## Concordances from transcriptions\n",
                "\n",
                "As many analyses in the language sciences use transcripts as their primary data and because transcripts have features that require additional processing, we will now perform concordancing based on on transcripts. As a first step, we load five example transcripts that represent the first five files from the Irish component of the [International Corpus of English](https://www.ice-corpora.uzh.ch/en.html).\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# define corpus files\n",
                "files <- paste(\"https://slcladal.github.io/data/ICEIrelandSample/S1A-00\", 1:5, \".txt\", sep = \"\")\n",
                "# load corpus files\n",
                "transcripts <- sapply(files, function(x){\n",
                "  x <- readLines(x)\n",
                "  })\n",
                "# inspect\n",
                "transcripts[[1]][1:10] %>%\n",
                "  as.data.frame()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The first ten lines shown above let us know that, after the header (`<S1A-001 Riding>`) and the symbol which indicates the start of the transcript (`<I>`), each utterance is preceded by a sequence which indicates the section, file, and speaker (e.g. `<S1A-001$A>`). The first utterance is thus uttered by speaker `A` in file `001` of section `S1A`. In addition, there are several sequences that provide meta-linguistic information which indicate the beginning of a speech unit (`<#>`), pauses (`<,>`), and laughter (`<&> laughter <\/&>`).\n",
                "\n",
                "To perform the concordancing, we need to change the format of the transcripts because the `kwic` function only works on character, corpus, tokens object- in their present form, the transcripts represent a list which contains vectors of strings. To change the format, we collapse the individual utterances into a single character vector for each transcript.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "transcripts_collapsed <- sapply(files, function(x){\n",
                "  # read-in text\n",
                "  x <- readLines(x)\n",
                "  # paste all lines together\n",
                "  x <- paste0(x, collapse = \" \")\n",
                "  # remove superfluous white spaces\n",
                "  x <- str_squish(x)\n",
                "})\n",
                "# inspect\n",
                "transcripts_collapsed %>%\n",
                "    substr(start=1, stop=500) %>%\n",
                "  as.data.frame()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We can now extract the concordances.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "kwic_trans <- quanteda::kwic(tokens(transcripts_collapsed), pattern = phrase(\"you know\"))\n",
                "# inspect\n",
                "kwic_trans %>%\n",
                "  as.data.frame() %>%\n",
                "  head() \n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The results show that each non-alphanumeric character is counted as a single word which reduces the context of the keyword substantially. Also, the *docname* column contains the full path to the data which make it hard to parse the content of the table. To address the first issue, we specify the tokenizer that we will use to not disrupt the annotation too much. In addition, we clean the *docname* column and extract only the file name. Lastly, we will expand the context window to 10 so that we have a better understanding of the context in which the phrase was used.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "kwic_trans <- quanteda::kwic(tokens(transcripts_collapsed, what = \"fasterword\"), \n",
                "  pattern = phrase(\"you know\"), window = 10) %>%\n",
                "  # clean docnames\n",
                "  dplyr::mutate(docname = str_replace_all(docname, \".*/([A-Z][0-9][A-Z]-[0-9]{1,3}).txt\", \"\\\\1\"))\n",
                "# inspect\n",
                "kwic_trans %>%\n",
                "  as.data.frame() %>%\n",
                "  head()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "[Back to LADAL](https://ladal.edu.au/kwics.html)\n",
                "\n"
            ]
        }
    ],
    "metadata": {
        "anaconda-cloud": "",
        "kernelspec": {
            "display_name": "R",
            "langauge": "R",
            "name": "ir"
        },
        "language_info": {
            "codemirror_mode": "r",
            "file_extension": ".r",
            "mimetype": "text/x-r-source",
            "name": "R",
            "pygments_lexer": "r",
            "version": "3.4.1"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}
