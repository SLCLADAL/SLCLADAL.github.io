{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "![An interactive LADAL notebook](https://slcladal.github.io/images/uq1.jpg)\n",
                "\n",
                "# Analyzing Co-Occurrences and Collocations\n",
                "\n",
                "This tutorial is the interactive Jupyter notebook accompanying the [*Language Technology and Data Analysis Laboratory* (LADAL) tutorial *Analyzing Co-Occurrences, N-grams, and Collocations in R*](https://ladal.edu.au/coll.html). \n",
                "\n",
                "\n",
                "**Preparation and session set up**\n",
                "\n",
                "We set up our session by activating the packages we need for this tutorial. \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# set options\n",
                "options(warn=-1)  # do not show warnings or messages\n",
                "# load packages\n",
                "library(GGally)        # extended functionality in ggplot2\n",
                "library(ggdendro)      # visualize dendrograms with ggplot2\n",
                "library(Matrix)        # sparse matrix operations\n",
                "library(quanteda)      # quantitative analysis of textual data\n",
                "library(quanteda.textplots)  # additional text visualization tools in quanteda\n",
                "library(dplyr)         # data manipulation and transformation\n",
                "library(stringr)       # string manipulation functions\n",
                "library(tm)            # text mining and analysis\n",
                "library(tidytext)      # tidy data principles applied to text data\n",
                "library(readxl)        # reading Excel files\n",
                "library(writexl)       # saving data in Excel format\n",
                "library(openxlsx)      # reading and saving Excel files\n",
                "library(here)          # for generating relative paths\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Using your own data\n",
                "\n",
                "<div class=\"warning\" style='padding:0.1em; background-color: rgba(215,209,204,.3); color:#51247a'>\n",
                "<span>\n",
                "<p style='margin-top:1em; text-align:center'>\n",
                "\n",
                "While the tutorial uses example data, you can also **use your own data**. To use your own data, click on the folder called `MyTexts` (it is in the menu to the left of the screen) and then simply drag and drop your txt-files into the folder. When you then execute the code chunk below, you will upload your own data and you can then use it in this notebook.<br>\n",
                "<br>\n",
                "You can upload <b>only txt-files<\/b> (simple unformatted files created in or saved by a text editor)! The notebook assumes that you upload some form of text data - not tabular data! <br>\n",
                "<br>\n",
                "<b>IMPORTANT<\/b>: Be sure to <b>replace `mytext` with `text` in the code chunk below and  do not execute the code chunk which loads an example text<\/b> so that you work with your and not the sample data!<\/b><br>\n",
                "<\/p>\n",
                "<p style='margin-left:1em;'>\n",
                "<\/p><\/span>\n",
                "<\/div>\n",
                "\n",
                "<br>\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "myfiles <- list.files(here::here(\"notebooks/MyTexts\"), # path to the corpus data\n",
                "                          # full paths - not just the names of the files\n",
                "                          full.names = T) \n",
                "# loop over the vector 'myfiles' that contains paths to the data\n",
                "mytext <- sapply(myfiles, function(x){\n",
                "\n",
                "  # read the content of each file using 'scan'\n",
                "  x <- scan(x, \n",
                "            what = \"char\",    # specify that the input is characters\n",
                "            sep = \"\",         # set separator to an empty string (read entire content)\n",
                "            quote = \"\",       # set quote to an empty string (no quoting)\n",
                "            quiet = T,        # suppress scan messages\n",
                "            skipNul = T)      # skip NUL bytes if encountered\n",
                "\n",
                "  # combine the character vector into a single string with spaces\n",
                "  x <- paste0(x, sep = \" \", collapse = \" \")\n",
                "\n",
                "  # remove extra whitespaces using 'str_squish' from the 'stringr' package\n",
                "  x <- stringr::str_squish(x)\n",
                "\n",
                "})\n",
                "\n",
                "# inspect the structure of the text object\n",
                "str(mytext)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Loading the example data\n",
                "\n",
                "We begin by loading the data which represents the text of Lewis Caroll's  *Alice's Adventures in Wonderland*. \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# load text\n",
                "text <- base::readRDS(url(\"https://slcladal.github.io/data/alice.rda\", \"rb\"))\n",
                "# inspect data\n",
                "head(text)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Extracting N-Grams\n",
                "\n",
                "The `quanteda.textstats` package offers the `textstat_collocations` function for extracting N-grams. This function uses the following main arguments\n",
                "\n",
                "+ `x`: a character, corpus, or tokens object.  \n",
                "+ `method`: association measure for detecting collocations. Currently this is limited to \"lambda\".  \n",
                "+ `size`: integer; the length of the ngram. The default is 2 - if you want to extract tri-grams set `size = 3` and if you want to extract four-grams set `size = 4` and so on.  \n",
                "+ `min_count`: numeric; minimum frequency of collocations that will be scored.  \n",
                "+ `smoothing`: numeric; a smoothing parameter added to the observed counts (default is 0.5).  \n",
                "+ `tolower`: logical; if TRUE, tokens are transformed to lower-case.  \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# concatenate the elements in the 'text' object\n",
                "text %>% \n",
                "  paste0(collapse = \" \") %>%\n",
                "  # convert the concatenated text into tokens\n",
                "  quanteda::tokens() %>%\n",
                "  # identify and extract bigrams \n",
                "  quanteda.textstats::textstat_collocations(size = 2, min_count = 1) %>%\n",
                "  # convert into a data frame and save results in an object called 'ngrams'\n",
                "  as.data.frame() %>%\n",
                "  # order by frequency\n",
                "  dplyr::arrange(-count) -> ngrams\n",
                "# inspect the first 10 rows\n",
                "head(ngrams, 10)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Exporting tables\n",
                "\n",
                "To export a table as an MS Excel spreadsheet, we use `write_xlsx`. \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# save data for MyOutput folder\n",
                "write_xlsx(ngrams, here::here(\"notebooks/MyOutput/ngrams.xlsx\"))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<div class=\"warning\" style='padding:0.1em; background-color: rgba(215,209,204,.3); color:#51247a'>\n",
                "<span>\n",
                "<p style='margin-top:1em; text-align:center'>\n",
                "<b>You will find the generated MS Excel spreadsheet named *ngrams.xlsx* in the `MyOutput` folder (located on the left side of the screen).<\/b> <br><br>Simply double-click the `MyOutput` folder icon, then right-click on the *ngrams.xlsx* file, and choose Download from the dropdown menu to download the file. <br>\n",
                "<\/p>\n",
                "<p style='margin-left:1em;'>\n",
                "<\/p><\/span>\n",
                "<\/div>\n",
                "\n",
                "<br>\n",
                "\n",
                "\n",
                "## Identifying Collocations\n",
                "\n",
                "There are various ways of finding collocations depending on the data provided, the context, and the association measure (which represents information of how strong the association between the words is). Below, you will see how to detect collocations in two different data structures:  \n",
                "+ a list of sentences  \n",
                "+ concordances  \n",
                "\n",
                "### Identifying collocations in sentences\n",
                "\n",
                "In the following, we will find collocations based on their co-occurrence in linguistic units (in this case sentences but this could also be speech units, tweets, or other paragraphs, etc.).\n",
                "\n",
                "**Data preparation**\n",
                "\n",
                "In a first step, we split our example text into sentences and clean the data (removing punctuation, converting to lower case, etc.).\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "text %>% \n",
                "  # concatenate the elements in the 'text' object\n",
                "  paste0(collapse = \" \") %>%\n",
                "  # separate possessives and contractions\n",
                "  stringr::str_replace_all(fixed(\"'\"), fixed(\" '\")) %>%\n",
                "  stringr::str_replace_all(fixed(\"’\"), fixed(\" '\")) %>%\n",
                "  # split text into sentences\n",
                "  tokenizers::tokenize_sentences() %>%\n",
                "  # unlist sentences\n",
                "  unlist() %>%\n",
                "  # remove non-word characters\n",
                "  stringr::str_replace_all(\"\\\\W\", \" \") %>%\n",
                "  stringr::str_replace_all(\"[^[:alnum:] ]\", \" \") %>%\n",
                "  # remove superfluous white spaces\n",
                "  stringr::str_squish() %>%\n",
                "  # convert to lower case and save in 'sentences' object\n",
                "  tolower() -> sentences\n",
                "# inspect first 10 sentences\n",
                "head(sentences, 10)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "*** \n",
                "\n",
                "**Loading your own data**\n",
                "\n",
                "<div class=\"warning\" style='padding:0.1em; background-color: rgba(215,209,204,.3); color:#51247a'>\n",
                "<span>\n",
                "<p style='margin-top:1em; text-align:center'>\n",
                "\n",
                "While the tutorial uses example data, you can also **use your own data**. To use your own data, click on the folder called `MyTexts` (it is in the menu to the left of the screen) and then simply drag and drop your txt-files into the folder. When you then execute the code chunk below, you will upload your own data and you can then use it in this notebook.<br>\n",
                "<br>\n",
                "You can upload <b>only txt-files<\/b> (simple unformatted files created in or saved by a text editor)! The notebook assumes that you upload some form of text data - not tabular data! <br>\n",
                "<br>\n",
                "<b>IMPORTANT<\/b>: Be sure to <b>replace `mytext` with `sentences` in the code chunk below and  do not execute the code chunk which loads an example text<\/b> so that you work with your and not the sample data!<\/b><br>\n",
                "<\/p>\n",
                "<p style='margin-left:1em;'>\n",
                "<\/p><\/span>\n",
                "<\/div>\n",
                "\n",
                "<br>\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "myfiles <- list.files(here::here(\"notebooks/MyTexts\"), # path to the corpus data\n",
                "                          # full paths - not just the names of the files\n",
                "                          full.names = T) \n",
                "# loop over the vector 'myfiles' that contains paths to the data\n",
                "mytext <- sapply(myfiles, function(x){\n",
                "\n",
                "  # read the content of each file using 'scan'\n",
                "  x <- scan(x, \n",
                "            what = \"char\",    # specify that the input is characters\n",
                "            sep = \"\",         # set separator to an empty string (read entire content)\n",
                "            quote = \"\",       # set quote to an empty string (no quoting)\n",
                "            quiet = T,        # suppress scan messages\n",
                "            skipNul = T)      # skip NUL bytes if encountered\n",
                "\n",
                "  # combine the character vector into a single string with spaces\n",
                "  x <- paste0(x, sep = \" \", collapse = \" \")\n",
                "\n",
                "  # WARNING: OPTIONAL\n",
                "  # remove non-word characters\n",
                "  x <- stringr::str_remove_all(x, \"[^[:alnum:] ]\")\n",
                "  \n",
                "  # remove extra white spaces using 'str_squish' from the 'stringr' package\n",
                "  x <- stringr::str_squish(x)\n",
                "\n",
                "})\n",
                "\n",
                "# inspect the structure of the text object\n",
                "str(mytext)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "***\n",
                "\n",
                "Next, we feed the sentences to a function that tabulates all words and their co-occurrences.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# load function that generates a co-occurrence table\n",
                "source(\"https://slcladal.github.io/rscripts/colldf.R\")\n",
                "cooctb <- colldf(x = sentences)\n",
                "# inspect results\n",
                "head(cooctb, 10)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "To determine which terms collocate significantly and with what association strength, we use the following information (that is provided by the table above):\n",
                "\n",
                "* O~11~ = Number of times *word~1~* occurs *with* *word~2~* (\\[ w_1 \\cap w_2 \\])\n",
                "\n",
                "* O~12~ = Number of times *word~1~* occurs *without* *word~2~* (\\[ w_1 \\cup w_2 \\])\n",
                "\n",
                "* O~21~ = Number of times `CoocTerm` occurs without `Term`\n",
                "\n",
                "* O~22~ = Number of terms that are not `coocTerm` or `Term`\n",
                "\n",
                "Example:\n",
                "\n",
                "\n",
                "|              | w~2~ present       |     w~2~ absent |      |\n",
                " :---          | :-----:    |   --------:  | ---\n",
                "| **w~1~ present**     | O~11~      | O~12~        |  = R~1~\n",
                "| **w~1~ absent** | O~21~      | O~22~        |  = R~2~\n",
                "|              |  = C~1~    |   = C~2~     |  = N |\n",
                "\n",
                "\n",
                "\n",
                "We could calculate all collocations in the corpus (based on co-occurrence within the same sentence) or we can find collocations of a specific term - here, we will find collocations fo the term *alice*.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# load function that extracts association statistics\n",
                "source(\"https://slcladal.github.io/rscripts/assocstats.R\")\n",
                "# extract association statistics\n",
                "coocStatz <- assocstats(cooctb, \"alice\", 10, 5)\n",
                "# inspect results\n",
                "coocStatz %>%\n",
                "  dplyr::mutate_at(9:16, round, 3) %>%\n",
                "  head(10)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The output shows that the words most strongly associated with *alice* (aside from *alice*) are *said*, *thought*, and *afraid*.\n",
                "\n",
                "\n",
                "To export a table as an MS Excel spreadsheet, we use `write_xlsx`. \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# save data for MyOutput folder\n",
                "write_xlsx(coocStatz, here::here(\"notebooks/MyOutput/coocStatz.xlsx\"))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<div class=\"warning\" style='padding:0.1em; background-color: rgba(215,209,204,.3); color:#51247a'>\n",
                "<span>\n",
                "<p style='margin-top:1em; text-align:center'>\n",
                "<b>You will find the generated MS Excel spreadsheet named *coocStatz.xlsx* in the `MyOutput` folder (located on the left side of the screen).<\/b> <br><br>Simply double-click the `MyOutput` folder icon, then right-click on the *coocStatz.xlsx* file, and choose Download from the dropdown menu to download the file. <br>\n",
                "<\/p>\n",
                "<p style='margin-left:1em;'>\n",
                "<\/p><\/span>\n",
                "<\/div>\n",
                "\n",
                "<br>\n",
                "\n",
                "### Identifying collocations using kwics\n",
                "\n",
                "In this section, we will extract collocations and calculate association measures based on  concordances and the corpus the concordances were extracted from.\n",
                "\n",
                "We start by cleaning our corpus and splitting it into chapters. \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# clean corpus\n",
                "text %>%\n",
                "  # concatenate the elements in the 'text' object\n",
                "  paste0(collapse = \" \") %>%\n",
                "  # separate possessives and contractions\n",
                "  stringr::str_replace_all(fixed(\"'\"), fixed(\" '\")) %>%\n",
                "  stringr::str_replace_all(fixed(\"’\"), fixed(\" '\")) %>%\n",
                "  # split text into different chapters\n",
                "  stringr::str_split(\"CHAPTER [IVX]{1,4}\") %>%\n",
                "  # unlist sentences\n",
                "  unlist() %>%\n",
                "  # remove non-word characters\n",
                "  stringr::str_replace_all(\"\\\\W\", \" \") %>%\n",
                "  stringr::str_replace_all(\"[^[:alnum:] ]\", \" \") %>%\n",
                "  # remove superfluous white spaces\n",
                "  stringr::str_squish() %>%\n",
                "  # convert to lower case and save in 'sentences' object\n",
                "  tolower() -> texts\n",
                "# inspect first 100 words the first 10 chapters\n",
                "head(substr(texts, 1, 100), 10)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<div class=\"warning\" style='padding:0.1em; background-color: rgba(215,209,204,.3); color:#51247a'>\n",
                "<span>\n",
                "<p style='margin-top:1em; text-align:center'>\n",
                "We split the corpus into chapter to mirror the fact that most text data will come in the form of corpora which consist of different files containing texts.\n",
                "<\/p>\n",
                "<p style='margin-left:1em;'>\n",
                "<\/p><\/span>\n",
                "<\/div>\n",
                "\n",
                "<br>\n",
                "\n",
                "\n",
                "***\n",
                "\n",
                "<div class=\"warning\" style='padding:0.1em; background-color: rgba(215,209,204,.3); color:#51247a'>\n",
                "<span>\n",
                "<p style='margin-top:1em; text-align:center'>\n",
                "\n",
                "If you have data consisting of cleaned, lower-case, texts with each text being in a different txt-file, you can use your own data! Simply click on the folder called `MyTexts`  (it is in the menu to the left of the screen) and then simply drag and drop your txt-files into the folder. When you then execute the code chunk below, you will upload your own data and you can then use it in this notebook.<br>\n",
                "<br>\n",
                "You can upload <b>only txt-files<\/b> (simple unformatted files created in or saved by a text editor)! The notebook assumes that you upload some form of text data - not tabular data! <br>\n",
                "<br>\n",
                "<b>IMPORTANT<\/b>: Be sure to <b>replace `mytext` with `text` in the code chunk below and  do not execute the code chunk which loads an example text<\/b> so that you work with your and not the sample data!<\/b><br>\n",
                "<\/p>\n",
                "<p style='margin-left:1em;'>\n",
                "<\/p><\/span>\n",
                "<\/div>\n",
                "\n",
                "<br>\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "myfiles <- list.files(here::here(\"notebooks/MyTexts\"), # path to the corpus data\n",
                "                          # full paths - not just the names of the files\n",
                "                          full.names = T) \n",
                "# loop over the vector 'myfiles' that contains paths to the data\n",
                "mytext <- sapply(myfiles, function(x){\n",
                "\n",
                "  # read the content of each file using 'scan'\n",
                "  x <- scan(x, \n",
                "            what = \"char\",    # specify that the input is characters\n",
                "            sep = \"\",         # set separator to an empty string (read entire content)\n",
                "            quote = \"\",       # set quote to an empty string (no quoting)\n",
                "            quiet = T,        # suppress scan messages\n",
                "            skipNul = T)      # skip NUL bytes if encountered\n",
                "\n",
                "  # combine the character vector into a single string with spaces\n",
                "  x <- paste0(x, sep = \" \", collapse = \" \")\n",
                "\n",
                "  # remove extra whitespaces using 'str_squish' from the 'stringr' package\n",
                "  x <- stringr::str_squish(x)\n",
                "\n",
                "})\n",
                "\n",
                "# inspect the structure of the text object\n",
                "str(mytext)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "***\n",
                "\n",
                "Next, we generate a frequency list of words that occur around a keyword (we use the keyword *alice* in this example but you can also choose a different word).\n",
                "\n",
                "for this we use the `tokens_select` function (from the `quanteda` package) which has the following arguments: \n",
                "\n",
                "+ `x`: a text or collection of texts. The text needs to be tokenised, i.e. split it into individual words, which is why we use the *text* in the `tokens()` function. \n",
                "+ `pattern`: a keyword defined by a search pattern  \n",
                "+ `window`: the size of the context window (how many word before and after)  \n",
                "+ `valuetype`: the type of pattern matching  \n",
                "  + \"glob\" for \"glob\"-style wildcard expressions;  \n",
                "  + \"regex\" for regular expressions; or  \n",
                "  + \"fixed\" for exact matching  \n",
                "+ `selection`: a character to define if the key word should be retained in the resulting frequency list or if it should be removed. The argument offers two options  \n",
                "  + \"keep\"  \n",
                "  + \"remove\"  \n",
                "+ `case_insensitive`: logical; if TRUE, ignore case when matching a pattern or dictionary values\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "kwic_words <- quanteda::tokens_select(tokens(texts), \n",
                "                                      pattern = \"alice\", \n",
                "                                      window = 5, \n",
                "                                      selection = \"keep\") %>%\n",
                "  unlist() %>%\n",
                "  # tabulate results\n",
                "  table() %>%\n",
                "  # convert into data frame\n",
                "  as.data.frame() %>%\n",
                "  # rename columns\n",
                "  dplyr::rename(token = 1,\n",
                "                n = 2) %>%\n",
                "  # add a coulm with type\n",
                "  dplyr::mutate(type = \"kwic\")\n",
                "# inspect results\n",
                "head(kwic_words, 10)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Next, we create a frequency table of the entire clean corpus.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# load and execute function that creates frequency lists from text data\n",
                "source(\"https://slcladal.github.io/rscripts/corpuswords.R\")\n",
                "corpus_words <- corpuswords(texts)\n",
                "# inspect the results by displaying the first 10 rows\n",
                "head(corpus_words, 10)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Next, we combine the two frequency lists.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# load and execute function that joins 'corpus_words' and 'kwic_words' data frames on the 'token' column\n",
                "source(\"https://slcladal.github.io/rscripts/combinefreq.R\")\n",
                "freq_df <- combinefreq(corpus_words, kwic_words) \n",
                "# inspect resulting frequency list\n",
                "head(freq_df, 10)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "To determine which terms collocate significantly and with what association strength, we use the following information (that is provided by the table above):\n",
                "\n",
                "* O11 = Number of times token occurs in `kwic`\n",
                "\n",
                "* O12 = Number of times token occurs in `corpus` (without `kwic`)\n",
                "\n",
                "* O21 = Number of times other tokens occur in `kwic`\n",
                "\n",
                "* O22 = Number of times  other tokens occur in `corpus`\n",
                "\n",
                "Example:\n",
                "\n",
                "\n",
                "|              | kwic       |     corpus |      |\n",
                " :---          | :-----:    |   --------:  | ---\n",
                "| **token **     | O~11~      | O~12~        |  = R~1~\n",
                "| **other tokens** | O~21~      | O~22~        |  = R~2~\n",
                "|              |  = C~1~    |   = C~2~     |  = N |\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# load and execute function that extracts association statistics\n",
                "source(\"https://slcladal.github.io/rscripts/assockwic.R\")\n",
                "coocStatz <- assockwic(freq_df)\n",
                "# inspect results\n",
                "head(coocStatz, 10)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Visualising collocations\n",
                "\n",
                "### Dotplots\n",
                "\n",
                "We can now visualize the association strengths in a dotplot as shown in the code chunk below.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "coocStatz %>%\n",
                "  dplyr::arrange(-phi) %>%\n",
                "  head(20) %>%\n",
                "  ggplot(aes(x = reorder(token, phi, mean), y = phi)) +\n",
                "  geom_point() +\n",
                "  coord_flip() +\n",
                "  theme_bw() +\n",
                "  labs(x = \"Token\", y = \"Association strength (phi)\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Exporting images\n",
                "\n",
                "To export image as an png-file, we use `ggsave`. Be aware that we use the `here` function to save the file in the `MyOutput` folder.\n",
                "\n",
                "The `ggsave` function has the following main arguments:\n",
                "\n",
                "+ `filename`: File name to create on disk.    \n",
                "+ `device`: Device to use. Can either be a device function (e.g. png), or one of \"eps\", \"ps\", \"tex\" (pictex), \"pdf\", \"jpeg\", \"tiff\", \"png\", \"bmp\", \"svg\" or \"wmf\" (windows only). If NULL (default), the device is guessed based on the filename extension  \n",
                "+ `path`: Path of the directory to save plot to: path and filename are combined to create the fully qualified file name. Defaults to the working directory.  \n",
                "+ `width, height`: Plot size in units expressed by the units argument. If not supplied, uses the size of the current graphics device.  \n",
                "+ `units`: One of the following units in which the width and height arguments are expressed: \"in\", \"cm\", \"mm\" or \"px\".  \n",
                "+ `dpi`: Plot resolution. Also accepts a string input: \"retina\" (320), \"print\" (300), or \"screen\" (72). Applies only to raster output types.  \n",
                "+ `bg`: Background color. If NULL, uses the plot.background fill value from the plot theme.  \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# save network graph for MyOutput folder\n",
                "ggsave(here::here(\"notebooks/MyOutput/image_01.png\"), bg = \"white\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<div class=\"warning\" style='padding:0.1em; background-color: rgba(215,209,204,.3); color:#51247a'>\n",
                "<span>\n",
                "<p style='margin-top:1em; text-align:center'>\n",
                "<b>You will find the image-file named *image_01.png* in the `MyOutput` folder (located on the left side of the screen).<\/b> <br><br>Simply double-click the `MyOutput` folder icon, then right-click on the *image_01.png* file, and choose Download from the dropdown menu to download the file. <br>\n",
                "<\/p>\n",
                "<p style='margin-left:1em;'>\n",
                "<\/p><\/span>\n",
                "<\/div>\n",
                "\n",
                "<br>\n",
                "\n",
                "### Barplots\n",
                "\n",
                "We can now visualize the association strengths in a barplot as shown in the code chunk below.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "coocStatz %>%\n",
                "  dplyr::arrange(-phi) %>%\n",
                "  head(20) %>%\n",
                "  ggplot(aes(x = reorder(token, phi, mean), y = phi, label = phi)) +\n",
                "  geom_bar(stat = \"identity\") +\n",
                "  geom_text(aes(y = phi-0.005, label = round(phi, 3)), color = \"white\", size=3) + \n",
                "  coord_flip() +\n",
                "  theme_bw() +\n",
                "  labs(x = \"Token\", y = \"Association strength (phi)\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "To export the image to the `MyOutput` folder, run the code chunk below (the image will appear as `image_02.png` in the `MyOutput` folder.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# save network graph for MyOutput folder\n",
                "ggsave(here::here(\"notebooks/MyOutput/image_02.png\"), bg = \"white\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Dendrograms\n",
                "\n",
                "Another method for visualizing collocations are dendrograms (tree-diagrams) which show how similarity  to indicate groupings based on numeric values (e.g., association strength). \n",
                "\n",
                "We start by extracting the tokens that we want to show (the top 20 collocates of *alice*).\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "top20colls <- coocStatz %>%\n",
                "  dplyr::arrange(-phi) %>%\n",
                "  head(20) %>%\n",
                "  dplyr::pull(token)\n",
                "top20colls\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We then need to generate a feature co-occurrence matrix from a document-feature matrix based on the cleaned, lower case sentences of our text.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "keyword_fcm <- sentences %>%\n",
                "  quanteda::tokens() %>%\n",
                "  quanteda::dfm() %>%\n",
                "  quanteda::dfm_select(pattern = c(top20colls, \"alice\")) %>%\n",
                "  quanteda::fcm(tri = FALSE)\n",
                "# inspect\n",
                "keyword_fcm[1:6, 1:6]\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Then we generate the dendrogram based on a  distance matrix generated from the feature co-occurrence matrix.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "hclust(dist(keyword_fcm),       # use distance matrix as data\n",
                "       method=\"ward.D2\") %>%    # ward.D as linkage method\n",
                "  ggdendrogram() +              # generate dendrogram\n",
                "  ggtitle(\"20 most strongly collocating terms of 'alice'\")  # add title\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "To export the image to the `MyOutput` folder, run the code chunk below (the image will appear as `image_03.png` in the `MyOutput` folder.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# save network graph for MyOutput folder\n",
                "ggsave(here::here(\"notebooks/MyOutput/image_03.png\"), bg = \"white\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Network Graphs\n",
                "\n",
                "Network graphs are a very useful tool to show relationships (or the absence of relationships) between elements. \n",
                "\n",
                "To generate a network graph of the collocates of a keyword, we \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "quanteda.textplots::textplot_network(keyword_fcm,  \n",
                "                                     edge_alpha = 0.8, \n",
                "                                     edge_color = \"gray\",\n",
                "                                     vertex_labelsize = log(rowSums(keyword_fcm)),\n",
                "                                     edge_size = 2)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "To export the image to the `MyOutput` folder, run the code chunk below (the image will appear as `image_03.png` in the `MyOutput` folder.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# save network graph for MyOutput folder\n",
                "ggsave(here::here(\"notebooks/MyOutput/image_03.png\"), bg = \"white\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "***\n",
                "\n",
                "[Back to LADAL](https://ladal.edu.au/coll.html)\n",
                "\n",
                "***\n"
            ]
        }
    ],
    "metadata": {
        "anaconda-cloud": "",
        "kernelspec": {
            "display_name": "R",
            "langauge": "R",
            "name": "ir"
        },
        "language_info": {
            "codemirror_mode": "r",
            "file_extension": ".r",
            "mimetype": "text/x-r-source",
            "name": "R",
            "pygments_lexer": "r",
            "version": "3.4.1"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}
