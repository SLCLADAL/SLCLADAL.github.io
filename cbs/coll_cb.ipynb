{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "![uq](https://slcladal.github.io/images/uq1.jpg)\n",
                "\n",
                "# Introduction\n",
                "\n",
                "This tutorial introduces collocation and co-occurrence analysis with R and shows how to extract and visualize semantic links between words. The entire R markdown document for the present tutorial can be downloaded [here](https://slcladal.github.io/coll.Rmd). Parts of this tutorial build on and use materials from  [this tutorial](https://tm4ss.github.io/docs/Tutorial_5_Co-occurrence.html) on co-occurrence analysis with R by Andreas Niekler and Gregor Wiedemann [see @WN17].\n",
                "\n",
                "How would you find words that are associated with a specific term and how can you visualize such word nets? This tutorial addresses this issue by focusing on co-occurrence and collocations of words. Collocations are words that occur very frequently together. For example, *Merry Christmas* is a collocation because *merry* and *Christmas* occur more frequently together than would be expected by chance. This means that if you were to shuffle all words in a corpus and would then test the frequency of how often *merry* and *Christmas* co-occurred, they would occur significantly less often in the shuffled or randomized corpus than in a corpus that contain non-shuffled natural speech. \n",
                "\n",
                "> How can you determine if words occur more frequently together than would be expected by chance? \n",
                "\n",
                "This tutorial aims to show how you can answer this question. \n",
                "\n",
                "**Preparation and session set up**\n",
                "\n",
                "This tutorial is based on R. If you have not installed R or are new to it, you will find an introduction to and more information how to use R [here](https://slcladal.github.io/intror.html). For this tutorials, we need to install certain *packages* from an R *library* so that the scripts shown below are executed without errors. Before turning to the code below, please install the packages by running the code below this paragraph. If you have already installed the packages mentioned below, then you can skip ahead and ignore this section. To install the necessary packages, simply run the following code - it may take some time (between 1 and 5 minutes to install all of the libraries so you do not need to worry if it takes some time).\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# set options\n",
                "options(stringsAsFactors = F)\n",
                "options(scipen = 999)\n",
                "options(max.print=1000)\n",
                "# install packages\n",
                "install.packages(\"cluster\")\n",
                "install.packages(\"corpus\")\n",
                "install.packages(\"FactoMineR\")\n",
                "install.packages(\"factoextra\")\n",
                "install.packages(\"flextable\")\n",
                "install.packages(\"GGally\")\n",
                "install.packages(\"ggdendro\")\n",
                "install.packages(\"igraph\")\n",
                "install.packages(\"network\")\n",
                "install.packages(\"Matrix\")\n",
                "install.packages(\"quanteda\")\n",
                "install.packages(\"sna\")\n",
                "install.packages(\"tidyverse\")\n",
                "install.packages(\"tm\")\n",
                "install.packages(\"tokenizers\")\n",
                "# install klippy for copy-to-clipboard button in code chunks\n",
                "remotes::install_github(\"rlesur/klippy\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "Next, we load the packages.\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# load packages\n",
                "library(cluster)\n",
                "library(corpus)\n",
                "library(FactoMineR)\n",
                "library(factoextra)\n",
                "library(flextable)\n",
                "library(GGally)\n",
                "library(ggdendro)\n",
                "library(igraph)\n",
                "library(network)\n",
                "library(Matrix)\n",
                "library(quanteda)\n",
                "library(sna)\n",
                "library(tidyverse)\n",
                "library(tm)\n",
                "library(tokenizers)\n",
                "# activate klippy for copy-to-clipboard button\n",
                "klippy::klippy()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "Finally, we need to manually install a package called `collustructions`. To install this package, go the [website of Susanne Flach](https://sfla.ch/collostructions/), who has written the `collustructions` package. On that website, you find different versions of that package for different operating systems (OS) like Windows and Mac. Next, download the version that is the right one for your OS, unzip the package file and copy it into your R library. Once you have copied the `collustructions` package in your R library, you can run the code chunks below to install and activate it.\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# install collostructions package\n",
                "install.packages(\"collostructions\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# load collostructions package\n",
                "library(collostructions)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "Once you have installed R, RStudio, and once you have initiated the session by executing the code shown above, you are good to go.\n",
                "\n",
                "# Extracting N-Grams and Collocations\n",
                "\n",
                "Collocations are terms that co-occur (significantly) more often together than would be expected by chance. A typical example of a collocation is *Merry Christmas* because the words *merry* and *Christmas* occur together more frequently together than would be expected, if words were just randomly stringed together.  \n",
                "\n",
                "N-grams are related to collocates in that they represent words that occur together (bi-grams are two words that occur together, tri-grams three words and so on). Fortunately, creating N-gram lists is very easy. We will use the Charles Darwin's *On the Origin of Species by Means of Natural Selection* as a data source and begin by generating a bi-gram list. As a first step, we load the data and split it into individual words.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# read in text\n",
                "darwin <- base::readRDS(url(\"https://slcladal.github.io/data/cdo.rda\", \"rb\")) %>%\n",
                "  paste0(collapse = \" \") %>%\n",
                "  stringr::str_squish() %>%\n",
                "  stringr::str_remove_all(\"- \")\n",
                "# further processing\n",
                "darwin_split <- darwin %>% \n",
                "  as_tibble() %>%\n",
                "  tidytext::unnest_tokens(words, value)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "darwin_split %>%\n",
                "  as.data.frame() %>%\n",
                "  head(15) %>%\n",
                "  flextable() %>%\n",
                "  flextable::set_table_properties(width = .5, layout = \"autofit\") %>%\n",
                "  flextable::theme_zebra() %>%\n",
                "  flextable::fontsize(size = 12) %>%\n",
                "  flextable::fontsize(size = 12, part = \"header\") %>%\n",
                "  flextable::align_text_col(align = \"center\") %>%\n",
                "  flextable::set_caption(caption = \"\")  %>%\n",
                "  flextable::border_outer()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "We can create bi-grams (N-grams consisting of two elements) by pasting every word together with the ford that immediately follows it. To do this, we could use a function that is already available but we can also very simply to this manually. The first step in creating bigrams manually consists in creating a table which holds every word in a corpus and the word that immediately precedes it. \n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# create data frame\n",
                "darwin_words <- darwin_split %>%\n",
                "  dplyr::rename(word1 = words) %>%\n",
                "  dplyr::mutate(word2 = c(word1[2:length(word1)], NA)) %>%\n",
                "  na.omit()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "darwin_words %>%\n",
                "  as.data.frame() %>%\n",
                "  head(15) %>%\n",
                "  flextable() %>%\n",
                "  flextable::set_table_properties(width = .5, layout = \"autofit\") %>%\n",
                "  flextable::theme_zebra() %>%\n",
                "  flextable::fontsize(size = 12) %>%\n",
                "  flextable::fontsize(size = 12, part = \"header\") %>%\n",
                "  flextable::align_text_col(align = \"center\") %>%\n",
                "  flextable::set_caption(caption = \"\")  %>%\n",
                "  flextable::border_outer()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "We can the paste the elements in these two column together and also inspect the frequency of each bigram.\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "darwin2grams <- darwin_words %>%\n",
                "  dplyr::mutate(bigram = paste(word1, word2, sep = \" \")) %>%\n",
                "  dplyr::group_by(bigram) %>%\n",
                "  dplyr::summarise(frequency = n()) %>%\n",
                "  dplyr::arrange(-frequency)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "darwin2grams %>%\n",
                "  as.data.frame() %>%\n",
                "  head(15) %>%\n",
                "  flextable() %>%\n",
                "  flextable::set_table_properties(width = .5, layout = \"autofit\") %>%\n",
                "  flextable::theme_zebra() %>%\n",
                "  flextable::fontsize(size = 12) %>%\n",
                "  flextable::fontsize(size = 12, part = \"header\") %>%\n",
                "  flextable::align_text_col(align = \"center\") %>%\n",
                "  flextable::set_caption(caption = \"\")  %>%\n",
                "  flextable::border_outer()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "The table show that among the 15 most frequent bigrams, there is only a single bigram (*natural selection*) which does not involve stop words (words that do not have referential but only relational meaning). \n",
                "\n",
                "We can remove bigrams that include stops words rather easily as shown below.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# define stopwords\n",
                "stps <- paste0(tm::stopwords(kind = \"en\"), collapse = \"\\\\b|\\\\b\")\n",
                "# clean bigram table\n",
                "darwin2grams_clean <- darwin2grams %>%\n",
                "  dplyr::filter(!str_detect(bigram, stps))\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "darwin2grams_clean %>%\n",
                "  as.data.frame() %>%\n",
                "  head(15) %>%\n",
                "  flextable() %>%\n",
                "  flextable::set_table_properties(width = .5, layout = \"autofit\") %>%\n",
                "  flextable::theme_zebra() %>%\n",
                "  flextable::fontsize(size = 12) %>%\n",
                "  flextable::fontsize(size = 12, part = \"header\") %>%\n",
                "  flextable::align_text_col(align = \"center\") %>%\n",
                "  flextable::set_caption(caption = \"\")  %>%\n",
                "  flextable::border_outer()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "## Extracting N-Grams with quanteda\n",
                "\n",
                "The `quanteda` package [see @quanteda] offers excellent and very fast functions for extracting bigrams.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#clean corpus\n",
                "darwin_clean <- darwin %>%\n",
                "  stringr::str_to_title()\n",
                "# tokenize corpus\n",
                "darwin_tokzd <- quanteda::tokens(darwin_clean)\n",
                "# extract bigrams\n",
                "BiGrams <- darwin_tokzd %>% \n",
                "       quanteda::tokens_remove(stopwords(\"en\")) %>% \n",
                "       quanteda::tokens_select(pattern = \"^[A-Z]\", \n",
                "                               valuetype = \"regex\",\n",
                "                               case_insensitive = FALSE, \n",
                "                               padding = TRUE) %>% \n",
                "       quanteda.textstats::textstat_collocations(min_count = 5, tolower = FALSE)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "BiGrams %>%\n",
                "  as.data.frame() %>%\n",
                "  head(15) %>%\n",
                "  flextable() %>%\n",
                "  flextable::set_table_properties(width = .5, layout = \"autofit\") %>%\n",
                "  flextable::theme_zebra() %>%\n",
                "  flextable::fontsize(size = 12) %>%\n",
                "  flextable::fontsize(size = 12, part = \"header\") %>%\n",
                "  flextable::align_text_col(align = \"center\") %>%\n",
                "  flextable::set_caption(caption = \"\")  %>%\n",
                "  flextable::border_outer()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "We can also extract bigrams very easily using the `tokens_compound` function which understands that we are looking for two-word expressions.\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "ngram_extract <- quanteda::tokens_compound(darwin_tokzd, pattern = BiGrams)\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "We can now generate concordances (and clean the resulting kwic table - the keyword-in-context table).\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "ngram_kwic <- kwic(ngram_extract, pattern = c(\"Natural_Selection\", \"South_America\")) %>%\n",
                "  as.data.frame() %>%\n",
                "  dplyr::select(-to, -from, -pattern)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "ngram_kwic %>%\n",
                "  head(15) %>%\n",
                "  flextable() %>%\n",
                "  flextable::set_table_properties(width = .95, layout = \"autofit\") %>%\n",
                "  flextable::theme_zebra() %>%\n",
                "  flextable::fontsize(size = 12) %>%\n",
                "  flextable::fontsize(size = 12, part = \"header\") %>%\n",
                "  flextable::align_text_col(align = \"center\") %>%\n",
                "  flextable::set_caption(caption = \"\")  %>%\n",
                "  flextable::border_outer()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "The disadvantage here is that we are strictly speaking only extracting N-Grams but not collocates as collocates do not necessarily have to occur in direct adjacency. The following section shoes how to expand the extraction of n-grams to the extraction of collocates.\n",
                "\n",
                "# Finding Collocations\n",
                "\n",
                "Both N-grams and collocations are not only an important concept in language teaching but they are also fundamental in Text Analysis and many other research areas working with language data. Unfortunately, words that collocate do not have to be immediately adjacent but can also encompass several slots. This is unfortunate because it makes retrieval of collocates substantially more difficult compared with a situation in which we only need to extract words that occur right next to each other.\n",
                "\n",
                "In the following, we will extract collocations from Charles Darwin's *On the Origin of Species by Means of Natural Selection*. In a first step, we will split the Origin into individual sentences.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# read in and process text\n",
                "darwinsentences <- darwin %>%\n",
                "  stringr::str_squish() %>%\n",
                "  tokenizers::tokenize_sentences(.) %>%\n",
                "  unlist() %>%\n",
                "  stringr::str_remove_all(\"- \") %>%\n",
                "  stringr::str_replace_all(\"\\\\W\", \" \") %>%\n",
                "  stringr::str_squish()\n",
                "# inspect data\n",
                "head(darwinsentences)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "The first element does not represent a full sentence because we selected a sample of the text which began in the middle of a sentence rather than at its beginning. In a next step, we will create a matrix that shows how often each word co-occurred with each other word in the data.\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# convert into corpus\n",
                "darwincorpus <- Corpus(VectorSource(darwinsentences))\n",
                "# create vector with words to remove\n",
                "extrawords <- c(\"the\", \"can\", \"get\", \"got\", \"can\", \"one\", \n",
                "                \"dont\", \"even\", \"may\", \"but\", \"will\", \n",
                "                \"much\", \"first\", \"but\", \"see\", \"new\", \n",
                "                \"many\", \"less\", \"now\", \"well\", \"like\", \n",
                "                \"often\", \"every\", \"said\", \"two\")\n",
                "# clean corpus\n",
                "darwincorpusclean <- darwincorpus %>%\n",
                "  tm::tm_map(removePunctuation) %>%\n",
                "  tm::tm_map(removeNumbers) %>%\n",
                "  tm::tm_map(tolower) %>%\n",
                "  tm::tm_map(removeWords, stopwords()) %>%\n",
                "  tm::tm_map(removeWords, extrawords)\n",
                "# create document term matrix\n",
                "darwindtm <- DocumentTermMatrix(darwincorpusclean, control=list(bounds = list(global=c(1, Inf)), weighting = weightBin))\n",
                "\n",
                "# convert dtm into sparse matrix\n",
                "darwinsdtm <- Matrix::sparseMatrix(i = darwindtm$i, j = darwindtm$j, \n",
                "                           x = darwindtm$v, \n",
                "                           dims = c(darwindtm$nrow, darwindtm$ncol),\n",
                "                           dimnames = dimnames(darwindtm))\n",
                "# calculate co-occurrence counts\n",
                "coocurrences <- t(darwinsdtm) %*% darwinsdtm\n",
                "# convert into matrix\n",
                "collocates <- as.matrix(coocurrences)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "collocates[1:10, 1:10] %>%\n",
                "  as.data.frame() %>%\n",
                "   tibble::rownames_to_column(\"Word\") %>%\n",
                "  flextable() %>%\n",
                "  flextable::set_table_properties(width = .5, layout = \"autofit\") %>%\n",
                "  flextable::theme_zebra() %>%\n",
                "  flextable::fontsize(size = 12) %>%\n",
                "  flextable::fontsize(size = 12, part = \"header\") %>%\n",
                "  flextable::align_text_col(align = \"center\") %>%\n",
                "  flextable::set_caption(caption = \"\")  %>%\n",
                "  flextable::bold(i = 1:10, j = 1, bold = TRUE, part = \"body\") %>%\n",
                "  flextable::border_outer()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "We can inspect this co-occurrence matrix and check how many terms (words or elements) it represents using the `ncol` function from base R. We can also check how often terms occur in the data using the `summary` function from base R. The output of the `summary` function tells us that the minimum frequency of a word in the data is 1 with a maximum of 25,435. The difference between the median (36.00) and the mean (74.47) indicates that the frequencies are distributed very non-normally - which is common for language data. \n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# inspect size of matrix\n",
                "ncol(collocates)\n",
                "summary(rowSums(collocates))\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "The `ncol` function reports that the data represents 8,638 words and that the most frequent word occurs 25,435 times in the text.\n",
                "\n",
                "# Visualizing Collocations\n",
                "\n",
                "We will now use an example of one individual word (*selection*) to show, how collocation strength for individual terms is calculated and how it can be visualized. The function `calculateCoocStatistics` is taken from [this tutorial](https://tm4ss.github.io/docs/Tutorial_5_Co-occurrence.html) [see also @WN17].\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# load function for co-occurrence calculation\n",
                "source(\"https://slcladal.github.io/rscripts/calculateCoocStatistics.R\")\n",
                "# define term\n",
                "coocTerm <- \"selection\"\n",
                "# calculate co-occurence statistics\n",
                "coocs <- calculateCoocStatistics(coocTerm, darwinsdtm, measure=\"LOGLIK\")\n",
                "# inspect results\n",
                "coocs[1:20]\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "The output shows that the word most strongly associated with *selection* in Charles Darwin's *Origin* is unsurprisingly *natural* - given the substantive strength of the association between *natural* and *selection* these term are definitely collocates and almost - if not already - a lexicalized construction (at least in this text).\n",
                "\n",
                "There are various visualizations options for collocations. Which visualization method is appropriate depends on what the visualizations should display. \n",
                "\n",
                "## Association Strength\n",
                "\n",
                "We start with the most basic and visualize the collocation strength using a simple dot chart. We use the vector of association strengths generated above and transform it into a table. Also, we exclude elements with an association strength lower than 30.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "coocdf <- coocs %>%\n",
                "  as.data.frame() %>%\n",
                "  dplyr::mutate(CollStrength = coocs,\n",
                "                Term = names(coocs)) %>%\n",
                "  dplyr::filter(CollStrength > 30)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "coocdf %>%\n",
                "  as.data.frame() %>%\n",
                "  dplyr::select(-.) %>%\n",
                "  dplyr::relocate(Term, CollStrength) %>%\n",
                "  head(15) %>%\n",
                "  flextable() %>%\n",
                "  flextable::set_table_properties(width = .5, layout = \"autofit\") %>%\n",
                "  flextable::theme_zebra() %>%\n",
                "  flextable::fontsize(size = 12) %>%\n",
                "  flextable::fontsize(size = 12, part = \"header\") %>%\n",
                "  flextable::align_text_col(align = \"center\") %>%\n",
                "  flextable::set_caption(caption = \"\")  %>%\n",
                "  flextable::border_outer()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "We can now visualize the association strengths as shown in the code chunk below.\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "ggplot(coocdf, aes(x = reorder(Term, CollStrength, mean), y = CollStrength)) +\n",
                "  geom_point() +\n",
                "  coord_flip() +\n",
                "  theme_bw() +\n",
                "  labs(y = \"\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "The dot chart shows that *natural* is collocating more strongly with *selection* compared to any other term. This confirms that *natural* and *selection* form a collocation in Darwin's *Origin*.\n",
                "\n",
                "## Dendrograms\n",
                "\n",
                "Another method for visualizing collocations are dendrograms. Dendrograms (also called tree-diagrams) show how similar elements are based on one or many features. As such, dendrograms are used to indicate groupings as they show elements (words) that are notably similar or different with respect to their association strength. To use this method, we first need to generate a distance matrix from our co-occurrence matrix.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "coolocs <- c(coocdf$Term, \"selection\")\n",
                "# remove non-collocating terms\n",
                "collocates_redux <- collocates[rownames(collocates) %in% coolocs, ]\n",
                "collocates_redux <- collocates_redux[, colnames(collocates_redux) %in% coolocs]\n",
                "# create distance matrix\n",
                "distmtx <- dist(collocates_redux)\n",
                "\n",
                "clustertexts <- hclust(    # hierarchical cluster object\n",
                "  distmtx,                 # use distance matrix as data\n",
                "  method=\"ward.D2\")        # ward.D as linkage method\n",
                "\n",
                "ggdendrogram(clustertexts) +\n",
                "  ggtitle(\"Terms strongly collocating with *selection*\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "## Network Graphs\n",
                "\n",
                "Network graphs are a very useful tool to show relationships (or the absence of relationships) between elements. Network graphs are highly useful when it comes to displaying the relationships that words have among each other and which properties these networks of words have.\n",
                "\n",
                "### Basic Network Graphs\n",
                "\n",
                "In order to display a network, we need to create a network graph by using the `network` function from the `network` package. \n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "net = network::network(collocates_redux, \n",
                "                       directed = FALSE,\n",
                "                       ignore.eval = FALSE,\n",
                "                       names.eval = \"weights\")\n",
                "# vertex names\n",
                "network.vertex.names(net) = rownames(collocates_redux)\n",
                "# inspect object\n",
                "net\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "Now that we have generated a network object, we visualize the network.\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "ggnet2(net, \n",
                "       label = TRUE, \n",
                "       label.size = 4,\n",
                "       alpha = 0.2,\n",
                "       size.cut = 3,\n",
                "       edge.alpha = 0.3) +\n",
                "  guides(color = FALSE, size = FALSE)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "The network is already informative but we will customize the network object so that the visualization becomes more appealing and informative. To add information, we create vector of words that contain different groups, e.g. terms that rarely, sometimes, and frequently collocate with *selection* (I used the dendrogram which displayed the cluster analysis as the basis for  the categorization). \n",
                "\n",
                "Based on these vectors, we can then change or adapt the default values of certain attributes or parameters of the network object (e.g. weights. linetypes, and colors).\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# create vectors with collocation occurrences as categories\n",
                "mid <- c(\"theory\", \"variations\", \"slight\", \"variation\")\n",
                "high <- c(\"natural\", \"selection\")\n",
                "infreq <- colnames(collocates_redux)[!colnames(collocates_redux) %in% mid & !colnames(collocates_redux) %in% high]\n",
                "# add color by group\n",
                "net %v% \"Collocation\" = ifelse(network.vertex.names(net) %in% infreq, \"weak\", \n",
                "                   ifelse(network.vertex.names(net) %in% mid, \"medium\", \n",
                "                   ifelse(network.vertex.names(net) %in% high, \"strong\", \"other\")))\n",
                "# modify color\n",
                "net %v% \"color\" = ifelse(net %v% \"Collocation\" == \"weak\", \"gray60\", \n",
                "                  ifelse(net %v% \"Collocation\" == \"medium\", \"orange\", \n",
                "                  ifelse(net %v% \"Collocation\" == \"strong\", \"indianred4\", \"gray60\")))\n",
                "# rescale edge size\n",
                "network::set.edge.attribute(net, \"weights\", ifelse(net %e% \"weights\" < 1, 0.1, \n",
                "                                   ifelse(net %e% \"weights\" <= 2, .5, 1)))\n",
                "# define line type\n",
                "network::set.edge.attribute(net, \"lty\", ifelse(net %e% \"weights\" <=.1, 3, \n",
                "                               ifelse(net %e% \"weights\" <= .5, 2, 1)))\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "We can now display the network object and make use of the added information.\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "ggnet2(net, \n",
                "       color = \"color\", \n",
                "       label = TRUE, \n",
                "       label.size = 4,\n",
                "       alpha = 0.2,\n",
                "       size = \"degree\",\n",
                "       edge.size = \"weights\",\n",
                "       edge.lty = \"lty\",\n",
                "       edge.alpha = 0.2) +\n",
                "  guides(color = FALSE, size = FALSE)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "## Biplots\n",
                "\n",
                "An alternative way to display co-occurrence patterns are bi-plots which are used to display the results of Correspondence Analyses. They are useful, in particular, when one is not interested in one particular key term and its collocations but in the overall similarity of many terms. Semantic similarity in this case refers to a shared semantic and this distributional profile. As such, words can be deemed semantically similar if they have a similar co-occurrence profile - i.e. they co-occur with the same elements. Biplots can be sued to visualize collocations because collocates co-occur and thus share semantic properties which renders then more similar to each other compared with other terms. \n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# perform correspondence analysis\n",
                "res.ca <- CA(collocates_redux, graph = FALSE)\n",
                "# plot results\n",
                "fviz_ca_row(res.ca, repel = TRUE, col.row = \"gray20\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "The bi-plot shows that *natural* and *selection* collocate as they are plotted in close proximity. The advantage of the biplot becomes apparent when we focus on other terms because the biplot also shows other collocates such as *vary* and *independently* or *might injurious*. \n",
                "\n",
                "# Determining Significance\n",
                "\n",
                "In order to identify which words occur together significantly more frequently than would be expected by chance, we have to determine if their co-occurrence frequency is statistical significant. This can be done wither for specific key terms or it can be done for the entire data. In this example, we will continue to focus on the key word *selection*.\n",
                "\n",
                "To determine which terms collocate significantly with the key term (*selection*), we use multiple (or repeated) Fisher's Exact tests which require the following information:\n",
                "\n",
                "* a = Number of times `coocTerm` occurs with term j\n",
                "\n",
                "* b = Number of times `coocTerm` occurs without  term j\n",
                "\n",
                "* c = Number of times other terms occur with term j\n",
                "\n",
                "* d = Number of terms that are not `coocTerm` or term j\n",
                "\n",
                "In a first step, we create a table which holds these quantities.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# convert to data frame\n",
                "coocdf <- as.data.frame(as.matrix(collocates))\n",
                "# reduce data\n",
                "diag(coocdf) <- 0\n",
                "coocdf <- coocdf[which(rowSums(coocdf) > 10),]\n",
                "coocdf <- coocdf[, which(colSums(coocdf) > 10)]\n",
                "# extract stats\n",
                "cooctb <- coocdf %>%\n",
                "  dplyr::mutate(Term = rownames(coocdf)) %>%\n",
                "  tidyr::gather(CoocTerm, TermCoocFreq,\n",
                "                colnames(coocdf)[1]:colnames(coocdf)[ncol(coocdf)]) %>%\n",
                "  dplyr::mutate(Term = factor(Term),\n",
                "                CoocTerm = factor(CoocTerm)) %>%\n",
                "  dplyr::mutate(AllFreq = sum(TermCoocFreq)) %>%\n",
                "  dplyr::group_by(Term) %>%\n",
                "  dplyr::mutate(TermFreq = sum(TermCoocFreq)) %>%\n",
                "  dplyr::ungroup(Term) %>%\n",
                "  dplyr::group_by(CoocTerm) %>%\n",
                "  dplyr::mutate(CoocFreq = sum(TermCoocFreq)) %>%\n",
                "  dplyr::arrange(Term) %>%\n",
                "  dplyr::mutate(a = TermCoocFreq,\n",
                "                b = TermFreq - a,\n",
                "                c = CoocFreq - a, \n",
                "                d = AllFreq - (a + b + c)) %>%\n",
                "  dplyr::mutate(NRows = nrow(coocdf))\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "cooctb %>%\n",
                "  as.data.frame() %>%\n",
                "  head(15) %>%\n",
                "  flextable() %>%\n",
                "  flextable::set_table_properties(width = .5, layout = \"autofit\") %>%\n",
                "  flextable::theme_zebra() %>%\n",
                "  flextable::fontsize(size = 12) %>%\n",
                "  flextable::fontsize(size = 12, part = \"header\") %>%\n",
                "  flextable::align_text_col(align = \"center\") %>%\n",
                "  flextable::set_caption(caption = \"\")  %>%\n",
                "  flextable::border_outer()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "We now select the key term (*selection*). If we wanted to find all collocations that are present in the data, we would use the entire data rather than only the subset that contains  *selection*. \n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "cooctb_redux <- cooctb %>%\n",
                "  dplyr::filter(Term == coocTerm)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "Next, we calculate which terms are (significantly) over- and under-proportionately used with *selection*. It is important to note that this procedure informs about both: over- and under-use! This is especially crucial when analyzing if specific words are attracted o repelled by certain constructions. Of course, this approach is not restricted to analyses of constructions and it can easily be generalized across domains and has also been used in machine learning applications.\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "coocStatz <- cooctb_redux %>%\n",
                "  dplyr::rowwise() %>%\n",
                "  dplyr::mutate(p = as.vector(unlist(fisher.test(matrix(c(a, b, c, d), \n",
                "                                                        ncol = 2, byrow = T))[1]))) %>%\n",
                "    dplyr::mutate(x2 = as.vector(unlist(chisq.test(matrix(c(a, b, c, d),                                                           ncol = 2, byrow = T))[1]))) %>%\n",
                "  dplyr::mutate(phi = sqrt((x2/(a + b + c + d)))) %>%\n",
                "      dplyr::mutate(expected = as.vector(unlist(chisq.test(matrix(c(a, b, c, d), ncol = 2, byrow = T))$expected[1]))) %>%\n",
                "  dplyr::mutate(Significance = dplyr::case_when(p <= .001 ~ \"p<.001\",\n",
                "                                                p <= .01 ~ \"p<.01\",\n",
                "                                                p <= .05 ~ \"p<.05\", \n",
                "                                                FALSE ~ \"n.s.\"))\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "coocStatz %>%\n",
                "  as.data.frame() %>%\n",
                "  head(15) %>%\n",
                "  flextable() %>%\n",
                "  flextable::set_table_properties(width = .5, layout = \"autofit\") %>%\n",
                "  flextable::theme_zebra() %>%\n",
                "  flextable::fontsize(size = 12) %>%\n",
                "  flextable::fontsize(size = 12, part = \"header\") %>%\n",
                "  flextable::align_text_col(align = \"center\") %>%\n",
                "  flextable::set_caption(caption = \"\")  %>%\n",
                "  flextable::border_outer()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "We now add information to the table and remove superfluous columns s that the table can be more easily parsed. \n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "coocStatz <- coocStatz %>%\n",
                "  dplyr::ungroup() %>%\n",
                "  dplyr::arrange(p) %>%\n",
                "  dplyr::mutate(j = 1:n()) %>%\n",
                "  # perform benjamini-hochberg correction\n",
                "  dplyr::mutate(corr05 = ((j/NRows)*0.05)) %>%\n",
                "  dplyr::mutate(corr01 = ((j/NRows)*0.01)) %>%\n",
                "  dplyr::mutate(corr001 = ((j/NRows)*0.001)) %>%\n",
                "  # calculate corrected significance status\n",
                "  dplyr::mutate(CorrSignificance = dplyr::case_when(p <= corr001 ~ \"p<.001\",\n",
                "                                                    p <= corr01 ~ \"p<.01\",\n",
                "                                                    p <= corr05 ~ \"p<.05\", \n",
                "                                                    FALSE ~ \"n.s.\")) %>%\n",
                "  dplyr::mutate(p = round(p, 6)) %>%\n",
                "  dplyr::mutate(x2 = round(x2, 1)) %>%\n",
                "  dplyr::mutate(phi = round(phi, 2)) %>%\n",
                "  dplyr::arrange(p) %>%\n",
                "  dplyr::select(-a, -b, -c, -d, -j, -NRows, -corr05, -corr01, -corr001) %>%\n",
                "  dplyr::mutate(Type = ifelse(expected > TermCoocFreq, \"Antitype\", \"Type\"))\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "coocStatz %>%\n",
                "  as.data.frame() %>%\n",
                "  head(15) %>%\n",
                "  flextable() %>%\n",
                "  flextable::set_table_properties(width = .5, layout = \"autofit\") %>%\n",
                "  flextable::theme_zebra() %>%\n",
                "  flextable::fontsize(size = 12) %>%\n",
                "  flextable::fontsize(size = 12, part = \"header\") %>%\n",
                "  flextable::align_text_col(align = \"center\") %>%\n",
                "  flextable::set_caption(caption = \"\")  %>%\n",
                "  flextable::border_outer()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "The results show that *selection* collocates significantly with *selection* (of course) but also, as expected, with *natural*. The corrected p-values shows that after Benjamini-Hochberg correction for multiple/repeated testing [see @field2012discovering] these are the only significant collocates of *selection*. Corrections are necessary when performing multiple tests because otherwise, the reliability of the test result would be strongly impaired as repeated testing causes substantive $\\alpha$-error inflation. The Benjamini-Hochberg correction that has been used here is preferrable over the more popular Bonferroni correction because it is less conservative and therefore less likely to result in $\\beta$-errors [see again @field2012discovering].\n",
                "\n",
                "# Changes in Collocation Strength\n",
                "\n",
                "We now turn to analyses of changes in collocation strength over apparent time. The example focuses on adjective amplification in Australian English. The issue we will analyze here is whether we can unearth changes in the collocation pattern of adjective amplifiers such as *very*, *really*, or *so*. In other words, we will investigate if amplifiers associate with different adjectives among speakers from different age groups. \n",
                "\n",
                "In a first step,  we activate packages and load the data.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# load functions\n",
                "source(\"https://SLCLADAL.github.io/rscripts/collexcovar.R\")\n",
                "# load data\n",
                "ampaus <- base::readRDS(url(\"https://slcladal.github.io/data/ozd.rda\", \"rb\"))\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "ampaus %>%\n",
                "  as.data.frame() %>%\n",
                "  head(15) %>%\n",
                "  flextable() %>%\n",
                "  flextable::set_table_properties(width = .5, layout = \"autofit\") %>%\n",
                "  flextable::theme_zebra() %>%\n",
                "  flextable::fontsize(size = 12) %>%\n",
                "  flextable::fontsize(size = 12, part = \"header\") %>%\n",
                "  flextable::align_text_col(align = \"center\") %>%\n",
                "  flextable::set_caption(caption = \"\")  %>%\n",
                "  flextable::border_outer()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "The data consists of three variables (`Adjective`, `Variant`, and `Age`). In a next step, we perform a co-varying collexeme analysis for *really* versus all other amplifiers. For this reason, we reduce the data set and retain only The function takes a data set consisting of three columns labeled `keys`, `colls`, and `time`.\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# rename data\n",
                "ampaus <- ampaus %>%\n",
                "  dplyr::rename(keys = Variant, colls = Adjective, time = Age)\n",
                "# perform analysis\n",
                "collexcovar_really <- collexcovar(data = ampaus, keyterm = \"really\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "collexcovar_really %>%\n",
                "  as.data.frame() %>%\n",
                "  head(15) %>%\n",
                "  flextable() %>%\n",
                "  flextable::set_table_properties(width = .5, layout = \"autofit\") %>%\n",
                "  flextable::theme_zebra() %>%\n",
                "  flextable::fontsize(size = 12) %>%\n",
                "  flextable::fontsize(size = 12, part = \"header\") %>%\n",
                "  flextable::align_text_col(align = \"center\") %>%\n",
                "  flextable::set_caption(caption = \"\")  %>%\n",
                "  flextable::border_outer()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "Now, that the data has the correct labels, we can continue with the implementation of the co-varying collexeme analysis. \n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# perform analysis\n",
                "collexcovar_pretty <- collexcovar(data = ampaus, keyterm = \"pretty\")\n",
                "collexcovar_so <- collexcovar(data = ampaus, keyterm = \"so\")\n",
                "collexcovar_very <- collexcovar(data = ampaus, keyterm = \"very\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "For other amplifiers, we have to change the label *other* to *bin* as the function already has a a label *other*. Once we have changed other to bin, we perform the analysis.\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "ampaus <- ampaus %>%\n",
                "  dplyr::mutate(keys = ifelse(keys == \"other\", \"bin\", keys))\n",
                "collexcovar_other <- collexcovar(data = ampaus, keyterm = \"bin\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "Next, we combine the results of the co-varying collexeme analysis into a single table.\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# combine tables\n",
                "collexcovar_ampaus <- rbind(collexcovar_really, collexcovar_very, \n",
                "                     collexcovar_so, collexcovar_pretty, collexcovar_other)\n",
                "collexcovar_ampaus <- collexcovar_ampaus %>%\n",
                "  dplyr::rename(Age = time,\n",
                "                Adjective = colls) %>%\n",
                "  dplyr::mutate(Variant = ifelse(Variant == \"bin\", \"other\", Variant)) %>%\n",
                "  dplyr::arrange(Age)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "collexcovar_ampaus %>%\n",
                "  as.data.frame() %>%\n",
                "  head(15) %>%\n",
                "  flextable() %>%\n",
                "  flextable::set_table_properties(width = .5, layout = \"autofit\") %>%\n",
                "  flextable::theme_zebra() %>%\n",
                "  flextable::fontsize(size = 12) %>%\n",
                "  flextable::fontsize(size = 12, part = \"header\") %>%\n",
                "  flextable::align_text_col(align = \"center\") %>%\n",
                "  flextable::set_caption(caption = \"\")  %>%\n",
                "  flextable::border_outer()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "We now modify the data set so that we can plot the collocation strength across apparent time.\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "ampauscoll <- collexcovar_ampaus %>%\n",
                "  dplyr::select(Age, Adjective, Variant, Type, phi) %>%\n",
                "  dplyr::mutate(phi = ifelse(Type == \"Antitype\", -phi, phi)) %>%\n",
                "  dplyr::select(-Type) %>%\n",
                "  tidyr::spread(Adjective, phi) %>%\n",
                "  tidyr::replace_na(list(bad = 0,\n",
                "                         funny = 0,\n",
                "                         hard = 0,\n",
                "                         good = 0,\n",
                "                         nice = 0,\n",
                "                         other = 0)) %>%\n",
                "  tidyr::gather(Adjective, phi, bad:other) %>%\n",
                "  tidyr::spread(Variant, phi) %>%\n",
                "  tidyr::replace_na(list(pretty = 0,\n",
                "                    really = 0,\n",
                "                    so = 0,\n",
                "                    very = 0,\n",
                "                    other = 0)) %>%\n",
                "  tidyr::gather(Variant, phi, other:very)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "ampauscoll %>%\n",
                "  as.data.frame() %>%\n",
                "  head(15) %>%\n",
                "  flextable() %>%\n",
                "  flextable::set_table_properties(width = .5, layout = \"autofit\") %>%\n",
                "  flextable::theme_zebra() %>%\n",
                "  flextable::fontsize(size = 12) %>%\n",
                "  flextable::fontsize(size = 12, part = \"header\") %>%\n",
                "  flextable::align_text_col(align = \"center\") %>%\n",
                "  flextable::set_caption(caption = \"\")  %>%\n",
                "  flextable::border_outer()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "In a final step, we visualize the results of our analysis.\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "p1 <- ggplot(ampauscoll, aes(x = reorder(Age, desc(Age)),\n",
                "                       y = phi, group = Variant)) +\n",
                "  facet_wrap(vars(Adjective)) +\n",
                "  geom_line() +\n",
                "  theme_set(theme_bw(base_size = 12)) +\n",
                "  coord_cartesian(ylim = c(-.2, .4))\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "ggplot(ampauscoll, aes(x = reorder(Age, desc(Age)),\n",
                "                       y = phi, group = Variant, \n",
                "                      color = Variant, linetype = Variant)) +\n",
                "  facet_wrap(vars(Adjective)) +\n",
                "  geom_line() +\n",
                "  guides(color=guide_legend(override.aes=list(fill=NA))) +\n",
                "  scale_color_manual(values = \n",
                "                       c(\"gray70\", \"gray70\", \"gray20\", \"gray70\", \"gray20\"),\n",
                "                        name=\"Variant\",\n",
                "                        breaks = c(\"other\", \"pretty\", \"really\", \"so\", \"very\"), \n",
                "                        labels = c(\"other\", \"pretty\", \"really\", \"so\", \"very\")) +\n",
                "  scale_linetype_manual(values = \n",
                "                          c(\"dotted\", \"dotdash\", \"longdash\", \"dashed\", \"solid\"),\n",
                "                        name=\"Variant\",\n",
                "                        breaks = c(\"other\", \"pretty\", \"really\", \"so\", \"very\"), \n",
                "                        labels = c(\"other\",  \"pretty\", \"really\", \"so\", \"very\")) +\n",
                "  theme(legend.position=\"top\", \n",
                "        axis.text.x = element_text(size=12),\n",
                "        panel.grid.major = element_blank(), \n",
                "        panel.grid.minor = element_blank()) +\n",
                "  theme_set(theme_bw(base_size = 12)) +\n",
                "  coord_cartesian(ylim = c(-.2, .4)) +\n",
                "  labs(x = \"Age\", y = \"Collocation Strength\") +\n",
                "  guides(size = FALSE)+\n",
                "  guides(alpha = FALSE)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "The results show that the collocation strength of different amplifier variants changes quite notably across age groups and we can also see that there is considerable variability in the way that the collocation strengths changes. For example, the collocation strengths between *bad* and *really* decreases from old to young speakers, while the reverse trend emerges for *good* which means that *really* is collocating more strongly with *good* among younger speakers than it is among older speakers.\n",
                "\n",
                "# Collostructional Analysis\n",
                "\n",
                "Collostructional  analysis [@stefanowitsch2003collostructions; @stefanowitsch2005covarying] investigates  the  lexicogrammatical associations between constructions and  lexical elements and there exist three basic subtypes of collostructional analysis: \n",
                "\n",
                "* Simple Collexeme Analysis\n",
                "\n",
                "* Distinctive Collexeme Analysis\n",
                "\n",
                "* Co-Varying Collexeme Analysis\n",
                "\n",
                "The analyses performed here are based on the `collostructions` package [@flach2017collostructions].\n",
                "\n",
                "## Simple Collexeme Analysis\n",
                "\n",
                "Simple Collexeme Analysis determines if a word is significantly attracted to a specific construction within a corpus. The idea is that the frequency of the word that is attracted to a construction is significantly higher within the construction than would be expected by chance.\n",
                "\n",
                "The example here analyzes the Go + Verb construction (e.g. *Go suck a nut!*). The question is which verbs are attracted to this constructions (in this case, if *suck* is attracted to this construction).\n",
                "\n",
                "To perform these analyses, we use the `collostructions` package. Information about how to download and install this package can be found [here](https://sfla.ch/collostructions/).\n",
                "\n",
                "In a first step, we load the `collostructions` package and inspect the data. In this case, we will only use a sample of 100 rows from the data set as the output would become hard to read.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# draw a sample of the data\n",
                "goVerb <- goVerb[sample(nrow(goVerb), 100),]\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "goVerb %>%\n",
                "  as.data.frame() %>%\n",
                "  head(15) %>%\n",
                "  flextable() %>%\n",
                "  flextable::set_table_properties(width = .5, layout = \"autofit\") %>%\n",
                "  flextable::theme_zebra() %>%\n",
                "  flextable::fontsize(size = 12) %>%\n",
                "  flextable::fontsize(size = 12, part = \"header\") %>%\n",
                "  flextable::align_text_col(align = \"center\") %>%\n",
                "  flextable::set_caption(caption = \"\")  %>%\n",
                "  flextable::border_outer()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "The `collex` function which calculates the results of a simple collexeme analysis requires a data frame consisting out of three columns that contain in column 1 the word to be tested, in column 2 the frequency of the word in the construction (CXN.FREQ), and in column 3 the frequency of the word in the corpus (CORP.FREQ).\n",
                "\n",
                "To perform the simple collexeme analysis, we need the overal size of the corpus, the frequency with which a word occurs in the construction under investigation and the frequency of that construction.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# define corpus size\n",
                "crpsiz <- sum(goVerb$CORP.FREQ)\n",
                "# perform simple collexeme analysis\n",
                "scollex_results <- collex(goVerb, corpsize = crpsiz, am = \"logl\", \n",
                "                          reverse = FALSE, decimals = 5,\n",
                "                          threshold = 1, cxn.freq = NULL, \n",
                "                          str.dir = FALSE)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "scollex_results %>%\n",
                "  as.data.frame() %>%\n",
                "  head(15) %>%\n",
                "  flextable() %>%\n",
                "  flextable::set_table_properties(width = .5, layout = \"autofit\") %>%\n",
                "  flextable::theme_zebra() %>%\n",
                "  flextable::fontsize(size = 12) %>%\n",
                "  flextable::fontsize(size = 12, part = \"header\") %>%\n",
                "  flextable::align_text_col(align = \"center\") %>%\n",
                "  flextable::set_caption(caption = \"\")  %>%\n",
                "  flextable::border_outer()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "The results show which words are significantly attracted to the construction. If the ASSOC column did not show *attr*, then the word would be repelled by the construction.\n",
                "\n",
                "## Covarying Collexeme Analysis\n",
                "\n",
                "Covarying collexeme analysis determines if the occurrence of a word in the first slot of a constructions correlates with the occurrence of a word in the second slot of the construction. As such, covarying collexeme analysis analyzes constructions with two slots and how the lexical elements within the two slots affect each other.\n",
                "\n",
                "The data we will use is called `vsmdata` and consist of 5,000 observations of adjectives and if the adjective is amplified. As such,  `vsmdata`  contains two columns: one column with the adjectives (Adjectives) and another column telling if the adjective has been amplified (0 means that the adjective occurred without an amplifier).  The first six rows of the data are shown below.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# load data\n",
                "vsmdata <- base::readRDS(url(\"https://slcladal.github.io/data/vsd.rda\", \"rb\")) %>%\n",
                "  dplyr::mutate(Amplifier = ifelse(Amplifier == 0, 0, 1))\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "vsmdata %>%\n",
                "  as.data.frame() %>%\n",
                "  head() %>%\n",
                "  flextable() %>%\n",
                "  flextable::set_table_properties(width = .5, layout = \"autofit\") %>%\n",
                "  flextable::theme_zebra() %>%\n",
                "  flextable::fontsize(size = 12) %>%\n",
                "  flextable::fontsize(size = 12, part = \"header\") %>%\n",
                "  flextable::align_text_col(align = \"center\") %>%\n",
                "  flextable::set_caption(caption = \"\")  %>%\n",
                "  flextable::border_outer()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "We now perform the collexeme analysis and oinspect the results.\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "covar_results <- collex.covar(vsmdata)\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "covar_results %>%\n",
                "  as.data.frame() %>%\n",
                "  head(10) %>%\n",
                "  flextable() %>%\n",
                "  flextable::set_table_properties(width = .5, layout = \"autofit\") %>%\n",
                "  flextable::theme_zebra() %>%\n",
                "  flextable::fontsize(size = 12) %>%\n",
                "  flextable::fontsize(size = 12, part = \"header\") %>%\n",
                "  flextable::align_text_col(align = \"center\") %>%\n",
                "  flextable::set_caption(caption = \"\")  %>%\n",
                "  flextable::border_outer()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "The results show if a words in the first and second slot attract or repel each other (ASSOC) and provide uncorrected significance levels.\n",
                "\n",
                "## Distinctive Collexeme Analysis\n",
                "\n",
                "Distinctive Collexeme Analysis determines if the frequencies of items in two alternating constructions or under two conditions differ significantly. This analysis can be extended to analyze if the use of a word differs between two corpora.\n",
                "\n",
                "Again, we use the `vsmdata` data.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "collexdist_results <- collex.dist(vsmdata, raw = TRUE)\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "collexdist_results %>%\n",
                "  as.data.frame() %>%\n",
                "  head(10) %>%\n",
                "  flextable() %>%\n",
                "  flextable::set_table_properties(width = .5, layout = \"autofit\") %>%\n",
                "  flextable::theme_zebra() %>%\n",
                "  flextable::fontsize(size = 12) %>%\n",
                "  flextable::fontsize(size = 12, part = \"header\") %>%\n",
                "  flextable::align_text_col(align = \"center\") %>%\n",
                "  flextable::set_caption(caption = \"\")  %>%\n",
                "  flextable::border_outer()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "The results show if words are significantly attracted or repelled by a modifier variant. \n",
                "\n",
                "# Citation & Session Info \n",
                "\n",
                "Schweinberger, Martin. `r format(Sys.time(), '%Y')`. *Analyzing Co-Occurrences and Collocations in R*. Brisbane: The University of Queensland. url: https://slcladal.github.io/coll.html (Version `r format(Sys.time(), '%Y.%m.%d')`).\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "@manual{schweinberger`r format(Sys.time(), '%Y')`coll,\n",
                "  author = {Schweinberger, Martin},\n",
                "  title = {Analyzing Co-Occurrences and Collocations in R},\n",
                "  note = {https://slcladal.github.io/coll.html},\n",
                "  year = {`r format(Sys.time(), '%Y')`},\n",
                "  organization = \"The University of Queensland, Australia. School of Languages and Cultures},\n",
                "  address = {Brisbane},\n",
                "  edition = {`r format(Sys.time(), '%Y.%m.%d')`}\n",
                "}\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "sessionInfo()\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "***\n",
                "\n",
                "[Back to top](#introduction)\n",
                "\n",
                "[Back to HOME](https://slcladal.github.io/index.html)\n",
                "\n",
                "***\n",
                "\n",
                "# References \n"
            ]
        }
    ],
    "metadata": {
        "anaconda-cloud": "",
        "kernelspec": {
            "display_name": "R",
            "langauge": "R",
            "name": "ir"
        },
        "language_info": {
            "codemirror_mode": "r",
            "file_extension": ".r",
            "mimetype": "text/x-r-source",
            "name": "R",
            "pygments_lexer": "r",
            "version": "3.4.1"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}
