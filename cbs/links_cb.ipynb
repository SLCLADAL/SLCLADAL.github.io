{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                " \n",
                " \n",
                " \n",
                " \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "knitr::include_graphics(\"https://slcladal.github.io/images/network.jpg\")\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "This page contains links to resources relevant for language technology, text analytics, data management and reproducibility, language data science and natural language processing.\n",
                "\n",
                "<br><br><br>\n",
                "\n",
                "# Tools \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "knitr::include_graphics(\"https://slcladal.github.io/images/AntConc_logo.png\")\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## AntConc (and other Ant tools)\n",
                "\n",
                "[AntConc](https://www.laurenceanthony.net/software/antconc/) is a freeware corpus analysis toolkit for concordancing and text analysis developed by Laurence Anthony. In addition to AntConc, Laurence Anthony's [AntLab](https://www.laurenceanthony.net/) contains a conglomeration of extremely useful and very user-friendly software tools, that help and facilitate the analysis of textual data. Laurence has really developed an [impressive, very user-friendly selection of tools](https://www.laurenceanthony.net/software.html) that assist anyone interested in working with language data.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "knitr::include_graphics(\"https://slcladal.github.io/images/SMARTool_logo.png\")\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## SMARTool\n",
                "\n",
                "[SMARTool](http://smartool.github.io/smartool-rus-eng/) is a corpus-based language learning and analysis tool for for English-speaking learners of Russian. It is linguist-built and thus informed by modern linguistic theory. SMARTool assists learners with coping with the rich Russian morphology and has user-friendly, corpus-based information and help for learning the most frequent forms of 3,000 basic vocabulary items.\n",
                "\n",
                "# Courses\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "knitr::include_graphics(\"https://slcladal.github.io/images/alt_logo.png\")\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Applied Language Technology\n",
                "\n",
                "[Applied Language Technology](https://applied-language-technology.readthedocs.io/en/latest/index.html) is a website hosting learning materials for two courses taught at the University of Helsinki: *Working with Text in Python* and *Natural Language Processing for Linguists*. Together, these two courses provide an introduction to applied language technology for audiences who are unfamiliar with language technology and programming. The learning materials assume no previous knowledge of the Python programming language.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "knitr::include_graphics(\"https://slcladal.github.io/images/cap_logo.png\")\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Cultural Analytics with Python\n",
                "\n",
                "[Introduction to Cultural Analytics & Python](https://melaniewalsh.github.io/Intro-Cultural-Analytics/welcome.html) is a website established by Melanie Walsh that hosts an online textbook which offers an introduction to the programming language Python that is specifically designed for people interested in the humanities and social sciences.\n",
                "\n",
                "## GLAM Workbench\n",
                "\n",
                "The [GLAM Workbench](https://glam-workbench.net/) is a collection of tools, tutorials, examples, and hacks to help you work with data from galleries, libraries, archives, and museums (the GLAM sector). The primary focus is Australia and New Zealand, but new collections are being added all the time. \n",
                "\n",
                "The resources in the GLAM Workbench are created and shared as Jupyter notebooks. Jupyter lets you combine narrative text and live code in an environment that encourages you to learn and explore. Jupyter notebooks run in your browser, and you can get started without installing any software!\n",
                "\n",
                "One really great advantage of the GLAM workbench is that it is interactive: if you click on one of the links that says *Run live on Binder*, this will open the notebook, ready to use, in a customised computing environment using the Binder service.\n",
                "\n",
                "## Programming Historian\n",
                "\n",
                "The [Programming Historian](https://programminghistorian.org/en/lessons/) is a collaborative blog that contains lessons on various topics associated with computational humanities. The blog  was founded in 2008 by William J. Turkel and Alan MacEachern. It focused heavily on the Python programming language and was published open access as a Network in Canadian History & Environment (NiCHE) *Digital Infrastructure* project. In 2012, Programming Historian expanded its editorial team and launched as an open access peer reviewed scholarly journal of methodology for digital historians. \n",
                "\n",
                "## TAPoR 3\n",
                "\n",
                "[TAPoR 3, the Text Analysis Portal for Research](http://tapor-test.artsrn.ualberta.ca/home) contains a unique, well-curated list of a wide variety of tools commonly used or widely respected groups of tools used by leading scholars in the various fields of Digital Humanities. TAPoR 3 was developed with support from the Text Mining the Novel Project. The tools mentioned in the list provided by TAPoR 3 represent both tried and trusted tools used by DH scholars, or new advancements that offer exciting new possibilities in their field. Curated lists are excellent places to start exploring TAPoR from, and can help with deciding what to include in your own lists. \n",
                "\n",
                "## Quick-R\n",
                "\n",
                "[Quick-R](https://www.statmethods.net/) by DataCamp and maintained by Rob Kabacoff that contains tutorial and R code on R, data management, statistics, and data visualization. The site is extremely helpful and recommendable for everyone that looks for code snippets that can be adapted for your own use.\n",
                "\n",
                "## STHDA\n",
                "\n",
                "[Statistical Tools For High-Throughput Data Analysis (STHDA)](http://www.sthda.com/english/) is a website containing various tutorials on the implementation of statistical methods in R. It is particularly useful due to the wide range of topics and procedures it covers.\n",
                "\n",
                "# Centers | Labs\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "knitr::include_graphics(\"https://slcladal.github.io/images/tcc.png\")\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Text Crunching Centre\n",
                "\n",
                "At the [Text Crunching Centre](https://www.cl.uzh.ch/en/TCC.html), a team of experts in Natural Language Processing supports your text technology needs. The TCC is part of the Department of Computational Linguistics at the University of Zurich and it is a service offered to all departments of the University of Zurich as well as to external partners or customers. \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "knitr::include_graphics(\"https://slcladal.github.io/images/varieng.png\")\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## VARIENG\n",
                "\n",
                "[VARIENG](https://www.helsinki.fi/en/researchgroups/varieng) stands for the *Research Unit for the Study of Variation, Contacts and Change in English*. It also stands for innovative thinking and team work in English corpus linguistics and the study of language variation and change. VARIENG members study the English language, its uses and users, both today and in the past. \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "knitr::include_graphics(\"https://slcladal.github.io/images/acqvalab.png\")\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## AcqVA Aurora Lab\n",
                "\n",
                "The [AcqVA Aurora Lab](https://site.uit.no/acqvalab), is the lab of the [UiT Aurora Center for Language Acquisition, Variation & Attrition](https://uit.no/research/acqva) at [The Arctic University of Norway in Tromsø](https://uit.no/startsida). Together with the Psycholinguistics of Language Representation (PoLaR) lab, the AcqVA Lab provides methodological support for researchers at the AcqVA Aurora Center.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "knitr::include_graphics(\"https://slcladal.github.io/images/scl.png\")\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Sydney Corpus Lab\n",
                "\n",
                "The [Sydney Corpus Lab](https://sydneycorpuslab.com/) aims to promote corpus linguistics in Australia. It’s a virtual, rather than a physical lab, and is an online platform for connecting computer-based linguists across the University of Sydney and beyond. Its mission is to build research capacity in corpus linguistics at the University of Sydney, to connect Australian corpus linguists, and to promote the method in Australia, both in linguistics and in other disciplines. We have strong links with the Sydney Centre for Language Research (Computational Approaches to Language node) and the Sydney Digital Humanities Research Group. \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "knitr::include_graphics(\"https://slcladal.github.io/images/AntConc_logo.png\")\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## AntLab\n",
                "\n",
                "Laurence Anthony's [AntLab](https://www.laurenceanthony.net/) contains a conglomeration of extremely useful and very user-friendly software tools, such as [AntConc](https://www.laurenceanthony.net/software/antconc/) that help and facilitate the analysis of textual data.\n",
                "\n",
                "<br>\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "knitr::include_graphics(\"https://slcladal.github.io/images/Logo_HBI.jpg\")\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Media Research Methods Lab \n",
                "\n",
                "The [Media Research Methods Lab (MRML)](https://leibniz-hbi.de/en/research/research-programmes/media-research-methods-lab) at the Leibniz Institute for Media Research │ Hans Bredow Institute (HBI) is designed as a method-oriented lab, which focuses on linking established social science methods (surveys, observations, content analysis, experiments) with new digital methods from the field of computational social science (e.g. automated content analysis, network analysis, log data analysis, experience sampling) across topics and disciplines. \n",
                "\n",
                "## National Centre for Text Mining\n",
                "\n",
                "[The National Centre for Text Mining (NaCTeM)](http://www.nactem.ac.uk/), located in the UK, is the first publicly-funded text mining centre in the world. We provide text mining services in response to the requirements of the UK academic community. On the NaCTeM website, you can find pointers to sources of information about text mining such as links to text mining services provided by NaCTeM, software tools (both those developed by the NaCTeM team and by other text mining groups), seminars, general events, conferences and workshops, tutorials and demonstrations, and text mining publications.\n",
                "\n",
                "# Blogs\n",
                "\n",
                "## Hypotheses\n",
                "\n",
                "[Hypotheses](https://corpling.hypotheses.org/) is a research blog by [Guillaume Desagulier](http://www2.univ-paris8.fr/desagulier/home/. In this blog, entitled *Around the word, A corpus linguist’s notebook*, Guillaume has recorded reflections and experiments on his practice as a usage-based corpus linguist and code for analyzing language using R.\n",
                "\n",
                "## Aneesha Bakharia\n",
                "\n",
                "[Aneesha Bakharia](https://aneesha.medium.com/) is a blog by [Aneesha Bakharia](https://itali.uq.edu.au/profile/554/aneesha-bakharia) about Data Science, Topic Modeling, Deep Learning, Algorithm Usability and Interpretation, Learning Analytics, and Electronics. Aneesha began her career as an electronics engineer but quickly transitioned to an educational software developer. Aneesha has worked in the higher education and vocational educational sectors in a variety of technical, innovation and project management roles. In her most recent role before commencing at UQ, Aneesha was a project manager for a large OLT Teaching and Learning grant on Learning Analytics at QUT. Aneesha’s primary responsibilities at ITaLI include directing the design, development and implementation of learning analytics initiatives (including the Course Insights teacher facing dashboard) at UQ.  \n",
                "\n",
                "***\n",
                "\n",
                "[Back to top](#announcements)\n",
                "\n",
                "[Back to HOME](https://slcladal.github.io/index.html)\n",
                "\n",
                "*** \n"
            ]
        }
    ],
    "metadata": {
        "anaconda-cloud": "",
        "kernelspec": {
            "display_name": "R",
            "langauge": "R",
            "name": "ir"
        },
        "language_info": {
            "codemirror_mode": "r",
            "file_extension": ".r",
            "mimetype": "text/x-r-source",
            "name": "R",
            "pygments_lexer": "r",
            "version": "3.4.1"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}
