<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />


<meta name="author" content="UQ SLC Digital Team" />

<meta name="date" content="2020-04-16" />

<title>Simple Linear Regression in R</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cosmo.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<link href="site_libs/font-awesome-5.1.0/css/all.css" rel="stylesheet" />
<link href="site_libs/font-awesome-5.1.0/css/v4-shims.css" rel="stylesheet" />

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>




<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #ffffff;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">LADAL</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    <span class="fa fa-play-circle"></span>
     
    Basics
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li class="dropdown-header">Basics</li>
    <li>
      <a href="introcomputer.html">General Tips on Computering</a>
    </li>
    <li class="divider"></li>
    <li>
      <a href="introquant.html">Introduction To Quantitative Reasoning</a>
    </li>
    <li>
      <a href="basicquant.html">Basic Concepts In Quantitative Research</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    <span class="fa fa-gear"></span>
     
    Data Processing
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li class="dropdown-header">Data Processing</li>
    <li>
      <a href="intror.html">Basics: Getting started with R</a>
    </li>
    <li>
      <a href="introloading.html">Loading and saving data</a>
    </li>
    <li>
      <a href="stringprocessing.html">String processing</a>
    </li>
    <li>
      <a href="regularexpressions.html">Regular expressions</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    <span class="fa fa-bar-chart"></span>
     
    Visualization
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li class="dropdown-header">Visualization</li>
    <li>
      <a href="basicgraphs.html">Visualizing Data with R</a>
    </li>
    <li>
      <a href="maps.html">Creating maps using R</a>
    </li>
    <li>
      <a href="motion.html">Motion Charts in R</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    <span class="fa fa-eye"></span>
     
    Statistics
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li class="dropdown-header">Statistics</li>
    <li>
      <a href="descriptivestatz.html">Descriptive Statistics</a>
    </li>
    <li class="divider"></li>
    <li class="dropdown-header">Basic Interential Statistics</li>
    <li>
      <a href="basicstatz.html">Basic Inferential Tests</a>
    </li>
    <li>
      <a href="basicstatzchi.html">The Chi-Square Family</a>
    </li>
    <li>
      <a href="basicstatzregression.html">Simple Linear Regression</a>
    </li>
    <li class="divider"></li>
    <li class="dropdown-header">Advanced Interential Statistics</li>
    <li>
      <a href="fixedregressions.html">Fixed-Effects Regression Models</a>
    </li>
    <li>
      <a href="mixedregressions.html">Mixed-Effects Regression Models</a>
    </li>
    <li>
      <a href="advancedstatztrees.html">Tree-Based Models</a>
    </li>
    <li>
      <a href="groupingstatz.html">Classification</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    <span class="fa fa-bars"></span>
     
    Text Analytics
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="textanalysis.html">Text Analysis and Distant Reading</a>
    </li>
    <li>
      <a href="webcrawling.html">Web Crawling</a>
    </li>
    <li>
      <a href="basicnetwork.html">Network Analysis</a>
    </li>
    <li>
      <a href="collocations.html">Co-occurrence and Collocation Analysis</a>
    </li>
    <li>
      <a href="topicmodels.html">Topic Modeling</a>
    </li>
    <li>
      <a href="classification.html">Classification</a>
    </li>
    <li>
      <a href="tagging.html">Tagging and Parsing</a>
    </li>
    <li>
      <a href="corplingr.html">Corpus Linguistics</a>
    </li>
  </ul>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="about.html">
    <span class="fa fa-info"></span>
     
    Contact
  </a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Simple Linear Regression in R</h1>
<h4 class="author">UQ SLC Digital Team</h4>
<h4 class="date">2020-04-16</h4>

</div>


<p><img src="images/uq1.jpg" width="100%" /></p>
<div id="introduction" class="section level1">
<h1><span class="header-section-number">1</span> Introduction</h1>
<p>This tutorial introduces linear regressions with R. The entire code for the sections below can be downloaded <a href="https://slcladal.github.io/rscripts/basicstatzchirscript.r">here</a>.</p>
<p><strong>Preparation and session set up</strong></p>
<p>As all visualizations in this tutorial rely on R, it is necessary to install R and RStudio. If these programs (or, in the case of R, environments) are not already installed on your machine, please search for them in your favorite search engine and add the term “download”. Open any of the first few links and follow the installation instructions (they are easy to follow, do not require any specifications, and are pretty much self-explanatory).</p>
<p>In addition, certain <em>packages</em> need to be installed from an R <em>library</em> so that the scripts shown below are executed without errors. Before turning to the code below, please install the packages by running the code below this paragraph. If you have already installed the packages mentioned below, then you can skip ahead ignore this section. To install the necessary packages, simply run the following code - it may take some time (between 1 and 5 minutes to install all of the packages so you do not need to worry if it takes some time).</p>
<pre class="r"><code># clean current workspace
rm(list=ls(all=T))
# set options
options(stringsAsFactors = F)
# install libraries
install.packages(c(&quot;calibrate&quot;, &quot;car&quot;, &quot;ggplot2&quot;, &quot;QuantPsyc&quot;))</code></pre>
<p>Once you have installed R, R-Studio, and have also initiated the session by executing the code shown above, you are good to go.</p>
</div>
<div id="simple-linear-regression" class="section level1">
<h1><span class="header-section-number">2</span> Simple Linear Regression</h1>
<p>This section focuses on a very widely used statistical method which is called regression. Regressions are used when we try to understand how independent variables correlate with a dependent or outcome variable. So, if you want to investigate how a certain factor affects an outcome, then a regression is the way to go. We will have a look at two simple examples to understand what the concepts underlying a regression mean and how a regression works. The R-Code, that we will use, is adapted from <span class="citation">Field, Miles, and Field (<a href="#ref-field2012discovering" role="doc-biblioref">2012</a>)</span> - which is highly recommended for understanding regression analyses! In addition to <span class="citation">Field, Miles, and Field (<a href="#ref-field2012discovering" role="doc-biblioref">2012</a>)</span>, there are various introductions which also focus on regression (among other types of analyses), for example, <span class="citation">Gries (<a href="#ref-gries2009statistics" role="doc-biblioref">2009</a>)</span>, <span class="citation">Levshina (<a href="#ref-levshina2015linguistics" role="doc-biblioref">2015</a>)</span>, <span class="citation">Wilcox (<a href="#ref-wilcox2009basic" role="doc-biblioref">2009</a>)</span> - <span class="citation">Baayen (<a href="#ref-baayen2008analyzing" role="doc-biblioref">2008</a>)</span> is also very good but probably not the first book one should read about statistics.</p>
<div id="introduction-1" class="section level2">
<h2><span class="header-section-number">2.1</span> Introduction</h2>
<p>Although the basic logic underlying regressions is identical to the conceptual underpinnings of <em>analysis of variance</em> (ANOVA), a related method, sociolinguistists have traditionally favoured regression analysis in their studies while ANOVAs have been the method of choice in psycholinguistics. The preference for either method is grounded in historical happenstances and the culture of these subdisciplines rather than in methodological reasoning.</p>
<p>A minor difference between regressions and ANOVA lies in the fact that regressions are based on the <span class="math inline">\(t\)</span>-distribution while ANOVAs use the <span class="math inline">\(F\)</span>-distribution (however, the <span class="math inline">\(F\)</span>-value is simply the value of <span class="math inline">\(t\)</span> squared or t<sup>2</sup>). Both <span class="math inline">\(t\)</span>- and <span class="math inline">\(F\)</span>-values report on the ratio between explained and unexplained variance.</p>
<p>The idea behind regression analysis is expressed formally in the equation below where<span class="math inline">\(f_{(x)}\)</span> is the <span class="math inline">\(y\)</span>-value we want to predict, <span class="math inline">\(\alpha\)</span> is the intercept (the point where the regression line crosses the <span class="math inline">\(y\)</span>-axis), <span class="math inline">\(\beta\)</span> is the coefficient (the slope of the regression line).</p>
<p><span class="math inline">\(f_{(x)} = \alpha + \beta_{1}x_{i} + \epsilon\)</span></p>
<p>In other words, to estimate how much some weights who is 180cm tall, we would multiply the coefficent (slope of the line) with 180 (<span class="math inline">\(x\)</span>) and add the value of the intercept (point where line crosses the <span class="math inline">\(y\)</span>-axis).</p>
<p>However, the idea behind regressions can best be described graphically: imagine a cloud of points (like the points in the scatterplot below). Regressions aim to find that line which has the minimal summed distance between points and the line (like the line in the right panel). Technically speaking, the aim of a regression is to find the line with the minimal deviance (or the line with the minimal sum of residuals). Residuals are the distance between the line and the points (the red lines) and it is also called <em>variance</em>.</p>
<p>Thus, regression lines are those lines where the sum of the red lines should be minimal. The slope of the regression line is called <em>coefficient</em> and the point where the regression line crosses the y-axis is called the <em>intercept</em>.</p>
<p><img src="basicstatzregression_files/figure-html/slr1-1.png" width="672" /></p>
<p>A word about standard errors (SE) is in order here because most commonly used statistics programs will provide SE values when reporting regression models. The SE is a measure that tells us how much the coefficients were to vary if the same regression were applied to many samples from the same population. A relatively small SE value therefore indicates that the coefficients will remain very stable if the same regression model is fitted to many different samples with identical parameters. In contrast, a large SE tells you that the model is volatile and not very stable or reliable as the coefficients vary substantially if the model is applied to many samples.</p>
</div>
</div>
<div id="example-1-preposition-use-across-real-time" class="section level1">
<h1><span class="header-section-number">3</span> Example 1: Preposition Use across Real-Time</h1>
<p>We will now turn to our first example. In this example, we will investigate whether the frequency of prepositions has changed from Middle English to Late Modern English. The reasoning behind this example is that Old English was highly synthetic compared with Present-Day English which comparatively analytic. In other words, while Old English speakers used case to indicate syntactic relations, speakers of Present-Day English use word order and prepositions to indicate syntactic relationships. This means that the loss of case had to be compensated by different strategies and maybe these strategies continued to develop and increase in frequency even after the change from synthetic to analytic had been mostly accomplished. And this prolonged change in compensatory strategies is what this example will focus on.</p>
<p>The analysis is based on data extracted from the <em>Penn Corpora of Historical English</em> (see <a href="http://www.ling.upenn.edu/hist-corpora/" class="uri">http://www.ling.upenn.edu/hist-corpora/</a>), that consists of 603 texts written between 1125 and 1900. In preparation of this example, all elements that were part-of-speech tagged as prepositions were extracted from the PennCorpora.</p>
<p>Then, the relative frequencies (per 1,000 words) of prepositions per text were calculated. This frequency of prepositions per 1,000 words represents our dependent variable. In a next step, the date when each letter had been written was extracted. The resulting two vectors were combined into a table which thus contained for each text, when it was written (independent variable) and its relative frequency of prepositions (dependent or outcome variable).</p>
<p>A regression analysis will follow the steps described below: 1. Extraction and processing of the data 2. Data visualization 3. Applying the regression analysis to the data 4. Diagnosing the regression model and checking whether or not basic model assumptions have been violated.</p>
<p>In a first step, we load the libraries and functions.</p>
<pre class="r"><code># load libraries
library(car)
library(dplyr)
library(ggplot2)
library(knitr)         
library(QuantPsyc)           
# load functions
source(&quot;https://slcladal.github.io/rscripts/multiplot.r&quot;)
source(&quot;https://slcladal.github.io/rscripts/slrsummary.r&quot;)</code></pre>
<p>After preparing our session, we can now load and inspect the data to get a first impression of its properties.</p>
<pre class="r"><code># load data
slrdata &lt;- read.delim(&quot;https://slcladal.github.io/data/lmmdata.txt&quot;, header = TRUE)
# inspect data
head(slrdata)                                   </code></pre>
<pre><code>##   Date         Genre    Text Prepositions Region
## 1 1736       Science   albin       166.01  North
## 2 1711     Education    anon       139.86  North
## 3 1808 PrivateLetter  austen       130.78  North
## 4 1878     Education    bain       151.29  North
## 5 1743     Education barclay       145.72  North
## 6 1908     Education  benson       120.77  North</code></pre>
<p>Inspecting the data is very important because it can happen that a data set may not load completely or that variables which should be numeric have been converted to character variables. If unchecked, then such issues could go unnoticed and cause trouble.</p>
<p>We will now plot the data to get a better understanding of what the data looks like.</p>
<pre class="r"><code>ggplot(slrdata, aes(Date, Prepositions)) +
  geom_point() +
  theme_bw() +
  labs(x = &quot;Year&quot;) +
  labs(y = &quot;Prepositions per 1,000 words&quot;) +
  geom_smooth()</code></pre>
<p><img src="basicstatzregression_files/figure-html/slr5-1.png" width="672" /></p>
<pre class="r"><code>ggplot(slrdata, aes(Date, Prepositions)) +
  geom_point() +
  theme_bw() +
  labs(x = &quot;Year&quot;) +
  labs(y = &quot;Prepositions per 1,000 words&quot;) +
  geom_smooth(method = &quot;lm&quot;) # with linear model smoothing!</code></pre>
<p><img src="basicstatzregression_files/figure-html/slr6-1.png" width="672" /></p>
<p>Before beginning with the regression analysis, we will scale the year. We scale by subtracting each value from the mean of year. This can be useful when dealing with numeric variables because if we did not scale year, we would get estimated values for year 0 (a year when English did not even exist yet). If a variable is scaled, the regression provides estimates of the model refer to the mean of that numeric variable. In other words, scaling can eb very helpful, especially with respect to the interpretation of the results that regression models report.</p>
<pre class="r"><code># scale date
slrdata$Date &lt;- slrdata$Date - mean(slrdata$Date) </code></pre>
<p>We will now begin the regression analysis by generating a first regression model. and inspect its results.</p>
<pre class="r"><code># create initial model
m1.lm &lt;- lm(Prepositions ~ Date, data = slrdata)
# inspect results
summary(m1.lm)</code></pre>
<pre><code>## 
## Call:
## lm(formula = Prepositions ~ Date, data = slrdata)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -69.101 -13.855   0.578  13.321  62.858 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 1.322e+02  8.386e-01 157.625   &lt;2e-16 ***
## Date        1.732e-02  7.267e-03   2.383   0.0175 *  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 19.43 on 535 degrees of freedom
## Multiple R-squared:  0.01051,    Adjusted R-squared:  0.008657 
## F-statistic: 5.681 on 1 and 535 DF,  p-value: 0.0175</code></pre>
<p>The summary output starts by repeating the regression equation. Then, the model provides the distribution of the residuals. The residuals should be sitributed normally with the Min and Max as well as the 1Q (first quartile) and 3Q (third quartile) being similar or ideally identical. In our case, the values are very similar which suggests that the residuals are distributed evenly and follow a normal distribution. The next part of the repost is the coefficients table. The Estimate for the intercept is the value where the regression line crosses the y-axis. The estimate for Date represents the slope of the regression line and tells us that with each year, the predicted frequency of prepositions incerase by .01732 prepositions. The t-value is the Estimate divided by the standard error (Std. Error). Based on the t-value, the p-value can be calculated manually as shown below.</p>
<pre class="r"><code># use pt function (which uses t-values and the degrees of freedom)
2*pt(-2.383, nrow(slrdata)-1)</code></pre>
<pre><code>## [1] 0.01751964</code></pre>
<p>The R<sup>2</sup>-values tell us how much variance is explained by our model compared to the overall variance (0.0105 means that our model explains only 1.05 percent of the variance - which is a tiny amount). The adjusted R<sup>2</sup>-value tell us how much variance the model would explain if we applied the model to new data of the same nature (which data points taken from the same population). Or, to be more precise, the adjusted R<sup>2</sup> takes the number of predictors into account: if a model contains predictors that do not explain much variance, then the adjusted R<sup>2</sup> will be lower than the multiple R<sup>2</sup> as the former penalizes models for having superfluous predictors. If there is a big difference between the two R<sup>2</sup>-values, then the model is overfitted which is not good. The F-statistic and the associated p-value tell us that the model, despite explaining almost no variance, is still significantly better than an intercept-only base-line model (or using the overall mean to predict the frequency of prepositions per text).</p>
<p>We can test this and also see where the F-values comes from by comparing the</p>
<pre class="r"><code># create intercept-only base-line model
m0.lm &lt;- lm(Prepositions ~ 1, data = slrdata)
# compare the base-line and the more saturated model
anova(m1.lm, m0.lm, test = &quot;F&quot;)</code></pre>
<pre><code>## Analysis of Variance Table
## 
## Model 1: Prepositions ~ Date
## Model 2: Prepositions ~ 1
##   Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)  
## 1    535 202058                             
## 2    536 204204 -1   -2145.6 5.6809 0.0175 *
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>The F- and p-values are exactly those reported by the summary which shows where the F-values comes from and what it means; namely it denote the difference between the base-line and the more saturated model.</p>
<p>The degrees of freedom associated with the residual standard error are the number of cases in the model minus the number of predictors (including the intercept). The residual standard error is square root of the sum of the squared residuals of the model divided by the degrees of freedom. Have a look at he following to clear this up:</p>
<pre class="r"><code># DF = N - number of predictors (including intercept)
DegreesOfFreedom &lt;- nrow(slrdata)-length(coef(m1.lm))
# sum of the squared residuals
SumSquaredResiduals &lt;- sum(resid(m1.lm)^2)
# Residual Standard Error
sqrt(SumSquaredResiduals/DegreesOfFreedom); DegreesOfFreedom</code></pre>
<pre><code>## [1] 19.43396</code></pre>
<pre><code>## [1] 535</code></pre>
<p>We will now check if mathematical assumptions have been violated (homogeneity of variance) or whether the data contains outliers. We check this using diagnostic plots.</p>
<pre class="r"><code># plot model: 3 plots per row in one window
par(mfrow = c(1, 3))
plot(resid(m1.lm))
plot(rstandard(m1.lm))
plot(rstudent(m1.lm)); par(mfrow = c(1, 1)) # restore default parameters</code></pre>
<p><img src="basicstatzregression_files/figure-html/slr9-1.png" width="672" /></p>
<p>The left graph shows the residuals of the model (i.e., the differences between the observed and the values predicted by the regression model). The problem with this plot is that the residuals are not standardized and so they cannot be compared to the residuals of other models. To remedy this deficiency, residuals are normalized by dividing the residuals by their standard deviation. Then, the normalized residuals can be plotted against the observed values (centre panel). In this way, not only are standardized residuals obtained, but the values of the residuals are transformed into z-values, and one can use the z-distribution to find problematic data points. There are three rules of thumb regarding finding problematic data points through standardized residuals <span class="citation">(Field, Miles, and Field <a href="#ref-field2012discovering" role="doc-biblioref">2012</a>, 268–69)</span>:</p>
<ul>
<li>Points with values higher than 3.29 should be removed from the data.</li>
<li>If more than 1% of the data points have values higher than 2.58, then the error rate of our model is too high.</li>
<li>If more than 5% of the data points have values greater than 1.96, then the error rate of our model is too high.</li>
</ul>
<p>The right panel shows the * studentized residuals* (adjusted predicted values: each data point is divided by the standard error of the residuals). In this way, it is possible to use Student’s t-distribution to diagnose our model.</p>
<p>Adjusted predicted values are residuals of a special kind: the model is calculated without a data point and then used to predict this data point. The difference between the observed data point and its predicted value is then called the adjusted predicted value. In summary, studentized residuals are very useful because they allow us to identify influential data points.</p>
<p>The plots show that there are two potentially problematic data points (the top-most and bottom-most point). These two points are clearly different from the other data points and may therefore be outliers. We will test later if these points need to be removed.</p>
<p>We will now generate more diagnostic plots.</p>
<pre class="r"><code>par(mfrow = c(2, 2)) # plot window: 2 plots/row, 2 plots/column
plot(m1.lm); par(mfrow = c(1, 1)) # restore normal plot window</code></pre>
<p><img src="basicstatzregression_files/figure-html/slr10-1.png" width="672" /></p>
<p>The diagnostic plots are very positive and we will go through why this is so for each panel. The graph in the upper left panel is useful for finding outliers or for determining the correlation between residuals and predicted values: when a trend becomes visible in the line or points (e.g., a rising trend or a zigzag line), then this would indicate that the model would be problematic (in such cases, it can help to remove data points that are too influential (outliers)).</p>
<p>The graphic in the upper right panel indicates whether the residuals are normally distributed (which is desirable) or whether the residuals do not follow a normal distribution. If the points lie on the line, the residuals follow a normal distribution. For example, if the points are not on the line at the top and bottom, it shows that the model does not predict small and large values well and that it therefore does not have a good fit.</p>
<p>The graphic in the lower left panel provides information about <em>homoscedasticity</em>. Homoscedasticity means that the variance of the residuals remains constant and does not correlate with any independent variable. In unproblematic cases, the graphic shows a flat line. If there is a trend in the line, we are dealing with heteroscedasticity, that is, a correlation between independent variables and the residuals, which is very problematic for regressions.</p>
<p>The graph in the lower right panel shows problematic influential data points that disproportionately affect the regression (this would be problematic). If such influential data points are present, they should be either weighted (one could generate a robust rather than a simple linear regression) or they must be removed. The graph displays Cook’s distance, which shows how the regression changes when a model without this data point is calculated. The cook distance thus shows the influence a data point has on the regression as a whole. Data points that have a Cook’s distance value greater than 1 are problematic <span class="citation">(Field, Miles, and Field <a href="#ref-field2012discovering" role="doc-biblioref">2012</a>, 269)</span>.</p>
<p>The so-called leverage is also a measure that indicates how strongly a data point affects the accuracy of the regression. Leverage values range between 0 (no influence) and 1 (strong influence: suboptimal!). To test whether a specific data point has a high leverage value, we calculate a cut-off point that indicates whether the leverage is too strong or still acceptable. The following two formulas are used for this:</p>
<p><span class="math display">\[\begin{equation}

\frac{3(k + 1)}{n}

\end{equation}\]</span></p>
<p>or</p>
<p><span class="math display">\[\begin{equation}

\frac{2(k + 1)}{n}

\end{equation}\]</span></p>
<p>We will look more closely at leverage in the context of multiple linear regression and will therefore end the current analysis by summarizing the results of the regression analysis in a table.</p>
<pre class="r"><code># create summary table
slrresults &lt;- slrsummary(m1.lm)  
# show summary table
slrresults</code></pre>
<table>
<caption>Results of a simple linear regression analysis.</caption>
<thead>
<tr class="header">
<th></th>
<th align="left">Estimate</th>
<th align="left">Pearson’s r</th>
<th align="left">Std. Error</th>
<th align="left">t value</th>
<th align="left">Pr(&gt;|t|)</th>
<th align="left">P-value sig.</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>(Intercept)</td>
<td align="left">132.19</td>
<td align="left"></td>
<td align="left">0.84</td>
<td align="left">157.62</td>
<td align="left">0</td>
<td align="left">p &lt; .001***</td>
</tr>
<tr class="even">
<td>Date</td>
<td align="left">0.02</td>
<td align="left">0.1</td>
<td align="left">0.01</td>
<td align="left">2.38</td>
<td align="left">0.0175</td>
<td align="left">p &lt; .05*</td>
</tr>
<tr class="odd">
<td>Model statistics</td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left">Value</td>
</tr>
<tr class="even">
<td>Number of cases in model</td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left">537</td>
</tr>
<tr class="odd">
<td>Residual standard error on 535 DF</td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left">19.43</td>
</tr>
<tr class="even">
<td>Multiple R-squared</td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left">0.0105</td>
</tr>
<tr class="odd">
<td>Adjusted R-squared</td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left">0.0087</td>
</tr>
<tr class="even">
<td>F-statistic (1, 535)</td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left">5.68</td>
</tr>
<tr class="odd">
<td>Model p-value</td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left">0.0175</td>
</tr>
</tbody>
</table>
<p>Typically, the results of regression analyses are presented in such tables as they include all important measures of model quality and significance, as well as the magnitude of the effects.</p>
<p>In addition, the results of simple linear regressions should be summarized in writing. An example of how the results of a regression analysis can be written up is provided below.</p>
<p>A simple linear regression has been fitted to the data. A visual assessment of the model diagnostic graphics did not indicate any problematic or disproportionately influential data points (outliers) and performed significantly better compared to an intercept-only base line model but only explained .87 percent of the vraiance (Adjusted R<sup>2</sup>: .0087, F-statistic (1, 535): 5,68, p-value: 0.0175*). The final minimal adequate linear regression model is based on 537 data points and confirms a significant and positive correlation between the year in which the text was written and the relative frequency of prepositions (coefficient estimate: .02, SE: 0.01, t-value: 2.38, p-value: .0175*).</p>
</div>
<div id="example-2-teaching-styles" class="section level1">
<h1><span class="header-section-number">4</span> Example 2: Teaching Styles</h1>
<p>In the previous example, we dealt with two numeric variables, while the following example deals with a categorical independent variable and a numeric dependent variable. The ability for regressions to handle very different types of variables makes regressions a widely used and robust method of analysis.</p>
<p>In this example, we are dealing with two groups of students that have been randomly assigned to be exposed to different teaching methods. Both groups undergo a language learning test after the lesson with a maximum score of 20 points.</p>
<p>The question that we will try to answer is whether the students in group A have performed significantly better than those in group B which would indicate that the teaching method to which group A was exposed works better than the teaching method to which group B was exposed.</p>
<p>Let’s move on to implementing the regression in “R”. In a first step, we load the data set and inspect its structure.</p>
<pre class="r"><code># load data
slrdata2 &lt;- read.delim(&quot;https://slcladal.github.io/data/slrdata2.txt&quot;, sep = &quot;\t&quot;, header = T)
# inspect data
head(slrdata2)</code></pre>
<pre><code>##   Group Score
## 1     A    15
## 2     A    12
## 3     A    11
## 4     A    18
## 5     A    15
## 6     A    15</code></pre>
<p>Now, we graphically display the data. In this case, a boxplot represents a good way to visualize the data.</p>
<pre class="r"><code># extract means
means &lt;- slrdata2 %&gt;%
  dplyr::group_by(Group) %&gt;%
  dplyr::summarise(Mean = round(mean(Score), 1), SD = round(sd(Score), 1))
# start plot
ggplot(slrdata2, aes(Group, Score)) + 
  geom_boxplot(fill=c(&quot;orange&quot;, &quot;darkgray&quot;)) +
  geom_text(data = means, aes(label = paste(&quot;M = &quot;, Mean, sep = &quot;&quot;), y = 1)) +
  geom_text(data = means, aes(label = paste(&quot;SD = &quot;, SD, sep = &quot;&quot;), y = 0)) +
  theme_set(theme_bw(base_size = 15)) +
  labs(x = &quot;Group&quot;) +                      
  labs(y = &quot;Test score (Points)&quot;, cex = .75) +   
  coord_cartesian(ylim = c(0, 20)) +  
  guides(fill = FALSE)                </code></pre>
<div class="figure">
<img src="basicstatzregression_files/figure-html/slr14-1.png" alt="Darstellung der Sprachtestdaten" width="672" />
<p class="caption">
Darstellung der Sprachtestdaten
</p>
</div>
<p>The data indicate that group A did significantly better than group B. We will test this impression by generating the regression model and creating the model and extracting the model summary.</p>
<pre class="r"><code># generate regression model
m2.lm &lt;- lm(Score ~ Group, data = slrdata2) 
# inspect results
summary(m2.lm)                             </code></pre>
<pre><code>## 
## Call:
## lm(formula = Score ~ Group, data = slrdata2)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -6.767 -1.933  0.150  2.067  6.233 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  14.9333     0.5346  27.935  &lt; 2e-16 ***
## GroupB       -3.1667     0.7560  -4.189 9.67e-05 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2.928 on 58 degrees of freedom
## Multiple R-squared:  0.2322, Adjusted R-squared:  0.219 
## F-statistic: 17.55 on 1 and 58 DF,  p-value: 9.669e-05</code></pre>
<p>The model summary reports that Groups B performed significantly (Pr(&gt;|t|) is smaller than .001 as indicated by the three * after the p-values) comapred with Groups A (the Estmate is negative). We will now create the diagnostic graphics.</p>
<pre class="r"><code>par(mfrow = c(1, 3))        # plot window: 1 plot/row, 3 plots/column
plot(resid(m2.lm))     # generate diagnostic plot
plot(rstandard(m2.lm)) # generate diagnostic plot
plot(rstudent(m2.lm)); par(mfrow = c(1, 1))  # restore normal plot window</code></pre>
<p><img src="basicstatzregression_files/figure-html/slr16-1.png" width="672" /></p>

<p>The graphics do not indicate outliers or other issues, so we can continue with more diagnostic graphics.</p>
<pre class="r"><code>par(mfrow = c(2, 2)) # generate a plot window with 2x2 panels
plot(m2.lm); par(mfrow = c(1, 1)) # restore normal plot window</code></pre>
<p><img src="basicstatzregression_files/figure-html/slr17-1.png" width="672" /></p>

<p>These graphics also show no problems. In this case, the data can be summarized in the next step.</p>
<pre class="r"><code># tabulate results
slrresults2 &lt;- slrsummary(m2.lm)
slrresults2</code></pre>
<pre><code>##                                  Estimate Pearson&#39;s r Std. Error t value
## (Intercept)                         14.93                   0.53   27.94
## GroupB                              -3.17        0.48       0.76   -4.19
## Model statistics                                                        
## Number of cases in model                                                
## Residual standard error on 58 DF                                        
## Multiple R-squared                                                      
## Adjusted R-squared                                                      
## F-statistic (1, 58)                                                     
## Model p-value                                                           
##                                  Pr(&gt;|t|) P-value sig.
## (Intercept)                             0  p &lt; .001***
## GroupB                              1e-04  p &lt; .001***
## Model statistics                                 Value
## Number of cases in model                            60
## Residual standard error on 58 DF                  2.93
## Multiple R-squared                              0.2322
## Adjusted R-squared                               0.219
## F-statistic (1, 58)                              17.55
## Model p-value                                    1e-04</code></pre>
<table>
<caption>Results of the regression model.</caption>
<thead>
<tr class="header">
<th></th>
<th align="left">Estimate</th>
<th align="left">Pearson’s r</th>
<th align="left">Std. Error</th>
<th align="left">t value</th>
<th align="left">Pr(&gt;|t|)</th>
<th align="left">P-value sig.</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>(Intercept)</td>
<td align="left">14.93</td>
<td align="left"></td>
<td align="left">0.53</td>
<td align="left">27.94</td>
<td align="left">0</td>
<td align="left">p &lt; .001***</td>
</tr>
<tr class="even">
<td>GroupB</td>
<td align="left">-3.17</td>
<td align="left">0.48</td>
<td align="left">0.76</td>
<td align="left">-4.19</td>
<td align="left">1e-04</td>
<td align="left">p &lt; .001***</td>
</tr>
<tr class="odd">
<td>Model statistics</td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left">Value</td>
</tr>
<tr class="even">
<td>Number of cases in model</td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left">60</td>
</tr>
<tr class="odd">
<td>Residual standard error on 58 DF</td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left">2.93</td>
</tr>
<tr class="even">
<td>Multiple R-squared</td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left">0.2322</td>
</tr>
<tr class="odd">
<td>Adjusted R-squared</td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left">0.219</td>
</tr>
<tr class="even">
<td>F-statistic (1, 58)</td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left">17.55</td>
</tr>
<tr class="odd">
<td>Model p-value</td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left">1e-04</td>
</tr>
</tbody>
</table>
<p>The results of this second simple linear regressions can be summarized as follows:</p>
<p>A simple linear regression was fitted to the data. A visual assessment of the model diagnostics did not indicate any problematic or disproportionately influential data points (outliers). The final linear regression model is based on 60 data points, performed significantly better than an intercept-only base line model (F (1, 58): 17.55, p-value &lt;. 001***), and reported that the model explained 21.9 percent of variance whichconfirmed a good model fit. According to this final model, group A scored significantly better on the language learning test than group B (coefficient: -3.17, SE: 0.48, t-value: -4.19, p-value &lt;. 001 ***).</p>
</div>
<div id="how-to-cite-this-tutorial" class="section level1 unnumbered">
<h1>How to cite this tutorial</h1>
<p>Schweinberger, Martin. 2020. <em>Simple Linear Regression in R</em>. Brisbane: The University of Queensland. url: <a href="https://slcladal.github.io/basicstatzregression.html" class="uri">https://slcladal.github.io/basicstatzregression.html</a>.</p>
</div>
<div id="references" class="section level1 unnumbered">
<h1>References</h1>
<div id="refs" class="references">
<div id="ref-baayen2008analyzing">
<p>Baayen, R Harald. 2008. <em>Analyzing Linguistic Data. A Practical Introduction to Statistics Using R</em>. Cambridge: Cambridge University press.</p>
</div>
<div id="ref-field2012discovering">
<p>Field, Andy, Jeremy Miles, and Zoe Field. 2012. <em>Discovering Statistics Using R</em>. Sage.</p>
</div>
<div id="ref-gries2009statistics">
<p>Gries, Stefan Th. 2009. <em>Statistics for Linguistics Using R: A Practical Introduction</em>. Berlin &amp; New York: Mouton de Gruyter.</p>
</div>
<div id="ref-levshina2015linguistics">
<p>Levshina, Natalia. 2015. <em>How to Do Linguistics with R: Data Exploration and Statistical Analysis</em>. Amsterdam: John Benjamins Publishing Company.</p>
</div>
<div id="ref-wilcox2009basic">
<p>Wilcox, Rand R. 2009. <em>Basic Statistics: Understanding Conventional Methods and Modern Insights</em>. Oxford University Press.</p>
</div>
</div>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_').toLowerCase();
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
