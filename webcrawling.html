<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />


<meta name="author" content="Martin Schweinberger" />

<meta name="date" content="2020-09-29" />

<title>Web Crawling and Scraping using R</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cosmo.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>




<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #ffffff;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">LADAL</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="people.html">OUR PEOPLE</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    NEWS
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="news.html">News &amp; Announcements</a>
    </li>
    <li>
      <a href="conferences.html">Events &amp; Presentations</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    DATA SCIENCE BASICS
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li class="dropdown-header">Introduction to Data Science</li>
    <li>
      <a href="introcomputer.html">Working with Computers: Tips and Tricks</a>
    </li>
    <li>
      <a href="reproducibility.html">Data Management, Version Control, and Reproducibility</a>
    </li>
    <li class="divider"></li>
    <li class="dropdown-header">Quantitative Research</li>
    <li>
      <a href="introquant.html">Introduction to Quantitative Reasoning</a>
    </li>
    <li>
      <a href="basicquant.html">Basic Concepts in Quantitative Research</a>
    </li>
    <li class="divider"></li>
    <li class="dropdown-header">Introduction to R</li>
    <li>
      <a href="IntroR_workshop.html">Getting started</a>
    </li>
    <li>
      <a href="stringprocessing.html">String Processing</a>
    </li>
    <li>
      <a href="regularexpressions.html">Regular Expressions</a>
    </li>
    <li>
      <a href="introtables.html">Working with Tables</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    TUTORIALS
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li class="dropdown-header">Data Visualization</li>
    <li>
      <a href="introviz.html">Introduction to Data Viz</a>
    </li>
    <li>
      <a href="basicgraphs.html">Common Visualization Types</a>
    </li>
    <li>
      <a href="basicgraphs.html">Advanced Visualization Methods</a>
    </li>
    <li>
      <a href="maps.html">Displaying Geo-Spatial Data</a>
    </li>
    <li>
      <a href="motion.html">Interactive Charts</a>
    </li>
    <li class="divider"></li>
    <li class="dropdown-header">Statistics</li>
    <li>
      <a href="descriptivestatz.html">Descriptive Statistics</a>
    </li>
    <li>
      <a href="basicstatz.html">Basic Inferential Statistics</a>
    </li>
    <li>
      <a href="regression.html">Regression Analysis</a>
    </li>
    <li>
      <a href="advancedstatztrees.html">Tree-Based Models</a>
    </li>
    <li>
      <a href="groupingstatz.html">Cluster and Correspondence Analysis</a>
    </li>
    <li>
      <a href="svm.html">Semantic Vector Space Models</a>
    </li>
    <li class="divider"></li>
    <li class="dropdown-header">Text Analytics</li>
    <li>
      <a href="textanalysis.html">Text Analysis and Distant Reading</a>
    </li>
    <li>
      <a href="kwics.html">Concordancing (keywords-in-context)</a>
    </li>
    <li>
      <a href="basicnetwork.html">Network Analysis</a>
    </li>
    <li>
      <a href="collocations.html">Co-occurrence and Collocation Analysis</a>
    </li>
    <li>
      <a href="topicmodels.html">Topic Modeling</a>
    </li>
    <li>
      <a href="sentiment.html">Sentiment Analysis</a>
    </li>
    <li>
      <a href="tagging.html">Tagging and Parsing</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    FOCUS &amp; CASE STUDIES
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="lex.html">Lexicography with R: Generating Dictionaries</a>
    </li>
    <li>
      <a href="surveys.html">Questionnaires and Surveys: Analyses with R</a>
    </li>
    <li>
      <a href="corplingr.html">Corpus Linguistics with R: Swearing in Irish English</a>
    </li>
    <li>
      <a href="convertpdf2txt.html">Converting PDFs to txt</a>
    </li>
    <li>
      <a href="webcrawling.html">Web Crawling using R</a>
    </li>
    <li>
      <a href="vc.html">Creating Vowel Charts with Praat and R</a>
    </li>
  </ul>
</li>
<li>
  <a href="services.html">SERVICES &amp; CONTACT</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Web Crawling and Scraping using R</h1>
<h4 class="author">Martin Schweinberger</h4>
<h4 class="date">2020-09-29</h4>

</div>


<p><img src="images/uq1.jpg" width="100%" /></p>
<div id="introduction" class="section level1 unnumbered">
<h1>Introduction</h1>
<p>This tutorial introduces how to extract and process text data from social media sites, web pages, or other documents for later analysis. The entire R markdown document for the present tutorial can be downloaded <a href="https://slcladal.github.io/webcrawling.Rdm">here</a>. This tutorial builds heavily on and uses materials from <a href="https://tm4ss.github.io/docs/Tutorial_2_Web_crawling.html">this tutorial</a> on web crawling and scraping using R by Andreas Niekler and Gregor Wiedemann <span class="citation">(see Wiedemann and Niekler <a href="#ref-WN17" role="doc-biblioref">2017</a>)</span>. <a href="https://tm4ss.github.io/docs/index.html">The tutorial</a> by Andreas Niekler and Gregor Wiedemann is more thorough, goes into more detail than this tutorial, and overs many more very useful text mining methods. An alternative approach for web crawling and scraping would be to use the <code>RCrawler</code> package <span class="citation">(Khalil and Fakir <a href="#ref-khalil2017rcrawler" role="doc-biblioref">2017</a>)</span> which is not introduced here thought (inspecting the <code>RCrawler</code> package and its functions is, however, also highly recommended). For a more in-depth introduction to web crawling in scraping, <span class="citation">Miner et al. (<a href="#ref-miner2012practical" role="doc-biblioref">2012</a>)</span> is a very useful introduction.</p>
<p>The automated download of HTML pages is called <em>Crawling</em>. The extraction of the textual data and/or metadata (for example, article date, headlines, author names, article text) from the HTML source code (or the DOM document object model of the website) is called <em>Scraping</em> <span class="citation">(see Olston and Najork <a href="#ref-olston2010web" role="doc-biblioref">2010</a>)</span>.</p>
<div id="preparation-and-session-set-up" class="section level2 unnumbered">
<h2>Preparation and session set up</h2>
<p>This tutorial is based on R. If you have not installed R or are new to it, you will find an introduction to and more information how to use R <a href="https://slcladal.github.io/IntroR_workshop.html">here</a>. For this tutorials, we need to install certain <em>packages</em> from an R <em>library</em> so that the scripts shown below are executed without errors. Before turning to the code below, please install the packages by running the code below this paragraph. If you have already installed the packages mentioned below, then you can skip ahead ignore this section. To install the necessary packages, simply run the following code - it may take some time (between 1 and 5 minutes to install all of the libraries so you do not need to worry if it takes some time).</p>
<pre class="r"><code># clean current workspace
rm(list=ls(all=T))
# set options
options(stringsAsFactors = F)
# install libraries
install.packages(c(&quot;rvest&quot;, &quot;readtext&quot;))</code></pre>
</div>
<div id="getting-started" class="section level2 unnumbered">
<h2>Getting started</h2>
<p>For web crawling and scraping, we use the package <code>rvest</code> and to extract text data from various formats such as PDF, DOC, DOCX and TXT files with the <code>readtext</code> package. The tasks described int his section consist of:</p>
<ol style="list-style-type: decimal">
<li><p>Download a single web page and extract its content</p></li>
<li><p>Extract links from a overview page and extract articles</p></li>
<li><p>Extract text data from PDF and other formats on disk</p></li>
</ol>
</div>
</div>
<div id="crawl-single-webpage" class="section level1 unnumbered">
<h1>Crawl single webpage</h1>
<p>As a first step, we will download a single web page from <em>The Guardian</em> and extract text together with relevant metadata such as the article date. Let’s define the URL of the article of interest and load the <code>rvest</code> package, which provides very useful functions for web crawling and scraping.</p>
<pre class="r"><code>url &lt;- &quot;https://www.theguardian.com/world/2017/jun/26/angela-merkel-and-donald-trump-head-for-clash-at-g20-summit&quot;
require(&quot;rvest&quot;)</code></pre>
<p>A convenient method to download and parse a webpage is provided by the function <code>read_html</code> which accepts a URL as its main argument. The function downloads the page and interprets the html source code as an HTML or XML object.</p>
<pre class="r"><code>html_document &lt;- read_html(url)</code></pre>
<p>HTML and XML objects are a structured representation of HTML/XML source code, which allows to extract single elements (headlines e.g. <code>&lt;h1&gt;</code>, paragraphs <code>&lt;p&gt;</code>, links <code>&lt;a&gt;</code>, …), their attributes (e.g. <code>&lt;a href="http://..."&gt;</code>) or text wrapped in between elements (e.g. <code>&lt;p&gt;my text...&lt;/p&gt;</code>). Elements can be extracted in XML objects with XPATH-expressions.</p>
<p>XPATH (see <a href="https://en.wikipedia.org/wiki/XPath">here</a>) is a query language that parses XML-tree structures and we use it to select the headline element from the HTML page.</p>
<p>The following xpath expression queries for first-order-headline elements <code>h1</code>, anywhere in the tree <code>//</code> which fulfill a certain condition <code>[...]</code>, namely that the <code>class</code> attribute of the <code>h1</code> element must contain the value <code>content__headline</code>.</p>
<p>The next expression uses R pipe operator %&gt;%, which takes the input from the left side of the expression and passes it on to the function ion the right side as its first argument. The result of this function is either passed onto the next function, via a pipe (<code>%&gt;%</code>) or it is assigned to variable, if it is the last operation in a pipe. Our pipe takes the <code>html_document</code> object, passes it to the <code>html_node</code> function, which extracts the first node fitting the given xpath expression. The resulting node object is passed to the <code>html_text</code> function which extracts the text wrapped in the <code>h1</code>-element.</p>
<pre class="r"><code>title_xpath &lt;- &quot;//h1[contains(@class, &#39;content__headline&#39;)]&quot;
title_text &lt;- html_document %&gt;%
  html_node(xpath = title_xpath) %&gt;%
  html_text(trim = T)</code></pre>
<p>Let’s see, what the title_text contains:</p>
<pre class="r"><code>cat(title_text)</code></pre>
<pre><code>## Angela Merkel and Donald Trump head for clash at G20 summit</code></pre>
<p>Now we modify the xpath expressions, to extract the article info, the paragraphs of the body text and the article date. Note that there are multiple paragraphs in the article. To extract not only the first, but all paragraphs we utilize the <code>html_nodes</code> function and glue the resulting single text vectors of each paragraph together with the <code>paste0</code> function.</p>
<pre class="r"><code>intro_xpath &lt;- &quot;//div[contains(@class, &#39;content__standfirst&#39;)]//p&quot;
intro_text &lt;- html_document %&gt;%
  html_node(xpath = intro_xpath) %&gt;%
  html_text(trim = T)
cat(intro_text)</code></pre>
<pre><code>## German chancellor plans to make climate change, free trade and mass migration key themes in Hamburg, putting her on collision course with US</code></pre>
<pre class="r"><code>body_xpath &lt;- &quot;//div[contains(@class, &#39;content__article-body&#39;)]//p&quot;
body_text &lt;- html_document %&gt;%
  html_nodes(xpath = body_xpath) %&gt;%
  html_text(trim = T) %&gt;%
  paste0(collapse = &quot;\n&quot;)</code></pre>
<p>Now, let’s inspect the first 150 elements of the text body.</p>
<pre class="r"><code>cat(substr(body_text, 0, 150))</code></pre>
<pre><code>## A clash between Angela Merkel and Donald Trump appears unavoidable after Germany signalled that it will make climate change, free trade and the manage</code></pre>
<p>We now extract the date from the html document.</p>
<pre class="r"><code>date_xpath &lt;- &quot;//time&quot;
date_object &lt;- html_document %&gt;%
  html_node(xpath = date_xpath) %&gt;%
  html_attr(name = &quot;datetime&quot;) %&gt;%
  as.Date()
cat(format(date_object, &quot;%Y-%m-%d&quot;))</code></pre>
<pre><code>## 2017-06-26</code></pre>
<p>The variables <code>title_text</code>, <code>intro_text</code>, <code>body_text</code> and <code>date_object</code> now contain the raw data for any subsequent text processing.</p>
</div>
<div id="following-links" class="section level1 unnumbered">
<h1>Following links</h1>
<p>Usually, we do not want download a single document, but a series of documents. In our second exercise, we want to download all Guardian articles tagged with <em>Angela Merkel</em>. Instead of a tag page, we could also be interested in downloading results of a site-search engine or any other link collection. The task is always two-fold: First, we download and parse the tag overview page to extract all links to articles of interest:</p>
<pre class="r"><code>url &lt;- &quot;https://www.theguardian.com/world/angela-merkel&quot;
html_document &lt;- read_html(url)</code></pre>
<p>Second, we download and scrape each individual article page. For this, we extract all <code>href</code>-attributes from <code>a</code>-elements fitting a certain CSS-class. To select the right contents via XPATH-selectors, you need to investigate the HTML-structure of your specific page. Modern browsers such as Firefox and Chrome support you in that task by a function called “Inspect Element” (or similar), available through a right-click on the page element.</p>
<pre class="r"><code>links &lt;- html_document %&gt;%
  html_nodes(xpath = &quot;//div[contains(@class, &#39;fc-item__container&#39;)]/a&quot;) %&gt;%
  html_attr(name = &quot;href&quot;)</code></pre>
<p>Now, <code>links</code> contains a list of 20 hyperlinks to single articles tagged with Angela Merkel.</p>
<pre class="r"><code>head(links, 3)</code></pre>
<pre><code>## [1] &quot;https://www.theguardian.com/world/2020/sep/28/alexei-navalny-angela-merkel-visit-berlin-hospital&quot;                            
## [2] &quot;https://www.theguardian.com/world/commentisfree/2020/sep/28/reunification-germany-30-years-eu-transatlantic-western-alliance&quot;
## [3] &quot;https://www.theguardian.com/environment/2020/sep/28/world-leaders-pledge-to-halt-earth-destruction-un-summit&quot;</code></pre>
<p>But stop! There is not only one page of links to tagged articles. If you have a look on the page in your browser, the tag overview page has several more than 60 sub pages, accessible via a paging navigator at the bottom. By clicking on the second page, we see a different URL-structure, which now contains a link to a specific paging number. We can use that format to create links to all sub pages by combining the base URL with the page numbers.</p>
<pre class="r"><code>page_numbers &lt;- 1:3
base_url &lt;- &quot;https://www.theguardian.com/world/angela-merkel?page=&quot;
paging_urls &lt;- paste0(base_url, page_numbers)
# View first 3 urls
head(paging_urls, 3)</code></pre>
<pre><code>## [1] &quot;https://www.theguardian.com/world/angela-merkel?page=1&quot;
## [2] &quot;https://www.theguardian.com/world/angela-merkel?page=2&quot;
## [3] &quot;https://www.theguardian.com/world/angela-merkel?page=3&quot;</code></pre>
<p>Now we can iterate over all URLs of tag overview pages, to collect more/all links to articles tagged with Angela Merkel. We iterate with a for-loop over all URLs and append results from each single URL to a vector of all links.</p>
<pre class="r"><code>all_links &lt;- NULL
for (url in paging_urls) {
  # download and parse single ta overview page
  html_document &lt;- read_html(url)
  # extract links to articles
  links &lt;- html_document %&gt;%
    html_nodes(xpath = &quot;//div[contains(@class, &#39;fc-item__container&#39;)]/a&quot;) %&gt;%
    html_attr(name = &quot;href&quot;)
  
  # append links to vector of all links
  all_links &lt;- c(all_links, links)
}</code></pre>
<p>An effective way of programming is to encapsulate repeatedly used code in a specific function. This function then can be called with specific parameters, process something and return a result. We use this here, to encapsulate the downloading and parsing of a Guardian article given a specific URL. The code is the same as in our exercise 1 above, only that we combine the extracted texts and metadata in a data.frame and wrap the entire process in a function-Block.</p>
<pre class="r"><code>scrape_guardian_article &lt;- function(url) {
  
  html_document &lt;- read_html(url)
  
  title_xpath &lt;- &quot;//h1[contains(@class, &#39;content__headline&#39;)]&quot;
  title_text &lt;- html_document %&gt;%
    html_node(xpath = title_xpath) %&gt;%
    html_text(trim = T)
  
  intro_xpath &lt;- &quot;//div[contains(@class, &#39;content__standfirst&#39;)]//p&quot;
  intro_text &lt;- html_document %&gt;%
    html_node(xpath = intro_xpath) %&gt;%
    html_text(trim = T)
  
  body_xpath &lt;- &quot;//div[contains(@class, &#39;content__article-body&#39;)]//p&quot;
  body_text &lt;- html_document %&gt;%
    html_nodes(xpath = body_xpath) %&gt;%
    html_text(trim = T) %&gt;%
    paste0(collapse = &quot;\n&quot;)
  
  date_xpath &lt;- &quot;//time&quot;
  date_text &lt;- html_document %&gt;%
    html_node(xpath = date_xpath) %&gt;%
    html_attr(name = &quot;datetime&quot;) %&gt;%
    as.Date()
  
  article &lt;- data.frame(
    url = url,
    date = date_text,
    title = title_text,
    body = paste0(intro_text, &quot;\n&quot;, body_text)
  )
  
  return(article)
  
}</code></pre>
<p>Now we can use that function <code>scrape_guardian_article</code> in any other part of our script. For instance, we can loop over each of our collected links. We use a running variable i, taking values from 1 to <code>length(all_links)</code> to access the single links in <code>all_links</code> and write some progress output.</p>
<pre class="r"><code>all_articles &lt;- data.frame()
for (i in 1:length(all_links)) {
  cat(&quot;Downloading&quot;, i, &quot;of&quot;, length(all_links), &quot;URL:&quot;, all_links[i], &quot;\n&quot;)
  article &lt;- scrape_guardian_article(all_links[i])
  # Append current article data.frame to the data.frame of all articles
  all_articles &lt;- rbind(all_articles, article)
}</code></pre>
<pre><code>## Downloading 1 of 60 URL: https://www.theguardian.com/world/2020/sep/28/alexei-navalny-angela-merkel-visit-berlin-hospital 
## Downloading 2 of 60 URL: https://www.theguardian.com/world/commentisfree/2020/sep/28/reunification-germany-30-years-eu-transatlantic-western-alliance 
## Downloading 3 of 60 URL: https://www.theguardian.com/environment/2020/sep/28/world-leaders-pledge-to-halt-earth-destruction-un-summit</code></pre>
<p>We now inspect the first three articles and save the data to our disc.</p>
<pre class="r"><code>head(all_articles, 3)
# Write articles to disk
write.csv2(all_articles, file = &quot;data/guardian_merkel.csv&quot;)</code></pre>
<p>The last command write the extracted articles to a CSV-file in the data directory for any later use.</p>
</div>
<div id="read-various-file-formats" class="section level1 unnumbered">
<h1>Read various file formats</h1>
<p>In case you have already a collection of text data files on your disk, you can import them into R in a very convenient provided by the <code>readtext</code> package. The package depends on some other programs or libraries in your system, to provide extraction for Word- and PDF-documents. So there may be some hurdles to install the package.</p>
<p>But once it is successfully installed, it allows for very easy extraction of text data from various file formats. Fist, we request a list of files in the directory to extract text from. For demonstration purpose, we provide in <code>data/documents</code> a random selection of various text formats.</p>
<pre class="r"><code>data_files &lt;- list.files(path = &quot;data/documents&quot;, full.names = T, recursive = T)
# View first file paths
head(data_files, 3)</code></pre>
<pre><code>## character(0)</code></pre>
<p>The <code>readtext</code> function from the package with the same name, detects automatically file formats of the given files list and extracts the content into a data.frame. The parameter <code>docvarsfrom</code> allows you to set metadata variables by splitting path names. In our example, <code>docvar3</code> contains a source type variable derived from the sub folder name.</p>
<pre class="r"><code>require(readtext)
extracted_texts &lt;- readtext(data_files, docvarsfrom = &quot;filepaths&quot;, dvsep = &quot;/&quot;)
# View first rows of the extracted texts
head(extracted_texts)
# View beginning of tenth extracted text
substr(trimws(extracted_texts$text[10]) , 0, 100)</code></pre>
<p>Again, the <code>extracted_texts</code> can be written by <code>write.csv2</code> to disk for later use.</p>
</div>
<div id="citation-session-info" class="section level1 unnumbered">
<h1>Citation &amp; Session Info</h1>
<p>Schweinberger, Martin. 2020. <em>Web Crawling and Scraping using R</em>. Brisbane: The University of Queensland. url: <a href="https://slcladal.github.io/webcrawling.html" class="uri">https://slcladal.github.io/webcrawling.html</a> (Version 2020.09.29).</p>
<pre><code>@manual{schweinberger2020webc,
  author = {Schweinberger, Martin},
  title = {Web Crawling and Scraping using R},
  note = {https://slcladal.github.io/webcrawling.html},
  year = {2020},
  organization = &quot;The University of Queensland, School of Languages and Cultures},
  address = {Brisbane},
  edition = {2020/09/29}
}</code></pre>
<pre class="r"><code>sessionInfo()</code></pre>
<pre><code>## R version 4.0.2 (2020-06-22)
## Platform: x86_64-w64-mingw32/x64 (64-bit)
## Running under: Windows 10 x64 (build 18362)
## 
## Matrix products: default
## 
## locale:
## [1] LC_COLLATE=German_Germany.1252  LC_CTYPE=German_Germany.1252   
## [3] LC_MONETARY=German_Germany.1252 LC_NUMERIC=C                   
## [5] LC_TIME=German_Germany.1252    
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
## [1] rvest_0.3.6 xml2_1.3.2 
## 
## loaded via a namespace (and not attached):
##  [1] digest_0.6.25   R6_2.4.1        magrittr_1.5    evaluate_0.14  
##  [5] httr_1.4.2      rlang_0.4.7     stringi_1.5.3   curl_4.3       
##  [9] rmarkdown_2.3   tools_4.0.2     stringr_1.4.0   xfun_0.16      
## [13] yaml_2.2.1      compiler_4.0.2  htmltools_0.5.0 knitr_1.30</code></pre>
</div>
<div id="references" class="section level1 unnumbered">
<h1>References</h1>
<hr />
<p><a href="https://slcladal.github.io/index.html">Main page</a></p>
<hr />
<div id="refs" class="references">
<div id="ref-khalil2017rcrawler">
<p>Khalil, Salim, and Mohamed Fakir. 2017. “RCrawler: An R Package for Parallel Web Crawling and Scraping.” <em>SoftwareX</em> 6: 98–106.</p>
</div>
<div id="ref-miner2012practical">
<p>Miner, Gary, John Elder IV, Andrew Fast, Thomas Hill, Robert Nisbet, and Dursun Delen. 2012. <em>Practical Text Mining and Statistical Analysis for Non-Structured Text Data Applications</em>. Academic Press.</p>
</div>
<div id="ref-olston2010web">
<p>Olston, Christopher, and Marc Najork. 2010. <em>Web Crawling</em>. Now Publishers Inc.</p>
</div>
<div id="ref-WN17">
<p>Wiedemann, Gregor, and Andreas Niekler. 2017. “Hands-on: A Five Day Text Mining Course for Humanists and Social Scientists in R.” In <em>Proceedings of the Workshop on Teaching NLP for Digital Humanities (Teach4DH2017), Berlin, Germany, September 12, 2017.</em>, 57–65. <a href="http://ceur-ws.org/Vol-1918/wiedemann.pdf">http://ceur-ws.org/Vol-1918/wiedemann.pdf</a>.</p>
</div>
</div>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
