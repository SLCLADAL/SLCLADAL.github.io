<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />


<meta name="author" content="Martin Schweinberger" />

<meta name="date" content="2022-05-21" />

<title>Web Crawling and Scraping using R</title>

<script src="site_libs/header-attrs-2.14/header-attrs.js"></script>
<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cosmo.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<script src="site_libs/clipboard-1.7.1/clipboard.min.js"></script>
<link href="site_libs/primer-tooltips-1.4.0/build.css" rel="stylesheet" />
<link href="site_libs/klippy-0.0.0.9500/css/klippy.min.css" rel="stylesheet" />
<script src="site_libs/klippy-0.0.0.9500/js/klippy.min.js"></script>
<link rel="stylesheet" href="styles.css" />

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-VSGK4KYDQZ"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-VSGK4KYDQZ');
</script>


<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>





<style type="text/css">
/* for pandoc --citeproc since 2.11 */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>




<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.tab('show');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">



<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  
  <!-- Added by SKC - LADAL image and thicker top with   -->
  <div class="container-fluid navbar-top" >
    <a href="index.html"> <!-- Make entire top row and text clickable home link  -->
        <div class="row">
            <div class="navbar-brand col-md-12">
              <img src="ladal_icon_cas_tran_white_trimed.png" class="navbar-icon" alt="LADAL"/>
              <span class="navbar-title-note navbar-collapse collapse" >Language Technology and Data Analysis Laboratory</span>
            </div>
        </div>
    </a>
  </div>
  
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <!-- SKC removed  navbar brand -->
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">HOME</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    ABOUT LADAL
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="people.html">People | Collabs</a>
    </li>
    <li>
      <a href="news.html">News</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    EVENTS
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="workshops.html">Workshops</a>
    </li>
    <li>
      <a href="webinars2022.html">LADAL Webinar Series 2022</a>
    </li>
    <li>
      <a href="opening.html">LADAL Webinar Series 2021</a>
    </li>
  </ul>
</li>
<li>
  <a href="tutorials.html">TUTORIALS</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    RESOURCES
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="links.html">Links</a>
    </li>
    <li>
      <a href="base.html">Tutorial stylesheet</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    CONTACT
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="contact.html">Contact</a>
    </li>
    <li>
      <a href="services.html">Services</a>
    </li>
  </ul>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">



<h1 class="title toc-ignore">Web Crawling and Scraping using R</h1>
<h4 class="author">Martin Schweinberger</h4>
<h4 class="date">2022-05-21</h4>

</div>


<p><img src="https://slcladal.github.io/images/uq1.jpg" width="100%" /></p>
<div id="introduction" class="section level1 unnumbered">
<h1 class="unnumbered">Introduction</h1>
<p>This tutorial introduces web crawling and web scraping with R. Web
crawling and web scraping are important and common procedures for
collecting text data from social media sites, web pages, or other
documents for later analysis. Regarding terminology, the automated
download of HTML pages is called <em>crawling</em> while the extraction
of the textual data and/or metadata (for example, article date,
headlines, author names, article text) from the HTML source code (or the
DOM document object model of the website) is called <em>scraping</em>
<span class="citation">(see <a href="#ref-olston2010web"
role="doc-biblioref">Olston and Najork 2010</a>)</span>.</p>
<p><img src="https://slcladal.github.io/images/yr_chili.jpg" width="15%" style="float:right; padding:10px" /></p>
<p>This tutorial is aimed at intermediate and advanced users of R with
the aim of showcasing how to crawl and scrape web data using R. The aim
is not to provide a fully-fledged analysis but rather to show and
exemplify selected useful methods associated with crawling and scraping
web data.</p>
<div class="warning"
style="padding:0.1em; background-color:#f2f2f2; color:#51247a">
<span>
<p style="margin-top:1em; text-align:center">
The entire R Notebook for the tutorial can be downloaded <a
href="https://slcladal.github.io/webcrawling.Rmd"><strong>here</strong></a>.
If you want to render the R Notebook on your machine, i.e. knitting the
document to html or a pdf, you need to make sure that you have R and
RStudio installed and you also need to download the <a
href="https://slcladal.github.io/bibliography.bib"><strong>bibliography
file</strong></a> and store it in the same folder where you store the
Rmd file. <br>
</p>
<p style="margin-left:1em;">
</p>
<p></span></p>
</div>
<p><br></p>
<p>This tutorial builds heavily on and uses materials from <a
href="https://tm4ss.github.io/docs/Tutorial_2_Web_crawling.html">this
tutorial</a> on web crawling and scraping using R by Andreas Niekler and
Gregor Wiedemann <span class="citation">(see <a href="#ref-WN17"
role="doc-biblioref">Wiedemann and Niekler 2017</a>)</span>. <a
href="https://tm4ss.github.io/docs/index.html">The tutorial</a> by
Andreas Niekler and Gregor Wiedemann is more thorough, goes into more
detail than this tutorial, and covers many more very useful text mining
methods. An alternative approach for web crawling and scraping would be
to use the <code>RCrawler</code> package <span class="citation">(<a
href="#ref-khalil2017rcrawler" role="doc-biblioref">Khalil and Fakir
2017</a>)</span> which is not introduced here though (inspecting the
<code>RCrawler</code> package and its functions is, however, also highly
recommended). For a more in-depth introduction to web crawling in
scraping, <span class="citation">Miner et al. (<a
href="#ref-miner2012practical" role="doc-biblioref">2012</a>)</span> is
a very useful introduction.</p>
<div id="preparation-and-session-set-up"
class="section level2 unnumbered">
<h2 class="unnumbered">Preparation and session set up</h2>
<p>This tutorial is based on R. If you have not installed R or are new
to it, you will find an introduction to and more information how to use
R <a href="https://slcladal.github.io/intror.html">here</a>. For this
tutorials, we need to install certain <em>packages</em> from an R
<em>library</em> so that the scripts shown below are executed without
errors. Before turning to the code below, please install the packages by
running the code below this paragraph. If you have already installed the
packages mentioned below, then you can skip ahead ignore this section.
To install the necessary packages, simply run the following code - it
may take some time (between 1 and 5 minutes to install all of the
libraries so you do not need to worry if it takes some time).</p>
<pre class="r"><code># install packages
install.packages(&quot;rvest&quot;)
install.packages(&quot;readtext&quot;)
install.packages(&quot;webdriver&quot;)
install.packages(&quot;tidyverse&quot;)
webdriver::install_phantomjs()
# install klippy for copy-to-clipboard button in code chunks
install.packages(&quot;remotes&quot;)
remotes::install_github(&quot;rlesur/klippy&quot;)</code></pre>
<p>If not done yet, please install the <a
href="https://phantomjs.org">phantomJS</a> headless browser. This needs
to be done only once.</p>
<p>Now that we have installed the packages (and the <a
href="https://phantomjs.org">phantomJS</a> headless browser), we can
activate them as shown below.</p>
<pre class="r"><code># set options
options(stringsAsFactors = F)         # no automatic data transformation
options(&quot;scipen&quot; = 100, &quot;digits&quot; = 4) # suppress math annotation
# load packages
library(tidyverse)
library(rvest)
library(readtext)
library(webdriver)
# activate klippy for copy-to-clipboard button
klippy::klippy()</code></pre>
<script>
  addClassKlippyTo("pre.r, pre.markdown");
  addKlippy('left', 'top', 'auto', '1', 'Copy code', 'Copied!');
</script>
<p>Once you have installed R and RStudio and once you have initiated the
session by executing the code shown above, you are good to go.</p>
</div>
</div>
<div id="scraping-a-single-website" class="section level1 unnumbered">
<h1 class="unnumbered">Scraping a single website</h1>
<p>For web crawling and scraping, we use the package <code>rvest</code>
and to extract text data from various formats such as PDF, DOC, DOCX and
TXT files with the <code>readtext</code> package. In a first exercise,
we will download a single web page from <em>The Guardian</em> and
extract text together with relevant metadata such as the article date.
Let’s define the URL of the article of interest and load the content
using the <code>read_html</code> function from the <code>rvest</code>
package, which provides very useful functions for web crawling and
scraping.</p>
<pre class="r"><code># define url
url &lt;- &quot;https://www.theguardian.com/world/2017/jun/26/angela-merkel-and-donald-trump-head-for-clash-at-g20-summit&quot;
# download content
webc &lt;- rvest::read_html(url)
# inspect
webc</code></pre>
<pre><code>## {html_document}
## &lt;html lang=&quot;en&quot;&gt;
## [1] &lt;head&gt;\n&lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; charset=UTF-8 ...
## [2] &lt;body&gt;\n                &lt;a href=&quot;#maincontent&quot; class=&quot;dcr-1y2qbjm&quot;&gt;Skip t ...</code></pre>
<p>We download and parse the webpage using the <code>read_html</code>
function which accepts a URL as a parameter. The function downloads the
page and interprets the html source code as an HTML / XML object.</p>
<p>However, the output contains a lot of information that we do not
really need. Thus, we process the data to extract only the text from the
webpage.</p>
<pre class="r"><code>webc %&gt;%
  # extract paragraphs
  rvest::html_nodes(&quot;p&quot;) %&gt;%
  # extract text
  rvest::html_text() -&gt; webtxt
# inspect
head(webtxt)</code></pre>
<pre><code>## [1] &quot;German chancellor plans to make climate change, free trade and mass migration key themes in Hamburg, putting her on collision course with US&quot;                                                                                                        
## [2] &quot;A clash between Angela Merkel and Donald Trump appears unavoidable after Germany signalled that it will make climate change, free trade and the management of forced mass global migration the key themes of the G20 summit in Hamburg next week.&quot;   
## [3] &quot;The G20 summit brings together the world’s biggest economies, representing 85% of global gross domestic product (GDP), and Merkel’s chosen agenda looks likely to maximise American isolation while attempting to minimise disunity amongst others. &quot;
## [4] &quot;The meeting, which is set to be the scene of large-scale street protests, will also mark the first meeting between Trump and the Russian president, Vladimir Putin, as world leaders.&quot;                                                               
## [5] &quot;Trump has already rowed with Europe once over climate change and refugees at the G7 summit in Italy, and now looks set to repeat the experience in Hamburg but on a bigger stage, as India and China join in the criticism of Washington. &quot;          
## [6] &quot;Last week, the new UN secretary-general, António Guterres, warned the Trump team if the US disengages from too many issues confronting the international community it will be replaced as world leader.&quot;</code></pre>
<p>The output shows the first 6 text elements of the website which means
that we were successful in scraping the text content of the web
page.</p>
<p>We can also extract the headline of the article by running the code
shown below.</p>
<pre class="r"><code>webc %&gt;%
  # extract paragraphs
  rvest::html_nodes(&quot;h1&quot;) %&gt;%
  # extract text
  rvest::html_text() -&gt; header
# inspect
head(header)</code></pre>
<pre><code>## [1] &quot;Angela Merkel and Donald Trump head for clash at G20 summit&quot;</code></pre>
</div>
<div id="following-links" class="section level1 unnumbered">
<h1 class="unnumbered">Following links</h1>
<p>Modern websites often do not contain the full content displayed in
the browser in their corresponding source files which are served by the
web-server. Instead, the browser loads additional content dynamically
via javascript code contained in the original source file. To be able to
scrape such content, we rely on a headless browser
<code>phantomJS</code> which renders a site for a given URL for us,
before we start the actual scraping, i.e. the extraction of certain
identifiable elements from the rendered site.</p>
<hr />
<div class="warning"
style="padding:0.1em; background-color:#f2f2f2; color:#51247a">
<span>
<p style="margin-top:1em; text-align:center">
<b>NOTE</b><br>In case the website does not fetch or alter the
to-be-scraped content dynamically, you can omit the PhantomJS webdriver
and just download the the static HTML source code to retrieve the
information from there. In this case, replace the following block of
code with a simple call of
<code>html_document &lt;- read_html(url)</code> where the
<code>read_html()</code> function downloads the unrendered page source
code directly.
</p>
<p style="margin-left:1em;">
</p>
<p></span></p>
</div>
<hr />
<p>Now we can start an instance of <code>PhantomJS</code> and create a
new browser session that awaits to load URLs to render the corresponding
websites.</p>
<pre class="r"><code>pjs_instance &lt;- run_phantomjs()
pjs_session &lt;- Session$new(port = pjs_instance$port)</code></pre>
<p>To make sure that we get the dynamically rendered HTML content of the
website, we pass the original source code downloaded from the URL to our
<code>PhantomJS</code> session first, and the use the rendered
source.</p>
<p>Usually, we do not want download a single document, but a series of
documents. In our second exercise, we want to download all Guardian
articles tagged with <em>Angela Merkel</em>. Instead of a tag page, we
could also be interested in downloading results of a site-search engine
or any other link collection. The task is always two-fold:</p>
<p>First, we download and parse the tag overview page to extract all
links to articles of interest:</p>
<pre class="r"><code>url &lt;- &quot;https://www.theguardian.com/world/angela-merkel&quot;
# go to URL
pjs_session$go(url)
# render page
rendered_source &lt;- pjs_session$getSource()
# download text and parse the source code into an XML object
html_document &lt;- read_html(rendered_source)</code></pre>
<p>Second, we download and scrape each individual article page. For
this, we extract all <code>href</code>-attributes from
<code>a</code>-elements fitting a certain CSS-class. To select the right
contents via XPATH-selectors, you need to investigate the HTML-structure
of your specific page. Modern browsers such as Firefox and Chrome
support you in that task by a function called “Inspect Element” (or
similar), available through a right-click on the page element.</p>
<pre class="r"><code>links &lt;- html_document %&gt;%
  html_nodes(xpath = &quot;//div[contains(@class, &#39;fc-item__container&#39;)]/a&quot;) %&gt;%
  html_attr(name = &quot;href&quot;)
# inspect 
links</code></pre>
<pre><code>##  [1] &quot;https://www.theguardian.com/world/2022/may/20/chat-group-leak-reveals-far-right-fantasies-of-germany-afd&quot;                  
##  [2] &quot;https://www.theguardian.com/commentisfree/2022/apr/17/observer-view-french-elections&quot;                                      
##  [3] &quot;https://www.theguardian.com/commentisfree/2022/apr/10/germany-role-against-delusional-putin&quot;                               
##  [4] &quot;https://www.theguardian.com/world/2022/mar/05/germany-angela-merkel-power-to-vladimir-putin-russia&quot;                        
##  [5] &quot;https://www.theguardian.com/commentisfree/2021/dec/26/nothing-like-a-dame-trump-tops-2021s-cast-of-clowns-and-baddies&quot;     
##  [6] &quot;https://www.theguardian.com/world/2021/dec/08/olaf-scholz-elected-succeed-angela-merkel-german-chancellor&quot;                 
##  [7] &quot;https://www.theguardian.com/world/2021/dec/08/after-16-years-at-the-top-of-german-politics-what-now-for-angela-merkel&quot;     
##  [8] &quot;https://www.theguardian.com/world/2021/dec/08/olaf-scholz-to-be-voted-in-as-german-chancellor-as-merkel-era-ends&quot;          
##  [9] &quot;https://www.theguardian.com/world/2021/dec/07/new-faces-policies-and-accents-germanys-next-coalition&quot;                      
## [10] &quot;https://www.theguardian.com/world/2021/dec/02/angela-merkel-bows-out-to-the-sound-of-beethoven-and-an-east-german-punk-hit&quot;
## [11] &quot;https://www.theguardian.com/world/2021/dec/02/germany-could-make-covid-vaccination-mandatory-says-merkel&quot;                  
## [12] &quot;https://www.theguardian.com/world/2021/dec/02/angela-merkel-to-bow-out-with-ceremony-live-on-german-tv&quot;                    
## [13] &quot;https://www.theguardian.com/world/2021/dec/01/long-reigns-often-leave-long-shadows-europeans-on-angela-merkel&quot;             
## [14] &quot;https://www.theguardian.com/world/2021/nov/30/slugs-angela-merkel-blair-barroso-prodi-on-germany-leader&quot;                   
## [15] &quot;https://www.theguardian.com/world/2021/nov/29/angela-merkel-punk-pick-for-leaving-ceremony-raises-eyebrows&quot;                
## [16] &quot;https://www.theguardian.com/world/2021/nov/28/a-new-german-era-dawns-but-collisions-lie-in-wait-for-coalition&quot;             
## [17] &quot;https://www.theguardian.com/commentisfree/2021/nov/25/the-guardian-view-on-germanys-government-green-light-for-change&quot;     
## [18] &quot;https://www.theguardian.com/world/2021/nov/22/austria-re-enters-covid-lockdown-as-europe-battles-virus-surge&quot;              
## [19] &quot;https://www.theguardian.com/world/2021/nov/19/lukashenko-says-belarusian-troops-may-have-helped-refugees-reach-europe&quot;     
## [20] &quot;https://www.theguardian.com/world/2021/nov/18/german-health-chief-urges-covid-crackdown-to-avert-very-bad-christmas&quot;</code></pre>
<p>Now, <code>links</code> contains a list of 20 hyperlinks to single
articles tagged with <em>Angela Merkel</em>.</p>
<p>But stop! There is not only one page of links to tagged articles. If
you have a look on the page in your browser, the tag overview page has
several more than 60 sub pages, accessible via a paging navigator at the
bottom. By clicking on the second page, we see a different
URL-structure, which now contains a link to a specific paging number. We
can use that format to create links to all sub pages by combining the
base URL with the page numbers.</p>
<pre class="r"><code>page_numbers &lt;- 1:3
base_url &lt;- &quot;https://www.theguardian.com/world/angela-merkel?page=&quot;
paging_urls &lt;- paste0(base_url, page_numbers)
# inspect
paging_urls</code></pre>
<pre><code>## [1] &quot;https://www.theguardian.com/world/angela-merkel?page=1&quot;
## [2] &quot;https://www.theguardian.com/world/angela-merkel?page=2&quot;
## [3] &quot;https://www.theguardian.com/world/angela-merkel?page=3&quot;</code></pre>
<p>Now we can iterate over all URLs of tag overview pages, to collect
more/all links to articles tagged with <em>Angela Merkel</em>. We
iterate with a for-loop over all URLs and append results from each
single URL to a vector of all links.</p>
<pre class="r"><code>all_links &lt;- NULL
for (url in paging_urls) {
  # download and parse single overview page
  pjs_session$go(url)
  rendered_source &lt;- pjs_session$getSource()
  html_document &lt;- read_html(rendered_source)
  # extract links to articles
  links &lt;- html_document %&gt;%
    html_nodes(xpath = &quot;//div[contains(@class, &#39;fc-item__container&#39;)]/a&quot;) %&gt;%
    html_attr(name = &quot;href&quot;)
  
  # append links to vector of all links
  all_links &lt;- c(all_links, links)
}
# inspect
head(all_links)</code></pre>
<pre><code>## [1] &quot;https://www.theguardian.com/world/2022/may/20/chat-group-leak-reveals-far-right-fantasies-of-germany-afd&quot;             
## [2] &quot;https://www.theguardian.com/commentisfree/2022/apr/17/observer-view-french-elections&quot;                                 
## [3] &quot;https://www.theguardian.com/commentisfree/2022/apr/10/germany-role-against-delusional-putin&quot;                          
## [4] &quot;https://www.theguardian.com/world/2022/mar/05/germany-angela-merkel-power-to-vladimir-putin-russia&quot;                   
## [5] &quot;https://www.theguardian.com/commentisfree/2021/dec/26/nothing-like-a-dame-trump-tops-2021s-cast-of-clowns-and-baddies&quot;
## [6] &quot;https://www.theguardian.com/world/2021/dec/08/olaf-scholz-elected-succeed-angela-merkel-german-chancellor&quot;</code></pre>
<p>An effective way of programming is to encapsulate repeatedly used
code in a specific function. This function then can be called with
specific parameters, process something and return a result. We use this
here, to encapsulate the downloading and parsing of a Guardian article
given a specific URL. The code is the same as in our exercise 1 above,
only that we combine the extracted texts and metadata in a data.frame
and wrap the entire process in a function-block.</p>
<pre class="r"><code>scrape_guardian_article &lt;- function(url) {
  # start PhantomJS
  pjs_session$go(url)
  rendered_source &lt;- pjs_session$getSource()
  # read raw html
  html_document &lt;- read_html(rendered_source)
  # extract title
  title &lt;- html_document %&gt;%
    rvest::html_node(&quot;h1&quot;) %&gt;%
    rvest::html_text(trim = T)
  # extract text
  text &lt;- html_document %&gt;%
    rvest::html_node(&quot;p&quot;) %&gt;%
    rvest::html_text(trim = T)
  # extract date
  date &lt;- url %&gt;%
    stringr::str_replace_all(&quot;.*([0-9]{4,4}/[a-z]{3,4}/[0-9]{1,2}).*&quot;, &quot;\\1&quot;)
  # generate data frame from results
  article &lt;- data.frame(
    url = url,
    date = date,
    title = title,
    body = text
  )
  
  return(article)
  
}</code></pre>
<p>Now we can use that function <code>scrape_guardian_article</code> in
any other part of our script. For instance, we can loop over each of our
collected links. We use a running variable i, taking values from 1 to
<code>length(all_links)</code> to access the single links in
<code>all_links</code> and write some progress output.</p>
<pre class="r"><code># create container for loop output
all_articles &lt;- data.frame()
# loop over links
for (i in 1:length(all_links)) {
  # print progress (optional)
  #cat(&quot;Downloading&quot;, i, &quot;of&quot;, length(all_links), &quot;URL:&quot;, all_links[i], &quot;\n&quot;)
  # scrape website
  article &lt;- scrape_guardian_article(all_links[i])
  # append current article data.frame to the data.frame of all articles
  all_articles &lt;- rbind(all_articles, article)
}
# inspect
head(all_articles)</code></pre>
<pre><code>##                                                                                                                     url
## 1              https://www.theguardian.com/world/2022/may/20/chat-group-leak-reveals-far-right-fantasies-of-germany-afd
## 2                                  https://www.theguardian.com/commentisfree/2022/apr/17/observer-view-french-elections
## 3                           https://www.theguardian.com/commentisfree/2022/apr/10/germany-role-against-delusional-putin
## 4                    https://www.theguardian.com/world/2022/mar/05/germany-angela-merkel-power-to-vladimir-putin-russia
## 5 https://www.theguardian.com/commentisfree/2021/dec/26/nothing-like-a-dame-trump-tops-2021s-cast-of-clowns-and-baddies
## 6             https://www.theguardian.com/world/2021/dec/08/olaf-scholz-elected-succeed-angela-merkel-german-chancellor
##          date
## 1 2022/may/20
## 2 2022/apr/17
## 3 2022/apr/10
## 4 2022/mar/05
## 5 2021/dec/26
## 6 2021/dec/08
##                                                                          title
## 1                 Chat group leak reveals far-right fantasies of Germany’s AfD
## 2                        The Observer view on the French presidential election
## 3       Germany must shake off its torpor and play its full role against Putin
## 4 Germany agonises over Merkel’s legacy: did she hand too much power to Putin?
## 5            Nothing like a dame: Trump tops 2021’s cast of clowns and baddies
## 6            Olaf Scholz elected to succeed Angela Merkel as German chancellor
##                                                                                                                                                                                                                                                                                                                                                                       body
## 1                                                                                                                                                                                                                                                            Among tens of thousands of posts is talk of jailing Angela Merkel, calls for armed resistance, and homophobia
## 2 For undecided French voters tempted to back the far-right populist Marine Le Pen in next Sunday’s presidential election, the situation resembles a midlife crisis. They’re fed up with the same old, same old – the boss is a pain and the bills keep on rising. How great it would be to throw it all up, escape the system, buy a smallholding somewhere and grow veg.
## 3                                                                                                                                                                                                                                 Only the boldest leadership can unite the EU against the delusional tyrant in Moscow. The German chancellor has the chance to provide it
## 4                                                                                                                                                                                                                                                            The war in Ukraine has prompted criticism of former chancellor’s decisions on Nord Stream pipeline and Russia
## 5                                                                                                                                                                                              This year’s political pantomime served up a rich cast of characters, with The Donald providing the laughs and  Xi Jinping as the baddest baddie of them all (oh yes he is!)
## 6                                                                                                                                                                                                                                                                                         Former Hamburg mayor secures 395 of 736 delegates’ ballots in parliamentary vote</code></pre>
<p>If you perform the web scraping on your own machine, you can now save
the table generated above on your machine using the code below. The code
chunk assumes that you have a folder called <code>data</code> in your
current working directory</p>
<pre class="r"><code>write.table(all_articles, here::here(&quot;data&quot;, &quot;all_articles.txt&quot;), sep = &quot;\t&quot;)</code></pre>
<p>The last command write the extracted articles to a tab-separated file
in the data directory on your machine for any later use.</p>
<hr />
<div class="warning"
style="padding:0.1em; background-color:#51247a; color:#f2f2f2">
<span>
<p style="margin-top:1em; text-align:center">
<b>EXERCISE TIME!</b>
</p>
<p style="margin-left:1em;">
</p>
<p></span></p>
</div>
<div class="question">
<p>`</p>
<ol style="list-style-type: decimal">
<li>Try to perform extraction of news articles from another web page,
e.g. <code>https://www.theaustralian.com.au</code>,
<code>https://www.nytimes.com</code>, or
<code>https://www.spiegel.de</code>. For this, investigate the URL
patterns of the page and look into the source code with the `inspect
element’ functionality of your browser to find appropriate XPATH
expressions.<br></li>
</ol>
</div>
<p>`</p>
<hr />
</div>
<div id="citation-session-info" class="section level1 unnumbered">
<h1 class="unnumbered">Citation &amp; Session Info</h1>
<p>Schweinberger, Martin. 2022. <em>Web Crawling and Scraping using
R</em>. Brisbane: The University of Queensland. url: <a
href="https://slcladal.github.io/webcrawling.html"
class="uri">https://slcladal.github.io/webcrawling.html</a> (Version
edition = {2022.05.21}).</p>
<pre><code>@manual{schweinberger2022webc,
  author = {Schweinberger, Martin},
  title = {Web Crawling and Scraping using R},
  note = {https://slcladal.github.io/webcrawling.html},
  year = {2022},
  organization = &quot;The University of Queensland, School of Languages and Cultures},
  address = {Brisbane},
  edition = {2022.05.21}
}</code></pre>
<pre class="r"><code>sessionInfo()</code></pre>
<pre><code>## R version 4.2.0 (2022-04-22 ucrt)
## Platform: x86_64-w64-mingw32/x64 (64-bit)
## Running under: Windows 10 x64 (build 19043)
## 
## Matrix products: default
## 
## locale:
## [1] LC_COLLATE=German_Germany.utf8  LC_CTYPE=German_Germany.utf8   
## [3] LC_MONETARY=German_Germany.utf8 LC_NUMERIC=C                   
## [5] LC_TIME=German_Germany.utf8    
## 
## attached base packages:
## [1] stats     graphics  grDevices datasets  utils     methods   base     
## 
## other attached packages:
##  [1] webdriver_1.0.6 readtext_0.81   rvest_1.0.2     forcats_0.5.1  
##  [5] stringr_1.4.0   dplyr_1.0.9     purrr_0.3.4     readr_2.1.2    
##  [9] tidyr_1.2.0     tibble_3.1.7    ggplot2_3.3.6   tidyverse_1.3.1
## 
## loaded via a namespace (and not attached):
##  [1] lubridate_1.8.0   png_0.1-7         ps_1.7.0          assertthat_0.2.1 
##  [5] digest_0.6.29     utf8_1.2.2        showimage_1.0.0   R6_2.5.1         
##  [9] cellranger_1.1.0  backports_1.4.1   reprex_2.0.1      evaluate_0.15    
## [13] httr_1.4.3        highr_0.9         pillar_1.7.0      rlang_1.0.2      
## [17] curl_4.3.2        readxl_1.4.0      rstudioapi_0.13   data.table_1.14.2
## [21] callr_3.7.0       jquerylib_0.1.4   klippy_0.0.0.9500 rmarkdown_2.14   
## [25] selectr_0.4-2     munsell_0.5.0     broom_0.8.0       compiler_4.2.0   
## [29] modelr_0.1.8      xfun_0.30         base64enc_0.1-3   pkgconfig_2.0.3  
## [33] htmltools_0.5.2   tidyselect_1.1.2  fansi_1.0.3       crayon_1.5.1     
## [37] tzdb_0.3.0        dbplyr_2.1.1      withr_2.5.0       grid_4.2.0       
## [41] jsonlite_1.8.0    gtable_0.3.0      lifecycle_1.0.1   DBI_1.1.2        
## [45] magrittr_2.0.3    scales_1.2.0      debugme_1.1.0     cli_3.3.0        
## [49] stringi_1.7.6     renv_0.15.4       fs_1.5.2          xml2_1.3.3       
## [53] bslib_0.3.1       ellipsis_0.3.2    generics_0.1.2    vctrs_0.4.1      
## [57] tools_4.2.0       glue_1.6.2        hms_1.1.1         processx_3.5.3   
## [61] fastmap_1.1.0     yaml_2.3.5        colorspace_2.0-3  knitr_1.39       
## [65] haven_2.5.0       sass_0.4.1</code></pre>
<hr />
<p><a href="#introduction">Back to top</a></p>
<p><a href="https://slcladal.github.io/index.html">Back to HOME</a></p>
<hr />
</div>
<div id="references" class="section level1 unnumbered">
<h1 class="unnumbered">References</h1>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-khalil2017rcrawler" class="csl-entry">
Khalil, Salim, and Mohamed Fakir. 2017. <span>“RCrawler: An r Package
for Parallel Web Crawling and Scraping.”</span> <em>SoftwareX</em> 6:
98–106.
</div>
<div id="ref-miner2012practical" class="csl-entry">
Miner, Gary, John Elder IV, Andrew Fast, Thomas Hill, Robert Nisbet, and
Dursun Delen. 2012. <em>Practical Text Mining and Statistical Analysis
for Non-Structured Text Data Applications</em>. Academic Press.
</div>
<div id="ref-olston2010web" class="csl-entry">
Olston, Christopher, and Marc Najork. 2010. <em>Web Crawling</em>. Now
Publishers Inc.
</div>
<div id="ref-WN17" class="csl-entry">
Wiedemann, Gregor, and Andreas Niekler. 2017. <span>“Hands-on:
<span>A</span> Five Day Text Mining Course for Humanists and Social
Scientists in <span>R</span>.”</span> In <em>Proceedings of the Workshop
on Teaching <span>NLP</span> for Digital Humanities
(<span>Teach4DH</span>), Berlin, Germany, September 12, 2017.</em>,
57–65. <a
href="http://ceur-ws.org/Vol-1918/wiedemann.pdf">http://ceur-ws.org/Vol-1918/wiedemann.pdf</a>.
</div>
</div>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // temporarily add toc-ignore selector to headers for the consistency with Pandoc
    $('.unlisted.unnumbered').addClass('toc-ignore')

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
