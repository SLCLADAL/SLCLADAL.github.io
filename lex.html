<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />


<meta name="author" content="Martin Schweinberger" />

<meta name="date" content="2020-09-29" />

<title>Lexicography with R</title>

<script src="site_libs/jquery-1.12.4/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cosmo.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<script src="site_libs/htmlwidgets-1.5.1/htmlwidgets.js"></script>
<link href="site_libs/datatables-css-0.0.0/datatables-crosstalk.css" rel="stylesheet" />
<script src="site_libs/datatables-binding-0.15/datatables.js"></script>
<link href="site_libs/dt-core-1.10.20/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="site_libs/dt-core-1.10.20/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="site_libs/dt-core-1.10.20/js/jquery.dataTables.min.js"></script>
<link href="site_libs/crosstalk-1.1.0.1/css/crosstalk.css" rel="stylesheet" />
<script src="site_libs/crosstalk-1.1.0.1/js/crosstalk.min.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>




<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #ffffff;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">LADAL</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="people.html">OUR PEOPLE</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    NEWS
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="news.html">News &amp; Announcements</a>
    </li>
    <li>
      <a href="conferences.html">Events &amp; Presentations</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    DATA SCIENCE BASICS
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li class="dropdown-header">Introduction to Data Science</li>
    <li>
      <a href="introcomputer.html">Working with Computers: Tips and Tricks</a>
    </li>
    <li>
      <a href="reproducibility.html">Data Management, Version Control, and Reproducibility</a>
    </li>
    <li class="divider"></li>
    <li class="dropdown-header">Quantitative Research</li>
    <li>
      <a href="introquant.html">Introduction to Quantitative Reasoning</a>
    </li>
    <li>
      <a href="basicquant.html">Basic Concepts in Quantitative Research</a>
    </li>
    <li class="divider"></li>
    <li class="dropdown-header">Introduction to R</li>
    <li>
      <a href="IntroR_workshop.html">Getting started</a>
    </li>
    <li>
      <a href="stringprocessing.html">String Processing</a>
    </li>
    <li>
      <a href="regularexpressions.html">Regular Expressions</a>
    </li>
    <li>
      <a href="introtables.html">Working with Tables</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    TUTORIALS
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li class="dropdown-header">Data Visualization</li>
    <li>
      <a href="introviz.html">Introduction to Data Viz</a>
    </li>
    <li>
      <a href="basicgraphs.html">Common Visualization Types</a>
    </li>
    <li>
      <a href="basicgraphs.html">Advanced Visualization Methods</a>
    </li>
    <li>
      <a href="maps.html">Displaying Geo-Spatial Data</a>
    </li>
    <li>
      <a href="motion.html">Interactive Charts</a>
    </li>
    <li class="divider"></li>
    <li class="dropdown-header">Statistics</li>
    <li>
      <a href="descriptivestatz.html">Descriptive Statistics</a>
    </li>
    <li>
      <a href="basicstatz.html">Basic Inferential Statistics</a>
    </li>
    <li>
      <a href="regression.html">Regression Analysis</a>
    </li>
    <li>
      <a href="advancedstatztrees.html">Tree-Based Models</a>
    </li>
    <li>
      <a href="groupingstatz.html">Cluster and Correspondence Analysis</a>
    </li>
    <li>
      <a href="svm.html">Semantic Vector Space Models</a>
    </li>
    <li class="divider"></li>
    <li class="dropdown-header">Text Analytics</li>
    <li>
      <a href="textanalysis.html">Text Analysis and Distant Reading</a>
    </li>
    <li>
      <a href="kwics.html">Concordancing (keywords-in-context)</a>
    </li>
    <li>
      <a href="basicnetwork.html">Network Analysis</a>
    </li>
    <li>
      <a href="collocations.html">Co-occurrence and Collocation Analysis</a>
    </li>
    <li>
      <a href="topicmodels.html">Topic Modeling</a>
    </li>
    <li>
      <a href="sentiment.html">Sentiment Analysis</a>
    </li>
    <li>
      <a href="tagging.html">Tagging and Parsing</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    FOCUS &amp; CASE STUDIES
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="lex.html">Lexicography with R: Generating Dictionaries</a>
    </li>
    <li>
      <a href="surveys.html">Questionnaires and Surveys: Analyses with R</a>
    </li>
    <li>
      <a href="corplingr.html">Corpus Linguistics with R: Swearing in Irish English</a>
    </li>
    <li>
      <a href="convertpdf2txt.html">Converting PDFs to txt</a>
    </li>
    <li>
      <a href="webcrawling.html">Web Crawling using R</a>
    </li>
    <li>
      <a href="vc.html">Creating Vowel Charts with Praat and R</a>
    </li>
  </ul>
</li>
<li>
  <a href="services.html">SERVICES &amp; CONTACT</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Lexicography with R</h1>
<h4 class="author">Martin Schweinberger</h4>
<h4 class="date">2020-09-29</h4>

</div>


<p><img src="images/uq1.jpg" width="100%" /></p>
<div id="introduction" class="section level1 unnumbered">
<h1>Introduction</h1>
<p>This tutorial introduces lexicography with R and shows how to use R to create dictionaries and find synonyms through determining semantic similarity in R. While the initial example focuses on English, subsequent sections show how easily this approach can be generalized to languages other than English (e.g. German, French, Spanish, Italian, or Dutch). The entire R-markdown document for the sections below can be downloaded <a href="https://slcladal.github.io/lex.Rmd">here</a>.</p>
<p>Traditionally, dictionaries are listing of words that are commonly arranged alphabetically, which may include information on definitions, usage, etymologies, pronunciations, translation, etc. <span class="citation">(see Agnes, Goldman, and Soltis <a href="#ref-agnes2002webster" role="doc-biblioref">2002</a>; Steiner <a href="#ref-steiner1985dictionaries" role="doc-biblioref">1985</a>)</span>. If such dictionaries, that are typically published as books contain translations of words in other languages, they are referred to as lexicons. Therefore, lexicographical references show the inter-relationships among lexical data, i.e. words.</p>
<p>Similarly, in computational linguistics, dictionaries represent a specific format of data where elements are linked to or paired with other elements in a systematic way. <em>Computational lexicology</em> refers to a branch of computational linguistics, which is concerned with the use of computers in the study of lexicons. Hence, computational lexicology has been defined as the use of computers in the study of machine-readable dictionaries <span class="citation">(see e.g. Amsler <a href="#ref-amsler1981structure" role="doc-biblioref">1981</a>)</span>. Computational lexicology is distinguished from <em>computational lexicography</em>, which can be defined as the use of computers in the construction of dictionaries which is the focus of this tutorial. It shoule be noted, thought, that computational lexicology and computational lexicography are often used synonymously.</p>
<div id="preparation-and-session-set-up" class="section level2 unnumbered">
<h2>Preparation and session set up</h2>
<p>This tutorial is based on R. If you have not installed R or are new to it, you will find an introduction to and more information how to use R <a href="https://slcladal.github.io/IntroR_workshop.html">here</a>. For this tutorials, we need to install certain <em>packages</em> from an R <em>library</em> so that the scripts shown below are executed without errors. Before turning to the code below, please install the packages by running the code below this paragraph. If you have already installed the packages mentioned below, then you can skip ahead ignore this section. To install the necessary packages, simply run the following code - it may take some time (between 1 and 5 minutes to install all of the libraries so you do not need to worry if it takes some time).</p>
<pre class="r"><code># clean current workspace
rm(list=ls(all=T))
# set options
options(stringsAsFactors = F)         # no automatic data transformation
options(&quot;scipen&quot; = 100, &quot;digits&quot; = 4) # suppress math annotation
# install libraries
install.packages(c(&quot;tidyr&quot;, &quot;tidytext&quot;, &quot;textdata&quot;, &quot;quanteda&quot;, &quot;koRpus&quot;, 
                   &quot;koRpus.lang.en&quot;, &quot;DT&quot;, &quot;hunspell&quot;, &quot;coop&quot;, &quot;dplyr&quot;, 
                   &quot;tm&quot;, &quot;cluster&quot;))</code></pre>
<p>Once you have installed R Studio and initiated the session by executing the code shown above, you are good to go.</p>
</div>
</div>
<div id="creating-dictionaries" class="section level1">
<h1><span class="header-section-number">1</span> Creating dictionaries</h1>
<p>In a first step, we load the necessary packages from the library and define the location of the engine which we use for the part-of-speech tagging. In this case, we will use the TreeTagger <span class="citation">(see Schmid <a href="#ref-schmid1994treetagger" role="doc-biblioref">1994</a>, <a href="#ref-schmid2013probabilistic" role="doc-biblioref">2013</a>; Schmid et al. <a href="#ref-schmid2007enriched" role="doc-biblioref">2007</a>,)</span>. How to install and then use the TreeTagger for English as well as for German, French, Spanish, Italian, and Dutch is demonstrated and explained <a href="https://slcladal.github.io/tagging.html#TreeTagger:_pos-tagging_languages_other_than_English">here</a>.</p>
<hr />
<p>NOTE</p>
<p>You will have to install TreeTagger and change the path used below (<code>"C:\\TreeTagger\\bin\\tag-english.bat"</code>) to the location where you have installed TreeTagger on your machine. If you do not know how to install TreeTagger or encounter problems, read <a href="https://slcladal.github.io/tagging.html">this tutorial</a>!</p>
<p>In addition, you can download the pos-tagged text <a href="https://slcladal.github.io/data/orwell.txt">here</a> so you can simply skip the next code chunk and load the data as shown below.</p>
<hr />
<pre class="r"><code># activate packages
library(tidyr)
library(tidytext)
library(stringr)
library(textdata)
library(quanteda)
library(koRpus)
library(koRpus.lang.en)
library(DT)
library(hunspell)
library(coop)
library(dplyr)
library(tm)
library(cluster)
# define location of pos-tagger engine
set.kRp.env(TT.cmd=&quot;C:\\TreeTagger\\bin\\tag-english.bat&quot;, lang=&quot;en&quot;) </code></pre>
<p>In a next step, we load and process the data which in this tutorial represents the text from George Orwell’s <em>Nineteen Eighty-Four</em>. We will not pre-process the data by for instance repairing broken or otherwise compromised words and continue by directly implementing the part-of-speech tagger.</p>
<pre class="r"><code># load and pos-tag data
orwell_pos &lt;- treetag(&quot;https://slcladal.github.io/data/orwell.txt&quot;)
# select data frame
orwell_pos &lt;- orwell_pos@tokens
# inspect  results
datatable(head(orwell_pos, 100), rownames = FALSE, options = list(pageLength = 10, scrollX=T), filter = &quot;none&quot;)</code></pre>
<p>If you could not pos-tag the text, you can simply execute the following code chunk which loads the pos-tagged text from the LADAL repository.</p>
<pre class="r"><code># load pos-taged data
orwell_pos &lt;- read.delim(&quot;https://slcladal.github.io/data/orwell_pos.txt&quot;, sep = &quot;\t&quot;, header = T)
# inspect  results
datatable(head(orwell_pos, 100), rownames = FALSE, options = list(pageLength = 10, scrollX=T), filter = &quot;none&quot;)</code></pre>
<div id="htmlwidget-85433af490a957f55beb" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-85433af490a957f55beb">{"x":{"filter":"none","data":[["orwell.txt","orwell.txt","orwell.txt","orwell.txt","orwell.txt","orwell.txt","orwell.txt","orwell.txt","orwell.txt","orwell.txt","orwell.txt","orwell.txt","orwell.txt","orwell.txt","orwell.txt","orwell.txt","orwell.txt","orwell.txt","orwell.txt","orwell.txt","orwell.txt","orwell.txt","orwell.txt","orwell.txt","orwell.txt","orwell.txt","orwell.txt","orwell.txt","orwell.txt","orwell.txt","orwell.txt","orwell.txt","orwell.txt","orwell.txt","orwell.txt","orwell.txt","orwell.txt","orwell.txt","orwell.txt","orwell.txt","orwell.txt","orwell.txt","orwell.txt","orwell.txt","orwell.txt","orwell.txt","orwell.txt","orwell.txt","orwell.txt","orwell.txt","orwell.txt","orwell.txt","orwell.txt","orwell.txt","orwell.txt","orwell.txt","orwell.txt","orwell.txt","orwell.txt","orwell.txt","orwell.txt","orwell.txt","orwell.txt","orwell.txt","orwell.txt","orwell.txt","orwell.txt","orwell.txt","orwell.txt","orwell.txt","orwell.txt","orwell.txt","orwell.txt","orwell.txt","orwell.txt","orwell.txt","orwell.txt","orwell.txt","orwell.txt","orwell.txt","orwell.txt","orwell.txt","orwell.txt","orwell.txt","orwell.txt","orwell.txt","orwell.txt","orwell.txt","orwell.txt","orwell.txt","orwell.txt","orwell.txt","orwell.txt","orwell.txt","orwell.txt","orwell.txt","orwell.txt","orwell.txt","orwell.txt","orwell.txt"],["1984","George","Orwell","Part","1",",","Chapter","1","It","was","a","bright","cold","day","in","April",",","and","the","clocks","were","striking","thirteen",".","Winston","Smith",",","his","chin","nuzzled","into","his","breast","in","an","effort","to","escape","the","vile","wind",",","slipped","quickly","through","the","glass","doors","of","Victory","Mansions",",","though","not","quickly","enough","to","prevent","a","swirl","of","gritty","dust","from","entering","along","with","him",".","The","hallway","smelt","of","boiled","cabbage","and","old","rag","mats",".","At","one","end","of","it","a","coloured","poster",",","too","large","for","indoor","display",",","had","been","tacked","to","the"],["CD","NP","NP","NP","CD",",","NP","CD","PP","VBD","DT","JJ","JJ","NN","IN","NP",",","CC","DT","NNS","VBD","JJ","CD","SENT","NP","NP",",","PP$","NN","VVN","IN","PP$","NN","IN","DT","NN","TO","VV","DT","JJ","NN",",","VVD","RB","IN","DT","NN","NNS","IN","NP","NNS",",","RB","RB","RB","JJ","TO","VV","DT","NN","IN","JJ","NN","IN","VVG","RP","IN","PP","SENT","DT","NN","VVN","IN","VVN","NN","CC","JJ","NN","NNS","SENT","IN","CD","NN","IN","PP","DT","JJ","NN",",","RB","JJ","IN","JJ","NN",",","VHD","VBN","VVN","TO","DT"],["@card@","George","Orwell","Part","@card@",",","Chapter","@card@","it","be","a","bright","cold","day","in","April",",","and","the","clock","be","striking","thirteen",".","Winston","Smith",",","his","chin","nuzzle","into","his","breast","in","an","effort","to","escape","the","vile","wind",",","slip","quickly","through","the","glass","door","of","Victory","mansion",",","though","not","quickly","enough","to","prevent","a","swirl","of","gritty","dust","from","enter","along","with","him",".","the","hallway","smell","of","boil","cabbage","and","old","rag","mat",".","at","one","end","of","it","a","coloured","poster",",","too","large","for","indoor","display",",","have","be","tack","to","the"],[4,6,6,4,1,1,7,1,2,3,1,6,4,3,2,5,1,3,3,6,4,8,8,1,7,5,1,3,4,7,4,3,6,2,2,6,2,6,3,4,4,1,7,7,7,3,5,5,2,7,8,1,6,3,7,6,2,7,1,5,2,6,4,4,8,5,4,3,1,3,7,5,2,6,7,3,3,3,4,1,2,3,3,2,2,1,8,6,1,3,5,3,6,7,1,3,4,6,2,3],["number","name","name","name","number","comma","name","number","pronoun","verb","determiner","adjective","adjective","noun","preposition","name","comma","conjunction","determiner","noun","verb","adjective","number","fullstop","name","name","comma","pronoun","noun","verb","preposition","pronoun","noun","preposition","determiner","noun","to","verb","determiner","adjective","noun","comma","verb","adverb","preposition","determiner","noun","noun","preposition","name","noun","comma","adverb","adverb","adverb","adjective","to","verb","determiner","noun","preposition","adjective","noun","preposition","verb","particle","preposition","pronoun","fullstop","determiner","noun","verb","preposition","verb","noun","conjunction","adjective","noun","noun","fullstop","preposition","number","noun","preposition","pronoun","determiner","adjective","noun","comma","adverb","adjective","preposition","adjective","noun","comma","verb","verb","verb","to","determiner"],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100],[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,3,3,3,3,3,3,3,3,3,3,3,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th>doc_id<\/th>\n      <th>token<\/th>\n      <th>tag<\/th>\n      <th>lemma<\/th>\n      <th>lttr<\/th>\n      <th>wclass<\/th>\n      <th>desc<\/th>\n      <th>stop<\/th>\n      <th>stem<\/th>\n      <th>idx<\/th>\n      <th>sntc<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"pageLength":10,"scrollX":true,"columnDefs":[{"className":"dt-right","targets":[4,7,8,9,10]}],"order":[],"autoWidth":false,"orderClasses":false}},"evals":[],"jsHooks":[]}</script>
<p>We can now use the resulting table to generate a first, basic dictionary that holds information about the word form (<em>token</em>), the part-of speech tag (<em>tag</em>), the lemmatized word type (<em>lemma</em>), the general word category (<em>wclass</em>), and the frequency with which the word form is used as that part-of speech.</p>
<pre class="r"><code># generate dictionary
orwell_dic_raw &lt;- orwell_pos %&gt;%
  dplyr::select(token, tag, lemma, wclass) %&gt;%
  dplyr::group_by(token, tag, lemma, wclass) %&gt;%
  dplyr::summarise(frequency = dplyr::n()) %&gt;%
  dplyr::arrange(lemma)
# inspect  results
datatable(head(orwell_dic_raw, 100), rownames = FALSE, options = list(pageLength = 10, scrollX=T), filter = &quot;none&quot;)</code></pre>
<div id="htmlwidget-95cb83c18fa38f8aa0be" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-95cb83c18fa38f8aa0be">{"x":{"filter":"none","data":[["'","'","'s","-","-'complete","--","-if","-said","-that","!","(",")",",",".","...",":",";","?","1","100","101","101","14.2.84","15","160","17.3.84","19.12.83","1900","1914","1920","1925","1930","1940","1944","1945","1960","1965","1968","1970","1973","1983","1984","2","2,000","20","200","2050","2713","3","3,000","3.12.83","300","4","40","5","6","600","6079","7","8","83","85","9","98","99","2+2=5","3rd","4th","a","A","A","Aaronson","aback","abandon","abandoned","abandoned","abashed","abbreviated","abiding","ability","abject","able","ablest","abnormality","abolish","abolished","abolition","abominably","about","about","About","About","above","above","Above","abreast","abruptly","Abruptly","absence","absent-minded"],["''","POS","POS",":","NN",":","NN","JJ","NP","SENT","(",")",",","SENT",":","SENT","SENT","SENT","CD","CD","CD","NP","CD","CD","CD","CD","CD","CD","CD","CD","CD","CD","CD","CD","CD","CD","CD","CD","CD","CD","CD","CD","CD","CD","CD","CD","CD","CD","CD","CD","CD","CD","CD","CD","CD","CD","CD","CD","CD","CD","CD","CD","CD","CD","CD","JJ","JJ","JJ","DT","DT","NP","NP","RB","VV","VVD","VVN","VVN","VVN","JJ","NN","JJ","JJ","JJS","NN","VV","VVN","NN","RB","IN","RB","IN","RB","IN","RB","IN","JJ","RB","RB","NN","JJ"],["'","'","'s","-","-'complete","--","-if","-said","-that","!","(",")",",",".","...",":",";","?","@card@","@card@","@card@","@card@","@card@","@card@","@card@","@card@","@card@","@card@","@card@","@card@","@card@","@card@","@card@","@card@","@card@","@card@","@card@","@card@","@card@","@card@","@card@","@card@","@card@","@card@","@card@","@card@","@card@","@card@","@card@","@card@","@card@","@card@","@card@","@card@","@card@","@card@","@card@","@card@","@card@","@card@","@card@","@card@","@card@","@card@","@card@","2+2=5","3rd","4th","a","a","A","Aaronson","aback","abandon","abandon","abandon","abash","abbreviate","abiding","ability","abject","able","able","abnormality","abolish","abolish","abolition","abominably","about","about","about","about","above","above","above","abreast","abruptly","abruptly","absence","absent-minded"],["punctuation","possessive","possessive","punctuation","noun","punctuation","noun","adjective","name","fullstop","punctuation","punctuation","comma","fullstop","punctuation","fullstop","fullstop","fullstop","number","number","number","name","number","number","number","number","number","number","number","number","number","number","number","number","number","number","number","number","number","number","number","number","number","number","number","number","number","number","number","number","number","number","number","number","number","number","number","number","number","number","number","number","number","number","number","adjective","adjective","adjective","determiner","determiner","name","name","adverb","verb","verb","verb","verb","verb","adjective","noun","adjective","adjective","adjective","noun","verb","verb","noun","adverb","preposition","adverb","preposition","adverb","preposition","adverb","preposition","adjective","adverb","adverb","noun","adjective"],[1826,104,272,9,1,363,1,1,1,250,35,35,5854,5607,7,219,143,376,11,3,14,1,1,1,1,1,1,1,1,1,2,2,1,1,1,2,1,1,1,2,2,7,13,1,2,1,3,1,10,1,2,2,3,1,3,3,1,3,2,2,1,2,1,1,1,1,1,3,2277,110,3,8,2,3,1,3,1,1,1,1,3,25,1,1,2,7,1,1,90,53,3,1,34,3,2,3,6,2,3,1]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th>token<\/th>\n      <th>tag<\/th>\n      <th>lemma<\/th>\n      <th>wclass<\/th>\n      <th>frequency<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"pageLength":10,"scrollX":true,"columnDefs":[{"className":"dt-right","targets":4}],"order":[],"autoWidth":false,"orderClasses":false}},"evals":[],"jsHooks":[]}</script>
<div id="cleaning-dictionary-entries" class="section level2 unnumbered">
<h2>Cleaning dictionary entries</h2>
<p>However, as the resulting table shows, the data is still very noisy, i.e. it contains a lot of <em>non-words</em>, i.e words that may be mis-spelled, broken, or otherwise compromised. In order to get rid of these, we can simply check if the word lemma exists in an existing dictionary. When you aim to identify exactly those words that are not yet part of an established dictionary, you could of course do it the other way around and remove all words that are already present in an existing dictionary.</p>
<pre class="r"><code># generate dictionary
orwell_dic_clean &lt;- orwell_dic_raw %&gt;%
  dplyr::filter(hunspell_check(lemma)) %&gt;%
  dplyr::filter(!stringr::str_detect(lemma, &quot;\\W\\w{1,}&quot;))
# inspect  results
datatable(head(orwell_dic_clean, 100), rownames = FALSE, options = list(pageLength = 10, scrollX=T), filter = &quot;none&quot;)</code></pre>
<div id="htmlwidget-256fbae95c8963962010" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-256fbae95c8963962010">{"x":{"filter":"none","data":[[".","...","3rd","4th","a","A","A","aback","abandon","abandoned","abandoned","abashed","abbreviated","abiding","ability","abject","able","ablest","abnormality","abolish","abolished","abolition","abominably","about","about","About","About","above","above","Above","abreast","abruptly","Abruptly","absence","absentmindedly","absolute","absolutely","absorption","abstainer","abstract","abstractedly","abstruse","absurd","absurdity","abundantly","abuse","abusing","abyss","accent","accents","accept","accepted","accepted","accepting","acceptable","acceptance","accident","accidentally","accompany","accomplices","accomplish","accomplished","accomplishes","accord","according","accordance","accordingly","account","account","accumulated","accumulation","accuracy","accurate","accurately","accused","accusing","ache","ached","aching","achieve","achieved","achieving","achievement","achievements","acid","acquaintance","acquire","acquired","acquiring","across","across","act","act","acted","acted","acting","acts","action","actions","active"],["SENT",":","JJ","JJ","DT","DT","NP","RB","VV","VVD","VVN","VVN","VVN","JJ","NN","JJ","JJ","JJS","NN","VV","VVN","NN","RB","IN","RB","IN","RB","IN","RB","IN","JJ","RB","RB","NN","RB","JJ","RB","NN","NN","JJ","RB","JJ","JJ","NN","RB","NN","VVG","NN","NN","NNS","VV","VVD","VVN","VVG","JJ","NN","NN","RB","VV","NNS","VV","VVN","VVZ","NN","VVG","NN","RB","NN","VV","VVN","NN","NN","JJ","RB","VVN","VVG","NN","VVD","VVG","VV","VVN","VVG","NN","NNS","NN","NN","VV","VVN","VVG","IN","RP","NN","VV","VVD","VVN","VVG","NNS","NN","NNS","JJ"],[".","...","3rd","4th","a","a","A","aback","abandon","abandon","abandon","abash","abbreviate","abiding","ability","abject","able","able","abnormality","abolish","abolish","abolition","abominably","about","about","about","about","above","above","above","abreast","abruptly","abruptly","absence","absentmindedly","absolute","absolutely","absorption","abstainer","abstract","abstractedly","abstruse","absurd","absurdity","abundantly","abuse","abuse","abyss","accent","accent","accept","accept","accept","accept","acceptable","acceptance","accident","accidentally","accompany","accomplice","accomplish","accomplish","accomplish","accord","accord","accordance","accordingly","account","account","accumulate","accumulation","accuracy","accurate","accurately","accuse","accuse","ache","ache","ache","achieve","achieve","achieve","achievement","achievement","acid","acquaintance","acquire","acquire","acquire","across","across","act","act","act","act","act","act","action","action","active"],["fullstop","punctuation","adjective","adjective","determiner","determiner","name","adverb","verb","verb","verb","verb","verb","adjective","noun","adjective","adjective","adjective","noun","verb","verb","noun","adverb","preposition","adverb","preposition","adverb","preposition","adverb","preposition","adjective","adverb","adverb","noun","adverb","adjective","adverb","noun","noun","adjective","adverb","adjective","adjective","noun","adverb","noun","verb","noun","noun","noun","verb","verb","verb","verb","adjective","noun","noun","adverb","verb","noun","verb","verb","verb","noun","verb","noun","adverb","noun","verb","verb","noun","noun","adjective","adverb","verb","verb","noun","verb","verb","verb","verb","verb","noun","noun","noun","noun","verb","verb","verb","preposition","particle","noun","verb","verb","verb","verb","noun","noun","noun","adjective"],[5607,7,1,3,2277,110,3,2,3,1,3,1,1,1,1,3,25,1,1,2,7,1,1,90,53,3,1,34,3,2,3,6,2,3,1,5,4,2,1,1,3,1,3,2,1,3,1,1,1,1,5,4,3,3,1,1,3,1,1,1,1,1,2,8,4,1,1,2,1,1,2,1,2,1,2,2,3,1,3,1,5,2,1,1,2,1,1,1,1,38,3,34,8,1,2,1,6,8,5,4]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th>token<\/th>\n      <th>tag<\/th>\n      <th>lemma<\/th>\n      <th>wclass<\/th>\n      <th>frequency<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"pageLength":10,"scrollX":true,"columnDefs":[{"className":"dt-right","targets":4}],"order":[],"autoWidth":false,"orderClasses":false}},"evals":[],"jsHooks":[]}</script>
<p>We have now checked the entries against an existing dictionary and removed non-word elements. As such, we are left with a clean dictionary based on George Orwell’s <em>Nineteen Eighty-Four</em>.</p>
</div>
<div id="extending-dictionaries" class="section level2 unnumbered">
<h2>Extending dictionaries</h2>
<p>Extending dictionaries, that is adding additional layers of information or other types of annotation, e.g. url’s to relevant references or sources, is fortunately very easy in R and can be done without much additional computing.</p>
<p>We will begin to extend our dictionary by adding an additional column (called <code>annotation</code>) in which we will add information.</p>
<pre class="r"><code># generate dictionary
orwell_dic_ext &lt;- orwell_dic_clean %&gt;%
  dplyr::mutate(annotation = NA) %&gt;%
  dplyr::mutate(annotation = ifelse(token == &quot;3rd&quot;, &quot;also 3.&quot;,
                                    ifelse(token == &quot;4th&quot;, &quot;also 4.&quot;, annotation)))
# inspect  results
datatable(head(orwell_dic_ext, 10), rownames = FALSE, options = list(pageLength = 10, scrollX=T), filter = &quot;none&quot;)</code></pre>
<div id="htmlwidget-040c277588cef5cf9256" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-040c277588cef5cf9256">{"x":{"filter":"none","data":[[".","...","3rd","4th","a","A","A","aback","abandon","abandoned"],["SENT",":","JJ","JJ","DT","DT","NP","RB","VV","VVD"],[".","...","3rd","4th","a","a","A","aback","abandon","abandon"],["fullstop","punctuation","adjective","adjective","determiner","determiner","name","adverb","verb","verb"],[5607,7,1,3,2277,110,3,2,3,1],[null,null,"also 3.","also 4.",null,null,null,null,null,null]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th>token<\/th>\n      <th>tag<\/th>\n      <th>lemma<\/th>\n      <th>wclass<\/th>\n      <th>frequency<\/th>\n      <th>annotation<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"pageLength":10,"scrollX":true,"columnDefs":[{"className":"dt-right","targets":4}],"order":[],"autoWidth":false,"orderClasses":false}},"evals":[],"jsHooks":[]}</script>
<p>To make it a bit more interesting but also keep this tutorial simple and straight-forward, we will add information about the polarity and emotionally of the words in our dictionary. We can do this by performing a sentiment analysis on the lemmas using the <code>tidytext</code> package.</p>
<p>The <code>tidytext</code> package contains three sentiment dictionaries (<code>nrc</code>, <code>bing</code>, and <code>afinn</code>). For the present purpose, we use the <code>ncr</code>dictionary which represents the Word-Emotion Association Lexicon <span class="citation">(@ Mohammad and Turney <a href="#ref-mohammad2013crowdsourcing" role="doc-biblioref">2013</a>)</span>. The Word-Emotion Association Lexicon which comprises 10,170 terms, and in which lexical elements are assigned scores based on ratings gathered through the crowd-sourced Amazon Mechanical Turk service. For the Word-Emotion Association Lexicon raters were asked whether a given word was associated with one of eight emotions. The resulting associations between terms and emotions are based on 38,726 ratings from 2,216 raters who answered a sequence of questions for each word which were then fed into the emotion association rating (see <span class="citation">Mohammad and Turney (<a href="#ref-mohammad2013crowdsourcing" role="doc-biblioref">2013</a>)</span>). Each term was rated 5 times. For 85 percent of words, at least 4 raters provided identical ratings. For instance, the word <em>cry</em> or <em>tragedy</em> are more readily associated with SADNESS while words such as <em>happy</em> or <em>beautiful</em> are indicative of JOY and words like <em>fit</em> or <em>burst</em> may indicate ANGER. This means that the sentiment analysis here allows us to investigate the expression of certain core emotions rather than merely classifying statements along the lines of a crude positive-negative distinction.</p>
<p>To be able to use the Word-Emotion Association Lexicon we need to add another column to our data frame called <code>word</code> which simply contains the lemmatized word. The reason is that the lexicon expects this column and only works if it finds a word column in the data. The code below shows how to add the emotion and polarity entries to our dictionary.</p>
<pre class="r"><code># generate dictionary
orwell_dic_ext &lt;- orwell_dic_ext %&gt;%
  dplyr::mutate(word = lemma) %&gt;%
  dplyr::left_join(get_sentiments(&quot;nrc&quot;)) %&gt;%
  tidyr::spread(sentiment, sentiment)
# inspect  results
datatable(head(orwell_dic_ext, 100), rownames = FALSE, options = list(pageLength = 10, scrollX=T), filter = &quot;none&quot;)</code></pre>
<div id="htmlwidget-245489c6bd1745b82bc7" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-245489c6bd1745b82bc7">{"x":{"filter":"none","data":[[".","...","3rd","4th","a","A","A","aback","abandon","abandoned","abandoned","abashed","abbreviated","abiding","ability","abject","able","ablest","abnormality","abolish","abolished","abolition","abominably","about","about","About","About","above","above","Above","abreast","abruptly","Abruptly","absence","absentmindedly","absolute","absolutely","absorption","abstainer","abstract","abstractedly","abstruse","absurd","absurdity","abundantly","abuse","abusing","abyss","accent","accents","accept","accepted","accepted","accepting","acceptable","acceptance","accident","accidentally","accompany","accomplices","accomplish","accomplished","accomplishes","accord","according","accordance","accordingly","account","account","accumulated","accumulation","accuracy","accurate","accurately","accused","accusing","ache","ached","aching","achieve","achieved","achieving","achievement","achievements","acid","acquaintance","acquire","acquired","acquiring","across","across","act","act","acted","acted","acting","acts","action","actions","active"],["SENT",":","JJ","JJ","DT","DT","NP","RB","VV","VVD","VVN","VVN","VVN","JJ","NN","JJ","JJ","JJS","NN","VV","VVN","NN","RB","IN","RB","IN","RB","IN","RB","IN","JJ","RB","RB","NN","RB","JJ","RB","NN","NN","JJ","RB","JJ","JJ","NN","RB","NN","VVG","NN","NN","NNS","VV","VVD","VVN","VVG","JJ","NN","NN","RB","VV","NNS","VV","VVN","VVZ","NN","VVG","NN","RB","NN","VV","VVN","NN","NN","JJ","RB","VVN","VVG","NN","VVD","VVG","VV","VVN","VVG","NN","NNS","NN","NN","VV","VVN","VVG","IN","RP","NN","VV","VVD","VVN","VVG","NNS","NN","NNS","JJ"],[".","...","3rd","4th","a","a","A","aback","abandon","abandon","abandon","abash","abbreviate","abiding","ability","abject","able","able","abnormality","abolish","abolish","abolition","abominably","about","about","about","about","above","above","above","abreast","abruptly","abruptly","absence","absentmindedly","absolute","absolutely","absorption","abstainer","abstract","abstractedly","abstruse","absurd","absurdity","abundantly","abuse","abuse","abyss","accent","accent","accept","accept","accept","accept","acceptable","acceptance","accident","accidentally","accompany","accomplice","accomplish","accomplish","accomplish","accord","accord","accordance","accordingly","account","account","accumulate","accumulation","accuracy","accurate","accurately","accuse","accuse","ache","ache","ache","achieve","achieve","achieve","achievement","achievement","acid","acquaintance","acquire","acquire","acquire","across","across","act","act","act","act","act","act","action","action","active"],["fullstop","punctuation","adjective","adjective","determiner","determiner","name","adverb","verb","verb","verb","verb","verb","adjective","noun","adjective","adjective","adjective","noun","verb","verb","noun","adverb","preposition","adverb","preposition","adverb","preposition","adverb","preposition","adjective","adverb","adverb","noun","adverb","adjective","adverb","noun","noun","adjective","adverb","adjective","adjective","noun","adverb","noun","verb","noun","noun","noun","verb","verb","verb","verb","adjective","noun","noun","adverb","verb","noun","verb","verb","verb","noun","verb","noun","adverb","noun","verb","verb","noun","noun","adjective","adverb","verb","verb","noun","verb","verb","verb","verb","verb","noun","noun","noun","noun","verb","verb","verb","preposition","particle","noun","verb","verb","verb","verb","noun","noun","noun","adjective"],[5607,7,1,3,2277,110,3,2,3,1,3,1,1,1,1,3,25,1,1,2,7,1,1,90,53,3,1,34,3,2,3,6,2,3,1,5,4,2,1,1,3,1,3,2,1,3,1,1,1,1,5,4,3,3,1,1,3,1,1,1,1,1,2,8,4,1,1,2,1,1,2,1,2,1,2,2,3,1,3,1,5,2,1,1,2,1,1,1,1,38,3,34,8,1,2,1,6,8,5,4],[null,null,"also 3.","also 4.",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[".","...","3rd","4th","a","a","A","aback","abandon","abandon","abandon","abash","abbreviate","abiding","ability","abject","able","able","abnormality","abolish","abolish","abolition","abominably","about","about","about","about","above","above","above","abreast","abruptly","abruptly","absence","absentmindedly","absolute","absolutely","absorption","abstainer","abstract","abstractedly","abstruse","absurd","absurdity","abundantly","abuse","abuse","abyss","accent","accent","accept","accept","accept","accept","acceptable","acceptance","accident","accidentally","accompany","accomplice","accomplish","accomplish","accomplish","accord","accord","accordance","accordingly","account","account","accumulate","accumulation","accuracy","accurate","accurately","accuse","accuse","ache","ache","ache","achieve","achieve","achieve","achievement","achievement","acid","acquaintance","acquire","acquire","acquire","across","across","act","act","act","act","act","act","action","action","active"],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"anger","anger",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"anger","anger",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"anticipation","anticipation",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"disgust",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"disgust","disgust",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,"fear","fear","fear",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"fear",null,null,null,null,null,null,null,null,null,null,null,"fear","fear","fear",null,null,null,null,null,null,null,null,"fear",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"joy","joy","joy",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"joy","joy","joy","joy","joy",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,"negative","negative","negative",null,null,null,null,"negative",null,null,null,"negative","negative","negative",null,null,null,null,null,null,null,null,null,null,null,"negative",null,null,null,null,null,null,null,null,"negative","negative",null,"negative","negative","negative",null,null,null,null,null,null,null,null,"negative",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"negative","negative","negative",null,null,null,null,null,"negative",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,"positive",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"positive",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"positive","positive",null,null,null,null,"positive","positive","positive","positive","positive",null,null,null,null,null,null,null,"positive",null,null,null,null,null,null,"positive","positive","positive","positive","positive",null,null,"positive","positive","positive",null,null,null,null,null,null,null,null,"positive","positive",null],[null,null,null,null,null,null,null,null,"sadness","sadness","sadness",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"sadness",null,null,null,null,null,null,null,null,null,null,null,"sadness","sadness","sadness",null,null,null,null,null,null,null,null,"sadness",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"sadness","sadness","sadness",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"surprise","surprise",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"trust","trust",null,null,"trust","trust",null,null,null,"trust",null,null,null,null,null,null,"trust","trust","trust","trust","trust",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th>token<\/th>\n      <th>tag<\/th>\n      <th>lemma<\/th>\n      <th>wclass<\/th>\n      <th>frequency<\/th>\n      <th>annotation<\/th>\n      <th>word<\/th>\n      <th>anger<\/th>\n      <th>anticipation<\/th>\n      <th>disgust<\/th>\n      <th>fear<\/th>\n      <th>joy<\/th>\n      <th>negative<\/th>\n      <th>positive<\/th>\n      <th>sadness<\/th>\n      <th>surprise<\/th>\n      <th>trust<\/th>\n      <th>&lt;NA&gt;<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"pageLength":10,"scrollX":true,"columnDefs":[{"className":"dt-right","targets":4}],"order":[],"autoWidth":false,"orderClasses":false}},"evals":[],"jsHooks":[]}</script>
<p>The resulting extended dictionary now contains not only the token, the pos-tag, the lemma, and the generalized word class, but also the emotional and polarity scores from the Word-Emotion Association Lexicon.</p>
</div>
<div id="generating-dictionaries-for-other-languages" class="section level2 unnumbered">
<h2>Generating dictionaries for other languages</h2>
<p>As mentioned above, the procedure for generating dictionaries can easily be applied to languages other than English. If you want to follow exactly the procedure described above, then the language set of the TreeTagger is the limiting factors as its R implementation only supports English, German, French, Italian, Spanish, and Dutch. fa part-of-speech tagged text in another language is already available to you, and you do not require the TreeTagger for the part-of-speech tagging, then you can skip the code chunk that is related to the tagging and you can modify the procedure described above to virtually any language.</p>
<p>We will now briefly create a German dictionary based on a subsection of the fairy tales collected by the brothers Grimm to show how the above procedure can be applied to a language other than English. In a first step, we load a German text into R.</p>
<pre class="r"><code># install german language package
#install.koRpus.lang(&quot;de&quot;)
# activate german language package
library(koRpus.lang.de)
# define location of pos-tagger engine
set.kRp.env(TT.cmd=&quot;C:\\TreeTagger\\bin\\tag-german.bat&quot;, lang=&quot;de&quot;) </code></pre>
<hr />
<p>NOTE</p>
<p>You would need to adapt the path to the TreeTagger engine (see previous code chunk) as well as the path to the data (see follwoing code chunk)! The paths below are specified to my computer.</p>
<hr />
<pre class="r"><code># pos-tag data
grimm_pos &lt;- treetag(file = &quot;data/grimm.txt&quot;)</code></pre>
<p>For other languages, you would need to adapt the path to the pos-tagger engine to match the language of your text. The code chunk below shows how you would do that for French, Spanish, Italian, and Dutch. Again note that you would need to adapt the path to the TreeTagger engine as well as the path to the data! The paths below are specified to my computer.</p>
<pre class="r"><code># French
# install french language package
install.koRpus.lang(&quot;fr&quot;)
# activate french language package
library(koRpus.lang.fr)
# define location of pos-tagger engine
set.kRp.env(TT.cmd=&quot;C:\\TreeTagger\\bin\\tag-french.bat&quot;, lang=&quot;fr&quot;) 
# pos-tag data
text_pos &lt;- treetag(&quot;PathToFrenchText.txt&quot;)

# Spanish
# install spanish language package
install.koRpus.lang(&quot;es&quot;)
# activate spanish language package
library(koRpus.lang.es)
# define location of pos-tagger engine
set.kRp.env(TT.cmd=&quot;C:\\TreeTagger\\bin\\tag-spanish.bat&quot;, lang=&quot;es&quot;) 
# pos-tag data
text_pos &lt;- treetag(&quot;PathToSpanishText.txt&quot;)

# Italian
# install italian language package
install.koRpus.lang(&quot;it&quot;)
# activate italian language package
library(koRpus.lang.it)
# define location of pos-tagger engine
set.kRp.env(TT.cmd=&quot;C:\\TreeTagger\\bin\\tag-italian.bat&quot;, lang=&quot;it&quot;) 
# pos-tag data
text_pos &lt;- treetag(&quot;PathToItalianText.txt&quot;)

# Dutch
# install dutch language package
install.koRpus.lang(&quot;nl&quot;)
# activate dutch language package
library(koRpus.lang.nl)
# define location of pos-tagger engine
set.kRp.env(TT.cmd=&quot;C:\\TreeTagger\\bin\\tag-dutch.bat&quot;, lang=&quot;nl&quot;) 
# pos-tag data
text_pos &lt;- treetag(&quot;PathToDutchText.txt&quot;)</code></pre>
<p>We will now continue with generating the dictionary based on teh brothers’ Grimm fairy tales. We go through the same steps as for the English dictionary and collapse all the steps into a single code block.</p>
<pre class="r"><code># select data frame
grimm_pos &lt;- grimm_pos@tokens
# generate dictionary
grimm_dic_raw &lt;- grimm_pos %&gt;%
  dplyr::select(token, tag, lemma, wclass) %&gt;%
  dplyr::group_by(token, tag, lemma, wclass) %&gt;%
  dplyr::summarise(frequency = dplyr::n()) %&gt;%
  dplyr::arrange(lemma)
# clean dictionary
grimm_dic_clean &lt;- grimm_dic_raw %&gt;%
  dplyr::filter(!stringr::str_detect(lemma, &quot;\\W\\w{1,}&quot;)) %&gt;%
  dplyr::filter(!stringr::str_detect(lemma, &quot;\\W{1,}&quot;)) %&gt;%
  dplyr::filter(!stringr::str_detect(lemma, &quot;[:digit:]{1,}&quot;)) %&gt;%
  dplyr::filter(nchar(token) &gt; 1)
# inspect  results
datatable(head(grimm_dic_clean, 100), rownames = FALSE, options = list(pageLength = 10, scrollX=T), filter = &quot;none&quot;)</code></pre>
<div id="htmlwidget-966b121a0ec771b0005a" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-966b121a0ec771b0005a">{"x":{"filter":"none","data":[["Aanten","Aar","ab","ab","abgebissen","abblasen","abbrechen","abbrechen","abgebrochen","Abend","Abend","Abends","Abendessen","Abends","Abentheuer","aber","aber","Aber","Aber","abermals","abgegessen","abfiel","abgehauenen","abgehen","abging","abgelo","abgestumpften","abgetragen","abhalten","abgehauen","abhaue","abhauen","abhauen","abgehoben","abhelfen","abholen","abholen","abgejagt","abjagen","abkommen","abzukommen","ableiten","ablo","abnahm","abnehmen","abpickte","Abraham","abgerechnet","Abreise","abgerissen","abroad","abgeschafft","abscheulich","abgeschickt","Abschied","abschlagen","abschlagen","abzuschlagen","abgeschlossen","abgeschnitten","abschneiden","absehen","absichtliche","absprangen","Abstammung","absterben","abstreift","abtragen","abtrocknen","abzuwaschen","abweichende","Abweichungen","abgewiesen","abwies","abwenden","abzuwenden","Abwesenheit","abgewickelt","abwickelst","abgewischt","abwischen","abziehen","abzulo","ach","Ach","acht","Acht","achte","achten","achter","Acht","achtet","achtete","geachtet","acker","ad","Ade","Ade","adelichen","Adern"],["NN","NN","APPR","PTKVZ","VVPP","VVINF","VVFIN","VVINF","VVPP","ADV","NN","NN","NN","ADV","NN","ADV","KON","ADV","KON","ADV","VVPP","VVFIN","ADJA","VVFIN","VVFIN","ADJD","ADJA","ADJD","VVINF","VVPP","VVFIN","VVFIN","VVINF","VVPP","VVINF","VVFIN","VVINF","VVPP","VVINF","VVINF","VVIZU","VVINF","NE","VVFIN","VVINF","ADJA","NE","VVPP","NN","VVPP","NN","VVPP","ADJD","VVPP","NN","VVFIN","VVINF","VVIZU","VVPP","VVPP","VVINF","VVINF","ADJA","VVFIN","NN","VVINF","VVFIN","VVFIN","VVINF","VVIZU","ADJA","NN","VVPP","VVFIN","VVINF","VVIZU","NN","VVPP","VVFIN","VVPP","VVINF","VVINF","VVIZU","ITJ","ITJ","CARD","ADJA","ADJA","ADJA","ADJA","NN","VVFIN","VVFIN","VVPP","VVIMP","FM","ITJ","NN","ADJA","NN"],["Aanten","Aar","ab","ab","abbeißen","abblasen","abbrechen","abbrechen","abbrechen","abend","Abend","Abend","Abendessen","abends","Abentheuer","aber","aber","aber","aber","abermals","abessen","abfallen","abgehauen","abgehen","abgehen","abgelo","abgestumpft","abgetragen","abhalten","abhauen","abhauen","abhauen","abhauen","abheben","abhelfen","abholen","abholen","abjagen","abjagen","abkommen","abkommen","ableiten","ablo","abnehmen","abnehmen","abpickte","Abraham","abrechnen","Abreise","abreißen","abroad","abschaffen","abscheulich","abschicken","Abschied","abschlagen","abschlagen","abschlagen","abschließen","abschneiden","abschneiden","absehen","absichtlich","abspringen","Abstammung","absterben","abstreifen","abtragen","abtrocknen","abwaschen","abweichend","Abweichung","abweisen","abweisen","abwenden","abwenden","Abwesenheit","abwickeln","abwickelst","abwischen","abwischen","abziehen","abzulo","ach","ach","acht","acht","acht","acht","acht","Acht","achten","achten","achten","ackern","ad","ade","Ade","adelichen","Ader"],["noun","noun","preposition","particle","verb","verb","verb","verb","verb","adverb","noun","noun","noun","adverb","noun","adverb","conjunction","adverb","conjunction","adverb","verb","verb","adjective","verb","verb","adjective","adjective","adjective","verb","verb","verb","verb","verb","verb","verb","verb","verb","verb","verb","verb","verb","verb","name","verb","verb","adjective","name","verb","noun","verb","noun","verb","adjective","verb","noun","verb","verb","verb","verb","verb","verb","verb","adjective","verb","noun","verb","verb","verb","verb","verb","adjective","noun","verb","verb","verb","verb","noun","verb","verb","verb","verb","verb","verb","interjection","interjection","number","adjective","adjective","adjective","adjective","noun","verb","verb","verb","verb","foreign","interjection","noun","adjective","noun"],[1,1,1,85,1,1,1,4,2,2,49,28,1,6,3,517,300,1,21,1,1,1,1,1,1,2,1,1,1,1,1,1,4,1,1,3,1,1,1,1,1,1,2,1,1,1,1,1,1,1,1,1,1,1,9,1,1,1,1,1,2,1,1,1,1,1,1,1,2,1,1,1,2,1,2,1,2,1,1,1,1,1,1,21,47,12,1,1,1,2,16,1,1,1,1,1,1,5,1,1]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th>token<\/th>\n      <th>tag<\/th>\n      <th>lemma<\/th>\n      <th>wclass<\/th>\n      <th>frequency<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"pageLength":10,"scrollX":true,"columnDefs":[{"className":"dt-right","targets":4}],"order":[],"autoWidth":false,"orderClasses":false}},"evals":[],"jsHooks":[]}</script>
<p>As with the English dictionary, we have created a customized German dictionary based of a subsample of the brothers’ Grimm fairy tales holding the word form(<em>token</em>), the part-of-speech tag (<em>tag</em>), the lemmatized word type (<em>lemma</em>), the general word class (<em>wclass</em>), ad the frequency with which a word form occurs as a part-of-speech in the data (<em>frequency</em>).</p>
</div>
</div>
<div id="finding-synonyms" class="section level1">
<h1><span class="header-section-number">2</span> Finding synonyms</h1>
<p>Another task that is quite common in lexicography is to determine if words share some form of relationship such as whether they are synonyms or antonyms. In computational linguistics, this is commonly determined based on the collocational profiles of words. These collocational profiles are also called <em>word vectors</em> or <em>word embeddings</em> and approaches which determine semantic similarity based on collocational profiles or word embeddings are called distributional approaches (or distributional semantics). The basic assumption of distributional approaches is that words that occur in the same context and therefore have similar collocational profiles are also semantically similar. In fact, various packages, such as <code>qdap</code> or , <code>wordnet</code> already provide synonyms for terms (all of which are based on similar collocational profiles) but we would like to determine if words are similar without knowing it in advance.</p>
<p>In this example, we want to determine if two degree adverbs (such as <em>very</em>, <em>really</em>, <em>so</em>, <em>completely</em>, <em>totally</em>, <em>amazingly</em>, etc.) are synonymous and can therefore be exchanged without changing the meaning of the sentence (or, at least, not changing it dramatically). This is relevant in lexicography as such terms can then be linked to each other and inform readers that these words are interchangeable.</p>
<p>As a first step, we load the data which contains three columns:</p>
<ul>
<li><p>one column holding the degree adverbs which is called <em>pint</em></p></li>
<li><p>one column called <em>adjs</em> holding the adjectives that the degree adverbs have modified</p></li>
<li><p>one column called <em>remove</em> which contains the word <em>keep</em> and which we will remove as it is not relevant for this tutorial</p></li>
</ul>
<p>When loading the data, we</p>
<ul>
<li><p>remove the <em>remove</em> column</p></li>
<li><p>rename the <em>pint</em> column as <em>degree_adverb</em></p></li>
<li><p>rename the <em>adjs</em> column as <em>adjectives</em></p></li>
<li><p>filter out all instances where the degree adverb column has the value <code>0</code> (which means that the adjective was not modified)</p></li>
<li><p>remove instances where <em>well</em> functions as a degree adverb (because it behaves rather differently from other degree adverbs)</p></li>
</ul>
<pre class="r"><code># load data
degree_adverbs &lt;- read.delim(&quot;https://slcladal.github.io/data/data04.txt&quot;, sep = &quot;\t&quot;, header = T) %&gt;%
  dplyr::select(-remove) %&gt;%
  dplyr::rename(degree_adverb = pint,
                adjective = adjs) %&gt;%
  dplyr::filter(degree_adverb != &quot;0&quot;,
                degree_adverb != &quot;well&quot;)
# inspect  results
datatable(head(degree_adverbs, 100), rownames = FALSE, options = list(pageLength = 10, scrollX=T), filter = &quot;none&quot;)</code></pre>
<div id="htmlwidget-7dafc71678bda6997b9c" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-7dafc71678bda6997b9c">{"x":{"filter":"none","data":[["real","really","very","really","really","really","so","really","pretty","really","pretty","really","really","really","really","really","really","pretty","really","pretty","really","really","real","very","very","pretty","really","really","very","very","really","really","really","really","so","very","very","really","really","so","very","very","pretty","really","really","very","really","pretty","really","really","really","real","pretty","really","really","very","really","very","really","really","so","so","real","very","really","pretty","really","very","pretty","really","real","real","really","really","really","really","really","really","really","really","really","really","so","really","really","very","really","pretty","very","totally","totally","really","really","really","really","really","so","so","really","so"],["bad","nice","good","early","bad","bad","long","wonderful","good","easy","strong","long","funny","good","nice","amazing","good","good","good","cheap","poor","good","high","sorry","good","good","hard","high","difficult","nice","good","old","hard","good","difficult","little","quick","easy","hard","good","short","difficult","easy","good","nice","good","nice","good","amazing","good","good","bad","good","nice","good","good","good","nice","good","good","poor","quick","good","short","nice","good","late","hard","full","open","late","real","good","good","good","good","good","good","good","nice","good","happy","next","good","good","nice","great","poor","good","white","white","funny","interested","interested","good","funny","good","funny","bad","funny"]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th>degree_adverb<\/th>\n      <th>adjective<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"pageLength":10,"scrollX":true,"order":[],"autoWidth":false,"orderClasses":false}},"evals":[],"jsHooks":[]}</script>
<p>In a next step, we create a matrix from this data frame which maps how often a given amplifier co-occurred with a given adjective. In text mining, this format is called a text-document matrix or tdm (which is a transposed <a href="https://en.wikipedia.org/wiki/Document-term_matrix">document-term matrix</a> of dtm).</p>
<pre class="r"><code># tabulate data (create term-document matrix)
tdm &lt;- ftable(degree_adverbs$adjective, degree_adverbs$degree_adverb)
# extract amplifiers and adjectives 
amplifiers &lt;- as.vector(unlist(attr(tdm, &quot;col.vars&quot;)[1]))
adjectives &lt;- as.vector(unlist(attr(tdm, &quot;row.vars&quot;)[1]))
# attach row and column names to tdm
rownames(tdm) &lt;- adjectives
colnames(tdm) &lt;- amplifiers
# inspect data
tdm[1:5, 1:5]</code></pre>
<pre><code>##           completely extremely pretty real really
## able               0         1      0    0      0
## actual             0         0      0    1      0
## amazing            0         0      0    0      4
## available          0         0      0    0      1
## bad                0         0      1    2      3</code></pre>
<p>In a next step, we extract the expected values of the co-occurrences if the amplifiers were distributed homogeneously and calculate the <em>Pointwise Mutual Information</em> (PMI) score and use that to then calculate the <em>Positive Pointwise Mutual Information</em> (PPMI) scores. According to <span class="citation">Levshina (<a href="#ref-levshina2015linguistics" role="doc-biblioref">2015</a>)</span> 327 - referring to <span class="citation">Bullinaria and Levy (<a href="#ref-bullinaria2007extracting" role="doc-biblioref">2007</a>)</span> - PPMI perform better than PMI as negative values are replaced with zeros. In a next step, we calculate the cosine similarity which will for the bases for the subsequent clustering.</p>
<pre class="r"><code># compute expected values
tdm.exp &lt;- chisq.test(tdm)$expected</code></pre>
<pre><code>## Warning in chisq.test(tdm): Chi-squared approximation may be incorrect</code></pre>
<pre class="r"><code># calculate PMI and PPMI
PMI &lt;- log2(tdm/tdm.exp)
PPMI &lt;- ifelse(PMI &lt; 0, 0, PMI)
# calculate cosine similarity
cosinesimilarity &lt;- cosine(PPMI)
# inspect cosine values
cosinesimilarity[1:5, 1:5]</code></pre>
<pre><code>##            completely   extremely      pretty       real      really
## completely 1.00000000 0.204188725 0.000000000 0.05304354 0.126668434
## extremely  0.20418873 1.000000000 0.007319316 0.00000000 0.004235346
## pretty     0.00000000 0.007319316 1.000000000 0.09441299 0.062323271
## real       0.05304354 0.000000000 0.094412995 1.00000000 0.131957473
## really     0.12666843 0.004235346 0.062323271 0.13195747 1.000000000</code></pre>
<p>As we have now obtained a similarity measure, we can go ahead and perform a cluster analysis on these similarity values. However, as we have to extract the maximum values in the similarity matrix that is not 1 as we will use this to create a distance matrix. While we could also have simply subtracted the cosine similarity values from 1 to convert the similarity matrix into a distance matrix, we follow the procedure proposed by <span class="citation">Levshina (<a href="#ref-levshina2015linguistics" role="doc-biblioref">2015</a>)</span>.</p>
<pre class="r"><code># find max value that is not 1
cosinesimilarity.test &lt;- apply(cosinesimilarity, 1, function(x){
  x &lt;- ifelse(x == 1, 0, x) } )
maxval &lt;- max(cosinesimilarity.test)
# create distance matrix
amplifier.dist &lt;- 1 - (cosinesimilarity/maxval)
clustd &lt;- as.dist(amplifier.dist)</code></pre>
<p>In a next step, we visualize the results of the semantic vector space model as a dendrogram.</p>
<pre class="r"><code># create cluster object
cd &lt;- hclust(clustd, method=&quot;ward.D&quot;)    
# plot cluster object
plot(cd, main = &quot;&quot;, sub = &quot;&quot;, yaxt = &quot;n&quot;, ylab = &quot;&quot;, xlab = &quot;&quot;, cex = .8)</code></pre>
<p><img src="lex_files/figure-html/vsm8-1.png" width="672" /></p>
<p>The clustering solution shows that, as expected, <em>completely</em>, <em>extremely</em>, and <em>totally</em> - while similar to each other and thus interchangeable with each other - form a separate cluster from all other amplifiers. In addition, <em>real</em> and <em>really</em> form a cluster together. The clustering of <em>very</em>, <em>pretty</em>, <em>so</em>, <em>really</em>, and <em>real</em> suggest that these amplifiers are more or less interchangeable with each other but not with <em>totally</em>, <em>completely</em>, and <em>extremely</em>.</p>
<hr />
<p>NOTE</p>
<p>Remember that this is only a tutorial! A proper study would have to take the syntactic context into account because, while we can say <em>This really great tutorial helped me a lot</em>. we probably would not say <em>This so great tutorial helped me a lot</em>. This is because so syntactically more restricted and is strongly disfavored in attributive contexts. Therefore, the syntactic context would have to be considered in a more thorough study.</p>
<hr />
<p>There are many more useful methods for identifying semantic similarity. A very useful method (which we have implemented here but only superficially is Semantci Vector Space Modelling. If you want to know more about this, this <a href="https://gederajeg.github.io/vector_space_model_indonesian/">tutorial by Gede Primahadi Wijaya Rajeg, Karlina Denistia, and Simon Musgrave</a> <span class="citation">(Rajeg, Denistia, and Musgrave <a href="#ref-rajeg2020semvec" role="doc-biblioref">2019</a>)</span> is highly recommended and will give a better understanding of SVM but this should suffice to get you started.</p>
</div>
<div id="going-further-crowd-sourced-dictionaries-with-r-and-git" class="section level1">
<h1><span class="header-section-number">3</span> Going further: crowd-sourced dictionaries with R and Git</h1>
<p>While it would go beyond the scope of this tutorial, it should be noted that the approach for creating dictionaries can be applied to crowed-sourced dictionaries. To do this, you could, e.g. upload your dictionary to a Git repository such as <a href="https://github.com/">GitHub</a> or <a href="https://about.gitlab.com/">GitLab</a> which would then allow everybody with an account on either of these platforms to add content to the dictionary. To add to the dictionary, contributors would simply have to fork the repository of the dictionary and then merge with the existing, original dictionary repository. The quality of the data would meanwhile remain under control of the owner of the original repository he they can decide on a case-by-case basis which change they would like to accept. In addition, and because Git is a version control environment, the owner could also go back to previous versions, if they think they erroneously accepted a change (merge).</p>
<p><img src="images/git.png" width="30%" style="float:right; padding:15px" /></p>
<p>This option is particularly interesting for the approach to creating dictionaries presented here because R Studio has an integrated and very easy to use pipeline to Git (see, e.g., <a href="https://support.rstudio.com/hc/en-us/articles/200532077-Version-Control-with-Git-and-SVN">here</a> and <a href="https://happygitwithr.com/rstudio-git-github.html">here</a>)</p>
<p>We have reached the end of this tutorial and you now know how to create and modify networks in R and how you can highlight aspects of your data.</p>
<p><br><br></p>
</div>
<div id="citation-session-info" class="section level1 unnumbered">
<h1>Citation &amp; Session Info</h1>
<p>Schweinberger, Martin. 2020. <em>Lexicography with R</em>. Brisbane: The University of Queensland. url: <a href="https://slcladal.github.io/lex.html" class="uri">https://slcladal.github.io/lex.html</a> (Version 2020.09.28).</p>
<pre><code>@manual{schweinberger2020lex,
  author = {Schweinberger, Martin},
  title = {Lexicography with R},
  note = {https://slcladal.github.io/lex.html},
  year = {2020},
  organization = &quot;The University of Queensland, Australia. School of Languages and Cultures},
  address = {Brisbane},
  edition = {2020/09/28}
}</code></pre>
<pre class="r"><code>sessionInfo()</code></pre>
<pre><code>## R version 4.0.2 (2020-06-22)
## Platform: x86_64-w64-mingw32/x64 (64-bit)
## Running under: Windows 10 x64 (build 18362)
## 
## Matrix products: default
## 
## locale:
## [1] LC_COLLATE=German_Germany.1252  LC_CTYPE=German_Germany.1252   
## [3] LC_MONETARY=German_Germany.1252 LC_NUMERIC=C                   
## [5] LC_TIME=German_Germany.1252    
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] koRpus.lang.de_0.1-1 cluster_2.1.0        tm_0.7-7            
##  [4] NLP_0.2-0            dplyr_1.0.2          coop_0.6-2          
##  [7] hunspell_3.0         DT_0.15              koRpus.lang.en_0.1-3
## [10] koRpus_0.13-2        sylly_0.1-6          quanteda_2.1.1      
## [13] textdata_0.4.1       stringr_1.4.0        tidytext_0.2.6      
## [16] tidyr_1.1.2         
## 
## loaded via a namespace (and not attached):
##  [1] tidyselect_1.1.0   xfun_0.16          slam_0.1-47        purrr_0.3.4       
##  [5] lattice_0.20-41    colorspace_1.4-1   vctrs_0.3.4        generics_0.0.2    
##  [9] htmltools_0.5.0    SnowballC_0.7.0    usethis_1.6.3      yaml_2.2.1        
## [13] rlang_0.4.7        pillar_1.4.6       glue_1.4.2         rappdirs_0.3.1    
## [17] lifecycle_0.2.0    munsell_0.5.0      gtable_0.3.0       htmlwidgets_1.5.1 
## [21] evaluate_0.14      knitr_1.30         crosstalk_1.1.0.1  parallel_4.0.2    
## [25] sylly.en_0.1-3     tokenizers_0.2.1   Rcpp_1.0.5         readr_1.3.1       
## [29] scales_1.1.1       jsonlite_1.7.1     RcppParallel_5.0.2 sylly.de_0.1-2    
## [33] fs_1.5.0           fastmatch_1.1-0    stopwords_2.0      ggplot2_3.3.2     
## [37] hms_0.5.3          digest_0.6.25      stringi_1.5.3      grid_4.0.2        
## [41] tools_4.0.2        magrittr_1.5       tibble_3.0.3       janeaustenr_0.1.5 
## [45] crayon_1.3.4       pkgconfig_2.0.3    ellipsis_0.3.1     Matrix_1.2-18     
## [49] xml2_1.3.2         data.table_1.13.0  rmarkdown_2.3      R6_2.4.1          
## [53] compiler_4.0.2</code></pre>
</div>
<div id="references" class="section level1 unnumbered">
<h1>References</h1>
<hr />
<p><a href="https://slcladal.github.io/index.html">Main page</a></p>
<hr />
<div id="refs" class="references">
<div id="ref-agnes2002webster">
<p>Agnes, Michael, Jonathan L Goldman, and Katherine Soltis. 2002. <em>Webster’s New World Compact Desk Dictionary and Style Guide</em>. Hungry Minds.</p>
</div>
<div id="ref-amsler1981structure">
<p>Amsler, Robert Alfred. 1981. <em>The Structure of the Merriam-Webster Pocket Dictionary</em>. Austin, TX: he University of Texas at Austin.</p>
</div>
<div id="ref-bullinaria2007extracting">
<p>Bullinaria, J. A., and J. P. Levy. 2007. “Extracting Semantic Representations from Word Co-Occurrence Statistics: A Computational Study.” <em>Behavior Research Methods</em> 39: 510–26.</p>
</div>
<div id="ref-levshina2015linguistics">
<p>Levshina, Natalia. 2015. <em>How to Do Linguistics with R: Data Exploration and Statistical Analysis</em>. Amsterdam: John Benjamins.</p>
</div>
<div id="ref-mohammad2013crowdsourcing">
<p>Mohammad, Saif M, and Peter D Turney. 2013. “Crowdsourcing a Word-Emotion Association Lexicon.” <em>Computational Intelligence</em> 29 (3): 436–65.</p>
</div>
<div id="ref-rajeg2020semvec">
<p>Rajeg, Gede Primahadi Wijaya, Karlina Denistia, and Simon Musgrave. 2019. “R Markdown Notebook for Vector Space Model and the Usage Patterns of Indonesian Denominal Verbs.” <a href="https://doi.org10.6084/m9.figshare.9970205.%20https://figshare.com/articles/R\%5FMarkdown\%5FNotebook\%5Ffor\%5Fi\%5FVector\%5Fspace\%5Fmodel\%5Fand\%5Fthe\%5Fusage\%5Fpatterns\%5Fof\%5FIndonesian\%5Fdenominal\%5Fverbs\%5Fi\%5F/9970205">https://doi.org10.6084/m9.figshare.9970205. https://figshare.com/articles/R\%5FMarkdown\%5FNotebook\%5Ffor\%5Fi\%5FVector\%5Fspace\%5Fmodel\%5Fand\%5Fthe\%5Fusage\%5Fpatterns\%5Fof\%5FIndonesian\%5Fdenominal\%5Fverbs\%5Fi\%5F/9970205</a>.</p>
</div>
<div id="ref-schmid1994treetagger">
<p>Schmid, Helmut. 1994. “TreeTagger-a Language Independent Part-of-Speech Tagger.” <em>Http://Www. Ims. Uni-Stuttgart. De/Projekte/Corplex/TreeTagger/</em>.</p>
</div>
<div id="ref-schmid2013probabilistic">
<p>———. 2013. “Probabilistic Part-Ofispeech Tagging Using Decision Trees.” In <em>New Methods in Language Processing</em>, 154.</p>
</div>
<div id="ref-schmid2007enriched">
<p>Schmid, Helmut, M Baroni, E Zanchetta, and A Stein. 2007. “The Enriched Treetagger System.” In <em>Proceedings of the Evalita 2007 Workshop</em>.</p>
</div>
<div id="ref-steiner1985dictionaries">
<p>Steiner, Roger J. 1985. “Dictionaries. The Art and Craft of Lexicography.” <em>Dictionaries: Journal of the Dictionary Society of North America</em> 7 (1): 294–300.</p>
</div>
</div>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
