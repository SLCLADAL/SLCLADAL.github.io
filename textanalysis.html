<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />


<meta name="author" content="Martin Scweinberger" />

<meta name="date" content="2020-09-29" />

<title>Text Analysis and Distant Reading using R</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cosmo.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>




<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #ffffff;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">LADAL</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="people.html">OUR PEOPLE</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    NEWS
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="news.html">News &amp; Announcements</a>
    </li>
    <li>
      <a href="conferences.html">Events &amp; Presentations</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    DATA SCIENCE BASICS
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li class="dropdown-header">Introduction to Data Science</li>
    <li>
      <a href="introcomputer.html">Working with Computers: Tips and Tricks</a>
    </li>
    <li>
      <a href="reproducibility.html">Data Management, Version Control, and Reproducibility</a>
    </li>
    <li class="divider"></li>
    <li class="dropdown-header">Quantitative Research</li>
    <li>
      <a href="introquant.html">Introduction to Quantitative Reasoning</a>
    </li>
    <li>
      <a href="basicquant.html">Basic Concepts in Quantitative Research</a>
    </li>
    <li class="divider"></li>
    <li class="dropdown-header">Introduction to R</li>
    <li>
      <a href="IntroR_workshop.html">Getting started</a>
    </li>
    <li>
      <a href="stringprocessing.html">String Processing</a>
    </li>
    <li>
      <a href="regularexpressions.html">Regular Expressions</a>
    </li>
    <li>
      <a href="introtables.html">Working with Tables</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    TUTORIALS
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li class="dropdown-header">Data Visualization</li>
    <li>
      <a href="introviz.html">Introduction to Data Viz</a>
    </li>
    <li>
      <a href="basicgraphs.html">Common Visualization Types</a>
    </li>
    <li>
      <a href="basicgraphs.html">Advanced Visualization Methods</a>
    </li>
    <li>
      <a href="maps.html">Displaying Geo-Spatial Data</a>
    </li>
    <li>
      <a href="motion.html">Interactive Charts</a>
    </li>
    <li class="divider"></li>
    <li class="dropdown-header">Statistics</li>
    <li>
      <a href="descriptivestatz.html">Descriptive Statistics</a>
    </li>
    <li>
      <a href="basicstatz.html">Basic Inferential Statistics</a>
    </li>
    <li>
      <a href="regression.html">Regression Analysis</a>
    </li>
    <li>
      <a href="advancedstatztrees.html">Tree-Based Models</a>
    </li>
    <li>
      <a href="groupingstatz.html">Cluster and Correspondence Analysis</a>
    </li>
    <li>
      <a href="svm.html">Semantic Vector Space Models</a>
    </li>
    <li class="divider"></li>
    <li class="dropdown-header">Text Analytics</li>
    <li>
      <a href="textanalysis.html">Text Analysis and Distant Reading</a>
    </li>
    <li>
      <a href="kwics.html">Concordancing (keywords-in-context)</a>
    </li>
    <li>
      <a href="basicnetwork.html">Network Analysis</a>
    </li>
    <li>
      <a href="collocations.html">Co-occurrence and Collocation Analysis</a>
    </li>
    <li>
      <a href="topicmodels.html">Topic Modeling</a>
    </li>
    <li>
      <a href="sentiment.html">Sentiment Analysis</a>
    </li>
    <li>
      <a href="tagging.html">Tagging and Parsing</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    FOCUS &amp; CASE STUDIES
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="lex.html">Lexicography with R: Generating Dictionaries</a>
    </li>
    <li>
      <a href="surveys.html">Questionnaires and Surveys: Analyses with R</a>
    </li>
    <li>
      <a href="corplingr.html">Corpus Linguistics with R: Swearing in Irish English</a>
    </li>
    <li>
      <a href="convertpdf2txt.html">Converting PDFs to txt</a>
    </li>
    <li>
      <a href="webcrawling.html">Web Crawling using R</a>
    </li>
    <li>
      <a href="vc.html">Creating Vowel Charts with Praat and R</a>
    </li>
  </ul>
</li>
<li>
  <a href="services.html">SERVICES &amp; CONTACT</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Text Analysis and Distant Reading using R</h1>
<h4 class="author">Martin Scweinberger</h4>
<h4 class="date">2020-09-29</h4>

</div>


<p><img src="images/uq1.jpg" width="100%" /></p>
<div id="introduction" class="section level1 unnumbered">
<h1>Introduction</h1>
<p>This tutorial introduces Text Analysis <span class="citation">(see Bernard and Ryan <a href="#ref-bernard1998text" role="doc-biblioref">1998</a>; Kabanoff <a href="#ref-kabanoff1997introduction" role="doc-biblioref">1997</a>; Popping <a href="#ref-popping2000computer" role="doc-biblioref">2000</a>)</span>, i.e. computer-based analysis of language data or the (semi-)automated extraction of information from text. The entire code for the sections below can be downloaded <a href="https://slcladal.github.io/textanalysis.Rmd">here</a>.</p>
<p>Since Text Analysis extracts and analyses information from language data, it can be considered a derivative of computational linguistics or an application of <em>Natural Language Processing</em> (NLP) to HASS research. As such, Text Analysis represents the application of computational methods in the humanities.</p>
<p>The advantage of Text Analysis over manual or traditional techniques (close reading) lies in the fact that Text Analysis allows the extraction of information from large sets of textual data and in a replicable manner. Other terms that are more or less synonymous with Text Analysis are Text Mining, Text Analytics, and Distant Reading. In some cases, Text Analysis is considered more qualitative while Text Analytics is considered to be quantitative. This distinction is not taken up here as Text Analysis, while allowing for qualitative analysis, builds upon quantitative information, i.e. information about frequencies or conditional probabilities.</p>
<p><img src="images/GoogleNgram.png" width="35%" style="float:right; padding:10px" /></p>
<p>Distant Reading is a cover term for applications of Text Analysis that allow to investigate literary and cultural trends using text data. Distant Reading contrasts with close reading, i.e. reading texts in the traditional sense whereas Distant Reading refers to the analysis of large amounts of text. Text Analysis and distant reading are similar with respect to the methods that are used but different with respect to their outlook. The outlook of distant reading is to extract information from text without close reading, i.e. reading the document(s) itself but rather focusing on emerging patterns in the language that is used.</p>
<p>Text Analysis or Distant Reading are rapidly growing in use and gaining popularity in the humanities because textual data is readily available and because computational methods can be applied to a huge variety of research questions. The attractiveness of computational text analysis is thus based on the availability of (large amounts of) digitally available texts and in their capability to provide insights that cannot be derived from close reading techniques.</p>
<p><img src="images/ShakespeareKingLearNwtwork.png" width="60%" style="float:right; padding:10px" /></p>
<p>While rapidly growing as a valid approach to analysing textual data, Text Analysis is <a href="https://www.chronicle.com/article/The-Digital-Humanities-Debacle/245986">critizised</a> for lack of "quantitative rigor and because its findings are either banal or, if interesting, not statistically robust (see <a href="https://www.chronicle.com/article/The-Digital-Humanities-Debacle/245986">here</a>. This criticism is correct in that most of the analysis that performed in <em>Computational Literary Studies</em> (CLS) are not yet as rigorous as analyses in fields that have a longer history of computational based, quantitative research, such as, for instance, corpus linguistics. However, the practices and methods used in CLS will be refined, adapted and show a rapid increase in quality if more research is devoted to these approaches. Also, Text Analysis simply offers an alternative way to analyse texts that is not in competition to traditional techniques but rather complements them.</p>
<p>Given it relatively recent emergence, so far, most of the applications of Text Analysis are based upon a relatively limited number of key procedures or concepts (e.g. concordancing, word frequencies, annotation or tagging, parsing, collocation, text classification, Sentiment Analysis, Entity Extraction, Topic Modelling, etc.). In the following, we will explore these procedures and introduce some basic tools that help you perform the introduced tasks.</p>
<div id="text-analysis-at-uq" class="section level2 unnumbered">
<h2>Text Analysis at UQ</h2>
<p>The <a href="https://www.library.uq.edu.au/">UQ Library</a> offers a very handy and attractive summary of <a href="https://guides.library.uq.edu.au/research-techniques/text-mining-analysis/introduction">resources, concepts, and tools</a> that can be used by researchers interested in Text Analysis and Distant Reading. Also, the UQ library site offers short video introductions and addresses issues that are not discussed here such as <a href="https://guides.library.uq.edu.au/research-techniques/text-mining-analysis/considerations">copyright issues</a>, <a href="https://guides.library.uq.edu.au/research-techniques/text-mining-analysis/sources-of-text-data">data sources available at the UQ library</a>, as well as <a href="https://guides.library.uq.edu.au/research-techniques/text-mining-analysis/sources-of-text-data">social media</a> and <a href="https://guides.library.uq.edu.au/research-techniques/text-mining-analysis/web-scraping">web scaping</a>.</p>
<p>In contrast to the UQ library site, the focus of this introduction lies on the practical how-to of text analysis. this means that the following concentrates on how to perform analyses rather than discussing their underlying concepts or evaluating their scientific merits.</p>
</div>
<div id="tools-versus-scripts" class="section level2 unnumbered">
<h2>Tools versus Scripts</h2>
<p>It is perfectly fine to use tools for the analyses exemplified below. However, the aim here is not primarily to show how to perform text analyses but how to perfrom text analyses in a way that complies with practices that guarantee sustainable, transparent, reproducible research. As R code can be readily shared and optimally contains all the data extraction, processing, vizualization, and analysis steps, using scripts is preferable over using (commercial) software.</p>
<p>In addition to being not as transparent and hindering reproduction of research, using tools can also lead to dependencies on third parties which does not arise when using open source software.</p>
<p>Finally, the widespread use of “R” particularly among data scientists, engineers, and analysts reduces the risk of software errors as a very active community corrects flawed functions typically quite rapidly.</p>
</div>
<div id="preparation-and-session-set-up" class="section level2 unnumbered">
<h2>Preparation and session set up</h2>
<p>This tutorial is based on R. If you have not installed R or are new to it, you will find an introduction to and more information how to use R <a href="https://slcladal.github.io/IntroR_workshop.html">here</a>. For this tutorials, we need to install certain <em>packages</em> from an R <em>library</em> so that the scripts shown below are executed without errors. Before turning to the code below, please install the packages by running the code below this paragraph. If you have already installed the packages mentioned below, then you can skip ahead ignore this section. To install the necessary packages, simply run the following code - it may take some time (between 1 and 5 minutes to install all of the libraries so you do not need to worry if it takes some time).</p>
<pre class="r"><code># clean current workspace
rm(list=ls(all=T))
# set options
options(stringsAsFactors = F)
# install libraries
install.packages(c(&quot;class&quot;, &quot;cluster&quot;, &quot;dplyr&quot;, &quot;factoextra&quot;, 
                   &quot;FactoMineR&quot;, &quot;ggplot2&quot;, &quot;ggraph&quot;, &quot;grid&quot;, 
                   &quot;gutenbergr&quot;, &quot;igraph&quot;, &quot;knitr&quot;, &quot;Matrix&quot;, 
                   &quot;NLP&quot;, &quot;openNLP&quot;, &quot;openNLPmodels.en&quot;, &quot;png&quot;, 
                   &quot;stringr&quot;, &quot;syuzhet&quot;, &quot;tidyr&quot;, &quot;tidytext&quot;, 
                   &quot;tm&quot;, &quot;topicmodels&quot;, &quot;wordcloud&quot;, &quot;xtable&quot;))</code></pre>
<p>Once you have installed R Studio and initiated the session by executing the code shown above, you are good to go.</p>
</div>
</div>
<div id="concordancing" class="section level1">
<h1><span class="header-section-number">1</span> Concordancing</h1>
<p>In Text Analysis, concordancing refers to the extraction of words from a given text or texts <span class="citation">(Lindquist <a href="#ref-lindquist2009corpus" role="doc-biblioref">2009</a>)</span>. Commonly, concordances are displayed in the form of key-word in contexts (KWIC)) where the search term is shown with some preceding and following context. Thus, such displays are referred to as key word in context concordances. A more elaborate tutorial on how to perform concordancing with R is available <a href="https://slcladal.github.io/kwics.html">here</a>.</p>
<p><br></p>
<p><img src="images/AntConcConcordance.png" width="60%" style="float:right; padding:10px" /></p>
<p><br></p>
<p>Concordancing is helpful for inspecting how often a given word occurs in a text or a collection of texts, for seeing how the term is used in the data, for extracting examples, and it also represents a basic procedure and often the first step in more sophisticated analyses of language data.</p>
<p>In the following, we will use R to create a KWIC display. More precisely, we will load Charles Darwin’s <em>On the origin of species by means of natural selection</em> and investigate his use of the phrase <em>natural selection</em> in across chapters.</p>
<pre class="r"><code># load libraries
library(dplyr)
library(stringr)
# read in text
darwin &lt;- readLines(&quot;https://slcladal.github.io/data/origindarwin.txt&quot;) %&gt;%
  paste(sep = &quot; &quot;, collapse = &quot; &quot;) %&gt;%
  str_replace_all(&quot;(CHAPTER [XVI]{1,7}\\.{0,1}) &quot;, &quot;qwertz\\1&quot;) %&gt;%
  tolower() %&gt;%  
  strsplit(&quot;qwertz&quot;) %&gt;%
  unlist()
# inspect data
nchar(darwin)</code></pre>
<pre><code>##  [1]  10133  76155  40735  37418 121497  75187  95601 102879  83597  81774
## [11]  72427  70938  74510  53292 114321  70663</code></pre>
<p>Now that we have the subsections of the data that we aim to investigate, we can perform the concordancing. To create a KWIC display, we load the function <code>ConcR</code> from a script called <code>ConcR_2.3_loadedfiles.R</code>. Then we define a pattern that we want to look for (the pattern can be a simple word or it contain regular expressions). Then, we define the amount of context that we want to have displayed (in our case 50 characters). Finally, we run the concordance function <code>ConcR</code> with the arguments <code>darwin</code> (the text elements that we want to inspect), the <code>search pattern</code>, and the <code>context</code>.</p>
<pre class="r"><code># load function for concordancing
source(&quot;https://slcladal.github.io/rscripts/ConcR_2.5_LoadedFiles.R&quot;)
# start concordancing
darwinorganism &lt;- ConcR(darwin, &quot;organism[s]{0,1}&quot;, 50)
# inspect data
darwinorganism[1:5, 2:ncol(darwinorganism)]</code></pre>
<pre><code>##                                           PreContext     Token
## 1 y generations. no case is on record of a variable   organism
## 2  there are two factors; namely, the nature of the   organism
## 3 ects of the conditions of life on each individual   organism
## 4 hat unlike their parents. i may add, that as some  organisms
## 5 e importance in comparison with the nature of the   organism
##                                          PostContext
## 1  ceasing to vary wnder cultivation. our oldest cul
## 2 , and the nature of the conditions. the former see
## 3 , in nearly the same manner as the chill affects d
## 4  breed freely under the most unnat- ural condition
## 5  in determining each particular form of variation</code></pre>
<p>We now want to extract the chapter in which the instance has occurred.</p>
<pre class="r"><code># clean data
darwinorganism &lt;- darwinorganism[complete.cases(darwinorganism),]
# determine chapter
darwinorganism$Chapter &lt;- ifelse(grepl(&quot;chapter [xvi]{1,7}\\.{0,1} .*&quot;, darwinorganism$OriginalString) == T, gsub(&quot;(chapter [xvi]{1,7})\\.{0,1} .*&quot;, &quot;\\1&quot;, darwinorganism$OriginalString), darwinorganism$OriginalString)
# remove OriginalString column 
darwinorganism$OriginalString &lt;- NULL
# inspect data
head(darwinorganism)</code></pre>
<pre><code>##                                           PreContext     Token
## 1 y generations. no case is on record of a variable   organism
## 2  there are two factors; namely, the nature of the   organism
## 3 ects of the conditions of life on each individual   organism
## 4 hat unlike their parents. i may add, that as some  organisms
## 5 e importance in comparison with the nature of the   organism
## 6 likewise neces- sarily occurs with closely allied  organisms
##                                          PostContext    Chapter
## 1  ceasing to vary wnder cultivation. our oldest cul  chapter i
## 2 , and the nature of the conditions. the former see  chapter i
## 3 , in nearly the same manner as the chill affects d  chapter i
## 4  breed freely under the most unnat- ural condition  chapter i
## 5  in determining each particular form of variation   chapter i
## 6 , which inhabit distinct continents or islands. wh chapter ii</code></pre>
<p>Now, the KWIC display is finished and we could go about investigating how Darwin has used the term “organism”.</p>
</div>
<div id="word-frequency" class="section level1">
<h1><span class="header-section-number">2</span> Word Frequency</h1>
<p>One basic aspect of Text Analysis consists in extracting word frequency lists, i.e. determining how often word forms occur in a given text or collection of texts. In fact, frequency information lies at the very core of Text Analysis.</p>
<p>To exemplify how frequency information can help us in an analysis, we will continue working with the KWIC display that we have created above.</p>
<p>In the following, we want to find out about changes in the frequency with which the term “organism” has been used across chapters in Darwin’s “Origin”.</p>
<p>In a first step, we extract the number of words in each chapter.</p>
<pre class="r"><code># extract number of words per chapter
library(dplyr)
darwinchapters &lt;- darwin %&gt;%
  strsplit(&quot; &quot;)
words &lt;- sapply(darwinchapters, function(x) length(x))
# inspect data
words</code></pre>
<pre><code>##  [1]  1855 14064  7455  7135 22316 13915 17780 19054 15846 14740 13312 12995
## [13] 13752  9816 20966 12986</code></pre>
<p>Next, we extract the number of matches in each chapter.</p>
<pre class="r"><code># extract number of matches per chapter
library(stringr)
matcheschapters &lt;- darwin %&gt;%
  str_extract_all(., &quot;organism[s]{0,1}&quot;) 
matches &lt;- sapply(matcheschapters, function(x) length(x))
# inspect data
matches</code></pre>
<pre><code>##  [1]  0  5  3  3  9  3  3  3  0  1  6  6 10  5  8  7</code></pre>
<p>Now, we extract the names of the chapters and create a table with the chapter names and the relative frequency of matches per 1,000 words.</p>
<pre class="r"><code># extract chapters
Chapters &lt;- as.vector(unlist(sapply(darwin, function(x){
  x &lt;- gsub(&quot;(chapter [xvi]{1,7})\\.{0,1} .*&quot;, &quot;\\1&quot;, x)
  x &lt;- ifelse(nchar(x) &gt; 50, &quot;chapter 0&quot;, x)
})))
# calculate rel. freq of serach term per chapter 
Frequency &lt;- matches/words*1000
# create table of results
tb &lt;- data.frame(Chapters, Frequency)
# inspect results
head(tb)</code></pre>
<pre><code>##      Chapters Frequency
## 1   chapter 0 0.0000000
## 2   chapter i 0.3555176
## 3  chapter ii 0.4024145
## 4 chapter iii 0.4204625
## 5  chapter iv 0.4032981
## 6   chapter v 0.2155947</code></pre>
<p>We can now visualize the relative frequencies of our search word per chapter.</p>
<pre class="r"><code># load library
library(ggplot2)
# create plot
ggplot(tb, aes(x=Chapters, y=Frequency, group =1)) + 
  geom_smooth(aes(y = Frequency, x = Chapters), color = &quot;goldenrod2&quot;)+
  geom_line(aes(y = Frequency, x = Chapters), color = &quot;indianred4&quot;) +         
  guides(color=guide_legend(override.aes=list(fill=NA))) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))+
  scale_y_continuous(name =&quot;Relative Frequency (per 1,000 words)&quot;)</code></pre>
<p><img src="textanalysis_files/figure-html/wf4-1.png" width="672" /></p>
<p>We will now briefly check an example where we simply extract a frequency list from a corpus.</p>
<pre class="r"><code># load library
library(tm)
# load and process corpus
corpuswords &lt;- readLines(&quot;https://slcladal.github.io/data/origindarwin.txt&quot;)  %&gt;%
  tolower() %&gt;%
  removeWords(stopwords(&quot;english&quot;)) %&gt;% 
  str_replace_all(&quot;[^[:alpha:][:space:]]*&quot;, &quot;&quot;)  %&gt;%
  paste(sep = &quot; &quot;, collapse = &quot; &quot;) %&gt;%
  str_replace_all(&quot; {2,}&quot;, &quot; &quot;) %&gt;%
  strsplit(&quot; &quot;) %&gt;%
  unlist()  
# create table
wordfreqs &lt;- corpuswords %&gt;%
  table() %&gt;%
  as.data.frame() %&gt;%
  arrange(desc(Freq))
# add column names
colnames(wordfreqs) &lt;- c(&quot;Word&quot;, &quot;Frequency&quot;)
# inspect data
head(wordfreqs)</code></pre>
<pre><code>##      Word Frequency
## 1 species      1755
## 2     one       777
## 3    will       757
## 4     may       650
## 5    many       590
## 6     can       583</code></pre>
<p>Such word frequency lists can be visualized, for example, as bargraphs.</p>
<pre class="r"><code># prepare data
wfd &lt;- table(corpuswords)
wfd &lt;- wfd[order(wfd, decreasing = T)]
wfd &lt;- wfd[1:10]
# start plot
barplot(wfd, las = 1, ylim = c(0,2000), las=2)
text(seq(0.7, 11.5, 1.2), wfd+150, wfd)</code></pre>
<p><img src="textanalysis_files/figure-html/wf6-1.png" width="672" /></p>
<p>Alternatively, word frequency lists can be visualized, although less informative, as word clouds.</p>
<pre class="r"><code># load library
library(&quot;wordcloud&quot;)
# create wordcloud
wordcloud(words = wordfreqs$Word, freq = wordfreqs$Frequency, 
          max.words=100, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, &quot;BrBG&quot;))</code></pre>
<p><img src="textanalysis_files/figure-html/wf7-1.png" width="672" /></p>
<p>Word lists can be used to determine differences between texts. For instance, we can load two different texts and check whether they differ with respect to word frequencies.</p>
<pre class="r"><code># load data
orwell &lt;- readLines(&quot;https://slcladal.github.io/data/orwell.txt&quot;)
melville &lt;- readLines(&quot;https://slcladal.github.io/data/melvillemobydick.txt&quot;)
# combine each text into one element
orwell &lt;- paste(as.vector(unlist(orwell)), sep = &quot; &quot;, collapse = &quot; &quot;)
melville &lt;- paste(as.vector(unlist(melville)), sep = &quot; &quot;, collapse = &quot; &quot;)
# load libraries
library(tm)
library(dplyr)
library(xtable)
# clean texts
docs &lt;- Corpus(VectorSource(c(orwell, melville))) %&gt;%
  tm_map(removePunctuation) %&gt;%
  tm_map(removeNumbers) %&gt;%
  tm_map(tolower)  %&gt;%
  tm_map(removeWords, stopwords(&quot;english&quot;)) %&gt;%
  tm_map(stripWhitespace) %&gt;%
  tm_map(PlainTextDocument)
# create term document matrix
tdm &lt;- TermDocumentMatrix(docs) %&gt;%
  as.matrix()
colnames(tdm) &lt;- c(&quot;Orwell&quot;,&quot;Melville&quot;)
# create comparison cloud
comparison.cloud(tdm, random.order=FALSE, 
                 colors = c(&quot;orange&quot;,&quot;lightblue&quot;),
                 title.size=2.5, max.words=200, 
                 title.bg.colors = &quot;white&quot;)</code></pre>
<p><img src="textanalysis_files/figure-html/wf8-1.png" width="672" /></p>
<p>Frequency information can also tell us something about the nature of a text. For instance, private dialogues will typically contain higher rates of second person pronouns compared with more format text types, such as, for instance, scripted monologues like speeches. For this reason, word frequency lists can be used in text classification and to determine the formality of texts.</p>
<p>As an example, below you find the number of the second person pronouns “you” and “your” and the number of all words except for these second person pronouns in private dialogues compared with scripted monologues in the Irish component of the International corpus of English (ICE).</p>
<table>
<thead>
<tr class="header">
<th align="center"></th>
<th align="center">Private dialogues</th>
<th align="center">Scripted monologues</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">you, your</td>
<td align="center">6761</td>
<td align="center">659</td>
</tr>
<tr class="even">
<td align="center">Other words</td>
<td align="center">259625</td>
<td align="center">105295</td>
</tr>
</tbody>
</table>
<p>If we calculate the percentage of second person pronouns in both text types and see whether private dialogues contain more of these second person pronouns than scripted monologues (i.e. speeches).</p>
<table>
<thead>
<tr class="header">
<th align="center"></th>
<th align="center">Private dialogues</th>
<th align="center">Scripted monologues</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">you, your</td>
<td align="center">6761</td>
<td align="center">659</td>
</tr>
<tr class="even">
<td align="center">Other words</td>
<td align="center">259625</td>
<td align="center">105295</td>
</tr>
<tr class="odd">
<td align="center">Percent</td>
<td align="center">2.60</td>
<td align="center">0.63</td>
</tr>
</tbody>
</table>
<p>This simple example shows that second person pronouns make up 2.6 percent of all words that are used in private dialogues while they only amount to 0.63 percent in scripted speeches. A handy way to present such differences visually are association and mosaic plots.</p>
<pre class="r"><code>d &lt;- matrix(c(6761, 659, 259625, 105295), nrow = 2, byrow = T)
colnames(d) &lt;- c(&quot;D&quot;, &quot;M&quot;)
rownames(d) &lt;- c(&quot;you, your&quot;, &quot;Other words&quot;)
assocplot(d)</code></pre>
<p><img src="textanalysis_files/figure-html/wf11-1.png" width="672" /></p>
<p>Bars above the dashed line indicate relative overuse while bars below the line suggest relative underuse. Therefore, the association plot indicates underuse of “you/your” and overuse of “other words” in monologues while the opposite trends holds true for dialogues, i.e. overuse of “you/your” and underuse of “Other words”.</p>
</div>
<div id="collocations-and-n-grams" class="section level1">
<h1><span class="header-section-number">3</span> Collocations and N-grams</h1>
<p>Collocation refers to the co-occurrence of words. A typical example of a collocation is “Merry Christmas” because the words merry and Christmas occur together more frequently together than would be expected by chance, if words were just randomly stringed together.</p>
<p>N-grams are related to collocates in that they represent words that occur together (bi-grams are two words that occur together, tri-grams three words and so on). Fortunately, creating N-gram lists is very easy. We will use the “Origin” to create a bi-gram list. As a first step, we load the data and split it into individual words.</p>
<pre class="r"><code># load libraries
library(dplyr)
library(stringr)
library(tm)
# read in text
darwin &lt;- readLines(&quot;https://slcladal.github.io/data/origindarwin.txt&quot;) %&gt;%
  paste(sep = &quot; &quot;, collapse = &quot; &quot;) %&gt;%
  removePunctuation() %&gt;%
  str_replace_all(&quot; {2,}&quot;, &quot; &quot;) %&gt;% 
  tolower() %&gt;%
  strsplit(&quot; &quot;) %&gt;%
  unlist()
# inspect data
head(darwin)</code></pre>
<pre><code>## [1] &quot;the&quot;     &quot;origin&quot;  &quot;of&quot;      &quot;species&quot; &quot;by&quot;      &quot;charles&quot;</code></pre>
<pre class="r"><code># create data frame
darwindf &lt;- data.frame(darwin[1:length(darwin)-1], 
                       darwin[2:length(darwin)])
# add column names
colnames(darwindf) &lt;- c(&quot;Word1&quot;, &quot;Word2&quot;)
# inspect data
head(darwindf)</code></pre>
<pre><code>##     Word1   Word2
## 1     the  origin
## 2  origin      of
## 3      of species
## 4 species      by
## 5      by charles
## 6 charles  darwin</code></pre>
<pre class="r"><code># create data frame
darwin2grams &lt;- paste(darwindf$Word1, darwindf$Word2, sep = &quot; &quot;)
# tabulate results
darwin2gramstb &lt;- table(darwin2grams)
# create data frame
darwin2gramsdf &lt;- data.frame(darwin2gramstb)
# order data frame
darwin2gramsdf &lt;- darwin2gramsdf[order(darwin2gramsdf$Freq, decreasing = T),]
# simplify column names
colnames(darwin2gramsdf) &lt;- c(&quot;Bigram&quot;, &quot;Frequency&quot;)
# inspect data
head(darwin2gramsdf)</code></pre>
<pre><code>##          Bigram Frequency
## 47490    of the      2673
## 34249    in the      1440
## 67399  the same       959
## 71688    to the       790
## 48173    on the       744
## 30694 have been       624</code></pre>
<p>Both N-grams and collocations are not only an important concept in language teaching but they are also fundamental in Text Analysis and many other research areas working with language data. Unfortunately, words that collocate do not have to be immediately adjacent but can also encompass several slots. This is unfortunate because it makes retrieval of collocates substantially more difficult compared with a situation in which we only need to extract words that occur right next to each other.</p>
</div>
<div id="text-classification" class="section level1">
<h1><span class="header-section-number">4</span> Text Classification</h1>
<p>Text classification refers to methods that allow to classify a given text to a predefined set of languages, genres, authors, or the like. Such classifications are typically based on the relative frequency of word classes, key words, phonemes, or other linguistic features such as average sentence length, words per line, etc.</p>
<p>As with most other methods that are used in text analysis, text classification typically builds upon a training set that is already annotated with the required tags. Training sets and the features that are derived from these training sets can be created by oneself or one can use build in training sets that are provided in the respective software packages or tools.</p>
<p>In the following, we will use the frequency of phonemes to classify a text. In a first step, we read in a German text, and split it into phonemes.</p>
<pre class="r"><code># read in German text
German &lt;- readLines(&quot;https://slcladal.github.io/data/phonemictext1.txt&quot;) %&gt;%
  str_replace_all(&quot; &quot;, &quot;&quot;) %&gt;%
  str_split(&quot;&quot;) %&gt;%
  unlist() %&gt;%
  as.vector()
# inspect data
head(German, 20)</code></pre>
<pre><code>##  [1] &quot;?&quot; &quot;a&quot; &quot;l&quot; &quot;s&quot; &quot;h&quot; &quot;E&quot; &quot;s&quot; &quot;@&quot; &quot;d&quot; &quot;e&quot; &quot;:&quot; &quot;n&quot; &quot;S&quot; &quot;t&quot; &quot;E&quot; &quot;p&quot; &quot;@&quot; &quot;n&quot; &quot;v&quot;
## [20] &quot;O&quot;</code></pre>
<p>We now do the same for three other texts - an English and a Spanish text as well as one text in a language that we will determine using classification.</p>
<pre class="r"><code># read in texts
English &lt;- readLines(&quot;https://slcladal.github.io/data/phonemictext2.txt&quot;)
Spanish &lt;- readLines(&quot;https://slcladal.github.io/data/phonemictext3.txt&quot;)
Unknown &lt;- readLines(&quot;https://slcladal.github.io/data/phonemictext4.txt&quot;)
# clean, split texts into phonemes, unlist and convert them into vectors
English &lt;- as.vector(unlist(strsplit(gsub(&quot; &quot;, &quot;&quot;, English), &quot;&quot;)))
Spanish &lt;- as.vector(unlist(strsplit(gsub(&quot; &quot;, &quot;&quot;, Spanish), &quot;&quot;)))
Unknown &lt;- as.vector(unlist(strsplit(gsub(&quot; &quot;, &quot;&quot;, Unknown), &quot;&quot;)))
# inspect data
head(English, 20)</code></pre>
<pre><code>##  [1] &quot;D&quot;  &quot;@&quot;  &quot;b&quot;  &quot;U&quot;  &quot;k&quot;  &quot;I&quot;  &quot;z&quot;  &quot;p&quot;  &quot;r&quot;  &quot;\\&quot; &quot;@&quot;  &quot;z&quot;  &quot;E&quot;  &quot;n&quot;  &quot;t&quot; 
## [16] &quot;@&quot;  &quot;d&quot;  &quot;{&quot;  &quot;z&quot;  &quot;@&quot;</code></pre>
<p>We will now create a table that represents the phonemes and their frequencies in each of the 4 texts. In addition, we will add the language and simply the column names.</p>
<pre class="r"><code># create data tables
German &lt;- data.frame(names(table(German)), as.vector(table(German)))
English &lt;- data.frame(names(table(English)), as.vector(table(English)))
Spanish &lt;- data.frame(names(table(Spanish)), as.vector(table(Spanish)))
Unknown &lt;- data.frame(names(table(Unknown)), as.vector(table(Unknown)))
# add column with language
German$Language &lt;- &quot;German&quot;
English$Language &lt;- &quot;English&quot;
Spanish$Language &lt;- &quot;Spanish&quot;
Unknown$Language &lt;- &quot;Unknown&quot;
# simlify column names
colnames(German)[1:2] &lt;- c(&quot;Phoneme&quot;, &quot;Frequency&quot;)
colnames(English)[1:2] &lt;- c(&quot;Phoneme&quot;, &quot;Frequency&quot;)
colnames(Spanish)[1:2] &lt;- c(&quot;Phoneme&quot;, &quot;Frequency&quot;)
colnames(Unknown)[1:2] &lt;- c(&quot;Phoneme&quot;, &quot;Frequency&quot;)
# comine all tables into a single table
classdata &lt;- rbind(German, English, Spanish, Unknown) 
# inspect table for English
head(classdata)</code></pre>
<pre><code>##   Phoneme Frequency Language
## 1       -         6   German
## 2       :       569   German
## 3       ?       556   German
## 4       @       565   German
## 5       ¼         6   German
## 6       2         6   German</code></pre>
<p>Now, we group the data so that we see, how often each phoneme is used in each language.</p>
<pre class="r"><code># activate package
library(tidyr)
# convert into wide format
classdw &lt;- classdata %&gt;%
  spread(Phoneme, Frequency) %&gt;%
  replace(is.na(.), 0)
# inspect data
head(classdw)</code></pre>
<pre><code>##   Language  &#39;  -   :   ?   @  \\   {   }  = © ¼  1 2  3  4  5   6 7  8  9   a
## 1  English  7  8 176   0 309 103 123  77 38 0 0  0 0 19  0 43   0 0  0  0 273
## 2   German  0  6 569 556 565   0   0   0  0 0 6  0 6 31 67  1 402 0 32 37 926
## 3  Spanish  0  5   0   0   0   0   0   0  0 1 0 13 0 23  0  0   0 0  0  9 631
## 4  Unknown 12 12 286   0 468 121 125 138 46 0 0  0 0 45  0 73   0 1  0  1 394
##    A Ã   b  B   c   C   d   D    e   E   f   F   g  G   h  H   i   I  j  J   k
## 1 52 0  92  2  93   0 205  74  442  99 127   1  76  2 281 33 390 154 20  0  88
## 2 27 6 242 28 204 165 577  34 1522 326 399 107 347 28 424 70 835 533 26  6 417
## 3  7 1  42 45 103   2 189  56  681  33  66  28  35 18  11 22 326   0 59  0  91
## 4 77 0 148 15 120   5 466 101  630 166 150   3 110  1 376 13 532 200 31 26 135
##    K   l  L   m  M    n  N   o   O   p  P  q   r  R    s   S    t  T   u   U
## 1  0 204  0 169  3  412 23 250  64 100  2  5 316  1  369  33  450 25  84  12
## 2 21 658 22 383 34 1545 60 232  99 147 10  0 753 12 1026 142 1001 14 353 205
## 3  0 284 16 119  2  393  5 414   0 149  2 17 379  1  374  62  290 49 175   2
## 4  1 324 18 194  9  696 40 408 126 165  2  2 410  3  507  70  679 19 123  10
##     v  V   w  W   x  y  Y   z  Z
## 1  68 28  95  3   8 67  0 124 14
## 2 157 15  86 21 233 14 28 245 11
## 3  31  0  23  1  80 21  0  12  1
## 4 126 45 137  8  14 67  0 177 47</code></pre>
<p>Now, we need to transform the data again, so that we have the frequency of each phoneme by language as the classifier will use “Language” as the dependent variable and the phoneme frequencies as predictors.</p>
<pre class="r"><code>numvar &lt;- colnames(classdw)[2:length(colnames(classdw))]
classdw[numvar] &lt;- lapply(classdw[numvar], as.numeric)
# function for normalizing numeric variables
normalize &lt;- function(x) { (x-min(x))/(max(x)-min(x))   }
# apply normalization
 classdw[numvar] &lt;- as.data.frame(lapply(classdw[numvar], normalize))
 # inspect data
classdw[, 1:5]</code></pre>
<pre><code>##   Language         &#39;         -         : ?
## 1  English 0.5833333 0.4285714 0.3093146 0
## 2   German 0.0000000 0.1428571 1.0000000 1
## 3  Spanish 0.0000000 0.0000000 0.0000000 0
## 4  Unknown 1.0000000 1.0000000 0.5026362 0</code></pre>
<p>Before turning to the actual classification, we will use a cluster analysis to see which texts the unknown text is most similar with.</p>
<pre class="r"><code># remove language column
textm &lt;- classdw[,2:ncol(classdw)]
# add languages as row names
rownames(textm) &lt;- classdw[,1]
# create distance matrix
distmtx &lt;- dist(textm)
# activate library
library(&quot;cluster&quot;)         # activate library
clustertexts &lt;- hclust(    # hierarchical cluster object
  distmtx,                 # use data diststudents
  method=&quot;ward.D&quot;)         # ward.D as linkage method
plot(clustertexts,         # plot result as dendrogram
     hang = .25,           # labels at split
     main = &quot;&quot;)            # no title</code></pre>
<p><img src="textanalysis_files/figure-html/tc6-1.png" width="672" /></p>
<p>According to the cluster analysis, the unknown text clusters together with the English texts which suggests that the unknown text is likely to be English.</p>
<p>Before we begin with the actual classification, we will split the data so that we have one data set without “Unknown” (this is our training set) and one data set with only “Unknown” (this is our test set).</p>
<pre class="r"><code>#load library
library(dplyr)
# create training set
train &lt;- classdw %&gt;%
  filter(Language != &quot;Unknown&quot;)
# increase training set size
#train &lt;- rbind(train, train, train, train, train, train, train, train)
# create test set
test &lt;- classdw %&gt;%
  filter(Language == &quot;Unknown&quot;)
# convert variables
#train$Language &lt;- as.factor(train$Language)
#train$Language &lt;- as.factor(train$Language)
# inspect data
train</code></pre>
<pre><code>##   Language         &#39;         -         : ?         @        \\     {        }
## 1  English 0.5833333 0.4285714 0.3093146 0 0.5469027 0.8512397 0.984 0.557971
## 2   German 0.0000000 0.1428571 1.0000000 1 1.0000000 0.0000000 0.000 0.000000
## 3  Spanish 0.0000000 0.0000000 0.0000000 0 0.0000000 0.0000000 0.000 0.000000
##          = © ¼ 1 2         3 4          5 6 7 8         9         a         A
## 1 0.826087 0 0 0 0 0.0000000 0 0.58904110 0 0 0 0.0000000 0.0000000 0.6428571
## 2 0.000000 0 1 0 1 0.4615385 1 0.01369863 1 0 1 1.0000000 1.0000000 0.2857143
## 3 0.000000 1 0 1 0 0.1538462 0 0.00000000 0 0 0 0.2432432 0.5482389 0.0000000
##           Ã    b         B          c          C          d         D         e
## 1 0.0000000 0.25 0.0000000 0.00000000 0.00000000 0.04123711 0.5970149 0.0000000
## 2 1.0000000 1.00 0.6046512 1.00000000 1.00000000 1.00000000 0.0000000 1.0000000
## 3 0.1666667 0.00 1.0000000 0.09009009 0.01212121 0.00000000 0.3283582 0.2212963
##          E         f        F         g          G        h         H         i
## 1 0.225256 0.1831832 0.000000 0.1314103 0.03703704 0.653753 0.3508772 0.1257367
## 2 1.000000 1.0000000 1.000000 1.0000000 1.00000000 1.000000 1.0000000 1.0000000
## 3 0.000000 0.0000000 0.254717 0.0000000 0.62962963 0.000000 0.1578947 0.0000000
##           I         j         J           k K         l         L         m
## 1 0.2889306 0.0000000 0.0000000 0.000000000 0 0.0000000 0.0000000 0.1893939
## 2 1.0000000 0.1538462 0.2307692 1.000000000 1 1.0000000 1.0000000 1.0000000
## 3 0.0000000 1.0000000 0.0000000 0.009118541 0 0.1762115 0.7272727 0.0000000
##         M          n         N         o         O         p P         q
## 1 0.03125 0.01649306 0.3272727 0.0989011 0.5079365 0.0000000 0 0.2941176
## 2 1.00000 1.00000000 1.0000000 0.0000000 0.7857143 0.7230769 1 0.0000000
## 3 0.00000 0.00000000 0.0000000 1.0000000 0.0000000 0.7538462 0 1.0000000
##           r R          s        S         t         T       u          U
## 1 0.0000000 0 0.00000000 0.000000 0.2250352 0.3142857 0.00000 0.04926108
## 2 1.0000000 1 1.00000000 1.000000 1.0000000 0.0000000 1.00000 1.00000000
## 3 0.1441648 0 0.00761035 0.266055 0.0000000 1.0000000 0.33829 0.00000000
##           v         V         w   W    x         y Y         z         Z
## 1 0.2936508 0.6222222 0.6315789 0.1 0.00 1.0000000 0 0.4806867 0.2826087
## 2 1.0000000 0.3333333 0.5526316 1.0 1.00 0.0000000 1 1.0000000 0.2173913
## 3 0.0000000 0.0000000 0.0000000 0.0 0.32 0.1320755 0 0.0000000 0.0000000</code></pre>
<p>Finally, we can apply our classifier to our data. The classifier we use is a k-nearest neighbour classifier as the underlying function will classify an unknown element given its proximity to the clusters in the training set.</p>
<pre class="r"><code># activate library
library(&quot;class&quot;)
# apply k-nearest-neighbor (knn) classifier
prediction &lt;- class::knn(train[,2:ncol(train)], test[,2:ncol(test)], cl = train[, 1], k = 3)
# inspect the result
prediction</code></pre>
<pre><code>## [1] German
## Levels: English German Spanish</code></pre>
<p>Based on the frequencies of phonemes in the unknown text, the knn-classifier predicts that the unknown text is English. This is in fact true as the text is a subsection of the Wikipedia article for Aldous Huxley’s <em>Brave New World</em>. The training texts were German, English, and Spanish translations of a subsection of Wikipedia’s article for Hermann Hesse’s <em>Steppenwolf</em>.</p>
</div>
<div id="entity-extraction" class="section level1">
<h1><span class="header-section-number">5</span> Entity Extraction</h1>
<p>Entity Extraction is a process during which textual elements which have characteristics that are common to proper nouns (locations, people, organizations, etc.) rather than other parts of speech, e.g. non-sentence initial capitalization, are extracted from texts. Retrieving entities is common in automated summarization and in Topic Modelling. Entity extraction can be achieved by simple feature extraction (e.g. extract all non-sentence initial capitalized words) or with the help of training sets. Using training sets, i.e. texts that are annotated for entities and non-entities, achieves better results when dealing with unknown data and data with inconsistent capitalization.</p>
<pre class="r"><code># load libraries
library(NLP)
library(openNLP)
#library(openNLPmodels.en)
library(openNLPdata)
# load text
orwell &lt;- readLines(&quot;https://slcladal.github.io/data/orwell.txt&quot;)
orwell &lt;- orwell  %&gt;%
  paste(sep = &quot; &quot;, collapse = &quot; &quot;) %&gt;%
  str_replace_all(&quot; {2,}&quot;, &quot; &quot;) %&gt;%
  str_replace_all(&quot;Part 2,.*&quot;, &quot;&quot;)
# convert text into string
orwell = as.String(orwell)
# define annotators
sent_annot = Maxent_Sent_Token_Annotator()
word_annot = Maxent_Word_Token_Annotator()
loc_annot = Maxent_Entity_Annotator(kind = &quot;location&quot;) 
people_annot = Maxent_Entity_Annotator(kind = &quot;person&quot;) 
# start annotation
orwellanno = NLP::annotate(orwell, list(sent_annot, word_annot, 
                                        loc_annot, people_annot))
# extract features
k &lt;- sapply(orwellanno$features, `[[`, &quot;kind&quot;)
# extract locations
orwelllocations = names(table(orwell[orwellanno[k == &quot;location&quot;]]))
# extract people
orwellpeople = names(table(orwell[orwellanno[k == &quot;person&quot;]]))
# inspect extract people
orwellpeople</code></pre>
<pre><code>##  [1] &quot;Adam&quot;                &quot;Ah&quot;                  &quot;Big Brother&quot;        
##  [4] &quot;Byron&quot;               &quot;Comrade Ogilvy&quot;      &quot;Floating Fortresses&quot;
##  [7] &quot;Goldstein&quot;           &quot;Ingsoc.&quot;             &quot;Jews&quot;               
## [10] &quot;Jones&quot;               &quot;Julius Caesar&quot;       &quot;Martin&quot;             
## [13] &quot;Milton&quot;              &quot;Parsons&quot;             &quot;Peace&quot;              
## [16] &quot;Rutherford&quot;          &quot;Saint Sebastian&quot;     &quot;Shakespeare&quot;        
## [19] &quot;Smith&quot;               &quot;St Martin&quot;           &quot;Syme&quot;               
## [22] &quot;Tom&quot;                 &quot;Winston&quot;             &quot;Winston Smith&quot;      
## [25] &quot;Withers&quot;</code></pre>
</div>
<div id="citation-session-info" class="section level1 unnumbered">
<h1>Citation &amp; Session Info</h1>
<p>Schweinberger, Martin. 2020. <em>Text Analysis and Distant Reading using R</em>. Brisbane: The University of Queensland. url: <a href="https://slcladal.github.io/textanalysis.html" class="uri">https://slcladal.github.io/textanalysis.html</a> (Version 2020.09.27).</p>
<pre><code>@manual{schweinberger2020ta,
  author = {Schweinberger, Martin},
  title = {Text Analysis and Distant Reading using R},
  note = {https://slcladal.github.io/textanalysis.html},
  year = {2020},
  organization = &quot;The University of Queensland, Australia. School of Languages and Cultures},
  address = {Brisbane},
  edition = {2020/09/27}
}</code></pre>
<pre class="r"><code>sessionInfo()</code></pre>
<pre><code>## R version 4.0.2 (2020-06-22)
## Platform: x86_64-w64-mingw32/x64 (64-bit)
## Running under: Windows 10 x64 (build 18362)
## 
## Matrix products: default
## 
## locale:
## [1] LC_COLLATE=German_Germany.1252  LC_CTYPE=German_Germany.1252   
## [3] LC_MONETARY=German_Germany.1252 LC_NUMERIC=C                   
## [5] LC_TIME=German_Germany.1252    
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] openNLPdata_1.5.3-4 openNLP_0.2-7       class_7.3-17       
##  [4] cluster_2.1.0       tidyr_1.1.2         knitr_1.30         
##  [7] xtable_1.8-4        wordcloud_2.6       RColorBrewer_1.1-2 
## [10] tm_0.7-7            NLP_0.2-0           ggplot2_3.3.2      
## [13] plyr_1.8.6          stringr_1.4.0       dplyr_1.0.2        
## 
## loaded via a namespace (and not attached):
##  [1] Rcpp_1.0.5       highr_0.8        pillar_1.4.6     compiler_4.0.2  
##  [5] tools_4.0.2      digest_0.6.25    evaluate_0.14    lifecycle_0.2.0 
##  [9] tibble_3.0.3     gtable_0.3.0     nlme_3.1-148     lattice_0.20-41 
## [13] mgcv_1.8-31      pkgconfig_2.0.3  rlang_0.4.7      Matrix_1.2-18   
## [17] parallel_4.0.2   yaml_2.2.1       xfun_0.16        rJava_0.9-13    
## [21] xml2_1.3.2       withr_2.3.0      generics_0.0.2   vctrs_0.3.4     
## [25] grid_4.0.2       tidyselect_1.1.0 glue_1.4.2       R6_2.4.1        
## [29] rmarkdown_2.3    purrr_0.3.4      farver_2.0.3     magrittr_1.5    
## [33] scales_1.1.1     ellipsis_0.3.1   htmltools_0.5.0  splines_4.0.2   
## [37] colorspace_1.4-1 labeling_0.3     stringi_1.5.3    munsell_0.5.0   
## [41] slam_0.1-47      crayon_1.3.4</code></pre>
<hr />
<p><a href="https://slcladal.github.io/index.html">Main page</a></p>
<hr />
</div>
<div id="references" class="section level1 unnumbered">
<h1>References</h1>
<div id="refs" class="references">
<div id="ref-bernard1998text">
<p>Bernard, H Russell, and Gery Ryan. 1998. “Text Analysis.” <em>Handbook of Methods in Cultural Anthropology</em> 613.</p>
</div>
<div id="ref-kabanoff1997introduction">
<p>Kabanoff, Boris. 1997. “Introduction: Computers Can Read as Well as Count: Computer-Aided Text Analysis in Organizational Research.” <em>Journal of Organizational Behavior</em>, 507–11.</p>
</div>
<div id="ref-lindquist2009corpus">
<p>Lindquist, Hans. 2009. <em>Corpus Linguistics and the Description of English</em>. Edinburgh: Edinburgh University Press.</p>
</div>
<div id="ref-popping2000computer">
<p>Popping, Roel. 2000. <em>Computer-Assisted Text Analysis</em>. Sage.</p>
</div>
</div>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
