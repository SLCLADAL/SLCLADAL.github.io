<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />


<meta name="author" content="UQ SLC Digital Team" />

<meta name="date" content="2019-07-10" />

<title>Text Analysis</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cosmo.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<link href="site_libs/font-awesome-5.0.13/css/fa-svg-with-js.css" rel="stylesheet" />
<script src="site_libs/font-awesome-5.0.13/js/fontawesome-all.min.js"></script>
<script src="site_libs/font-awesome-5.0.13/js/fa-v4-shims.min.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>


</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}

.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>


<div class="container-fluid main-container">

<!-- tabsets -->
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->




<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_').toLowerCase();
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}


.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
  padding-left: 25px;
  text-indent: 0;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>

<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">LADAL</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    <span class="fa fa-play-circle"></span>
     
    Basics
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li class="dropdown-header">Basics</li>
    <li>
      <a href="introquant.html">Introduction To Quantitative Reasoning</a>
    </li>
    <li>
      <a href="basicquant.html">Basic Concepts In Quantitative Reasoning</a>
    </li>
    <li class="divider"></li>
    <li class="dropdown-header">Research Designs</li>
    <li>
      <a href="researchdesigns.html">Overview</a>
    </li>
    <li>
      <a href="corpling.html">Corpus Linguistics</a>
    </li>
    <li>
      <a href="experiments.html">Experimental Designs</a>
    </li>
    <li>
      <a href="ca.html">Conversation Analysis</a>
    </li>
    <li>
      <a href="acoustic.html">Acoustic Analysis</a>
    </li>
    <li class="divider"></li>
    <li class="dropdown-header">Data Collection</li>
    <li>
      <a href="introdatacollection.html">Introduction</a>
    </li>
    <li>
      <a href="fieldwork.html">Field Work</a>
    </li>
    <li>
      <a href="interviews.html">Interviews</a>
    </li>
    <li>
      <a href="questionnaires.html">Questionnaires and Surveys</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    <span class="fa fa-gear"></span>
     
    Data Processing
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li class="dropdown-header">Data Processing</li>
    <li>
      <a href="intror.html">Basics: Getting started with R</a>
    </li>
    <li>
      <a href="introloading.html">Loading and saving data</a>
    </li>
    <li>
      <a href="introtables.html">Tabulating data</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    <span class="fa fa-bar-chart"></span>
     
    Visualization
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li class="dropdown-header">Visualization</li>
    <li>
      <a href="basicgraphs.html">Basic Visualization Techniques</a>
    </li>
    <li>
      <a href="advancedgraphs.html">Advanced Visualization Techniques</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    <span class="fa fa-eye"></span>
     
    Statistics
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li class="dropdown-header">Statistics</li>
    <li>
      <a href="descriptivestatz.html">Descriptive Statistics</a>
    </li>
    <li class="divider"></li>
    <li class="dropdown-header">Basic Interential Statistics</li>
    <li>
      <a href="basicstatz.html">Basic Inferential Tests</a>
    </li>
    <li>
      <a href="basicstatzchi.html">The Chi-Square Family</a>
    </li>
    <li>
      <a href="basicstatzregression.html">Simple Linear Regression</a>
    </li>
    <li class="divider"></li>
    <li class="dropdown-header">Advanced Interential Statistics</li>
    <li>
      <a href="advancedstatzregressions.html">Regression Analysis</a>
    </li>
    <li>
      <a href="advancedstatztrees.html">Tree-Based Models</a>
    </li>
    <li>
      <a href="groupingstatz.html">Agglomerative Procedures</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    <span class="fa fa-bars"></span>
     
    Text Analysis/Corpus Linguistics
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li class="dropdown-header">Text Analysis</li>
    <li>
      <a href="textanalysis.html">Introduction</a>
    </li>
    <li>
      <a href="webcrawling.html">Web Crawling</a>
    </li>
    <li>
      <a href="network.html">Network Analysis</a>
    </li>
    <li>
      <a href="topic.html">Topic Modeling</a>
    </li>
    <li>
      <a href="classification.html">Classification</a>
    </li>
    <li>
      <a href="tagging.html">Tagging and Parsing</a>
    </li>
    <li class="divider"></li>
    <li class="dropdown-header">Corpus Linguistics</li>
    <li>
      <a href="corplingr.html">Corpus Linguistics in R</a>
    </li>
    <li>
      <a href="corplingantconcexcel.html">Corpus Linguistics with AntConc, TextPad and Excel</a>
    </li>
    <li>
      <a href="available.html">Available Software</a>
    </li>
  </ul>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="about.html">
    <span class="fa fa-info"></span>
     
    Contact
  </a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Text Analysis</h1>
<h4 class="author"><em>UQ SLC Digital Team</em></h4>
<h4 class="date"><em>2019-07-10</em></h4>

</div>


<p><img src="images/uq1.jpg" width="100%" /></p>
<div id="introduction" class="section level1">
<h1><span class="header-section-number">1</span> Introduction</h1>
<p>This section introduces Text Analysis, i.e. computer-based analysis of language data or the (semi-)automated extraction of information from text. The advantage of Text Analysis over manual techniques lies in the fact that Text Analysis allows to extract information from large sets of textual data and in a replicable manner. Other terms that are more or less synonymous with Text Analysis are Text Mining or Text Analytics (sometimes, Text Analysis is considered more qualitative while Text Analytics is considered to be quantitative). This distinction is not taken up here as Text Analysis, while allowing for qualitative analysis, builds upon quantitative information, i.e. information about frequencies or conditional probabilities.</p>
<p>Most of the applications of Text Analysis are based upon a relatively limited number of key procedures or concepts (e.g. concordancing, word frequencies, annotation or tagging, parsing, collocation, text classification, Sentiment Analysis, Entity Extraction, Topic Modelling, etc.). In the following, we will explore these procedures and introduce some basic tools that help you perform the introduced tasks.</p>
</div>
<div id="distant-reading" class="section level1">
<h1><span class="header-section-number">2</span> Distant Reading</h1>
<p>Distant Reading is a cover term for applications of Text Analysis that allow to investigate literary and cultural trends using text data. Distant Reading contrasts with close reading, i.e. reading texts in the traditional sense whereas Distant Reading refers to the analysis of large amounts of text. Text Analysis and distant reading are similar with respect to the methods that are used but different with respect to their outlook. The outlook of distant reading is to extract information from text without close reading, i.e. reading the document(s) itself but rather focusing on emerging patterns in the language that is used.</p>
<div class="figure">
<img src="textanalysis_files/figure-html/Fig1-1.png" alt="\label{fig:Fig1} Google N-Gram Viewer results for “ communist “,” terror “,” democratic “,” liberal “, and “ terror “ from 1820 to 2008." width="768" />
<p class="caption">
 Google N-Gram Viewer results for “ communist “,” terror “,” democratic “,” liberal “, and “ terror “ from 1820 to 2008.
</p>
</div>
<p>Distant Reading is very attractive because it can be applied to so many different research questions as the only requirements are that the data is present in text format and is sufficiently large. If these conditions are met, then Distant Reading or Text Analysis methods can offer a myriad of insights that would not be derivable from close reading techniques.</p>
</div>
<div id="concordancing" class="section level1">
<h1><span class="header-section-number">3</span> Concordancing</h1>
<p>In Text Analysis, concordancing refers to the extraction of words from a given text or texts. Commonly, concordances are displayed in the form of KWIC displays (Key Word in Context) where the search term is shown with some preceding and following context.</p>
<div class="figure">
<img src="textanalysis_files/figure-html/Fig2-1.png" alt="\label{fig:Fig2} KWIC display of the search term language extracted from the BROWN corpus in the program AntConc." width="768" />
<p class="caption">
 KWIC display of the search term language extracted from the BROWN corpus in the program AntConc.
</p>
</div>
<p>Concordancing is helpful for inspecting how often a given word occurs in a text or a collection of texts, for seeing how the term is used in the data, for extracting examples, and it also represents a basic procedure and often the first step in more sophisticated analyses of language data.</p>
<div id="practical-example" class="section level2">
<h2><span class="header-section-number">3.1</span> Practical example</h2>
<p>In this example, we will use “R” to create a KWIC display. More precisely, we will load Charles Darwin’s “On the origin of species” and investigate his use of the term “natural selection” in across chapters.</p>
<pre class="r"><code># read in text
darwin &lt;- readLines(&quot;D:\\Uni\\UQ\\LADAL\\SLCLADAL.github.io\\data/origindarwin.txt&quot;)
# inspect data
str(darwin)</code></pre>
<pre><code>##  chr [1:20001] &quot;THE ORIGIN OF SPECIES &quot; &quot;BY &quot; &quot;CHARLES DARWIN &quot; ...</code></pre>
<p>The output shows that we are dealing with 20001 individual elements. Therefore, we combine these elements into one element and remove everything element that occurs after the header “CHAPTER II”.</p>
<pre class="r"><code># create a single element 
darwin1 &lt;- paste(darwin, sep = &quot; &quot;, collapse = &quot; &quot;)
# split monograph into chapters
darwin2 &lt;- as.vector(unlist(sapply(darwin1, function(x) {
  x &lt;- strsplit(gsub(&quot;(CHAPTER [xviXVI]{1,7})&quot;, &quot;qwertz\\1&quot;, x), &quot;qwertz&quot; )
}))) 
# convert everything into lower case
darwin3 &lt;- tolower(darwin2)
# inspect data
nchar(darwin1); nchar(darwin2)</code></pre>
<pre><code>## [1] 1181142</code></pre>
<pre><code>##  [1]  10133  76156  40736  37419 121498  75188  95602 102880  83598  81775
## [11]  72428  70939  74511  53293 114322  70664</code></pre>
<p>We now have the subsection of the data that we aim to investigate and can now perform the concordaning.</p>
<pre class="r"><code># load function for concordancing
source(&quot;D:\\R/ConcR_2.3_loadedfiles.R&quot;)
# set parameters for concordancing
pattern &lt;- &quot;organism[s]{0,1}&quot;
context &lt;- 50
# extract all adjectives (concordance)
darwinnatsel &lt;- ConcR(darwin3, pattern, context)
# inspect data
darwinnatsel[1:5, 2:ncol(darwinnatsel)]</code></pre>
<pre><code>##                                           PreContext     Token
## 1                                               &lt;NA&gt;      &lt;NA&gt;
## 2 y generations. no case is on record of a variable   organism
## 3  there are two factors; namely, the nature of the   organism
## 4 ects of the conditions of life on each individual   organism
## 5 hat unlike their parents. i may add, that as some  organisms
##                                          PostContext
## 1                                               &lt;NA&gt;
## 2  ceasing to vary wnder cultivation. our oldest cul
## 3 , and the nature of the conditions. the former see
## 4 , in nearly the same manner as the chill affects d
## 5  breed freely under the most unnat- ural condition</code></pre>
<pre class="r"><code># clean data
darwinnatsel1 &lt;- darwinnatsel[complete.cases(darwinnatsel),]
# determine chapter
darwinnatsel1$Chapter &lt;- ifelse(grepl(&quot;chapter [xvi]{1,7}\\.{0,1} .*&quot;, darwinnatsel1$OriginalString) == T, gsub(&quot;(chapter [xvi]{1,7})\\.{0,1} .*&quot;, &quot;\\1&quot;, darwinnatsel1$OriginalString), darwinnatsel1$OriginalString)
# remove OriginalString column 
darwinnatsel1$OriginalString &lt;- NULL
# inspect data
head(darwinnatsel1)</code></pre>
<pre><code>##                                           PreContext     Token
## 2 y generations. no case is on record of a variable   organism
## 3  there are two factors; namely, the nature of the   organism
## 4 ects of the conditions of life on each individual   organism
## 5 hat unlike their parents. i may add, that as some  organisms
## 6 e importance in comparison with the nature of the   organism
## 7 likewise neces- sarily occurs with closely allied  organisms
##                                          PostContext    Chapter
## 2  ceasing to vary wnder cultivation. our oldest cul  chapter i
## 3 , and the nature of the conditions. the former see  chapter i
## 4 , in nearly the same manner as the chill affects d  chapter i
## 5  breed freely under the most unnat- ural condition  chapter i
## 6  in determining each particular form of variation   chapter i
## 7 , which inhabit distinct continents or islands. wh chapter ii</code></pre>
<pre class="r"><code># extract number of words per chapter
wordfreq &lt;- as.vector(sapply(darwin3, function(x){
  x &lt;- as.vector(unlist(strsplit(x, &quot; &quot;)))
  x &lt;- length(x)
}))
# extract number of search terms per chapter
termfreq &lt;- as.vector(unlist(sapply(darwin3, function(x){
  x &lt;- str_extract_all(x, pattern)
  x &lt;- length(as.vector(unlist(x)))
})))
# extract chapters
chapters &lt;- as.vector(unlist(sapply(darwin3, function(x){
  x &lt;- gsub(&quot;(chapter [xvi]{1,7})\\.{0,1} .*&quot;, &quot;\\1&quot;, x)
  x &lt;- ifelse(nchar(x) &gt;50, &quot;chapter 0&quot;, x)
})))
# calculate rel. freq of serach term per chapter 
relfreq &lt;- termfreq/wordfreq*1000
# create table of results
tb &lt;- data.frame(chapters, relfreq)
tb$chapters &lt;- factor(tb$chapters, 
                         levels = c(&quot;chapter 0&quot;, &quot;chapter i&quot;, &quot;chapter ii&quot;,
                            &quot;chapter iii&quot;, &quot;chapter iv&quot;, &quot;chapter v&quot;,
                            &quot;chapter vi&quot;, &quot;chapter vii&quot;, &quot;chapter viii&quot;,
                            &quot;chapter ix&quot;, &quot;chapter x&quot;, &quot;chapter xi&quot;,
                            &quot;chapter xii&quot;, &quot;chapter xiii&quot;, &quot;chapter xiv&quot;,
                            &quot;chapter xv&quot;))
# inspect results
tb</code></pre>
<pre><code>##        chapters   relfreq
## 1     chapter 0 0.0000000
## 2     chapter i 0.3554924
## 3    chapter ii 0.4023605
## 4   chapter iii 0.4204036
## 5    chapter iv 0.4032800
## 6     chapter v 0.2155792
## 7    chapter vi 0.1687194
## 8   chapter vii 0.1574390
## 9  chapter viii 0.0000000
## 10   chapter ix 0.0678380
## 11    chapter x 0.4506873
## 12   chapter xi 0.4616805
## 13  chapter xii 0.7271141
## 14 chapter xiii 0.5093206
## 15  chapter xiv 0.3815520
## 16   chapter xv 0.5390005</code></pre>
<p>We can now vizualize the relative frequencies of our search word per chapter.</p>
<pre class="r"><code>library(ggplot2)
ggplot(tb, aes(x=chapters, y=relfreq, group =1)) + 
  geom_smooth(aes(y = relfreq, x = chapters), color = &quot;goldenrod2&quot;)+
  geom_line(aes(y = relfreq, x = chapters), color = &quot;indianred4&quot;) +         
  guides(color=guide_legend(override.aes=list(fill=NA))) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))+
  scale_x_discrete(name =&quot;&quot;,
                   breaks=c(&quot;chapter 0&quot;, &quot;chapter i&quot;, &quot;chapter ii&quot;,
                            &quot;chapter iii&quot;, &quot;chapter iv&quot;, &quot;chapter v&quot;,
                            &quot;chapter vi&quot;, &quot;chapter vii&quot;, &quot;chapter viii&quot;,
                            &quot;chapter ix&quot;, &quot;chapter x&quot;, &quot;chapter xi&quot;,
                            &quot;chapter xii&quot;, &quot;chapter xiii&quot;, &quot;chapter xiv&quot;,
                            &quot;chapter xv&quot;),
                    labels=c(&quot;Chapter 0&quot;, &quot;Chapter i&quot;, &quot;Chapter ii&quot;,
                             &quot;Chapter iii&quot;, &quot;Chapter iv&quot;, &quot;Chapter v&quot;,
                             &quot;Chapter vi&quot;, &quot;Chapter vii&quot;, &quot;Chapter viii&quot;,
                             &quot;Chapter ix&quot;, &quot;Chapter x&quot;, &quot;Chapter xi&quot;,
                             &quot;Chapter xii&quot;, &quot;Chapter xiii&quot;, &quot;Chapter xiv&quot;,
                             &quot;Chapter xv&quot;)) +
  scale_y_continuous(name =&quot;Relative Frequency (per 1,000 words)&quot;,
                   breaks=seq(0, 1, 0.2),
                   labels=seq(0, 1, 0.2),
                   limits=c(-0.05, 1))</code></pre>
<p><img src="textanalysis_files/figure-html/conc6-1.png" width="672" /></p>
</div>
</div>
<div id="word-frequency" class="section level1">
<h1><span class="header-section-number">4</span> Word Frequency</h1>
<p>One basic aspect of Text Analysis consists in extracting word frequency lists, i.e. determining how often word forms occur in a given text or collection of texts.</p>
<pre><code>##       Word Frequency
## 1       of        54
## 2      the        50
## 3      and        46
## 4 language        29
## 5       in        19
## 6        a        16</code></pre>
<p>Such word frequency lists can be visualized as word clouds.</p>
<p><img src="textanalysis_files/figure-html/wf2-1.png" width="672" /></p>
<p>Such word lists can be used to determine differences between texts. For instance, we can load two different texts and check whether they differ with respect to word frequencies.</p>
<p><img src="textanalysis_files/figure-html/wf3-1.png" width="672" /></p>
<p>Also, private dialogue data will contain second person pronouns substantially more frequently than, for instance, scripted monologues such as speeches. Thus, word lists can be used in text classification and to determine the formality of texts.</p>
<p>As an example, below you find the number of the second person pronouns you and your and the number of all words except for these second person pronouns in private dialogues compared with scripted monologues in the Irish component of the International corpus of English (ICE).</p>
<pre><code>Private dialogues   Scripted monologues</code></pre>
<p>Second person pronouns (you, your) 6761 659 Other words 259625 105295 Total 266386 105954</p>
<p>If we calculate the percentage of second person pronouns in both text types and see whether private dialogues contain more of these second person pronouns than scripted monologues (i.e. speeches).</p>
<pre><code>Private dialogues   Scripted monologues</code></pre>
<p>Second person pronouns (you, your) 6761 659 Other words 259625 105295 Percent 2.60 0.63</p>
<p>This simple example shows that second person pronouns make up 2.6 percent of all words that are used in private dialogues while they only amount to 0.63 percent in scripted speeches.</p>
</div>
<div id="term-document-matrices" class="section level1">
<h1><span class="header-section-number">5</span> Term-Document Matrices</h1>
<p>Term-Document Matrices are an extension of word frequencies.</p>
</div>
<div id="collocation" class="section level1">
<h1><span class="header-section-number">6</span> Collocation</h1>
<p>Collocation refers to the co-occurrence of words. A typical example of a collocation is “Merry Christmas” because the words merry and Christmas occur together more frequently together than would be expected by chance, if words were just randomly stringed together. Collocations are not only an important concept in language teaching but they are also fundamental in Text Analysis and many other research areas working with language data. In addition, words that collocate do not have to be immediately adjacent but can also encompass several slots. For instance,</p>
</div>
<div id="tagging-and-annotation" class="section level1">
<h1><span class="header-section-number">7</span> Tagging and Annotation</h1>
<p>Tagging or annotation refers to a process in which information is added to existing text. The annotation can be very different depending on the task at hand. The most common type of annotation when it comes to language data is part-of-speech tagging where the word class is determined for each word in a text and the word class is then added to the word as a tag. However, there are many different ways to tag or annotate texts. Sentiment Analysis, for instance, also annotates texts or words with respect to its or their emotional value or polarity. In fact, annotation is required in many machine-learning contexts because annotated texts represent a training set on which an algorithm is trained that then predicts for unknown items what values they would most likely be assigned if the annotation were done manually.</p>
<div id="part-of-speech-tagging-pos-tagging" class="section level2">
<h2><span class="header-section-number">7.1</span> Part-of-speech tagging (pos tagging)</h2>
<p>For many analyses that use language data it is useful or even important to differentiate between different parts of speech. In order to determine the word class of a certain word, we use a procedure which is called part-of-speech tagging or pos-tagging for short.</p>
<p><img src="textanalysis_files/figure-html/unnamed-chunk-1-1.png" width="384" /> Figure 3: Example of the first lines of the pos-tagged ICE Ireland.</p>
</div>
<div id="syntactic-parsing" class="section level2">
<h2><span class="header-section-number">7.2</span> Syntactic Parsing</h2>
<p>Parsing refers to another type of annotation in which typically syntactic relations or functions are added to text. Parsing builds on PoS-tagging and allows drawing syntactic trees or dependencies.</p>
<p><img src="textanalysis_files/figure-html/unnamed-chunk-2-1.png" width="384" /></p>
<p>The parsed information can then be used to create e.g. syntax trees.</p>
<p><img src="textanalysis_files/figure-html/unnamed-chunk-3-1.png" width="384" /></p>
</div>
</div>
<div id="text-classification" class="section level1">
<h1><span class="header-section-number">8</span> Text Classification</h1>
<p>Text classification refers to methods that allow to classify a given text to a predefined set of languages, genres, authors, or the like. Such classifications are typically based on the relative frequency of word classes, key words, phonemes, or other linguistic features such as average sentence length, words per line, etc.</p>
<p>As with most other methods that are used in text analysis, text classification typically builds upon a training set that is already annotated with the required tags. Training sets and the features that are derived from these training sets can be created by oneself or one can use build in training sets that are provided in the respective software packages or tools.</p>
<div id="practical-example-1" class="section level2">
<h2><span class="header-section-number">8.1</span> Practical example</h2>
<p>In the following, we will use the frequency of phonemes to classify a text. In a first step, we read in a German text, and split it into phonemes.</p>
<pre class="r"><code># read in German text
German &lt;- readLines(&quot;D:\\Uni\\UQ\\LADAL\\SLCLADAL.github.io\\data/phonemictext1.txt&quot;)
# clean text
German &lt;- gsub(&quot; &quot;, &quot;&quot;, German)
# split text into phonemes
German &lt;- strsplit(German, &quot;&quot;)
# unlist and convert into vector
German &lt;- as.vector(unlist(German))
# inspect data
head(German)</code></pre>
<pre><code>## [1] &quot;?&quot; &quot;a&quot; &quot;l&quot; &quot;s&quot; &quot;h&quot; &quot;E&quot;</code></pre>
<p>We now do the same for three other texts - an English and a Spanish text as well as one text in a language that we will determine using classification.</p>
<pre class="r"><code># read in texts
English &lt;- readLines(&quot;D:\\Uni\\UQ\\LADAL\\SLCLADAL.github.io\\data/phonemictext2.txt&quot;)
Spanish &lt;- readLines(&quot;D:\\Uni\\UQ\\LADAL\\SLCLADAL.github.io\\data/phonemictext3.txt&quot;)
Unknown &lt;- readLines(&quot;D:\\Uni\\UQ\\LADAL\\SLCLADAL.github.io\\data/phonemictext4.txt&quot;)
# clean, split texts into phonemes, unlist and convert them into vectors
English &lt;- as.vector(unlist(strsplit(gsub(&quot; &quot;, &quot;&quot;, English), &quot;&quot;)))
Spanish &lt;- as.vector(unlist(strsplit(gsub(&quot; &quot;, &quot;&quot;, Spanish), &quot;&quot;)))
Unknown &lt;- as.vector(unlist(strsplit(gsub(&quot; &quot;, &quot;&quot;, Unknown), &quot;&quot;)))
# inspect data
head(English)</code></pre>
<pre><code>## [1] &quot;D&quot; &quot;@&quot; &quot;b&quot; &quot;U&quot; &quot;k&quot; &quot;I&quot;</code></pre>
<p>We will now create a table that represents the phonemes and their frequencies in each of the 4 texts. In addition, we will add the language and simpily the column names.</p>
<pre class="r"><code># create data tables
German &lt;- data.frame(names(table(German)), as.vector(table(German)))
English &lt;- data.frame(names(table(English)), as.vector(table(English)))
Spanish &lt;- data.frame(names(table(Spanish)), as.vector(table(Spanish)))
Unknown &lt;- data.frame(names(table(Unknown)), as.vector(table(Unknown)))
# add column with language
German$Language &lt;- &quot;German&quot;
English$Language &lt;- &quot;English&quot;
Spanish$Language &lt;- &quot;Spanish&quot;
Unknown$Language &lt;- &quot;Unknown&quot;
# simlify column names
colnames(German)[1:2] &lt;- c(&quot;Phoneme&quot;, &quot;Frequency&quot;)
colnames(English)[1:2] &lt;- c(&quot;Phoneme&quot;, &quot;Frequency&quot;)
colnames(Spanish)[1:2] &lt;- c(&quot;Phoneme&quot;, &quot;Frequency&quot;)
colnames(Unknown)[1:2] &lt;- c(&quot;Phoneme&quot;, &quot;Frequency&quot;)
# comine all tables into a single table
classdata &lt;- rbind(German, English, Spanish, Unknown) 
# inspect table for English
head(classdata)</code></pre>
<pre><code>##   Phoneme Frequency Language
## 1       -         6   German
## 2       :       569   German
## 3       ?       556   German
## 4       @       565   German
## 5       ¼         6   German
## 6       2         6   German</code></pre>
<p>Now, we group the data so that we see, how often each phoneme is used in each language.</p>
<pre class="r"><code># set options
options(stringsAsFactors = F)
# create wide format
classdatanew &lt;- reshape(classdata, idvar = &quot;Language&quot;, timevar = &quot;Phoneme&quot;,direction = &quot;wide&quot;)
classdw &lt;- t(apply(classdatanew, 1, function(x){ 
  x &lt;- ifelse(is.na(x) == T, 0, x)}))
# simplify column names
colnames(classdw) &lt;- gsub(&quot;Frequency.&quot;, &quot;&quot;, colnames(classdw))
# convert into data frame
classdw &lt;- as.data.frame(classdw)
# inspect data
classdw[, 1:6]</code></pre>
<pre><code>##     Language  -   :   ?   @  ¼
## 1     German  6 569 556 565  6
## 63   English  8 176   0 309  0
## 118  Spanish  5   0   0   0  0
## 168  Unknown 12 286   0 468  0</code></pre>
<p>Now, we need to transform the data again, so that we have the frequency of each phoneme by language as the classifier will use “Language” as the dependent varianble and the phoneme frequencies as predictors.</p>
<pre class="r"><code>numvar &lt;- colnames(classdw)[2:length(colnames(classdw))]
classdw[numvar] &lt;- lapply(classdw[numvar], as.numeric)
# function for normalizing numeric variables
normalize &lt;- function(x) { (x -min(x))/(max(x)-min(x))   }
# apply normalization
 classdw[numvar] &lt;- as.data.frame(lapply(classdw[numvar], normalize))
 # inspect data
classdw[, 1:5]</code></pre>
<pre><code>##     Language         -         : ?         @
## 1     German 0.1428571 1.0000000 1 1.0000000
## 63   English 0.4285714 0.3093146 0 0.5469027
## 118  Spanish 0.0000000 0.0000000 0 0.0000000
## 168  Unknown 1.0000000 0.5026362 0 0.8283186</code></pre>
<p>Before we begin with the actual classification, we will split the data so that we have one data set without “Unknown” (this is our training set) and one data set with only “Unknown” (this is our test set).</p>
<pre class="r"><code>#load library
library(dplyr)
# create training set
train &lt;- classdw %&gt;%
  filter(Language != &quot;Unknown&quot;)
# create test set
test &lt;- classdw %&gt;%
  filter(Language == &quot;Unknown&quot;)
# convert variables
train$Language &lt;- as.factor(train$Language)
train$Language &lt;- as.factor(train$Language)
# inspect data
train[, 1:3]; test[, 1:3]</code></pre>
<pre><code>##   Language         -         :
## 1   German 0.1428571 1.0000000
## 2  English 0.4285714 0.3093146
## 3  Spanish 0.0000000 0.0000000</code></pre>
<pre><code>##   Language -         :
## 1  Unknown 1 0.5026362</code></pre>
<p>Finally, we can apply our classifier to our data. The classifier we use is a k-nearest neighbor classifier as the underlying function will classify an unknown element given its proximity to the clusters in the training set.</p>
<pre class="r"><code># activate library
library(&quot;class&quot;)
# apply k-nearest-neighbor (knn) classifier
prediction &lt;- knn(train[2:ncol(train)], test[2:ncol(train)], cl = train[, 1], k = 3)
# inspect the result
prediction</code></pre>
<pre><code>## [1] Spanish
## Levels: English German Spanish</code></pre>
<p>Based on the frequenies of phonemes in the unknown text, the knn-classifier predicts that the unknown text is English. This is in fact true as the text is a subsection of the Wikipedia article for Aldous Huxley’s “Brave New World”. The training texts were German, English, and Spanish translations of a subsection of Wikipedia’s article for Hermann Hesse’s “Steppenwolf”.</p>
</div>
</div>
<div id="sentiment-analysis" class="section level1">
<h1><span class="header-section-number">9</span> Sentiment Analysis</h1>
<p>Sentiment Analysis is a cover term for approaches which extract information on emotion or opinion from natural language. Sentiment analyses have been successfully applied to analysis of language data in a wide range of disciplines such as psychology, economics, education, as well as political and social sciences. Commonly sentiment analyses are used to determine the stance of a larger group of speakers towards a given phenomenon such as political candidates or parties, product lines or situations. Crucially, sentiment analyses are employed in these domains because they have advantages compared to alternative methods investigating the verbal expression of emotion. One advantage of sentiment analyses is that the emotion coding of sentiment analysis is fully replicable.</p>
<p>Typically, Sentiment Analysis represents a type of classifier only provide information about positive or negative polarity, e.g. whether a tweet is “positive” or “negative”. Therefore, Sentiment Analysis is often regarded as rather coarse-grained and, thus, rather irrelevant for the types of research questions in linguistics.</p>
<p>In the language sciences, Sentiment Analysis can also be a very helpful tool if the type of Sentiment Analysis provides more fine-grained information. In the following, we will perform such a information-rich Sentiment Analysis. The Sentiment Analysis used here does not only provide information about polarity but it will also provide association values for eight core emotions.</p>
<p>The more fine-grained output is made possible by relying an the Word-Emotion Association Lexicon (Mohammad &amp; Turney 2013), which comprises 10,170 terms, and in which lexical elements are assigned scores based on ratings gathered through the crowd-sourced Amazon Mechanical Turk service. For the Word-Emotion Association Lexicon raters were asked whether a given word was associated with one of eight emotions. The resulting associations between terms and emotions are based on 38,726 ratings from 2,216 raters who answered a sequence of questions for each word which were then fed into the emotion association rating (cf. Mohammad &amp; Turney 2013). Each term was rated 5 times. For 85 percent of words, at least 4 raters provided identical ratings. For instance, the word “cry” or “tragedy” are more readily associated with SADNESS while words such as “happy” or “beautiful” are indicative of JOY and words like “fit” or “burst” may indicate ANGER. This means that the sentiment analysis here allows us to investigate the expression of certain core emotions rather than merely classifying statements along the lines of a crude positive-negative distinction.</p>
<div id="practical-example-2" class="section level2">
<h2><span class="header-section-number">9.1</span> Practical example</h2>
<p>In the following, we will perform two sentiment analses. In the first example, we will investigate the emotionality of three different novels while we will analyse how the sentiment changes throughout the course of a anovel in the second example.</p>
<p>We will start with the first example and load five pieces of literature.</p>
<pre class="r"><code># read in texts
darwin &lt;- readLines(&quot;D:\\Uni\\UQ\\LADAL\\SLCLADAL.github.io\\data/origindarwin.txt&quot;)
twain &lt;- readLines(&quot;D:\\Uni\\UQ\\LADAL\\SLCLADAL.github.io\\data/twainhuckfinn.txt&quot;)
orwell &lt;- readLines(&quot;D:\\Uni\\UQ\\LADAL\\SLCLADAL.github.io\\data/orwell.txt&quot;)
lovecraft &lt;- readLines(&quot;D:\\Uni\\UQ\\LADAL\\SLCLADAL.github.io\\data/lovecraftcolor.txt&quot;)
husband &lt;- readLines(&quot;D:\\Uni\\UQ\\LADAL\\SLCLADAL.github.io\\data/husbandsregret.txt&quot;)</code></pre>
<p>In a next step, we clean the data, convert it to lower case, and split it into individual words.</p>
<pre class="r"><code># clean and split files into words 
darwin &lt;- tolower(as.vector(unlist(strsplit(paste(gsub(&quot; {2,}&quot;, &quot; &quot;, darwin), sep = &quot; &quot;), &quot; &quot;))))
twain &lt;- tolower(as.vector(unlist(strsplit(paste(gsub(&quot; {2,}&quot;, &quot; &quot;, twain), sep = &quot; &quot;), &quot; &quot;))))
orwell &lt;- tolower(as.vector(unlist(strsplit(paste(gsub(&quot; {2,}&quot;, &quot; &quot;, orwell), sep = &quot; &quot;), &quot; &quot;))))
lovecraft &lt;- tolower(as.vector(unlist(strsplit(paste(gsub(&quot; {2,}&quot;, &quot; &quot;, lovecraft), sep = &quot; &quot;), &quot; &quot;))))
husband &lt;- tolower(as.vector(unlist(strsplit(paste(gsub(&quot; {2,}&quot;, &quot; &quot;, husband), sep = &quot; &quot;), &quot; &quot;))))</code></pre>
<p>Now, we extract samples from each data set.</p>
<pre class="r"><code>darwin &lt;- sample(darwin, 5000, replace = F)
twain &lt;- sample(twain, 5000, replace = F)
orwell &lt;- sample(orwell, 5000, replace = F)
lovecraft &lt;- sample(lovecraft, 5000, replace = F)
husband &lt;- sample(husband, 5000, replace = F)</code></pre>
<p>We now load the “syuzhet” package and apply the “get_nrc_sentiment” function to the data which performs the Sentiment Analysis.</p>
<pre class="r"><code># load library
library(syuzhet)
# perform sentiment analysis
darwinemo &lt;- get_nrc_sentiment(darwin)
twainemo &lt;- get_nrc_sentiment(twain)
orwellemo &lt;- get_nrc_sentiment(orwell)
lovecraftemo &lt;- get_nrc_sentiment(lovecraft)
husbandemo &lt;- get_nrc_sentiment(husband)
# inspect data
head(darwinemo)</code></pre>
<pre><code>##   anger anticipation disgust fear joy sadness surprise trust negative
## 1     0            0       0    0   0       0        0     0        0
## 2     0            0       0    0   0       0        0     0        0
## 3     0            0       0    0   0       0        0     0        0
## 4     0            0       0    0   0       0        0     1        0
## 5     0            0       0    0   0       0        0     0        0
## 6     0            0       0    0   0       0        0     0        0
##   positive
## 1        0
## 2        0
## 3        0
## 4        1
## 5        0
## 6        0</code></pre>
<p>After performing the Sentiment Analysis, we prepare the data for visualizations</p>
<pre class="r"><code># extract percentages of emotional words
darwinemos &lt;- colSums(darwinemo)/50
twainemos &lt;- colSums(twainemo)/50
orwellemos &lt;- colSums(orwellemo)/50
lovecraftemos &lt;- colSums(lovecraftemo)/50
husbandemos &lt;- colSums(husbandemo)/50
# collapse into a single table
emolit &lt;- data.frame(darwinemos, twainemos, orwellemos, lovecraftemos, husbandemos)
# transpose data
emo &lt;- t(emolit)
# clean row names
rownames(emo) &lt;- gsub(&quot;emos&quot;, &quot;&quot;, rownames(emo))
# inspect data
head(emo)</code></pre>
<pre><code>##           anger anticipation disgust fear  joy sadness surprise trust
## darwin     0.88         2.06    0.46 1.62 1.36    1.44     1.08  2.90
## twain      1.24         2.20    0.76 1.50 1.88    1.20     1.36  2.28
## orwell     1.38         2.22    1.28 2.16 1.56    1.90     0.96  2.48
## lovecraft  1.84         1.82    1.62 2.70 1.10    2.34     1.00  1.52
## husband    2.08         2.38    1.16 2.34 2.16    2.30     1.22  2.32
##           negative positive
## darwin        2.80     4.44
## twain         2.44     3.70
## orwell        3.72     3.76
## lovecraft     4.78     2.68
## husband       4.46     4.38</code></pre>
<pre class="r"><code>#convert into data frame
emo &lt;- as.data.frame(emo)
# add author column
emo$Author &lt;- c(&quot;Darwin&quot;, &quot;Twain&quot;, &quot;Orwell&quot;, &quot;Lovecraft&quot;, &quot;Husband&quot;)
# load library
library(tidyr)
# convert data from wide to long
emol &lt;- gather(emo, Emotion, Score, anger:positive, factor_key=TRUE)
# inspect data
head(emol)</code></pre>
<pre><code>##      Author      Emotion Score
## 1    Darwin        anger  0.88
## 2     Twain        anger  1.24
## 3    Orwell        anger  1.38
## 4 Lovecraft        anger  1.84
## 5   Husband        anger  2.08
## 6    Darwin anticipation  2.06</code></pre>
<pre class="r"><code># load library
library(ggplot2)
# extract subset
emol2 &lt;- emol %&gt;%
  filter(Emotion != &quot;positive&quot;) %&gt;%
  filter(Emotion != &quot;negative&quot;)
# start plot
ggplot(emol2,                   # plot barplotdatagg1
       aes(Emotion, Score,      # define x- and y-axis
           fill = Author)) +    # define grouping variable
  geom_bar(stat=&quot;identity&quot;,     # determine type of plot
           position=position_dodge()) +  # determine grouping
  scale_fill_manual(values=c(&quot;goldenrod2&quot;, &quot;gray70&quot;, &quot;indianred4&quot;, &quot;grey30&quot;, &quot;lightblue&quot;)) +                 # define colours
  theme_bw()                    # define theme (black and white)</code></pre>
<p><img src="textanalysis_files/figure-html/unnamed-chunk-17-1.png" width="672" /></p>
</div>
</div>
<div id="entity-extraction" class="section level1">
<h1><span class="header-section-number">10</span> Entity Extraction</h1>
<p>Entity Extraction is a process during which textual elements which have characteristics that are common to proper nouns (locations, people, organizations, etc.) rather than other parts of speech, e.g. non-sentence initial capitalization, are extracted from texts. Retrieving entities is common in automated summarization and in Topic Modelling. Entity extraction can be achieved by simple feature extraction (e.g. extract all non-sentence initial capitalized words) or with the help of training sets. Using training sets, i.e. texts that are annotated for entities and non-entities, achieves better results when dealing with unknown data and data with inconsistent capitalization.</p>
</div>
<div id="topic-modelling" class="section level1">
<h1><span class="header-section-number">11</span> Topic Modelling</h1>
<p>Topic Modelling is a procedure that allows to extract clusters of key words. These key word clusters can represent topics that texts deal with. This procedure builds on word frequencies and correlations between word frequencies.</p>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
