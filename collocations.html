<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />


<meta name="author" content="Martin Schweinberger" />

<meta name="date" content="2020-04-15" />

<title>Analyzing co-occurrences and collocations using R</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cosmo.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<link href="site_libs/font-awesome-5.1.0/css/all.css" rel="stylesheet" />
<link href="site_libs/font-awesome-5.1.0/css/v4-shims.css" rel="stylesheet" />

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>




<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #ffffff;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">LADAL</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    <span class="fa fa-play-circle"></span>
     
    Basics
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li class="dropdown-header">Basics</li>
    <li>
      <a href="introcomputer.html">General Tips on Computering</a>
    </li>
    <li class="divider"></li>
    <li>
      <a href="introquant.html">Introduction To Quantitative Reasoning</a>
    </li>
    <li>
      <a href="basicquant.html">Basic Concepts In Quantitative Reasoning</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    <span class="fa fa-gear"></span>
     
    Data Processing
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li class="dropdown-header">Data Processing</li>
    <li>
      <a href="intror.html">Basics: Getting started with R</a>
    </li>
    <li>
      <a href="introloading.html">Loading and saving data</a>
    </li>
    <li>
      <a href="stringprocessing.html">String processing</a>
    </li>
    <li>
      <a href="regularexpressions.html">Regular expressions</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    <span class="fa fa-bar-chart"></span>
     
    Visualization
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li class="dropdown-header">Visualization</li>
    <li>
      <a href="basicgraphs.html">Visualizing Data with R</a>
    </li>
    <li>
      <a href="maps.html">Geo-Spatial Data Visualization in R</a>
    </li>
    <li>
      <a href="motion.html">Motion Charts in R</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    <span class="fa fa-eye"></span>
     
    Statistics
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li class="dropdown-header">Statistics</li>
    <li>
      <a href="descriptivestatz.html">Descriptive Statistics</a>
    </li>
    <li class="divider"></li>
    <li class="dropdown-header">Basic Interential Statistics</li>
    <li>
      <a href="basicstatz.html">Basic Inferential Tests</a>
    </li>
    <li>
      <a href="basicstatzchi.html">The Chi-Square Family</a>
    </li>
    <li>
      <a href="basicstatzregression.html">Simple Linear Regression</a>
    </li>
    <li class="divider"></li>
    <li class="dropdown-header">Advanced Interential Statistics</li>
    <li>
      <a href="fixedregressions.html">Fixed-Effects Regression Models</a>
    </li>
    <li>
      <a href="mixedregressions.html">Mixed-Effects Regression Models</a>
    </li>
    <li>
      <a href="advancedstatztrees.html">Tree-Based Models</a>
    </li>
    <li>
      <a href="groupingstatz.html">Classification</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    <span class="fa fa-bars"></span>
     
    Text Analytics
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="textanalysis.html">Text Analysis and Distant Reading</a>
    </li>
    <li>
      <a href="webcrawling.html">Web Crawling</a>
    </li>
    <li>
      <a href="basicnetwork.html">Network Analysis</a>
    </li>
    <li>
      <a href="collocations.html">Co-occurrence and Collocation Analysis</a>
    </li>
    <li>
      <a href="topicmodels.html">Topic Modeling</a>
    </li>
    <li>
      <a href="classification.html">Classification</a>
    </li>
    <li>
      <a href="tagging.html">Tagging and Parsing</a>
    </li>
    <li>
      <a href="corplingr.html">Corpus Linguistics</a>
    </li>
  </ul>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="about.html">
    <span class="fa fa-info"></span>
     
    Contact
  </a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Analyzing co-occurrences and collocations using R</h1>
<h4 class="author">Martin Schweinberger</h4>
<h4 class="date">2020-04-15</h4>

</div>


<p><img src="images/uq1.jpg" width="100%" /></p>
<div id="introduction" class="section level1">
<h1><span class="header-section-number">1</span> Introduction</h1>
<p>This tutorial introduces collocation and co-occurrence analysis with <em>R</em> and shows how to extract and visualize semantic links between words. Parts of this tutorial build on <span class="citation">Wiedemann and Niekler (<a href="#ref-wiedemann2017textmining" role="doc-biblioref">2017</a>)</span>. The code for this stutorial can be downloaded <a href="https://slcladal.github.io/rscripts/collocations.r">here</a>.</p>
<p>How would you find words that are associated with a specific term and how can you visualize such word nets? This tutorial addresses this issue by focusing on co-occurrence and collocations of words. Collocations are words that occur very frequently together. For example, <em>Merry Christmas</em> is a collocation because <em>merry</em> and <em>Christmas</em> occur more frequently together than would be expected by chance. This means that if you were to shuffle all words in a corpus and would then test the frequency of how often <em>merry</em> and <em>Christmas</em> co-occurred, they would occur significantly less often in the shuffled or randomized corpus than in a corpus that contain non-shuffled natural speech.</p>
<p>But how can you determine if words occur more frequently together than would be expected by chance? This tutorial will answer this question.</p>
<p><strong>Preparation and session set up</strong></p>
<p>As all calculations and visualizations in this tutorial rely on <em>R</em>, it is necessary to install <em>R</em> and <em>RStudio</em>. If these programs (or, in the case of <em>R</em>, environments) are not already installed on your machine, please search for them in your favorite search engine and add the term <em>download</em>. Open any of the first few links and follow the installation instructions (they are easy to follow, do not require any specifications, and are pretty much self-explanatory).</p>
<p>In addition, certain <em>packages</em> need to be installed so that the scripts shown below are executed without errors. Before turning to the code below, please install the packages by running the code below this paragraph. If you have already installed the packages mentioned below, then you can skip ahead ignore this section. To install the necessary packages, simply run the following code - it may take some time (between 1 and 5 minutes to install all of the packages so you do not need to worry if it takes some time).</p>
<pre class="r"><code># clean current workspace
rm(list=ls(all=T))
# set options
options(stringsAsFactors = F)
options(scipen = 999)
options(max.print=10000)
# install packages
install.packages(&quot;corpus&quot;, &quot;dplyr&quot;, &quot;quanteda&quot;, &quot;stringr&quot;, &quot;tidyr&quot;, &quot;tm&quot;)
# for visualization
install.packages(&quot;GGally&quot;, &quot;network&quot;, &quot;sna&quot;, &quot;ggplot2&quot;)
# for statistics
install.packages(&quot;collostructions&quot;)</code></pre>
<p>Once you have installed <em>R</em> and <em>R-Studio</em>, and have also initiated the session by executing the code shown above, you are good to go.</p>
</div>
<div id="visualizing-collocations-of-america-in-trumps-2020-sotu-address" class="section level1">
<h1><span class="header-section-number">2</span> Visualizing Collocations of <em>America</em> in Trump’s 2020 SOTU Address</h1>
<p>In the following example, we will analyze which words collocate with the term <em>america</em> in Trump’s 2020 State of the Union address. The analysis consists of the following steps:</p>
<ul>
<li><p>Data processing</p></li>
<li><p>Creating a co-occurence matrix</p></li>
<li><p>Finding significant collocations</p></li>
<li><p>Extracting the network of collocations</p></li>
<li><p>Visualizing the network of collocations</p></li>
</ul>
<div id="data-processing" class="section level2">
<h2><span class="header-section-number">2.1</span> Data processing</h2>
<p>We start by initiating the session and loading the packages from the R library that we are going to use in this tutorial.</p>
<pre class="r"><code># clean current workspace
rm(list=ls(all=T))
# set options
options(stringsAsFactors = F)
options(scipen = 999)
options(max.print=10000)
# load packages
# for data porcessing
library(corpus)
library(dplyr)
library(quanteda)
library(stringr)
library(tidyr)
library(tm)
# for visualization
library(GGally)
library(network)
library(sna)
library(ggplot2)</code></pre>
<p>We continue the analysis by loading the data and defining the key term. The key term forms the center of the collocation network.</p>
<pre class="r"><code># load data
textdata &lt;- readLines(&quot;https://SLCLADAL.github.io/data/sotutrump.txt&quot;, 
                     skipNul = T, encoding = &quot;unknown&quot;)
# define collocterm
collocterm &lt;- &quot;america&quot;
# inspect data
str(textdata)</code></pre>
<pre><code>##  chr [1:257] &quot;THE PRESIDENT:  Thank you very much.  Thank you.  Thank you very much.&quot; ...</code></pre>
<p>The separation of the text into semantic analysis units is important for co-occurrence analysis. Context windows can be for instance documents, paragraphs or sentences or neighboring words. One of the most frequently used context window is the sentence. The sentence segmentation must take place <em>before</em> the other preprocessing steps because the sentence-segmentation-model relies on intact word forms and punctuation marks.</p>
<pre class="r"><code># clean data
textdata &lt;- textdata %&gt;%
  stringr::str_replace_all(&quot;AUDIENCE:.*&quot;, &quot;&quot;) %&gt;%
  stringr::str_replace_all(&quot; {2,}&quot;, &quot; &quot;) %&gt;%
  stringr::str_replace_all(&quot;THE PRESIDENT:&quot;, &quot;&quot;) %&gt;%
  stringr::str_replace_all(&quot;Mr.&quot;, &quot;Mr&quot;) %&gt;%
  stringr::str_replace_all(&quot;\\(applause.{0,1}\\)&quot;, &quot;&quot;) %&gt;%
  stringr::str_replace_all(&quot;\\(APPLAUSE.{0,1}\\)&quot;, &quot;&quot;) %&gt;%
  paste(collapse = &quot; &quot;) %&gt;%
  str_squish() %&gt;%
  stripWhitespace()
# split text into sentences
sentences &lt;- unlist(strsplit(as.character(textdata), &quot;(?&lt;=\\.)\\s(?=[A-Z])&quot;, perl = T))
# inspect data
head(sentences)</code></pre>
<pre><code>## [1] &quot;Thank you very much.&quot;                                                                                                                                                                                                                                                                    
## [2] &quot;Thank you.&quot;                                                                                                                                                                                                                                                                              
## [3] &quot;Thank you very much.&quot;                                                                                                                                                                                                                                                                    
## [4] &quot;Madam Speaker, Mr Vice President, members of Congress, the First Lady of the United States — — and my fellow citizens: Three years ago, we launched the great American comeback.&quot;                                                                                                        
## [5] &quot;Tonight, I stand before you to share the incredible results.&quot;                                                                                                                                                                                                                            
## [6] &quot;Jobs are booming, incomes are soaring, poverty is plummeting, crime is falling, confidence is surging, and our country is thriving and highly respected again. (Applause.) America’s enemies are on the run, America’s fortunes are on the rise, and America’s future is blazing bright.&quot;</code></pre>
<p>A word of warning is in order here: the newly decomposed corpus has now reached a considerable size of 268 sentences. Older computers may get in trouble because of insufficient memory during this preprocessing step.</p>
<p>Now we going to implement a pre-processing chain and apply it on the separated sentences. Preprocessing consists of cleaning the data by removing punctuation, numbers, superfluous white spaces, and so-called stop words which do not have semantic meaning.</p>
<pre class="r"><code># convert to lower case
sentencesclean &lt;- sentences %&gt;%
  tolower() %&gt;%
  # remove punctuation
  removePunctuation() %&gt;%
  # remove non alphanumeric characters
  stringr::str_replace_all(&quot;[^[:alnum:][:space:]_]&quot;, &quot;&quot;) %&gt;%
  # remove numbers
  removeNumbers()
# remove stop words
english_stopwords &lt;- readLines(&quot;https://slcladal.github.io/resources/stopwords_en.txt&quot;, encoding = &quot;UTF-8&quot;)
sentencesclean &lt;- removeWords(sentencesclean, english_stopwords) %&gt;%
  # strip white spaces
  str_squish() %&gt;%
  stripWhitespace()
# remove emplty elements
sentencesclean &lt;- sentencesclean[sentencesclean != &quot;&quot;]
# stem words
sentencesclean &lt;- as.vector(sapply(sentencesclean, function(x){
  x &lt;- text_tokens(x, stemmer = &quot;en&quot;)
  x &lt;- as.vector(unlist(x))
  x &lt;- stringr::str_c(x, sep = &quot; &quot;, collapse = &quot; &quot;)}))
# select a sample
#sentencesclean &lt;- sample(sentencesclean, 100)
# inspect data
head(sentencesclean)</code></pre>
<pre><code>## [1] &quot;madam speaker mr vice presid member congress ladi unit state fellow citizen year ago launch great american comeback&quot;                                                      
## [2] &quot;tonight stand share incred result&quot;                                                                                                                                        
## [3] &quot;job boom incom soar poverti plummet crime fall confid surg countri thrive high respect applaus america enemi run america fortun rise america futur blaze bright&quot;          
## [4] &quot;year econom decay applaus day countri advantag scorn nation long applaus broken promis jobless recoveri tire platitud constant excus deplet american wealth power prestig&quot;
## [5] &quot;short year shatter mental american declin reject downsiz america destini&quot;                                                                                                 
## [6] &quot;total reject downsiz&quot;</code></pre>
</div>
<div id="creating-a-co-occurrence-matrix" class="section level2">
<h2><span class="header-section-number">2.2</span> Creating a co-occurrence matrix</h2>
<p>In a next step, we create a co-occurence matrix from the words in the sentences.</p>
<pre class="r"><code>sentencecorpus &lt;- Corpus(VectorSource(sentencesclean))
DTM &lt;- DocumentTermMatrix(sentencecorpus, control=list(bounds = list(global=c(1, Inf)), weighting = weightBin)) 
# Convert to sparseMatrix matrix
require(Matrix)
DTM &lt;- sparseMatrix(i = DTM$i, 
                    j = DTM$j, 
                    x = DTM$v, 
                    dims = c(DTM$nrow, DTM$ncol), 
                    dimnames = dimnames(DTM))
# Matrix multiplication for co-occurrence counts
coocCounts &lt;- t(DTM) %*% DTM 
cooc &lt;- as.matrix(coocCounts)
cooc[1:5, 1:5]</code></pre>
<pre><code>##          ago american citizen comeback congress
## ago       12        2       1        1        2
## american   2       45       2        1        8
## citizen    1        2       8        1        2
## comeback   1        1       1        1        1
## congress   2        8       2        1       12</code></pre>
<p>The matrix has <code>nrow(cooc)</code> rows and columns and is symmetric. Each cell contains the number of joint occurrences. In the diagonal, the frequencies of single occurrences of each term are encoded.</p>
<p>The part of the co-occurrence matrix that is displayed shows that comeback appears together 1 times with congress in the 1222 sentences of the SUTO addresses. comeback alone occurs 1 times.</p>
<p>We can also use this co-occurrence table to create a co-occurence graph.</p>
<pre class="r"><code>simplecooctb &lt;- cooc[rownames(cooc) == &quot;american&quot;,]
simplecoocdf &lt;- data.frame(rep(&quot;american&quot;, length(simplecooctb)),
                           names(simplecooctb),
                           simplecooctb)
colnames(simplecoocdf) &lt;- c(&quot;Term&quot;, &quot;Cooc&quot;, &quot;Freq&quot;)
# inspect results
head(simplecoocdf)</code></pre>
<pre><code>##              Term     Cooc Freq
## ago      american      ago    2
## american american american   45
## citizen  american  citizen    2
## comeback american comeback    1
## congress american congress    8
## fellow   american   fellow    2</code></pre>
<pre class="r"><code>simplecooc &lt;- simplecoocdf %&gt;%
  dplyr::select(Term, Cooc, Freq)
simplecoocterms &lt;- rep(simplecooc$Term, simplecooc$Freq)
simplecooccoocs &lt;- rep(simplecooc$Cooc, simplecooc$Freq)
simplecoocdf &lt;- data.frame(simplecoocterms, simplecooccoocs)
colnames(simplecoocdf) &lt;- c(&quot;Term&quot;, &quot;CoocTerm&quot;)
simplecoocnet &lt;- network(simplecoocdf,
              directed = FALSE,
              ignore.eval = FALSE,
              names.eval = &quot;weights&quot;
              )
ggnet2(simplecoocnet, 
       size = 6, 
       color = &quot;goldenrod&quot;, 
       edge.size = .5,
       edge.color = &quot;lightgrey&quot;, 
       label = TRUE, 
       label.size = 3)</code></pre>
<p><img src="collocations_files/figure-html/coll_01_13-1.png" width="672" /></p>
<p>However, this network is not really useful as it does not distinguish between co-occurences and collocates, i.e. terms that occur significantly more frequently with the keyterm than would be expected by chance. Therefore, we now proceed to extract collocations.</p>
</div>
<div id="finding-significant-collocates" class="section level2">
<h2><span class="header-section-number">2.3</span> Finding significant collocates</h2>
<p>In order to identify which words occur together more frequently than would be expected by chance, we have to determine if their co-occurence frequency is statistical significant.</p>
<p>In a first step, we will determine which terms collocate significantly with the key term we are intersted in. In a second step, we will extract the network of collocations around this key term.</p>
<p>To determine which terms collocate significantly with the key term ("america), we will use multiple Fisher’s Exact tests which require the following information:</p>
<ul>
<li><p>a = Number of times <code>coocTerm</code> occurs with term j</p></li>
<li><p>b = Number of times <code>coocTerm</code> occurs without term j</p></li>
<li><p>c = Number of times other terms occur with term j</p></li>
<li><p>d = Number of terms that are not <code>coocTerm</code> or term j</p></li>
</ul>
<p>In a first step, we create a table which holds these quantities.</p>
<pre class="r"><code>coocdf &lt;- as.data.frame(as.matrix(cooc))
cooctb &lt;- coocdf %&gt;%
  dplyr::mutate(Term = rownames(coocdf)) %&gt;%
  tidyr::gather(CoocTerm, TermCoocFreq,
                colnames(coocdf)[1]:colnames(coocdf)[ncol(coocdf)]) %&gt;%
  dplyr::mutate(Term = factor(Term),
                CoocTerm = factor(CoocTerm)) %&gt;%
  dplyr::mutate(AllFreq = sum(TermCoocFreq)) %&gt;%
  dplyr::group_by(Term) %&gt;%
  dplyr::mutate(TermFreq = sum(TermCoocFreq)) %&gt;%
  dplyr::ungroup(Term) %&gt;%
  dplyr::group_by(CoocTerm) %&gt;%
  dplyr::mutate(CoocFreq = sum(TermCoocFreq)) %&gt;%
  dplyr::arrange(Term) %&gt;%
  dplyr::mutate(a = TermCoocFreq,
                b = TermFreq - a,
                c = CoocFreq - a, 
                d = AllFreq - (a + b + c)) %&gt;%
  dplyr::mutate(NRows = nrow(coocdf))
cooctb</code></pre>
<pre><code>## # A tibble: 1,493,284 x 11
## # Groups:   CoocTerm [1,222]
##    Term  CoocTerm TermCoocFreq AllFreq TermFreq CoocFreq     a     b     c     d
##    &lt;fct&gt; &lt;fct&gt;           &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
##  1 aban~ ago                 0   55525       15      262     0    15   262 55248
##  2 aban~ american            0   55525       15      891     0    15   891 54619
##  3 aban~ citizen             0   55525       15      118     0    15   118 55392
##  4 aban~ comeback            0   55525       15       17     0    15    17 55493
##  5 aban~ congress            0   55525       15      352     0    15   352 55158
##  6 aban~ fellow              0   55525       15       27     0    15    27 55483
##  7 aban~ great               0   55525       15      160     0    15   160 55350
##  8 aban~ ladi                0   55525       15       78     0    15    78 55432
##  9 aban~ launch              0   55525       15       79     0    15    79 55431
## 10 aban~ madam               0   55525       15       17     0    15    17 55493
## # ... with 1,493,274 more rows, and 1 more variable: NRows &lt;int&gt;</code></pre>
<p>We now select the term for which we want to check the collocation network. In this example, we want to analyze the network of <code>collocterm</code>. Thus, we remove all rows from the data that do not involve <code>collocterm</code>.</p>
<pre class="r"><code>cooctb_redux &lt;- cooctb %&gt;%
  dplyr::filter(Term == collocterm)</code></pre>
<p>Next, we calculate which terms are collocating with <code>collocterm</code> but we also test which terms are not used with <code>collocterm</code>.</p>
<pre class="r"><code>coocStatz &lt;- cooctb_redux %&gt;%
  dplyr::rowwise() %&gt;%
  dplyr::mutate(p = as.vector(unlist(fisher.test(matrix(c(a, b, c, d), 
                                                        ncol = 2, byrow = T))[1]))) %&gt;%
    dplyr::mutate(x2 = as.vector(unlist(chisq.test(matrix(c(a, b, c, d),                                                           ncol = 2, byrow = T))[1]))) %&gt;%
  dplyr::mutate(phi = sqrt((x2/(a + b + c + d)))) %&gt;%
      dplyr::mutate(expected = as.vector(unlist(chisq.test(matrix(c(a, b, c, d), ncol = 2, byrow = T))$expected[1]))) %&gt;%
  dplyr::mutate(Significance = ifelse(p &lt;= .05, &quot;p&lt;.05&quot;,
                               ifelse(p &lt;= .01, &quot;p&lt;.01&quot;,
                               ifelse(p &lt;= .001, &quot;p&lt;.001&quot;, &quot;n.s.&quot;))))
# inspect results
coocStatz</code></pre>
<pre><code>## Source: local data frame [1,222 x 16]
## Groups: &lt;by row&gt;
## 
## # A tibble: 1,222 x 16
##    Term  CoocTerm TermCoocFreq AllFreq TermFreq CoocFreq     a     b     c     d
##    &lt;fct&gt; &lt;fct&gt;           &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
##  1 amer~ ago                 2   55525      594      262     2   592   260 54671
##  2 amer~ american           10   55525      594      891    10   584   881 54050
##  3 amer~ citizen             3   55525      594      118     3   591   115 54816
##  4 amer~ comeback            0   55525      594       17     0   594    17 54914
##  5 amer~ congress            6   55525      594      352     6   588   346 54585
##  6 amer~ fellow              0   55525      594       27     0   594    27 54904
##  7 amer~ great               2   55525      594      160     2   592   158 54773
##  8 amer~ ladi                0   55525      594       78     0   594    78 54853
##  9 amer~ launch              1   55525      594       79     1   593    78 54853
## 10 amer~ madam               0   55525      594       17     0   594    17 54914
## # ... with 1,212 more rows, and 6 more variables: NRows &lt;int&gt;, p &lt;dbl&gt;,
## #   x2 &lt;dbl&gt;, phi &lt;dbl&gt;, expected &lt;dbl&gt;, Significance &lt;chr&gt;</code></pre>
<pre class="r"><code>coocStatz &lt;- coocStatz %&gt;%
  dplyr::ungroup() %&gt;%
  dplyr::arrange(p) %&gt;%
  dplyr::mutate(j = 1:n()) %&gt;%
  # perform benjamini-holm correction
  dplyr::mutate(corr05 = ((j/NRows)*0.05)) %&gt;%
  dplyr::mutate(corr01 = ((j/NRows)*0.01)) %&gt;%
  dplyr::mutate(corr001 = ((j/NRows)*0.001)) %&gt;%
  # calculate corrected significance status
  dplyr::mutate(CorrSignificance = ifelse(p &lt;= corr001, &quot;p&lt;.001&quot;,
                ifelse(p &lt;= corr01, &quot;p&lt;.01&quot;,
                       ifelse(p &lt;= corr05, &quot;p&lt;.05&quot;, &quot;n.s.&quot;)))) %&gt;%
  dplyr::mutate(p = round(p, 6)) %&gt;%
  dplyr::mutate(x2 = round(x2, 1)) %&gt;%
  dplyr::mutate(phi = round(phi, 2)) %&gt;%
  dplyr::arrange(p) %&gt;%
  dplyr::select(-a, -b, -c, -d, -j, -NRows, -corr05, -corr01, -corr001) %&gt;%
  dplyr::mutate(Type = ifelse(expected &gt; TermCoocFreq, &quot;Antitype&quot;, &quot;Type&quot;))
# inspect results
coocStatz</code></pre>
<pre><code>## # A tibble: 1,222 x 13
##    Term  CoocTerm TermCoocFreq AllFreq TermFreq CoocFreq      p    x2   phi
##    &lt;fct&gt; &lt;fct&gt;           &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
##  1 amer~ america            30   55525      594      594 0       86.1  0.04
##  2 amer~ god                 4   55525      594       93 0.0179   6.4  0.01
##  3 amer~ bear                2   55525      594       21 0.0210   7.3  0.01
##  4 amer~ war                 3   55525      594       55 0.0212   6.3  0.01
##  5 amer~ destini             2   55525      594       23 0.0249   6.5  0.01
##  6 amer~ pray                2   55525      594       23 0.0249   6.5  0.01
##  7 amer~ place               4   55525      594      106 0.0274   5    0.01
##  8 amer~ embrac              2   55525      594       25 0.0291   5.7  0.01
##  9 amer~ extraor~            3   55525      594       65 0.0326   4.7  0.01
## 10 amer~ back                5   55525      594      182 0.0468   3.4  0.01
## # ... with 1,212 more rows, and 4 more variables: expected &lt;dbl&gt;,
## #   Significance &lt;chr&gt;, CorrSignificance &lt;chr&gt;, Type &lt;chr&gt;</code></pre>
</div>
<div id="extracting-a-collocation-network" class="section level2">
<h2><span class="header-section-number">2.4</span> Extracting a collocation network</h2>
<p>Now that we have determined which words collocate with the key term (<code>collocterm</code>), we extract the collocation network.</p>
<pre class="r"><code>colloctermtb &lt;- cooctb %&gt;%
  dplyr::filter(Term == collocterm,
                TermCoocFreq &gt; 0)
colloctermtb</code></pre>
<pre><code>## # A tibble: 388 x 11
## # Groups:   CoocTerm [388]
##    Term  CoocTerm TermCoocFreq AllFreq TermFreq CoocFreq     a     b     c     d
##    &lt;fct&gt; &lt;fct&gt;           &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
##  1 amer~ ago                 2   55525      594      262     2   592   260 54671
##  2 amer~ american           10   55525      594      891    10   584   881 54050
##  3 amer~ citizen             3   55525      594      118     3   591   115 54816
##  4 amer~ congress            6   55525      594      352     6   588   346 54585
##  5 amer~ great               2   55525      594      160     2   592   158 54773
##  6 amer~ launch              1   55525      594       79     1   593    78 54853
##  7 amer~ member              2   55525      594      145     2   592   143 54788
##  8 amer~ presid              2   55525      594      125     2   592   123 54808
##  9 amer~ state               5   55525      594      442     5   589   437 54494
## 10 amer~ unit                4   55525      594      349     4   590   345 54586
## # ... with 378 more rows, and 1 more variable: NRows &lt;int&gt;</code></pre>
</div>
<div id="extracting-collocations" class="section level2">
<h2><span class="header-section-number">2.5</span> Extracting collocations</h2>
<p>We now extract terms that collocate with the key term (<code>collocterm</code>).</p>
<pre class="r"><code>coocStatz_redux &lt;- coocStatz %&gt;%
  dplyr::filter(Significance != &quot;n.s.&quot;)
coocStatz_redux </code></pre>
<pre><code>## # A tibble: 10 x 13
##    Term  CoocTerm TermCoocFreq AllFreq TermFreq CoocFreq      p    x2   phi
##    &lt;fct&gt; &lt;fct&gt;           &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
##  1 amer~ america            30   55525      594      594 0       86.1  0.04
##  2 amer~ god                 4   55525      594       93 0.0179   6.4  0.01
##  3 amer~ bear                2   55525      594       21 0.0210   7.3  0.01
##  4 amer~ war                 3   55525      594       55 0.0212   6.3  0.01
##  5 amer~ destini             2   55525      594       23 0.0249   6.5  0.01
##  6 amer~ pray                2   55525      594       23 0.0249   6.5  0.01
##  7 amer~ place               4   55525      594      106 0.0274   5    0.01
##  8 amer~ embrac              2   55525      594       25 0.0291   5.7  0.01
##  9 amer~ extraor~            3   55525      594       65 0.0326   4.7  0.01
## 10 amer~ back                5   55525      594      182 0.0468   3.4  0.01
## # ... with 4 more variables: expected &lt;dbl&gt;, Significance &lt;chr&gt;,
## #   CorrSignificance &lt;chr&gt;, Type &lt;chr&gt;</code></pre>
<pre class="r"><code>coocnet &lt;- cooc[which(rownames(cooc) %in% coocStatz_redux$CoocTerm), which(colnames(cooc) %in% coocStatz_redux$CoocTerm)]
coocnet[1:5, 1:5]</code></pre>
<pre><code>##               america destini back extraordinari place
## america            30       2    5             3     4
## destini             2       3    0             0     1
## back                5       0    9             1     2
## extraordinari       3       0    1             3     1
## place               4       1    2             1     6</code></pre>
</div>
<div id="visualizing-the-collocation-network" class="section level2">
<h2><span class="header-section-number">2.6</span> Visualizing the collocation network</h2>
<p>Now, we can visualize the collocation network of the key term. In a first step, we create a network object.</p>
<pre class="r"><code>collocnet = network(coocnet,
              directed = FALSE,
              ignore.eval = FALSE,
              names.eval = &quot;weights&quot;
              )</code></pre>
<p>Now, we create a simple network of the collocations.</p>
<pre class="r"><code>ggnet2(collocnet, 
       size = 6, 
       color = &quot;goldenrod&quot;, 
       edge.size = .5,
       edge.color = &quot;lightgrey&quot;, 
       label = TRUE, 
       label.size = 3)</code></pre>
<p><img src="collocations_files/figure-html/coll_01_37-1.png" width="672" /></p>
<p>The network only shows the collocates (as nodes) and which of these terms collocate. In a next step, we add information to the graph to show the collocation strength. In order to add information, we inspect the data that is underlying the simple network graph.</p>
<pre class="r"><code>ggnet2(collocnet)$data</code></pre>
<pre><code>##            label alpha  color shape size          x         y
## 1        america     1 grey75    19    9 0.43790844 0.5002818
## 2        destini     1 grey75    19    9 0.89974715 0.4573185
## 3           back     1 grey75    19    9 0.01646705 0.8668169
## 4  extraordinari     1 grey75    19    9 0.14521774 1.0000000
## 5          place     1 grey75    19    9 0.55264614 0.8882082
## 6            war     1 grey75    19    9 0.00000000 0.6930952
## 7           pray     1 grey75    19    9 0.01764997 0.1054978
## 8            god     1 grey75    19    9 0.95862609 0.8704725
## 9           bear     1 grey75    19    9 0.64763441 0.0000000
## 10        embrac     1 grey75    19    9 1.00000000 0.6490009</code></pre>
<p>Next, we modify the size of the icicles which should represent the frequency of the co-occurrence.</p>
<pre class="r"><code># rescale edge size
set.edge.attribute(collocnet, &quot;weights&quot;, ifelse(collocnet %e% &quot;weights&quot; &lt;= 1,
                                                1, 
                                   ifelse(collocnet %e% &quot;weights&quot; &lt;= 3, 2, 3)))
# define line type
set.edge.attribute(collocnet, &quot;lty&quot;, ifelse(collocnet %e% &quot;weights&quot; == 0.25, 3, 
                                      ifelse(collocnet %e% &quot;weights&quot; == .5, 2, 1)))</code></pre>
<p>Now that we have set specified the edge size, and line type we visualize the network again.</p>
<pre class="r"><code>ggnet2(collocnet,
       label = TRUE,
       color = &quot;red&quot;,
       label.size = 3,
       alpha = .5,
       size = &quot;degree&quot;,
       edge.size = &quot;weights&quot;,
       edge.lty = &quot;lty&quot;,
       edge.alpha = .2,
legend.position = &quot;bottom&quot;) +
  guides(color = FALSE, size = FALSE)</code></pre>
<p><img src="collocations_files/figure-html/coll_01_57-1.png" width="672" /></p>
<p>We now have a meaningful collocation network. All of the terms that are shown collocates with <code>collocterm</code> and the thickness of the lines shows how often the key term (<code>collocterm</code>) co-occurs with each of the terms. The size of the icicles shows how frequent the terms are.</p>
</div>
</div>
<div id="analyzing-changes-in-collocation-strength-across-time" class="section level1">
<h1><span class="header-section-number">3</span> Analyzing Changes in Collocation Strength across Time</h1>
<p>This section focuses on changes in collocation strength across apparent time. The example focuses on adjective amplification in Australian English. The issue we will analyse here is whether we can unearth changes in the collocation pattern of adjective amplifiers such as <em>very</em>, <em>really</em>, or <em>so</em>. In other words, we will investigate if amplifiers associate with different adjectives among speakers from different age groups.</p>
<p>In a first step, we activate packages and load the data.</p>
<pre class="r"><code># load packages
library(Rling)
library(dplyr)
library(ggplot2)
# load functions
source(&quot;https://SLCLADAL.github.io/rscripts/collexcovar.R&quot;)
# load data
ampaus &lt;- read.table(&quot;https://SLCLADAL.github.io/data/ampaus.txt&quot;, sep = &quot;\t&quot;, header = T)
# inspect data
str(ampaus)</code></pre>
<pre><code>## &#39;data.frame&#39;:    582 obs. of  3 variables:
##  $ Adjective: chr  &quot;good&quot; &quot;good&quot; &quot;good&quot; &quot;other&quot; ...
##  $ Variant  : chr  &quot;really&quot; &quot;other&quot; &quot;other&quot; &quot;pretty&quot; ...
##  $ Age      : chr  &quot;26-40&quot; &quot;26-40&quot; &quot;26-40&quot; &quot;26-40&quot; ...</code></pre>
<p>The data consists of three variables (Adjective, Variant, and Age).</p>
<p>Now, we perform a co-varying collexeme analysis for really versus other amplifiers. The function takes a data set consisting of three columns labelled keys, colls, and time</p>
<pre class="r"><code># rename data
ampaus &lt;- ampaus %&gt;%
  dplyr::rename(keys = Variant, colls = Adjective, time = Age)
# perform analysis
collexcovar_really &lt;- collexcovar(data = ampaus, keyterm = &quot;really&quot;)
# inspect results
collexcovar_really</code></pre>
<pre><code>## # A tibble: 18 x 12
##    time  colls Freq_key Freq_other Freq_Colls       p    x2   phi expected
##    &lt;chr&gt; &lt;chr&gt;    &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;
##  1 17-25 other       83        144        227 3.50e-5  17.8  0.21  104.   
##  2 17-25 good        53         31         84 5.21e-4  12.8  0.18   38.5  
##  3 17-25 nice        22         17         39 1.79e-1   2    0.07   17.9  
##  4 26-40 other       29         58         87 2.09e-1   2    0.13   32.4  
##  5 41-80 bad          1          0          1 2.46e-1   3.1  0.23    0.246
##  6 41-80 hard         2          2          4 2.50e-1   1.5  0.16    0.982
##  7 26-40 bad          3          2          5 3.59e-1   1.2  0.1     1.86 
##  8 26-40 hard         1          0          1 3.72e-1   1.7  0.12    0.372
##  9 17-25 funny       13         11         24 4.07e-1   0.7  0.04   11.0  
## 10 41-80 other        7         26         33 5.44e-1   0.5  0.09    8.11 
## 11 26-40 good         8         10         18 5.99e-1   0.5  0.06    6.69 
## 12 17-25 bad          9         10         19 1.00e+0   0    0.01    8.70 
## 13 17-25 hard         5          6         11 1.00e+0   0    0       5.04 
## 14 26-40 funny        1          1          2 1.00e+0   0.1  0.03    0.744
## 15 26-40 nice         3          5          8 1.00e+0   0    0       2.98 
## 16 41-80 funny        0          1          1 1.00e+0   0.3  0.08    0.246
## 17 41-80 good         4         12         16 1.00e+0   0    0.01    3.93 
## 18 41-80 nice         0          2          2 1.00e+0   0.7  0.11    0.491
## # ... with 3 more variables: CorrSignificance &lt;chr&gt;, Type &lt;chr&gt;, Variant &lt;chr&gt;</code></pre>
<pre class="r"><code># perform analysis
collexcovar_pretty &lt;- collexcovar(data = ampaus, keyterm = &quot;pretty&quot;)
collexcovar_so &lt;- collexcovar(data = ampaus, keyterm = &quot;so&quot;)
collexcovar_very &lt;- collexcovar(data = ampaus, keyterm = &quot;very&quot;)</code></pre>
<p>For other amplifiers, we have to change the label “other” to “bin” as the function already has a a label “other”. Once we have changed other to bin, we perform the analysis.</p>
<pre class="r"><code>ampaus &lt;- ampaus %&gt;%
  dplyr::mutate(keys = ifelse(keys == &quot;other&quot;, &quot;bin&quot;, keys))
collexcovar_other &lt;- collexcovar(data = ampaus, keyterm = &quot;bin&quot;)</code></pre>
<p>Next, we combine the results of the co-varying collexeme analysis into a single table.</p>
<pre class="r"><code># combine tables
collexcovar_ampaus &lt;- rbind(collexcovar_really, collexcovar_very, 
                     collexcovar_so, collexcovar_pretty, collexcovar_other)
collexcovar_ampaus &lt;- collexcovar_ampaus %&gt;%
  dplyr::rename(Age = time,
                Adjective = colls) %&gt;%
  dplyr::mutate(Variant = ifelse(Variant == &quot;bin&quot;, &quot;other&quot;, Variant)) %&gt;%
  dplyr::arrange(Age)
# inspect results
collexcovar_ampaus</code></pre>
<pre><code>## # A tibble: 90 x 12
##    Age   Adjective Freq_key Freq_other Freq_Colls       p    x2   phi expected
##    &lt;chr&gt; &lt;chr&gt;        &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;
##  1 17-25 other           83        144        227 3.50e-5  17.8  0.21   104.  
##  2 17-25 good            53         31         84 5.21e-4  12.8  0.18    38.5 
##  3 17-25 nice            22         17         39 1.79e-1   2    0.07    17.9 
##  4 17-25 funny           13         11         24 4.07e-1   0.7  0.04    11.0 
##  5 17-25 bad              9         10         19 1.00e+0   0    0.01     8.70
##  6 17-25 hard             5          6         11 1.00e+0   0    0        5.04
##  7 17-25 funny            0         24         24 3.43e-2   4.5  0.1      3.56
##  8 17-25 other           40        187        227 9.06e-2   3.1  0.09    33.7 
##  9 17-25 bad              0         19         19 9.09e-2   3.5  0.09     2.82
## 10 17-25 hard             3          8         11 2.15e-1   1.4  0.06     1.63
## # ... with 80 more rows, and 3 more variables: CorrSignificance &lt;chr&gt;,
## #   Type &lt;chr&gt;, Variant &lt;chr&gt;</code></pre>
<p>We now modify the data set so that we can plot the collocation strength across apparent time.</p>
<pre class="r"><code>ampauscoll &lt;- collexcovar_ampaus %&gt;%
  dplyr::select(Age, Adjective, Variant, Type, phi) %&gt;%
  dplyr::mutate(phi = ifelse(Type == &quot;Antitype&quot;, -phi, phi)) %&gt;%
  dplyr::select(-Type) %&gt;%
  tidyr::spread(Adjective, phi) %&gt;%
  tidyr::replace_na(list(bad = 0,
                         funny = 0,
                         hard = 0,
                         good = 0,
                         nice = 0,
                         other = 0)) %&gt;%
  tidyr::gather(Adjective, phi, bad:other) %&gt;%
  tidyr::spread(Variant, phi) %&gt;%
  tidyr::replace_na(list(pretty = 0,
                    really = 0,
                    so = 0,
                    very = 0,
                    other = 0)) %&gt;%
  tidyr::gather(Variant, phi, other:very)
ampauscoll</code></pre>
<pre><code>## # A tibble: 90 x 4
##    Age   Adjective Variant   phi
##    &lt;chr&gt; &lt;chr&gt;     &lt;chr&gt;   &lt;dbl&gt;
##  1 17-25 bad       other   -0.05
##  2 17-25 funny     other   -0.05
##  3 17-25 good      other   -0.05
##  4 17-25 hard      other    0.04
##  5 17-25 nice      other   -0.07
##  6 17-25 other     other    0.12
##  7 26-40 bad       other    0.02
##  8 26-40 funny     other   -0.06
##  9 26-40 good      other    0.01
## 10 26-40 hard      other   -0.04
## # ... with 80 more rows</code></pre>
<p>In a final step, we visualize the results of our analysis.</p>
<pre class="r"><code>ggplot(ampauscoll, aes(x = reorder(Age, desc(Age)), 
                             y = phi, group = Variant, 
                      color = Variant, linetype = Variant)) +
  facet_wrap(vars(Adjective)) +
  geom_line() +
  guides(color=guide_legend(override.aes=list(fill=NA))) +
  scale_color_manual(values = 
                       c(&quot;gray70&quot;, &quot;gray70&quot;, &quot;gray20&quot;, &quot;gray70&quot;, &quot;gray20&quot;),
                        name=&quot;Variant&quot;,
                        breaks = c(&quot;other&quot;, &quot;pretty&quot;, &quot;really&quot;, &quot;so&quot;, &quot;very&quot;), 
                        labels = c(&quot;other&quot;, &quot;pretty&quot;, &quot;really&quot;, &quot;so&quot;, &quot;very&quot;)) +
  scale_linetype_manual(values = 
                          c(&quot;dotted&quot;, &quot;dotdash&quot;, &quot;longdash&quot;, &quot;dashed&quot;, &quot;solid&quot;),
                        name=&quot;Variant&quot;,
                        breaks = c(&quot;other&quot;, &quot;pretty&quot;, &quot;really&quot;, &quot;so&quot;, &quot;very&quot;), 
                        labels = c(&quot;other&quot;,  &quot;pretty&quot;, &quot;really&quot;, &quot;so&quot;, &quot;very&quot;)) +
  theme_set(theme_bw(base_size = 12)) +
  theme(legend.position=&quot;top&quot;, 
        axis.text.x = element_text(size=12),
        panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank()) +
  coord_cartesian(ylim = c(-.2, .4)) +
  labs(x = &quot;Age&quot;, y = &quot;Collocation Strength&quot;) +
  guides(size = FALSE)+
  guides(alpha = FALSE)</code></pre>
<p><img src="collocations_files/figure-html/coll_02_17-1.png" width="672" /></p>
<p>The results show that the collocation strength of different amplifier variants changes quite notably across age groups and we can also see that there is considerable variability in the way that the collocation strengths changes. For example, the collocation strengths between <em>bad</em> and <em>really</em> decreases from old to young speakers, while the reverse trend emerges for <em>good</em> which means that <em>really</em> is collocating more strongly with <em>good</em> among younger speakers than it is among older speakers.</p>
</div>
<div id="collostructional-analysis" class="section level1">
<h1><span class="header-section-number">4</span> Collostructional Analysis</h1>
<p>Collostructional analysis <span class="citation">(Stefanowitsch and Gries <a href="#ref-stefanowitsch2003collostructions" role="doc-biblioref">2003</a>, <a href="#ref-stefanowitsch2005covarying" role="doc-biblioref">2005</a>)</span> investigates the lexicogrammatical associations between constructions and lexical elements and there exist three basic subtypes of collostructional analysis:</p>
<ul>
<li><p>Simple Collexeme Analysis</p></li>
<li><p>Distinctive Collexeme Analysis</p></li>
<li><p>Co-Varying Collexeme Analysis</p></li>
</ul>
<p>The analyses performed here are based on the collostructions package <span class="citation">(Flach <a href="#ref-flach2017collostructions" role="doc-biblioref">2017</a>)</span>.</p>
<div id="simple-collexeme-analysis" class="section level2">
<h2><span class="header-section-number">4.1</span> Simple Collexeme Analysis</h2>
<p>Simple Collexeme Analysis determines if a word is significantly attracted to a specific construction within a corpus. The idea is that the frequency of the word that is attracted to a construction is significantly higher within the construction than would be expected by chance.</p>
<p>The example here analyzses the Go + Verb construction (e.g. “Go suck a nut!”). The question is which verbs are attracted to this constructions (in this case, if <em>suck</em> is attracted to this construction).</p>
<p>In a first step, we load the collostructions package and inspect the data. In this case, we will only use a sample of 100 rows from the data set as the output would become hard to read.</p>
<pre class="r"><code># load library
library(collostructions)
# draw a sample of the data
goVerb &lt;- goVerb[sample(nrow(goVerb), 100),]
# inspect data
str(goVerb)</code></pre>
<pre><code>## &#39;data.frame&#39;:    100 obs. of  3 variables:
##  $ WORD     : Factor w/ 752 levels &quot;accept&quot;,&quot;accomplish&quot;,..: 596 458 567 49 36 693 334 592 376 58 ...
##  $ CXN.FREQ : int  1 12 1 1 3 10 1 9 1 1 ...
##  $ CORP.FREQ: int  380 34842 11565 34970 6322 38282 295 16297 1011 3753 ...</code></pre>
<p>The collex function which calculates the results of a simple collexeme analysis requires a data frame consisting out of three columns that contain in column 1 the word to be tested, in column 2 the frequency of the word in the construction (CXN.FREQ), and in column 3 the frequency of the word in the corpus (CORP.FREQ).</p>
<p>To perform the simple collexeme analysis, we need the overal size of the corpus, the frequency with which a word occurs in the construction under investigation and the frequency of that construction.</p>
<pre class="r"><code># define corpus size
crpsiz &lt;- sum(goVerb$CORP.FREQ)
# perform simple collexeme analysis
scollex_results &lt;- collex(goVerb, corpsize = crpsiz, am = &quot;logl&quot;, 
                          reverse = FALSE, decimals = 5,
                          threshold = 1, cxn.freq = NULL, 
                          str.dir = FALSE)
# inspect results
head(scollex_results)</code></pre>
<pre><code>##   COLLEX CORP.FREQ OBS  EXP ASSOC COLL.STR.LOGL SIGNIF
## 1   fuck      7236 140  1.9  attr     941.14129  *****
## 2  visit     77696 234 20.9  attr     748.00636  *****
## 3   talk    100009 189 26.9  attr     437.72210  *****
## 4   grab     10759  76  2.9  attr     355.86476  *****
## 5    sit     35886 109  9.6  attr     339.03691  *****
## 6   wash     10714  22  2.9  attr      51.57311  *****</code></pre>
<p>The results show which words are significantly attracted to the construction. If the ASSOC column did not show <em>attr</em>, then the word would be repelled by the construction.</p>
</div>
<div id="covarying-collexeme-analysis" class="section level2">
<h2><span class="header-section-number">4.2</span> Covarying Collexeme Analysis</h2>
<p>Covarying collexeme analysis determines if the occurrence of a word in the first slot of a constructions affects the occurrence of another word in the second slot of the construction. As such, covarying collexeme analysis analyzes constructions with two slots and how the lexical elemenst within the two slots affect each other.</p>
<p>The data we will use consists of two columns which contain in the first column (CXN.TYPE) the word in the first slot (either <em>cannot</em> or <em>can’t</em>) and in the second slot (COLLEXEME) the word in the second slot, i.e. the collexeme, which the verb that follows after <em>cannot</em> or <em>can’t</em>. The first six rows of the data are shown below.</p>
<pre class="r"><code>head(cannot)</code></pre>
<pre><code>##   CXN.TYPE COLLEXEME
## 1   cannot       see
## 2    can&#39;t    figure
## 3    can&#39;t        do
## 4   cannot      deal
## 5   cannot     bring
## 6   cannot        be</code></pre>
<p>We now perform the collexeme analysis and oinspect the results.</p>
<pre class="r"><code>covar_results &lt;- collex.covar(cannot)
# inspect results
head(covar_results)</code></pre>
<pre><code>##    SLOT1    SLOT2  fS1 fS2 OBS   EXP ASSOC COLL.STR.LOGL SIGNIF
## 1 cannot       be  614 359 234  86.7  attr     328.47123  *****
## 2  can&#39;t remember 1929 127 127  96.3  attr      72.29136  *****
## 3  can&#39;t       do 1929 184 170 139.6  attr      36.83365  *****
## 4  can&#39;t      get 1929 132 123 100.1  attr      29.02213  *****
## 5  can&#39;t      see 1929 125 115  94.8  attr      23.20114  *****
## 6  can&#39;t     hear 1929  34  34  25.8  attr      18.93751   ****</code></pre>
<p>The results show if a words in the first and second slot attract or repel each other (ASSOC) and provide uncorrceted significance levels.</p>
</div>
<div id="distinctive-collexeme-analysis" class="section level2">
<h2><span class="header-section-number">4.3</span> Distinctive Collexeme Analysis</h2>
<p>Distinctive Collexeme Analysis determines if the frequencies of items in two alternating constructions or under two conditions differ significantly. This analysis can be extended to analyze if the use of a word differs between two corpora.</p>
<p>Again, we use the cannot data.</p>
<pre class="r"><code>collexdist_results &lt;- collex.dist(cannot, raw = TRUE)
# inspect results
head(collexdist_results)</code></pre>
<pre><code>##     COLLEX O.CXN1 E.CXN1 O.CXN2 E.CXN2 ASSOC COLL.STR.LOGL SIGNIF SHARED
## 1 remember    127   96.3      0   30.7 can&#39;t      72.29136  *****      N
## 2       do    170  139.6     14   44.4 can&#39;t      36.83365  *****      Y
## 3      get    123  100.1      9   31.9 can&#39;t      29.02213  *****      Y
## 4      see    115   94.8     10   30.2 can&#39;t      23.20114  *****      Y
## 5     hear     34   25.8      0    8.2 can&#39;t      18.93751   ****      N
## 6    think     64   52.3      5   16.7 can&#39;t      14.00636    ***      Y</code></pre>
<p>The results show if words are significantly attracted or repelled by either the contraction (<em>can’t</em>) or the full form (<em>cannot</em>). In this example, <em>remember</em> - like all other words shown in the results - is significantly attracted to the contraction (<em>can’t</em>).</p>
</div>
</div>
<div id="how-to-cite-this-tutorial" class="section level1 unnumbered">
<h1>How to cite this tutorial</h1>
<p>Schweinberger, Martin. 2020. <em>Analyzing co-occurrences and collocations using R</em>. Brisbane: The University of Queensland. url: <a href="https://slcladal.github.io/collocations.html" class="uri">https://slcladal.github.io/collocations.html</a>.</p>
</div>
<div id="references" class="section level1 unnumbered">
<h1>References</h1>
<div id="refs" class="references">
<div id="ref-flach2017collostructions">
<p>Flach, Susanne. 2017. “Collostructions: An R Implementation for the Family of Collostructional Methods.” Package version v.0.1.0. <a href="https://sfla.ch/collostructions/">https://sfla.ch/collostructions/</a>.</p>
</div>
<div id="ref-stefanowitsch2003collostructions">
<p>Stefanowitsch, Anatol, and Stefan Th. Gries. 2003. “Collostructions: Investigating the Interaction of Words and Constructions.” <em>International Journal of Corpus Linguistics</em> 8 (2): 209–43.</p>
</div>
<div id="ref-stefanowitsch2005covarying">
<p>Stefanowitsch, Anatol, and Stefan Th Gries. 2005. “Covarying Collexemes.” <em>Corpus Linguistics and Linguistic Theory</em> 1 (1): 1–43.</p>
</div>
<div id="ref-wiedemann2017textmining">
<p>Wiedemann, Gregor, and Andreas Niekler. 2017. “Hands-on: A Five Day Text Mining Course for Humanists and Social Scientists in R.” Berlin: Proceedings of the 1st Workshop Teaching NLP for Digital Humanities (Teach4DH@GSCL 2017). <a href="https://tm4ss.github.io/docs/index.html">https://tm4ss.github.io/docs/index.html</a>.</p>
</div>
</div>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_').toLowerCase();
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
