<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />


<meta name="author" content="UQ SLC Digital Team" />

<meta name="date" content="2019-07-25" />

<title>Classification</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cosmo.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<link href="site_libs/font-awesome-5.0.13/css/fa-svg-with-js.css" rel="stylesheet" />
<script src="site_libs/font-awesome-5.0.13/js/fontawesome-all.min.js"></script>
<script src="site_libs/font-awesome-5.0.13/js/fa-v4-shims.min.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>


</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}

.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>


<div class="container-fluid main-container">

<!-- tabsets -->
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->




<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_').toLowerCase();
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}


.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
  padding-left: 25px;
  text-indent: 0;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>

<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">LADAL</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    <span class="fa fa-play-circle"></span>
     
    Basics
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li class="dropdown-header">Basics</li>
    <li>
      <a href="introquant.html">Introduction To Quantitative Reasoning</a>
    </li>
    <li>
      <a href="basicquant.html">Basic Concepts In Quantitative Reasoning</a>
    </li>
    <li class="divider"></li>
    <li class="dropdown-header">Research Designs</li>
    <li>
      <a href="researchdesigns.html">Overview</a>
    </li>
    <li>
      <a href="corpling.html">Corpus Linguistics</a>
    </li>
    <li>
      <a href="experiments.html">Experimental Designs</a>
    </li>
    <li>
      <a href="acoustic.html">Acoustic Analysis</a>
    </li>
    <li class="divider"></li>
    <li class="dropdown-header">Data Collection</li>
    <li>
      <a href="introdatacollection.html">Introduction</a>
    </li>
    <li>
      <a href="questionnaires.html">Questionnaires and Surveys</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    <span class="fa fa-gear"></span>
     
    Data Processing
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li class="dropdown-header">Data Processing</li>
    <li>
      <a href="intror.html">Basics: Getting started with R</a>
    </li>
    <li>
      <a href="introloading.html">Loading and saving data</a>
    </li>
    <li>
      <a href="stringprocessing.html">String processing</a>
    </li>
    <li>
      <a href="regularexpressions.html">Regular expressions</a>
    </li>
    <li>
      <a href="introtables.html">Tabulating data</a>
    </li>
    <li class="divider"></li>
    <li>
      <a href="dataprocessingexcel.html">Data Processing with Excel</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    <span class="fa fa-bar-chart"></span>
     
    Visualization
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li class="dropdown-header">Visualization</li>
    <li>
      <a href="basicgraphs.html">Basic Visualization with R</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    <span class="fa fa-eye"></span>
     
    Statistics
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li class="dropdown-header">Statistics</li>
    <li>
      <a href="descriptivestatz.html">Descriptive Statistics</a>
    </li>
    <li class="divider"></li>
    <li class="dropdown-header">Basic Interential Statistics</li>
    <li>
      <a href="basicstatz.html">Basic Inferential Tests</a>
    </li>
    <li>
      <a href="basicstatzchi.html">The Chi-Square Family</a>
    </li>
    <li>
      <a href="basicstatzregression.html">Simple Linear Regression</a>
    </li>
    <li class="divider"></li>
    <li class="dropdown-header">Advanced Interential Statistics</li>
    <li>
      <a href="fixedregressions.html">Fixed-Effects Regression Models</a>
    </li>
    <li>
      <a href="mixedregressions.html">Mixed-Effects Regression Models</a>
    </li>
    <li>
      <a href="advancedstatztrees.html">Tree-Based Models</a>
    </li>
    <li>
      <a href="groupingstatz.html">Classification</a>
    </li>
    <li>
      <a href="collostructionalanalysis.html">Collostructional Analysis</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    <span class="fa fa-bars"></span>
     
    Text Analysis/Corpus Linguistics
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li class="dropdown-header">Text Analysis</li>
    <li>
      <a href="textanalysis.html">Introduction</a>
    </li>
    <li>
      <a href="webcrawling.html">Web Crawling</a>
    </li>
    <li>
      <a href="network.html">Network Analysis</a>
    </li>
    <li>
      <a href="topicmodels.html">Topic Modeling</a>
    </li>
    <li>
      <a href="classification.html">Classification</a>
    </li>
    <li>
      <a href="tagging.html">Tagging and Parsing</a>
    </li>
    <li class="divider"></li>
    <li class="dropdown-header">Corpus Linguistics</li>
    <li>
      <a href="corplingr.html">Corpus Linguistics in R</a>
    </li>
  </ul>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="about.html">
    <span class="fa fa-info"></span>
     
    Contact
  </a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Classification</h1>
<h4 class="author"><em>UQ SLC Digital Team</em></h4>
<h4 class="date"><em>2019-07-25</em></h4>

</div>


<p><img src="images/uq1.jpg" width="100%" /></p>
<div id="introduction" class="section level1">
<h1><span class="header-section-number">1</span> Introduction</h1>
<p>This tutorial introduces classification using “R”. The entire code for the sections below can be downloaded <a href="https://slcladal.github.io/rscripts/classificationrscript.r">here</a>.</p>
</div>
<div id="preparation" class="section level1">
<h1><span class="header-section-number">2</span> Preparation</h1>
<p>As all caluculations and visualizations in this tutorial rely on “R”, it is necessary to install “R”, “RStudio”, and “Tinn-R”. If these programms (or, in the case of “R”, environments) are not installed yet, please search for them in your favorite search engine and add the term “download”. Open any of the first few links and follow the installation instructions (they are easy to follow, do not require any specifications, and are pretty much self-explanatory).</p>
<p>In addition, certain “libraries” need to be installed so that the scripts shown below are executed without errors. Before turning to the code below, please install the libraries needed for running the code below. If you have already installed the libraries mentioned below, then you can skip ahead ignore this section. To install the necessary libraries, simply run the following code - it may take some time (between 1 and 5 minutes to install all of the libraries so you do not need to worry if it takes some time).</p>
<pre class="r"><code># clean current workspace
rm(list=ls(all=T))
# set options
options(stringsAsFactors = F)
# install libraries
install.packages(c(&quot;LiblineaR&quot;, &quot;tm&quot;))</code></pre>
<p>Once you have installed “R”, “R-Studio”, “Tinn-R”, and have also initiated the session by executing the code shown above, you are good to go.</p>
</div>
<div id="classification-of-textual-data" class="section level1">
<h1><span class="header-section-number">3</span> Classification of textual data</h1>
<p>In this section we show the use of supervised machine learning for text classification. The basic idea is to compute a model based on training data. Training data usually are hand-coded documents or text snippets associated with a specific category (class). From these texts, features (e.g. words) are extracted and associated with categories in the model. The model then can be utilized to categorize new texts.</p>
<p>We cover basic principles of the process such as cross-validation and feature engineering in the following steps:</p>
<ol style="list-style-type: decimal">
<li>Read text data</li>
<li>Read training data</li>
<li>Build feature matrix</li>
<li>Classify (LiblineaR)</li>
<li>K-fold cross validation</li>
<li>Optimize C</li>
<li>Optimize features: stopwords, bi-grams, stemming</li>
<li>Final classification</li>
</ol>
<p>As data, again we use the “State of the Union”-addresses. But this time, we operate on paragraphs instead of documents. The file <code>data/sotu_paragraphs.csv</code> provides the speeches in the appropriate format. For each paragraph, we want to know whether it covers content related to <strong>domestic or foreign affairs</strong>.</p>
</div>
<div id="read-paragraphs" class="section level1">
<h1><span class="header-section-number">4</span> Read paragraphs</h1>
<p>As already known, we read the text source (21334 paragraphs from 231 speeches). For the moment, we apply very basic preprocessing.</p>
<pre class="r"><code># load library
library(tm)
library(quanteda)
# load data
testdata &lt;- read.delim(&quot;D:\\Uni\\UQ\\LADAL\\SLCLADAL.github.io\\data/testparagraphs.txt&quot;, sep = &quot;\t&quot;, header = T)
str(testdata)</code></pre>
<pre><code>## &#39;data.frame&#39;:    38577 obs. of  6 variables:
##  $ doc_id       : Factor w/ 38459 levels &quot;&quot;,&quot;$1,000 being usually, on account of their small compensation, filled by&quot;,..: 336 1631 2773 3757 3879 3997 4118 4236 4699 353 ...
##  $ speech_doc_id: Factor w/ 84 levels &quot;&quot;,&quot;$60,624,464 2. Newspapers and periodicals, 1 cent per pound&quot;,..: 3 3 3 3 3 3 3 3 3 3 ...
##  $ speech_type  : Factor w/ 5 levels &quot;&quot;,&quot;10,324,069 4. Parcels, etc., 16 cents a pound&quot;,..: 5 5 5 5 5 5 5 5 5 5 ...
##  $ president    : Factor w/ 18 levels &quot;&quot;,&quot;512,977,326 &quot;,..: 5 5 5 5 5 5 5 5 5 5 ...
##  $ date         : Factor w/ 76 levels &quot;&quot;,&quot;1790-01-08&quot;,..: 2 2 2 2 2 2 2 2 2 2 ...
##  $ text         : Factor w/ 4070 levels &quot;&quot;,&quot;\\Friendly relations with all, but entangling alliances with none,\\ has long\nbeen a maxim with us. Our true m&quot;| __truncated__,..: 653 895 1336 235 45 3145 3529 2903 3785 3766 ...</code></pre>
<p>Now that the test data is loaded into “R”, we load a list of stopwords into “R”.</p>
<pre class="r"><code># load stop words
english_stopwords &lt;- readLines(&quot;https://slcladal.github.io/resources/stopwords_en.txt&quot;, encoding = &quot;UTF-8&quot;)
# inspect stop words
head(english_stopwords)</code></pre>
<pre><code>## [1] &quot;a&quot;         &quot;a&#39;s&quot;       &quot;able&quot;      &quot;about&quot;     &quot;above&quot;     &quot;according&quot;</code></pre>
<pre class="r"><code># create corpus object
corpus &lt;- Corpus(DataframeSource(testdata))
# preprocessing chain
processedCorpus &lt;- tm_map(corpus, content_transformer(tolower))
processedCorpus &lt;- tm_map(processedCorpus, removePunctuation, preserve_intra_word_dashes = TRUE)
processedCorpus &lt;- tm_map(processedCorpus, removeNumbers)
processedCorpus &lt;- tm_map(processedCorpus, stripWhitespace)
# inspect corpus
as.character(processedCorpus[[2]])</code></pre>
<pre><code>## [1] &quot;i embrace with great satisfaction the opportunity which now presents itself of congratulating you on the present favorable prospects of our public affairs the recent accession of the important state of north carolina to the constitution of the united states of which official information has been received the rising credit and respectability of our country the general and increasing good will toward the government of the union and the concord peace and plenty with which we are blessed are circumstances auspicious in an eminent degree to our national prosperity&quot;</code></pre>
</div>
<div id="load-training-data" class="section level1">
<h1><span class="header-section-number">5</span> Load training data</h1>
<p>We provide 300 manually annotated example paragraphs as training data. In the file “paragraph_training_data.txt”, the paragraph id and the corresponding category is stored.</p>
<pre class="r"><code># load training data (already annotated)
trainingData &lt;- read.delim(&quot;https://slcladal.github.io/data/paragraph_training_data.txt&quot;, sep = &quot;\t&quot;, header = T, quote = &quot;&quot;)
# inspect data 
colnames(trainingData); str(trainingData)</code></pre>
<pre><code>## [1] &quot;ID&quot;    &quot;LABEL&quot;</code></pre>
<pre><code>## &#39;data.frame&#39;:    300 obs. of  2 variables:
##  $ ID   : int  15155 5251 8312 1950 20521 234 12249 16303 18626 876 ...
##  $ LABEL: Factor w/ 2 levels &quot;DOMESTIC&quot;,&quot;FOREIGN&quot;: 1 1 2 1 1 2 1 1 2 1 ...</code></pre>
<p>How is the ratio between domestic and foreign content in the training data?</p>
<pre class="r"><code>classCounts &lt;- table(trainingData[, &quot;LABEL&quot;])
print(classCounts)</code></pre>
<pre><code>## 
## DOMESTIC  FOREIGN 
##      209       91</code></pre>
<pre class="r"><code>numberOfDocuments &lt;- nrow(trainingData)</code></pre>
<p>For our first classification attempt, we create a Document-Term Matrix from the preprocessed corpus and use the extracted single words (unigrams) as features for the classification. Since the resulting DTM might contain too many words, we restrict the vocabulary to a minimum frequency.</p>
<pre class="r"><code># Base line: create feature set out of unigrams
DTM &lt;- DocumentTermMatrix(processedCorpus)
# How many features do we have?
dim(DTM)</code></pre>
<pre><code>## [1] 38577 12886</code></pre>
<pre class="r"><code># Probably the DTM is too big for the classifier. Let us reduce it
minimumFrequency &lt;- 5
DTM &lt;- DocumentTermMatrix(processedCorpus, control = list(bounds = list(global = c(minimumFrequency, Inf))))
dim(DTM)</code></pre>
<pre><code>## [1] 38577  5409</code></pre>
</div>
<div id="classification" class="section level1">
<h1><span class="header-section-number">6</span> Classification</h1>
<p>Now we build a linear classification model with the LiblineaR package. The package LiblineaR wraps around the open source library LIBLINEAR which provides a very fast implementations for two text classification algorithms: 1. Logistic Regression, and 2. Support Vector Machines (SVM) with a linear kernel. For both algorithms, different regularization methods exist (e.g. L1, and L2-regularization). The combination of algorithms and regularization can be controlled by the <code>type</code> parameter of the <code>LiblineaR</code> function. We stick to the default type, L2-regularized logistic regression (LR), since it usually achieves good performance in text classification.</p>
<p>First, we load the packages. Since Liblinear requires a special Sparse Matrix format, we also load the “SparseM” package and a conversion function which allows to convert <code>slam</code>-matrices (as used in the <code>tm</code> package) into <code>SparseM</code>-matrices.</p>
<p>Then, we split the annotated data into a training set (80%) and a test set (20%) using a boolean selector. The expression assigned to <code>selector_idx</code> creates a boolean vector of length 300 containing a FALSE value in every fifths position. This selector is used to select to training set. Its inverted vector (!) is used to select the test set.</p>
<pre class="r"><code>require(LiblineaR)
require(SparseM)
source(&quot;https://slcladal.github.io/rscripts/utils.R&quot;)
annotatedDTM &lt;- DTM[trainingData[, &quot;ID&quot;], ]
annotatedDTM &lt;- convertSlamToSparseM(annotatedDTM)
annotatedLabels &lt;- trainingData[, &quot;LABEL&quot;]
# split into training and test set
selector_idx &lt;- rep(c(rep(TRUE, 4), FALSE), length.out = numberOfDocuments)
trainingDTM &lt;- annotatedDTM[selector_idx, ]
trainingLabels &lt;- annotatedLabels[selector_idx]
testDTM &lt;- annotatedDTM[!selector_idx, ]
testLabels &lt;- annotatedLabels[!selector_idx]
# create LR classification model
model &lt;- LiblineaR(trainingDTM, trainingLabels)
summary(model)</code></pre>
<pre><code>##            Length Class  Mode     
## TypeDetail    1   -none- character
## Type          1   -none- numeric  
## W          5410   -none- numeric  
## Bias          1   -none- numeric  
## ClassNames    2   factor numeric  
## NbClass       1   -none- numeric</code></pre>
<p>The model created by the LiblineaR function can now be utilized to predict the labels of the test set. Then we compare the result of the automatic classification to our known labels to determine the accuracy of the process.</p>
<pre class="r"><code>classification &lt;- predict(model, testDTM) 
predictedLabels &lt;- classification$predictions
contingencyTable &lt;- table(predictedLabels, testLabels)
print(contingencyTable) </code></pre>
<pre><code>##                testLabels
## predictedLabels DOMESTIC FOREIGN
##        DOMESTIC        7       1
##        FOREIGN        38      14</code></pre>
<pre class="r"><code># calculate accuracy
accuracy &lt;- sum(diag(contingencyTable)) / length(testLabels)
print(accuracy) </code></pre>
<pre><code>## [1] 0.35</code></pre>
<p>The accuracy of 0.35 appears moderate for a first try. But how does it actually relate to a base line? Think of the imbalanced class proportions in our training set. Let us create a pseudo classification as base line, in which we do not classify at all, but simply assume the label “DOMESTIC” or “FOREIGN” for each paragraph.</p>
<p>We further employ a function called “F.measure” which gives more differentiated measures than simple accuracy (“A”)to determine the classification quality. The F.measure (“F”) is the harmonic mean of Precision (“P”) and Recall (“R”) (see <a href="https://en.wikipedia.org/wiki/Precision_and_recall#Definition_.28classification_context.29" class="uri">https://en.wikipedia.org/wiki/Precision_and_recall#Definition_.28classification_context.29</a> for Details).</p>
<pre class="r"><code># Create pseudo classification
pseudoLabelsDOM &lt;- factor(rep(&quot;DOMESTIC&quot;, length(testLabels)), levels(testLabels))
pseudoLabelsFOR &lt;- factor(rep(&quot;FOREIGN&quot;, length(testLabels)), levels(testLabels))
# Evaluation of former LR classification with F-measures
F.measure(predictedLabels, testLabels, positiveClassName = &quot;DOMESTIC&quot;)</code></pre>
<pre><code>##          P          R          S          F          A       Pos. 
##  0.8750000  0.1555556  0.9333333  0.2641509  0.3500000 45.0000000</code></pre>
<pre class="r"><code>F.measure(predictedLabels, testLabels, positiveClassName = &quot;FOREIGN&quot;)</code></pre>
<pre><code>##          P          R          S          F          A       Pos. 
##  0.2692308  0.9333333  0.1555556  0.4179104  0.3500000 15.0000000</code></pre>
<pre class="r"><code># Evaluation of pseudo classification with F-measures
F.measure(pseudoLabelsDOM, testLabels, positiveClassName = &quot;DOMESTIC&quot;)</code></pre>
<pre><code>##          P          R          S          F          A       Pos. 
##  0.7500000  1.0000000  0.0000000  0.8571429  0.7500000 45.0000000</code></pre>
<pre class="r"><code>F.measure(pseudoLabelsFOR, testLabels, positiveClassName = &quot;FOREIGN&quot;)</code></pre>
<pre><code>##     P     R     S     F     A  Pos. 
##  0.25  1.00  0.00  0.40  0.25 15.00</code></pre>
<p>This little experiment shows that depending on the definition of our positive class, the accuracy is either 25% or 75% if not classifying at all. In both cases the <em>specificity</em> (<code>S</code>), the true negative rate, is zero. From this, we can learn two things:</p>
<ol style="list-style-type: decimal">
<li>If classes in training/test sets are imbalanced, accuracy might be a misleading measurement. Other measure should be considered additionally.</li>
<li>To utilize accuracy and F-measure in a meaningful way, the less frequent class should be defined as POSITIVE class (FOREIGN in our case).</li>
</ol>
</div>
<div id="k-fold-cross-validation" class="section level1">
<h1><span class="header-section-number">7</span> K-fold cross validation</h1>
<p>To evaluate a classifier, the training data can be divided into training and test data. The model learns on the former and is evaluated with the latter. In this procedure, we unfortunately lose the test data to learn from. If there is little training data available, the k-fold cross-validation is a more suitable procedure.</p>
<p>For this, training data is split into e.g. K = 10 parts. Then k-1 parts are used for training and 1 part is used for testing. This process is repeated k times, with another split of the overall data set for testing in each iteration.</p>
<p>The final result is determined from the average of the quality of the k runs. This allows a good approximation to the classification quality, including all training data.</p>
<p>The <code>get_k_fold_logical_indexes</code> function introduced below returns a logical vector for the fold <code>j</code> for cross-validation. It splits a training data record of the size <code>n</code> into <code>k</code> folds. The resulting vector and its negated vector can be used for easy training data / test data selection.</p>
<pre class="r"><code>get_k_fold_logical_indexes &lt;- function(j, k, n) {
  if (j &gt; k) stop(&quot;Cannot select fold larger than nFolds&quot;)
  fold_lidx &lt;- rep(FALSE, k)
  fold_lidx[j] &lt;- TRUE
  fold_lidx &lt;- rep(fold_lidx, length.out = n)
  return(fold_lidx)
}
# Example usage
get_k_fold_logical_indexes(1, k = 10, n = 12)</code></pre>
<pre><code>##  [1]  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE
## [12] FALSE</code></pre>
<pre class="r"><code>get_k_fold_logical_indexes(2, k = 10, n = 12)</code></pre>
<pre><code>##  [1] FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
## [12]  TRUE</code></pre>
<pre class="r"><code>get_k_fold_logical_indexes(3, k = 10, n = 12)</code></pre>
<pre><code>##  [1] FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
## [12] FALSE</code></pre>
<p>Now we run 1) splitting of the annotated data, 2) model computation and testing in one for-loop.</p>
<pre class="r"><code>k &lt;- 10
evalMeasures &lt;- NULL
for (j in 1:k) {
  # create j-th boolean selection vector
  currentFold &lt;- get_k_fold_logical_indexes(j, k, nrow(trainingDTM))
  
  # select training data split
  foldDTM &lt;- annotatedDTM[!currentFold, ]
  foldLabels &lt;- annotatedLabels[!currentFold]
  
  # create model
  model &lt;- LiblineaR(foldDTM, foldLabels)
  
  # select test data split
  testSet &lt;- annotatedDTM[currentFold, ]
  testLabels &lt;- annotatedLabels[currentFold]
  
  # predict test labels
  predictedLabels &lt;- predict(model, testSet)$predictions
  
  # evaluate predicted against test labels
  kthEvaluation &lt;- F.measure(predictedLabels, testLabels, positiveClassName = &quot;FOREIGN&quot;)
  
  # combine evaluation measures for k runs
  evalMeasures &lt;- rbind(evalMeasures, kthEvaluation)
}
# Final evaluation values of k runs:
print(evalMeasures)</code></pre>
<pre><code>##                       P         R          S         F         A Pos.
## kthEvaluation 0.3214286 0.9000000 0.05000000 0.4736842 0.3333333   10
## kthEvaluation 0.4800000 0.7500000 0.07142857 0.5853659 0.4333333   16
## kthEvaluation 0.1538462 0.5714286 0.04347826 0.2424242 0.1666667    7
## kthEvaluation 0.2916667 0.7777778 0.19047619 0.4242424 0.3666667    9
## kthEvaluation 0.2400000 0.8571429 0.17391304 0.3750000 0.3333333    7
## kthEvaluation 0.2592593 1.0000000 0.13043478 0.4117647 0.3333333    7
## kthEvaluation 0.3703704 0.9090909 0.10526316 0.5263158 0.4000000   11
## kthEvaluation 0.2222222 1.0000000 0.12500000 0.3636364 0.3000000    6
## kthEvaluation 0.3448276 1.0000000 0.05000000 0.5128205 0.3666667   10
## kthEvaluation 0.2962963 1.0000000 0.13636364 0.4571429 0.3666667    8</code></pre>
<pre class="r"><code># Average over all folds
print(colMeans(evalMeasures))</code></pre>
<pre><code>##         P         R         S         F         A      Pos. 
## 0.2979917 0.8765440 0.1076358 0.4372397 0.3400000 9.1000000</code></pre>
<p>Accuracy is around 0%, F-measure is around 0%. Let’s try some approaches to optimize the automatic classification.</p>
</div>
<div id="optimization" class="section level1">
<h1><span class="header-section-number">8</span> Optimization</h1>
<p>These first tries have clarified how to utilize and evaluate machine learning functions for text in R. Now we concentrate on optimization strategies to get better results from the automatic classification process.</p>
<div id="c-parameter" class="section level2">
<h2><span class="header-section-number">8.1</span> C-Parameter</h2>
<p>For a linear classification model, the cost parameter (C-parameter) is the most important parameter to tweak (for other SVM kernels such as the radial or polynomial kernel there are other important parameters which influence the shape of the kernel function). The <strong>C-parameter</strong> determines the cost of classifications on the training data during training.</p>
<p>High values of C lead to a high costs of misclassification. The decision boundary which the classifier learns, will try to avoid any misclassification. But, values too high can lead to an overfitting of the model. This means, it adapts well to the training data, but classification will more likely fail on new test data.</p>
<p>Low values of C lead to less strict decision boundaries, which accepts some misclassifications. Such a model might generalize better on unseen data. But in the end, there is now exact method to determine a good C-value beforehand. It rather is an empirical question. To choose an optimal C-value, we simply try from a range of values, run k-fold-cross-validation for each single value and decide for the C which resulted in the best classification accuracy / F-measure. This is realized in the following for-loop, which utilizes the function <code>k_fold_cross_validation</code>. The function (have a look into <code>F.measure.R</code>) simply wraps the code for cross-validation, we used above.</p>
<pre class="r"><code>cParameterValues &lt;- c(0.003, 0.01, 0.03, 0.1, 0.3, 1, 3 , 10, 30, 100)
fValues &lt;- NULL
for (cParameter in cParameterValues) {
  print(paste0(&quot;C = &quot;, cParameter))
  evalMeasures &lt;- k_fold_cross_validation(annotatedDTM, annotatedLabels, cost = cParameter)
  fValues &lt;- c(fValues, evalMeasures[&quot;F&quot;])
}</code></pre>
<pre><code>## [1] &quot;C = 0.003&quot;
## [1] &quot;C = 0.01&quot;
## [1] &quot;C = 0.03&quot;
## [1] &quot;C = 0.1&quot;
## [1] &quot;C = 0.3&quot;
## [1] &quot;C = 1&quot;
## [1] &quot;C = 3&quot;
## [1] &quot;C = 10&quot;
## [1] &quot;C = 30&quot;
## [1] &quot;C = 100&quot;</code></pre>
<pre class="r"><code>plot(fValues, type=&quot;o&quot;, col=&quot;green&quot;, xaxt=&quot;n&quot;)
axis(1,at=1:length(cParameterValues), labels = cParameterValues)</code></pre>
<p><img src="classification_files/figure-html/class12-1.png" width="672" /></p>
<pre class="r"><code>bestC &lt;- cParameterValues[which.max(fValues)]
print(paste0(&quot;Best C value: &quot;, bestC, &quot;, F1 = &quot;, max(fValues)))</code></pre>
<pre><code>## [1] &quot;Best C value: 1, F1 = 0.437239695980729&quot;</code></pre>
<p>From the empirical test, we can obtain C = 1 as optimal choice for the cost parameter. On the current training data set with the current features it achieves 0.4372397 F-score.</p>
</div>
<div id="optimized-preprocessing" class="section level2">
<h2><span class="header-section-number">8.2</span> Optimized Preprocessing</h2>
<p>Not only the classification model has parameters which can be optimized to improve the results. More important are the features used for classification. In our preprocessing chain above, we extracted single types and transformed them into lower case. We now add different preprocessing steps and check on the results. To get an optimal cost parameter for each new feature set, we wrapped the code for C-parameter optimization into the <code>optimize_C</code> function.</p>
<p><strong>Stop word removal</strong></p>
<p>Stop words often do not contribute to the meaning of a text. For the decision between DOMESTIC and FOREIGN affairs, we do not expect any useful information from them. So let’s remove them and if it improves the classifier.</p>
<pre class="r"><code>processedCorpus &lt;- tm_map(corpus, content_transformer(tolower))
processedCorpus &lt;- tm_map(processedCorpus, removeWords, english_stopwords)
processedCorpus &lt;- tm_map(processedCorpus, removePunctuation, preserve_intra_word_dashes = TRUE)
processedCorpus &lt;- tm_map(processedCorpus, removeNumbers)
processedCorpus &lt;- tm_map(processedCorpus, stripWhitespace)
as.character(processedCorpus[[4963]])</code></pre>
<pre><code>## [1] &quot;&quot;</code></pre>
<pre class="r"><code>minimumFrequency &lt;- 5
DTM &lt;- DocumentTermMatrix(processedCorpus, control = list(bounds = list(global = c(minimumFrequency, Inf))))
dim(DTM)</code></pre>
<pre><code>## [1] 38577  5045</code></pre>
<pre class="r"><code>annotatedDTM &lt;- convertSlamToSparseM(DTM[trainingData[, &quot;ID&quot;], ])
best_C &lt;- optimize_C(annotatedDTM, annotatedLabels)</code></pre>
<pre><code>## [1] &quot;C = 0.003&quot;
## [1] &quot;C = 0.01&quot;
## [1] &quot;C = 0.03&quot;
## [1] &quot;C = 0.1&quot;
## [1] &quot;C = 0.3&quot;
## [1] &quot;C = 1&quot;
## [1] &quot;C = 3&quot;
## [1] &quot;C = 10&quot;
## [1] &quot;C = 30&quot;
## [1] &quot;C = 100&quot;
## [1] &quot;Best C value: 1, F1 = 0.443462480072387&quot;</code></pre>
<pre class="r"><code>k_fold_cross_validation(annotatedDTM, annotatedLabels, cost = best_C)</code></pre>
<pre><code>##         P         R         S         F         A      Pos. 
## 0.3026075 0.8837031 0.1175201 0.4434625 0.3500000 9.1000000</code></pre>
<p>Now let us see, if the use of bigrams, i.e. concatenations of sequences of two words can improve the result. Bigrams help to overcome a little bit the bag-of-words assumption of Document-Term-Matrices. With them, we can learn multi-word units such as <em>great britain</em>, <em>international affairs</em> or <em>united nations</em> as meaningful features for our task. The package <code>tokenizers</code> provides a simple and fast implementation to generate n-grams.</p>
<p><strong>Bigrams</strong></p>
<pre class="r"><code>require(tokenizers)
tokenize_ngrams(&quot;This is a test&quot;, n = 2, n_min = 1, ngram_delim = &quot;_&quot;, simplify = T)</code></pre>
<pre><code>## [1] &quot;this&quot;    &quot;this_is&quot; &quot;is&quot;      &quot;is_a&quot;    &quot;a&quot;       &quot;a_test&quot;  &quot;test&quot;</code></pre>
<pre class="r"><code>bigram_corpus &lt;- tm_map(processedCorpus, content_transformer(tokenize_ngrams), n = 2, n_min = 1, ngram_delim = &quot;_&quot;, simplify = T)
minimumFrequency &lt;- 5
DTM &lt;- DocumentTermMatrix(bigram_corpus, control = list(bounds = list(global = c(minimumFrequency, Inf))))
dim(DTM)</code></pre>
<pre><code>## [1] 38577  7929</code></pre>
<pre class="r"><code>sample(colnames(DTM), 10)</code></pre>
<pre><code>##  [1] &quot;\&quot;exchange\&quot;,&quot;          &quot;\&quot;revoked\&quot;,&quot;          
##  [3] &quot;\&quot;creation_national\&quot;,&quot; &quot;\&quot;militia_system\&quot;,&quot;   
##  [5] &quot;\&quot;stipulated_treaty\&quot;,&quot; &quot;\&quot;amicable\&quot;,&quot;         
##  [7] &quot;\&quot;agriculture\&quot;,&quot;       &quot;\&quot;statute\&quot;,&quot;          
##  [9] &quot;\&quot;remained\&quot;,&quot;          &quot;\&quot;prepared\&quot;,&quot;</code></pre>
<pre class="r"><code>annotatedDTM &lt;- convertSlamToSparseM(DTM[trainingData[, &quot;ID&quot;], ])
best_C &lt;- optimize_C(annotatedDTM, annotatedLabels)</code></pre>
<pre><code>## [1] &quot;C = 0.003&quot;
## [1] &quot;C = 0.01&quot;
## [1] &quot;C = 0.03&quot;
## [1] &quot;C = 0.1&quot;
## [1] &quot;C = 0.3&quot;
## [1] &quot;C = 1&quot;
## [1] &quot;C = 3&quot;
## [1] &quot;C = 10&quot;
## [1] &quot;C = 30&quot;
## [1] &quot;C = 100&quot;
## [1] &quot;Best C value: 30, F1 = 0.44114326849621&quot;</code></pre>
<pre class="r"><code>k_fold_cross_validation(annotatedDTM, annotatedLabels, cost = best_C)</code></pre>
<pre><code>##         P         R         S         F         A      Pos. 
## 0.3007760 0.8817388 0.1155749 0.4411433 0.3466667 9.1000000</code></pre>
<p><strong>Minimum feature frequency</strong></p>
<p>Up to this point, we dropped features occurring less than five times in our data. Let’s see if we include more features by decreasing the minimum frequency.</p>
<pre class="r"><code># More features
minimumFrequency &lt;- 2
DTM &lt;- DocumentTermMatrix(bigram_corpus, control = list(bounds = list(global = c(minimumFrequency, Inf))))
dim(DTM)</code></pre>
<pre><code>## [1] 38577 28773</code></pre>
<pre class="r"><code>annotatedDTM &lt;- convertSlamToSparseM(DTM[trainingData[, &quot;ID&quot;], ])
best_C &lt;- optimize_C(annotatedDTM, annotatedLabels)</code></pre>
<pre><code>## [1] &quot;C = 0.003&quot;
## [1] &quot;C = 0.01&quot;
## [1] &quot;C = 0.03&quot;
## [1] &quot;C = 0.1&quot;
## [1] &quot;C = 0.3&quot;
## [1] &quot;C = 1&quot;
## [1] &quot;C = 3&quot;
## [1] &quot;C = 10&quot;
## [1] &quot;C = 30&quot;
## [1] &quot;C = 100&quot;
## [1] &quot;Best C value: 3, F1 = 0.44247683286872&quot;</code></pre>
<pre class="r"><code>k_fold_cross_validation(annotatedDTM, annotatedLabels, cost = best_C)</code></pre>
<pre><code>##         P         R         S         F         A      Pos. 
## 0.3025640 0.8817388 0.1225201 0.4424768 0.3500000 9.1000000</code></pre>
<p><strong>Stemming</strong></p>
<p>As a last method, we utilize stemming (before bigram-concatenation) to unify different variants of the same semantic form (such as <em>nation</em> and <em>nations</em>).</p>
<pre class="r"><code># Stemming
stemmed_corpus &lt;- tm_map(processedCorpus, stemDocument, language = &quot;english&quot;)
stemmed_bigram_corpus &lt;- tm_map(stemmed_corpus, content_transformer(tokenize_ngrams), n = 2, n_min = 1, ngram_delim = &quot;_&quot;, simplify = T)
minimumFrequency &lt;- 2
DTM &lt;- DocumentTermMatrix(stemmed_bigram_corpus, control = list(bounds = list(global = c(minimumFrequency, Inf))))
dim(DTM)</code></pre>
<pre><code>## [1] 38577 30588</code></pre>
<pre class="r"><code>annotatedDTM &lt;- convertSlamToSparseM(DTM[trainingData[, &quot;ID&quot;], ])
best_C &lt;- optimize_C(annotatedDTM, annotatedLabels)</code></pre>
<pre><code>## [1] &quot;C = 0.003&quot;
## [1] &quot;C = 0.01&quot;
## [1] &quot;C = 0.03&quot;
## [1] &quot;C = 0.1&quot;
## [1] &quot;C = 0.3&quot;
## [1] &quot;C = 1&quot;
## [1] &quot;C = 3&quot;
## [1] &quot;C = 10&quot;
## [1] &quot;C = 30&quot;
## [1] &quot;C = 100&quot;
## [1] &quot;Best C value: 10, F1 = 0.434645029121356&quot;</code></pre>
<pre class="r"><code>k_fold_cross_validation(annotatedDTM, annotatedLabels, cost = best_C)</code></pre>
<pre><code>##         P         R         S         F         A      Pos. 
## 0.2982734 0.8507864 0.1305801 0.4346450 0.3500000 9.1000000</code></pre>
<p>Each individual approach to optimize our text features for classification has some effect on the results. Best results could be obtained from a combination of bigram features with minimum frequency 2 and stop word removal. Stemming did not have a positive effect on the performance.</p>
<p>But, testing different features must be done for each new task / language individually, since there is no “one-size fits all” approach to this.</p>
<p>GENERAL ADVISE: For this tutorial we utilized a rather small training set of 300 examples, 91 of them in the positive class. Better classification accuracy can be expected, if more training data is available. Hence, instead of spending too much time on feature optimization, it will probably be a better idea to invest into generation of more training data first.</p>
</div>
</div>
<div id="final-classification" class="section level1">
<h1><span class="header-section-number">9</span> Final classification</h1>
<p>We now apply our best classification model to the entire data set, to determine the occurrence of FORGEIN/DOMESTIC affairs related content in each single speech.</p>
<pre class="r"><code># Final classification
DTM &lt;- DocumentTermMatrix(bigram_corpus, control = list(bounds = list(global = c(2, Inf))))
annotatedDTM &lt;- convertSlamToSparseM(DTM[trainingData[, &quot;ID&quot;], ])
best_C &lt;- optimize_C(annotatedDTM, annotatedLabels)</code></pre>
<pre><code>## [1] &quot;C = 0.003&quot;
## [1] &quot;C = 0.01&quot;
## [1] &quot;C = 0.03&quot;
## [1] &quot;C = 0.1&quot;
## [1] &quot;C = 0.3&quot;
## [1] &quot;C = 1&quot;
## [1] &quot;C = 3&quot;
## [1] &quot;C = 10&quot;
## [1] &quot;C = 30&quot;
## [1] &quot;C = 100&quot;
## [1] &quot;Best C value: 3, F1 = 0.44247683286872&quot;</code></pre>
<pre class="r"><code>final_model &lt;- LiblineaR(annotatedDTM, annotatedLabels, cost = best_C)
final_labels &lt;- predict(final_model, convertSlamToSparseM(DTM))$predictions
table(final_labels) / sum(table(final_labels))</code></pre>
<pre><code>## final_labels
##   DOMESTIC    FOREIGN 
## 0.07784431 0.92215569</code></pre>
<p>We see that the classifier puts the majority of the around 21,000 paragraphs into the DOMESTIC category. We can visualize the result as a bar chart with <code>ggplot2</code>. For better readability</p>
<pre class="r"><code>speech_year &lt;- substr(testdata$date, 0, 4)
speech_id &lt;- testdata$speech_doc_id
paragraph_position &lt;- unlist(sapply(table(speech_id), FUN = function(x) {1:x}))
presidents_df &lt;- data.frame(
  paragraph_position = paragraph_position,
  speech = paste0(speech_id, &quot;: &quot;, testdata$president, &quot;_&quot;, speech_year),
  category = final_labels
)
# preserve speech order in chart by using factors
presidents_df$speech &lt;- factor(presidents_df$speech, levels = unique(presidents_df$speech))
# Remove two very long speeches to beautify the plot (you can also skipt this)
presidents_df &lt;- presidents_df[!grepl(&quot;Carter_1981|Truman_1946&quot;, presidents_df$speech), ]
# plot classes of paragraphs for each speech as tile
require(ggplot2)
ggplot(presidents_df, aes(x = speech, y = paragraph_position, fill = category)) + 
  geom_tile(stat=&quot;identity&quot;) + coord_flip()</code></pre>
<p><img src="classification_files/figure-html/class18-1.png" width="960" /></p>
<p>Can you see how DOMESTIC affairs related content gets more important over the course of centuries? Also the position of FOREIGN policy statements changes around the turn from the 19th to 20th century from the begginning of a speech to more dispersed positions thoughout the speech, and finally a tendency to rather place them at the end of speeches.</p>
</div>
<div id="optional-exercises" class="section level1">
<h1><span class="header-section-number">10</span> Optional exercises</h1>
<ol style="list-style-type: decimal">
<li>Divide the training data into a 80% training set and 20% test set. Train a classifier on the training set and evaluate the performance on the test set. As performance measure use <strong>Krippendorff’a alpha</strong> and <strong>Cohen’s Kappa</strong> as provided in the <code>irr</code> package for R.</li>
<li>Use the <code>proba = T</code> parameter for the <code>predict</code> method during the final classification to evaluate on label probabilities instead of concrete label decisions. Use the output of probabilities for the label “FOREIGN” to classify paragraphs as “FOREIGN” only if the label probability is greater than 60%. Visualize the result. What can you observe compared to the previous plot (decision boundary around 50%)?</li>
<li>Find a visualization that shows the position of FOREIGN and DOMESTIC paragraphs in each speech.</li>
</ol>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
