---
title: "Errors and Corrections"
author: "Martin Schweinberger"
date: "2 Dezember 2018"
output: html_document
---

\subsection{Exkurs: alpha- und beta-Fehler}\label{fehler}
Die Frage ist nun, warum man im vorherigen Beispiel nicht einen einfachen $\chi$^{2}-Test rechnen sollte. Das stärkste Argument dagegen ist hat mit einem sehr gewichtigen Problem zu tun, welches dazu unter anderem geführt hat, dass multivariate Verfahren entwickelt wurden: Dem Ansteigen der Fehlerrate bei wiederholten Tests. Wir haben in Sektion \ref{signifikanz} gesehen, dass wir gewöhnlich ein Signifikanzniveau von 5\% annehmen. Dies bedeutet aber auch, dass durchschnittlich bei jedem 20.sten Test, der einen Signifikanzwert von .05 hat eine Fehleinschätzung vorliegt, da durchschnittlich eines von 20 als signifikant \textquote{erwiesenen} Ergebnissen in Wirklichkeit auf einer zufälligen Verteilung beruht und damit nicht durch die gemessenen Faktoren verursacht wurde. Rechnen wir nun mehrere Tests in Folge, so summieren sich diese Wahrscheinlichkeiten und schon bei 4 Tests, die aufeinander folgen 18.5\% wie man mit Formel \ref{eq:inflatederrors} leicht errechnen kann (cf. Formel \ref{eq:inflatederrorsbsp}).

\begin{equation}
1 - .95^{n} = Fehler
\label{eq:inflatederrors}
\end{equation}

\begin{equation}
1 - .95^{4} = 1 - 0.814 = 0.185
\label{eq:inflatederrorsbsp}
\end{equation}

Wir werden auf dieses Beispiel zurückkommen, aber zuerst werden wir unterschiedliche Arten von Fehlern betrachten.

Man unterscheidet zwischen $\alpha$- (alpha) und $\beta$-Fehlern (beta), wobei $\alpha$-Fehler aussagen, dass ein Zusammenhang besteht, obwohl er in der empirischen Wirklichkeit nicht besteht. Bei $\beta$-Fehlern wird davon ausgegangen, dass ein Zusammenhang nicht besteht, obwohl er in der empirischen Wirklichkeit vorhanden ist (vgl. Tabelle \ref{tab:fehler}).

\begin{table}[H]
\begin{minipage}{\textwidth}
\begin{center}
\begin{footnotesize}
\begin{tabular}{l c c c }
& & \multicolumn{2}{c}{\textbf{Wirklichkeit}} \\
& & Zusammenhang & Kein Zusammenhang \\
\hline\\
\multirow{3}{*}{\textbf{Test}} & Zusammenhang & \textcolor{green}{$\surd$} & $\alpha$-Fehler \\\\
& Kein Zusammenhang & $\beta$-Fehler & \textcolor{green}{$\surd$} \\\\
\hline
\end{tabular}
\end{footnotesize}
\caption{Darstellung von Fehlern.}
\label{tab:fehler}
\end{center}
\end{minipage}
\end{table}

Zu dem Unterschied zwischen $\alpha$-- und $\beta$--Fehlern läßt sich sagen, dass generell $\beta$--Fehler zu bevorzugen sind, da sie lediglich aussagen, dass aufgrund der Datenlage nicht davon ausgegangen werde kann, dass $X$ oder $Y$ der Fall ist, während bei $\alpha$--Fehlern falsche Aussagen Teil des anerkannten Wissens werden. Als Daumenregel gilt, dass konservativeres und zurückhaltenderes Verhalten wissenschaftstheoretisch weniger problematisch ist und somit eher $\alpha$-- als $\beta$--Fehler vermieden werden sollten.

Nachdem wir nun geklärt haben, welche Arten von Fehlern es gibt und dass sich Fehler aufsummieren, betrachten wir einen verwandten Begriff: \textit{Unabhängigkeit} (Independence).

\subsubsection{Unabhängigkeit}\label{unabhangigkeit}
Wäre es so, dass sich Fehler immer aufaddieren, dann wäre Statistik nicht möglich, da jeder neue Test alle vorhergehenden Tests berücksichtigen müsste und durch das Aufsummieren der Fehler die Fehlerrate gegen Unendlich tendieren würde. Es kann also nicht so sein, dass sich Fehler immer aufsummieren. Was bestimmt nun, ob sich Fehler aufsummieren oder nicht? Die Antwort lautet: Unabhängigkeit.

Wenn Tests voneinander unabhängig sind, dann summieren sich deren Fehler nicht auf, wenn Sie allerdings miteinander in einem Zusammenhang stehen, dann summieren sich die Fehler. Unabhängigkeit hat in der Statistik allerdings eine von der Alltagssprache abgewandelte Bedeutung: In der Statistik versteht man unter Unabhängigkeit die Unabhängigkeit der Hypothesen. Handelt es sich um Spezifikationen von allgemeineren Hypothesen, so sind die spezifizierten Hypothesen nicht unabhängig von der allgemeinen Hypothese, und sie sind nicht unabhängig von den anderen spezifizierten Hypothesen. In anderen Worten, wenn wir mehrere spezielle Hypothesen im Rahmen einer allgemeineren Hypothese testen, dann sind die Hypothesen streng genommen nicht unabhängig und können nicht so behandelt werden.

Ein verwandtes Phänomen haben wir in Sektion \ref{chi2zk} kennengelernt: Der Grund warum wir nicht einfach einen neuen $\chi$^{2}-Test rechnen konnten liegt darin, dass erst ein Test durchgeführt würde, bzw. wurde eine Tabelle erstellt unter der Annahme, dass sich die Emotionsmetaphern in unterschiedlichen Registern verschieden Realisiert werden. Der folgende Test baute darauf auf und testete die spezifizierte Hypothese, dass sich zwei bestimmte Metaphernarten in zwei bestimmten Registern unterscheiden würden. Wir haben es also mit zwei Hypothesen zu tun, wobei die zweite Hypothese eine Spezifikation der ersten Hypothese darstellte. Die Hypothesen standen also in einem Zusammenhang und waren nicht unabhängig. Folglich hätten sich die Fehler addiert, wenn wir nicht berücksichtigt hätten, dass nicht nur die Teiltabelle aus den Daten extrahiert wurde, sondern wir eine Teiltabelle einer größeren Tabelle testen wollen.

Ein zweites und vielleicht noch wichtigeres Merkmal von Unabhängigkeit ist, dass unabhängige Variablen nicht miteinander korrelieren dürfen. Tun sie dies dennoch, so spricht man von Multikollinearität (mehr dazu in Sektion \ref{mlr}).

\subsubsection{Korrekturen}\label{korrekturen}
Wie kann man nun damit umgehen, dass man mehrere Hypothesen testen möchte? Zum einen kann man multivariate Verfahren verwenden, wie wir in Sektion \ref{mlr} sehen werden. Eine andere Methode besteht darin, Korrekturen einzubauen, um so zu garantieren, dass das $\alpha$--Niveau auch bei wiederholten Tests bei 5\% bleibt.

Die bekannteste und wahrscheinlich auch am weitesten verbreitete Korrektur ist die Bonferroni-Korrektur, bei der das $\alpha$--Niveau durch die Anzahl der Tests geteilt wird. Beispielsweise sollen bei einer Untersuchung 4 Tests durchgeführt werden, dann wird das $\alpha$--Niveau auf .05/4 = .0125 gesenkt, sodass die Summer der $\alpha$--Niveaus der vier Tests wieder das gewohnte 5\%-Niveau herstellt. Der Nachteil dieser Korrektur ist, dass Sie eher konservativ ist und daher zu einem relativ hohen $\beta$--Fehlerrate führt.

%\subsection{Nicht-parametrische Tests}
%Wenn die vorliegende abhängige Variable nicht nominal oder kategorial, sondern ordinal ist, d.h. wenn die abhängige Variable Rangdaten darstellt, dann ist der $\chi$^{2}-Test nicht anzuwenden. In solchen Fällen sollte man auf Tests zurückgreifen, die für ordinale Variablen ausgelegt sind. Im Folgenden werden die wichtigsten bi-variaten Tests für Rangdaten dargestellt.

%\subsubsection{Der Mann-Whitney U Test}
%Häufig kommt es vor, dass numerische abhängige Variablen entweder in Rangdaten transformiert werden müssen (bspw. da die Verteilung der Residuen/Fehler keine einfache lineare Regression oder einfache Varianzanalyse zulässt) oder die abhängige Variable von vornherein nach Rängen gestuft ist. In solchen Fällen kann kein $\chi$^{2}-Test gerechnet werden, da dieser von nominalen oder kategorialen Variablen ausgeht und nicht auf Rangdaten angepasst werden darf. Man nutzt in solche Fällen den Mann-Whitney U Test, der sowohl in Fällen anwendbar ist, wo es sich um abhängige (dieselben Versuchspersonen werden unter mindestens zwei Bedingungen getestet), wie auch unabhängige Gruppen handelt (unterschiedliche Versuchspersonen werden unter mindestens zwei Bedingungen getestet). Wenn man es mit unabhängigen Gruppen, d.h. unterschiedlichen Versuchspersonen in den Gruppen zu tun hat, und die abhängige Variable eine Rangfolge darstellt, nutzt man folgenden Code, um den Mann-Whitney U Test in \verb!R! zu implementieren.

%\begin{lstlisting}
%# y ist aV und numerisch, x ist uV und ein binaerer faktor
%wilcox.test(y ~ x)

%# y1 ist aV, y2 ist uV und beide sind numerisch
%wilcox.test(y1, y2)
%\end{lstlisting}

%Hat man es mit abhängigen Gruppen zu tun, muss man das Argument \verb!paired! gleich \verb!TRUE! setzen (siehe unten).

%\begin{lstlisting}
%# dependent 2-group Wilcoxon Signed Rank Test
%wilcox.test(y1,y2,paired=TRUE) # where y1 and y2 are numeric
%\end{lstlisting}

%\subsubsection{Kruskal Wallis Test}
%Bei dem Kruskal Wallis Test handelt es sich um eine einfache Varianzanalyse, die anstatt auf numerische Werte auf die Rangdaten angewandt wird.

%\begin{lstlisting}
%y <- c(15, 13, 10, 8, 37, 23, 31, 52, 11, 17)
%x <- c("A", "A", "A", "A", "A", "B", "B", "B", "B", "B")
%# Kruskal Wallis Test One Way Anova by Ranks
%kruskal.test(y~x) # where y1 is numeric and x is a factor
%\end{lstlisting}

%\begin{lstlisting}
%# Randomized Block Design - Friedman Test
%friedman.test(y~x|z)
%# where y are the data values, x is a grouping factor
%# and z is a blocking factor
%\end{lstlisting}


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r cars}
summary(cars)
```

## Including Plots

You can also embed plots, for example:

```{r pressure, echo=FALSE}
plot(pressure)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
