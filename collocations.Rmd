---
title: "Analyzing co-occurrences and collocations using R"
author: "Martin Schweinberger"
date: "`r format(Sys.time(), '%Y-%m-%d')`"
output:
  bookdown::html_document2: default
bibliography: bibliography.bib
link-citations: yes
---
```{r uq1, echo=F, fig.cap="", message=FALSE, warning=FALSE, out.width='100%'}
knitr::include_graphics("images/uq1.jpg")
```

# Introduction

This tutorial introduces collocation and co-occurrence analysis with *R* and shows how to extract and visualize semantic links between words. Parts of this tutorial build on @wiedemann2017textmining. The code for this stutorial can be downloaded [here](https://slcladal.github.io/rscripts/collocations.r).  

How would you find words that are associated with a specific term and how can you visualize such word nets? This tutorial addresses this issue by focusing on co-occurrence and collocations of words. Collocations are words that occur very frequently together. For example, *Merry Christmas* is a collocation because *merry* and *Christmas* occur more frequently together than would be expected by chance. This means that if you were to shuffle all words in a corpus and would then test the frequency of how often *merry* and *Christmas* co-occurred, they would occur significantly less often in the shuffled or randomized corpus than in a corpus that contain non-shuffled natural speech. 

But how can you determine if words occur more frequently together than would be expected by chance? This tutorial will answer this question. 

**Preparation and session set up**

As all calculations and visualizations in this tutorial rely on *R*, it is necessary to install *R* and *RStudio*. If these programs (or, in the case of *R*, environments) are not already installed on your machine, please search for them in your favorite search engine and add the term *download*. Open any of the first few links and follow the installation instructions (they are easy to follow, do not require any specifications, and are pretty much self-explanatory).

In addition, certain *packages* need to be installed so that the scripts shown below are executed without errors. Before turning to the code below, please install the packages by running the code below this paragraph. If you have already installed the packages mentioned below, then you can skip ahead ignore this section. To install the necessary packages, simply run the following code - it may take some time (between 1 and 5 minutes to install all of the packages so you do not need to worry if it takes some time).

```{r prep1, echo=T, eval = F, message=FALSE, warning=FALSE}
# clean current workspace
rm(list=ls(all=T))
# set options
options(stringsAsFactors = F)
options(scipen = 999)
options(max.print=10000)
# install packages
install.packages("corpus", "dplyr", "quanteda", "stringr", "tidyr", "tm")
# for visualization
install.packages("GGally", "network", "sna", "ggplot2")
# for statistics
install.packages("collostructions")
```

Once you have installed *R* and *R-Studio*, and have also initiated the session by executing the code shown above, you are good to go.

# Visualizing Collocations of *America* in Trump's 2020 SOTU Address

In the following example, we will analyze which words collocate with the term *america* in Trump's 2020 State of the Union address. The analysis consists of the following steps:

* Data processing

* Creating a co-occurence matrix

* Finding significant collocations 

* Extracting the network of collocations

* Visualizing the network of collocations


## Data processing

We start by initiating the session and loading the packages from the R library that we are going to use in this tutorial.

```{r prep2, echo=T, eval = T, message=FALSE, warning=FALSE}
# clean current workspace
rm(list=ls(all=T))
# set options
options(stringsAsFactors = F)
options(scipen = 999)
options(max.print=10000)
# load packages
# for data porcessing
library(corpus)
library(dplyr)
library(quanteda)
library(stringr)
library(tidyr)
library(tm)
# for visualization
library(GGally)
library(network)
library(sna)
library(ggplot2)
```

We continue the analysis by loading the data and defining the key term. The key term forms the center of the collocation network. 

```{r coll_01_01, echo=T, eval = T, message=FALSE, warning=FALSE}
# load data
textdata <- readLines("https://SLCLADAL.github.io/data/sotutrump.txt", 
                     skipNul = T, encoding = "unknown")
# define collocterm
collocterm <- "america"
# inspect data
str(textdata)
```

The separation of the text into semantic analysis units is important for co-occurrence analysis. Context windows can be for instance documents, paragraphs or sentences or neighboring words. One of the most frequently used context window is the sentence. The sentence segmentation must take place *before* the other preprocessing steps because the sentence-segmentation-model relies on intact word forms and punctuation marks.

```{r coll_01_03, echo=T, eval = T, message=FALSE, warning=FALSE}
# clean data
textdata <- textdata %>%
  stringr::str_replace_all("AUDIENCE:.*", "") %>%
  stringr::str_replace_all(" {2,}", " ") %>%
  stringr::str_replace_all("THE PRESIDENT:", "") %>%
  stringr::str_replace_all("Mr.", "Mr") %>%
  stringr::str_replace_all("\\(applause.{0,1}\\)", "") %>%
  stringr::str_replace_all("\\(APPLAUSE.{0,1}\\)", "") %>%
  paste(collapse = " ") %>%
  str_squish() %>%
  stripWhitespace()
# split text into sentences
sentences <- unlist(strsplit(as.character(textdata), "(?<=\\.)\\s(?=[A-Z])", perl = T))
# inspect data
head(sentences)
```

A word of warning is in order here: the newly decomposed corpus has now reached a considerable size of `r length(sentences)` sentences. Older computers may get in trouble because of insufficient memory during this preprocessing step.

Now we going to implement a pre-processing chain and apply it on the separated sentences. Preprocessing consists of cleaning the data by removing punctuation, numbers, superfluous white spaces, and so-called stop words which do not have semantic meaning.

```{r coll_01_05, echo=T, eval = T, message=FALSE, warning=FALSE}
# convert to lower case
sentencesclean <- sentences %>%
  tolower() %>%
  # remove punctuation
  removePunctuation() %>%
  # remove non alphanumeric characters
  stringr::str_replace_all("[^[:alnum:][:space:]_]", "") %>%
  # remove numbers
  removeNumbers()
# remove stop words
english_stopwords <- readLines("https://slcladal.github.io/resources/stopwords_en.txt", encoding = "UTF-8")
sentencesclean <- removeWords(sentencesclean, english_stopwords) %>%
  # strip white spaces
  str_squish() %>%
  stripWhitespace()
# remove emplty elements
sentencesclean <- sentencesclean[sentencesclean != ""]
# stem words
sentencesclean <- as.vector(sapply(sentencesclean, function(x){
  x <- text_tokens(x, stemmer = "en")
  x <- as.vector(unlist(x))
  x <- stringr::str_c(x, sep = " ", collapse = " ")}))
# select a sample
#sentencesclean <- sample(sentencesclean, 100)
# inspect data
head(sentencesclean)
```
 
## Creating a co-occurrence matrix

In a next step, we create a co-occurence matrix from the words in the sentences.

```{r coll_01_09, echo=T, eval = T, message=FALSE, warning=FALSE}
sentencecorpus <- Corpus(VectorSource(sentencesclean))
DTM <- DocumentTermMatrix(sentencecorpus, control=list(bounds = list(global=c(1, Inf)), weighting = weightBin)) 
# Convert to sparseMatrix matrix
require(Matrix)
DTM <- sparseMatrix(i = DTM$i, 
                    j = DTM$j, 
                    x = DTM$v, 
                    dims = c(DTM$nrow, DTM$ncol), 
                    dimnames = dimnames(DTM))
# Matrix multiplication for co-occurrence counts
coocCounts <- t(DTM) %*% DTM 
cooc <- as.matrix(coocCounts)
cooc[1:5, 1:5]
```

The matrix has `nrow(cooc)` rows and columns and is symmetric. Each cell contains the number of joint occurrences. In the diagonal, the frequencies of single occurrences of each term are encoded.

The part of the co-occurrence matrix that is displayed shows that `r colnames(cooc)[4]` appears together `r cooc[4, 5]` times with `r colnames(cooc)[5]` in the `r nrow(cooc)` sentences of the SUTO addresses. `r colnames(cooc)[4]` alone occurs `r cooc[4, 4]` times.

We can also use this co-occurrence table to create a co-occurence graph.

```{r coll_01_11, echo=T, eval = T, message=FALSE, warning=FALSE}
simplecooctb <- cooc[rownames(cooc) == "american",]
simplecoocdf <- data.frame(rep("american", length(simplecooctb)),
                           names(simplecooctb),
                           simplecooctb)
colnames(simplecoocdf) <- c("Term", "Cooc", "Freq")
# inspect results
head(simplecoocdf)
```
  
```{r coll_01_13, echo=T, eval = T, message=FALSE, warning=FALSE}
simplecooc <- simplecoocdf %>%
  dplyr::select(Term, Cooc, Freq)
simplecoocterms <- rep(simplecooc$Term, simplecooc$Freq)
simplecooccoocs <- rep(simplecooc$Cooc, simplecooc$Freq)
simplecoocdf <- data.frame(simplecoocterms, simplecooccoocs)
colnames(simplecoocdf) <- c("Term", "CoocTerm")
simplecoocnet <- network(simplecoocdf,
              directed = FALSE,
              ignore.eval = FALSE,
              names.eval = "weights"
              )
ggnet2(simplecoocnet, 
       size = 6, 
       color = "goldenrod", 
       edge.size = .5,
       edge.color = "lightgrey", 
       label = TRUE, 
       label.size = 3)
```

However, this network is not really useful as it does not distinguish between co-occurences and collocates, i.e. terms that occur significantly more frequently with the keyterm than would be expected by chance. Therefore, we now proceed to extract collocations.

## Finding significant collocates

In order to identify which words occur together more frequently than would be expected by chance, we have to determine if their co-occurence frequency is statistical significant. 

In a first step, we will determine which terms collocate significantly with the key term we are intersted in. In a second step, we will extract the network of collocations around this key term.

To determine which terms collocate significantly with the key term ("america), we will use multiple Fisher's Exact tests which require the following information:

* a = Number of times `coocTerm` occurs with term j

* b = Number of times `coocTerm` occurs without  term j

* c = Number of times other terms occur with term j

* d = Number of terms that are not `coocTerm` or term j

In a first step, we create a table which holds these quantities.

```{r coll_01_15, echo=T, eval = T, message=FALSE, warning=FALSE}
coocdf <- as.data.frame(as.matrix(cooc))
cooctb <- coocdf %>%
  dplyr::mutate(Term = rownames(coocdf)) %>%
  tidyr::gather(CoocTerm, TermCoocFreq,
                colnames(coocdf)[1]:colnames(coocdf)[ncol(coocdf)]) %>%
  dplyr::mutate(Term = factor(Term),
                CoocTerm = factor(CoocTerm)) %>%
  dplyr::mutate(AllFreq = sum(TermCoocFreq)) %>%
  dplyr::group_by(Term) %>%
  dplyr::mutate(TermFreq = sum(TermCoocFreq)) %>%
  dplyr::ungroup(Term) %>%
  dplyr::group_by(CoocTerm) %>%
  dplyr::mutate(CoocFreq = sum(TermCoocFreq)) %>%
  dplyr::arrange(Term) %>%
  dplyr::mutate(a = TermCoocFreq,
                b = TermFreq - a,
                c = CoocFreq - a, 
                d = AllFreq - (a + b + c)) %>%
  dplyr::mutate(NRows = nrow(coocdf))
cooctb
```

We now select the term for which we want to check the collocation network. In this example, we want to analyze the network of `collocterm`. Thus, we remove all rows from the data that do not involve `collocterm`. 

```{r coll_01_23, echo=T, eval = T, message=FALSE, warning=FALSE}
cooctb_redux <- cooctb %>%
  dplyr::filter(Term == collocterm)
```

Next, we calculate which terms are collocating with `collocterm` but we also test which terms are not used with `collocterm`.

```{r coll_01_25, echo=T, eval = T, message=FALSE, warning=FALSE}
coocStatz <- cooctb_redux %>%
  dplyr::rowwise() %>%
  dplyr::mutate(p = as.vector(unlist(fisher.test(matrix(c(a, b, c, d), 
                                                        ncol = 2, byrow = T))[1]))) %>%
    dplyr::mutate(x2 = as.vector(unlist(chisq.test(matrix(c(a, b, c, d),                                                           ncol = 2, byrow = T))[1]))) %>%
  dplyr::mutate(phi = sqrt((x2/(a + b + c + d)))) %>%
      dplyr::mutate(expected = as.vector(unlist(chisq.test(matrix(c(a, b, c, d), ncol = 2, byrow = T))$expected[1]))) %>%
  dplyr::mutate(Significance = ifelse(p <= .05, "p<.05",
                               ifelse(p <= .01, "p<.01",
                               ifelse(p <= .001, "p<.001", "n.s."))))
# inspect results
coocStatz
```

```{r coll_01_27, echo=T, eval = T, message=FALSE, warning=FALSE}
coocStatz <- coocStatz %>%
  dplyr::ungroup() %>%
  dplyr::arrange(p) %>%
  dplyr::mutate(j = 1:n()) %>%
  # perform benjamini-holm correction
  dplyr::mutate(corr05 = ((j/NRows)*0.05)) %>%
  dplyr::mutate(corr01 = ((j/NRows)*0.01)) %>%
  dplyr::mutate(corr001 = ((j/NRows)*0.001)) %>%
  # calculate corrected significance status
  dplyr::mutate(CorrSignificance = ifelse(p <= corr001, "p<.001",
                ifelse(p <= corr01, "p<.01",
                       ifelse(p <= corr05, "p<.05", "n.s.")))) %>%
  dplyr::mutate(p = round(p, 6)) %>%
  dplyr::mutate(x2 = round(x2, 1)) %>%
  dplyr::mutate(phi = round(phi, 2)) %>%
  dplyr::arrange(p) %>%
  dplyr::select(-a, -b, -c, -d, -j, -NRows, -corr05, -corr01, -corr001) %>%
  dplyr::mutate(Type = ifelse(expected > TermCoocFreq, "Antitype", "Type"))
# inspect results
coocStatz
```

## Extracting a collocation network

Now that we have determined which words collocate with the key term (`collocterm`), we extract the collocation network.

```{r coll_01_29, echo=T, eval = T, message=FALSE, warning=FALSE}
colloctermtb <- cooctb %>%
  dplyr::filter(Term == collocterm,
                TermCoocFreq > 0)
colloctermtb
```

## Extracting collocations

We now extract terms that collocate with the key term (`collocterm`).

```{r coll_01_31, echo=T, eval = T, message=FALSE, warning=FALSE}
coocStatz_redux <- coocStatz %>%
  dplyr::filter(Significance != "n.s.")
coocStatz_redux 
```

```{r coll_01_33, echo=T, eval = T, message=FALSE, warning=FALSE}
coocnet <- cooc[which(rownames(cooc) %in% coocStatz_redux$CoocTerm), which(colnames(cooc) %in% coocStatz_redux$CoocTerm)]
coocnet[1:5, 1:5]
```

## Visualizing the collocation network

Now, we can visualize the collocation network of the key term. In a first step, we create a network object.

```{r coll_01_35, echo=T, eval = T, message=FALSE, warning=FALSE}
collocnet = network(coocnet,
              directed = FALSE,
              ignore.eval = FALSE,
              names.eval = "weights"
              )
```

Now, we create a simple network of the collocations.

```{r coll_01_37, echo=T, eval = T, message=FALSE, warning=FALSE}
ggnet2(collocnet, 
       size = 6, 
       color = "goldenrod", 
       edge.size = .5,
       edge.color = "lightgrey", 
       label = TRUE, 
       label.size = 3)
```

The network only shows the collocates (as nodes) and which of these terms collocate. In a next step, we add information to the graph to show the collocation strength.  In order to add information, we inspect the data that is underlying the simple network graph.

```{r coll_01_41, echo=T, eval = T, message=FALSE, warning=FALSE}
ggnet2(collocnet)$data
```

Next, we modify the size of the icicles which should represent the frequency of the co-occurrence.

```{r coll_01_53, echo=T, eval = T, message=FALSE, warning=FALSE}
# rescale edge size
set.edge.attribute(collocnet, "weights", ifelse(collocnet %e% "weights" <= 1,
                                                1, 
                                   ifelse(collocnet %e% "weights" <= 3, 2, 3)))
# define line type
set.edge.attribute(collocnet, "lty", ifelse(collocnet %e% "weights" == 0.25, 3, 
                                      ifelse(collocnet %e% "weights" == .5, 2, 1)))
```

Now that we have set specified the edge size, and line type we visualize the network again. 

```{r coll_01_57, echo=T, eval = T, message=FALSE, warning=FALSE}
ggnet2(collocnet,
       label = TRUE,
       color = "red",
       label.size = 3,
       alpha = .5,
       size = "degree",
       edge.size = "weights",
       edge.lty = "lty",
       edge.alpha = .2,
legend.position = "bottom") +
  guides(color = FALSE, size = FALSE)
```

We now have a meaningful collocation network. All of the terms that are shown collocates with `collocterm` and the thickness of the lines shows how often the key term (`collocterm`) co-occurs with each of the terms. The size of the icicles shows how frequent the terms are. 

# Analyzing Changes in Collocation Strength across Time

This section focuses on changes in collocation strength across apparent time. The example focuses on adjective amplification in Australian English. The issue we will analyse here is whether we can unearth changes in the collocation pattern of adjective amplifiers such as *very*, *really*, or *so*. In other words, we will investigate if amplifiers associate with different adjectives among speakers from different age groups. 

In a first step,  we activate packages and load the data.

```{r coll_02_01, echo=T, eval = T, message=FALSE, warning=FALSE}
# load packages
library(dplyr)
library(ggplot2)
# load functions
source("https://SLCLADAL.github.io/rscripts/collexcovar.R")
# load data
ampaus <- read.table("https://SLCLADAL.github.io/data/ampaus.txt", sep = "\t", header = T)
# inspect data
str(ampaus)
```

The data consists of three variables (Adjective, Variant, and Age).


Now, we perform a co-varying collexeme analysis for really versus other amplifiers. The function takes a data set consisting of three columns labelled keys, colls, and time

```{r coll_02_05, echo=T, eval = T, message=FALSE, warning=FALSE}
# rename data
ampaus <- ampaus %>%
  dplyr::rename(keys = Variant, colls = Adjective, time = Age)
# perform analysis
collexcovar_really <- collexcovar(data = ampaus, keyterm = "really")
# inspect results
collexcovar_really
```


```{r coll_02_07, echo=T, eval = T, message=FALSE, warning=FALSE}
# perform analysis
collexcovar_pretty <- collexcovar(data = ampaus, keyterm = "pretty")
collexcovar_so <- collexcovar(data = ampaus, keyterm = "so")
collexcovar_very <- collexcovar(data = ampaus, keyterm = "very")
```

For other amplifiers, we have to change the label "other" to "bin" as the function already has a a label "other". Once we have changed other to bin, we perform the analysis.

```{r coll_02_09, echo=T, eval = T, message=FALSE, warning=FALSE}
ampaus <- ampaus %>%
  dplyr::mutate(keys = ifelse(keys == "other", "bin", keys))
collexcovar_other <- collexcovar(data = ampaus, keyterm = "bin")
```

Next, we combine the results of the co-varying collexeme analysis into a single table.

```{r coll_02_11, echo=T, eval = T, message=FALSE, warning=FALSE}
# combine tables
collexcovar_ampaus <- rbind(collexcovar_really, collexcovar_very, 
                     collexcovar_so, collexcovar_pretty, collexcovar_other)
collexcovar_ampaus <- collexcovar_ampaus %>%
  dplyr::rename(Age = time,
                Adjective = colls) %>%
  dplyr::mutate(Variant = ifelse(Variant == "bin", "other", Variant)) %>%
  dplyr::arrange(Age)
# inspect results
collexcovar_ampaus
```

We now modify the data set so that we can plot the collocation strength across apparent time.

```{r coll_02_13, echo=T, eval = T, message=FALSE, warning=FALSE}
ampauscoll <- collexcovar_ampaus %>%
  dplyr::select(Age, Adjective, Variant, Type, phi) %>%
  dplyr::mutate(phi = ifelse(Type == "Antitype", -phi, phi)) %>%
  dplyr::select(-Type) %>%
  tidyr::spread(Adjective, phi) %>%
  tidyr::replace_na(list(bad = 0,
                         funny = 0,
                         hard = 0,
                         good = 0,
                         nice = 0,
                         other = 0)) %>%
  tidyr::gather(Adjective, phi, bad:other) %>%
  tidyr::spread(Variant, phi) %>%
  tidyr::replace_na(list(pretty = 0,
                    really = 0,
                    so = 0,
                    very = 0,
                    other = 0)) %>%
  tidyr::gather(Variant, phi, other:very)
ampauscoll
```

In a final step, we visualize the results of our analysis.

```{r coll_02_15, echo=F, eval = T, message=FALSE, warning=FALSE}
p1 <-ggplot(ampauscoll, aes(x = reorder(Age, desc(Age)), 
                             y = phi, group = Variant, 
                      color = Variant, linetype = Variant)) +
  facet_wrap(vars(Adjective)) +
  geom_line() +
  guides(color=guide_legend(override.aes=list(fill=NA))) +
  scale_color_manual(values = 
                       c("gray70", "gray70", "gray20", "gray70", "gray20"),
                        name="Variant",
                        breaks = c("other", "pretty", "really", "so", "very"), 
                        labels = c("other", "pretty", "really", "so", "very")) +
  scale_linetype_manual(values = 
                          c("dotted", "dotdash", "longdash", "dashed", "solid"),
                        name="Variant",
                        breaks = c("other", "pretty", "really", "so", "very"), 
                        labels = c("other",  "pretty", "really", "so", "very")) +
  theme_set(theme_bw(base_size = 12)) +
  theme(legend.position="top", 
        axis.text.x = element_text(size=12),
        panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank()) +
  coord_cartesian(ylim = c(-.2, .4)) +
  labs(x = "Age", y = "Collocation Strength") +
  guides(size = FALSE)+
  guides(alpha = FALSE)
```

```{r coll_02_17, echo=T, eval = T, message=FALSE, warning=FALSE}
ggplot(ampauscoll, aes(x = reorder(Age, desc(Age)), 
                             y = phi, group = Variant, 
                      color = Variant, linetype = Variant)) +
  facet_wrap(vars(Adjective)) +
  geom_line() +
  guides(color=guide_legend(override.aes=list(fill=NA))) +
  scale_color_manual(values = 
                       c("gray70", "gray70", "gray20", "gray70", "gray20"),
                        name="Variant",
                        breaks = c("other", "pretty", "really", "so", "very"), 
                        labels = c("other", "pretty", "really", "so", "very")) +
  scale_linetype_manual(values = 
                          c("dotted", "dotdash", "longdash", "dashed", "solid"),
                        name="Variant",
                        breaks = c("other", "pretty", "really", "so", "very"), 
                        labels = c("other",  "pretty", "really", "so", "very")) +
  theme_set(theme_bw(base_size = 12)) +
  theme(legend.position="top", 
        axis.text.x = element_text(size=12),
        panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank()) +
  coord_cartesian(ylim = c(-.2, .4)) +
  labs(x = "Age", y = "Collocation Strength") +
  guides(size = FALSE)+
  guides(alpha = FALSE)
```

The results show that the collocation strength of different amplifier variants changes quite notably across age groups and we can also see that there is considerable variability in the way that the collocation strengths changes. For example, the collocation strengths between *bad* and *really* decreases from old to young speakers, while the reverse trend emerges for *good* which means that *really* is collocating more strongly with *good* among younger speakers than it is among older speakers.

# Collostructional Analysis

Collostructional  analysis [@stefanowitsch2003collostructions; @stefanowitsch2005covarying] investigates  the  lexicogrammatical associations between constructions and  lexical elements and there exist three basic subtypes of collostructional analysis: 

* Simple Collexeme Analysis

* Distinctive Collexeme Analysis

* Co-Varying Collexeme Analysis

The analyses performed here are based on the collostructions package [@flach2017collostructions].

## Simple Collexeme Analysis

Simple Collexeme Analysis determines if a word is significantly attracted to a specific construction within a corpus. The idea is that the frequency of the word that is attracted to a construction is significantly higher within the construction than would be expected by chance.

The example here analyzses the Go + Verb construction (e.g. "Go suck a nut!"). The question is which verbs are attracted to this constructions (in this case, if *suck* is attracted to this construction).

In a first step, we load the collostructions package and inspect the data. In this case, we will only use a sample of 100 rows from the data set as the output would become hard to read.

```{r coll_03_01, echo=T, eval = T, message=FALSE, warning=FALSE}
# load library
library(collostructions)
# draw a sample of the data
goVerb <- goVerb[sample(nrow(goVerb), 100),]
# inspect data
str(goVerb)
```

The collex function which calculates the results of a simple collexeme analysis requires a data frame consisting out of three columns that contain in column 1 the word to be tested, in column 2 the frequency of the word in the construction (CXN.FREQ), and in column 3 the frequency of the word in the corpus (CORP.FREQ).

To perform the simple collexeme analysis, we need the overal size of the corpus, the frequency with which a word occurs in the construction under investigation and the frequency of that construction.

```{r coll_03_03, echo=T, eval = T, message=FALSE, warning=FALSE}
# define corpus size
crpsiz <- sum(goVerb$CORP.FREQ)
# perform simple collexeme analysis
scollex_results <- collex(goVerb, corpsize = crpsiz, am = "logl", 
                          reverse = FALSE, decimals = 5,
                          threshold = 1, cxn.freq = NULL, 
                          str.dir = FALSE)
# inspect results
head(scollex_results)
```

The results show which words are significantly attracted to the construction. If the ASSOC column did not show *attr*, then the word would be repelled by the construction.

## Covarying Collexeme Analysis

Covarying collexeme analysis determines if the occurrence of a word in the first slot of a constructions affects the occurrence of another word in the second slot of the construction. As such, covarying collexeme analysis analyzes constructions with two slots and how the lexical elemenst within the two slots affect each other.

The data we will use consists of two columns which contain in the first column (CXN.TYPE) the word in the first slot (either *cannot* or *can't*) and in the second slot (COLLEXEME) the word in the second slot, i.e. the collexeme, which the verb that follows after *cannot* or *can't*. The first six rows of the data are shown below.

```{r coll_03_07, echo=T, eval = T, message=FALSE, warning=FALSE}
head(cannot)
```

We now perform the collexeme analysis and oinspect the results.

```{r coll_03_09, echo=T, eval = T, message=FALSE, warning=FALSE}
covar_results <- collex.covar(cannot)
# inspect results
head(covar_results)
```

The results show if a words in the first and second slot attract or repel each other (ASSOC) and provide uncorrceted significance levels.

## Distinctive Collexeme Analysis

Distinctive Collexeme Analysis determines if the frequencies of items in two alternating constructions or under two conditions differ significantly. This analysis can be extended to analyze if the use of a word differs between two corpora.

Again, we use the cannot data.

```{r coll_03_11, echo=T, eval = T, message=FALSE, warning=FALSE}
collexdist_results <- collex.dist(cannot, raw = TRUE)
# inspect results
head(collexdist_results)
```

The results show if words are significantly attracted or repelled by either the contraction (*can't*) or the full form (*cannot*). In this example, *remember* - like all other words shown in the results - is significantly attracted to the contraction (*can't*).

# How to cite this tutorial {-}

Schweinberger, Martin. 2020. *Analyzing co-occurrences and collocations using R*. Brisbane: The University of Queensland. url: https://slcladal.github.io/collocations.html.

# References
