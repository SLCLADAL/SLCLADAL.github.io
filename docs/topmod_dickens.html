<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />


<meta name="author" content="Gerold Schneider, Max Lauber" />

<meta name="date" content="2022-11-15" />

<title>Topic Modelling of Charles Dickens’ novels</title>

<script src="site_libs/header-attrs-2.21/header-attrs.js"></script>
<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cosmo.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<link rel="stylesheet" href="styles.css" />

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-VSGK4KYDQZ"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-VSGK4KYDQZ');
</script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>





<style type="text/css">
/* for pandoc --citeproc since 2.11 */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>




<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark the anchor link active (and if it's in a dropdown, also mark that active)
  var dropdown = menuAnchor.closest('li.dropdown');
  if (window.bootstrap) { // Bootstrap 4+
    menuAnchor.addClass('active');
    dropdown.find('> .dropdown-toggle').addClass('active');
  } else { // Bootstrap 3
    menuAnchor.parent().addClass('active');
    dropdown.addClass('active');
  }

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before, .tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "\e259";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "\e258";
  font-family: 'Glyphicons Halflings';
  border: none;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">



<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  
  <!-- Added by SKC - LADAL image and thicker top with   -->
  <div class="container-fluid navbar-top" >
    <a href="index.html"> <!-- Make entire top row and text clickable home link  -->
        <div class="row">
            <div class="navbar-brand col-md-12">
              <img src="/content/ladal_icon_cas_tran_white_trimed.png" class="navbar-icon" alt="LADAL"/>
              <span class="navbar-title-note navbar-collapse collapse" >Language Technology and Data Analysis Laboratory</span>
            </div>
        </div>
    </a>
  </div>
  
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <!-- SKC removed  navbar brand -->
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">HOME</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    ABOUT LADAL
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="people.html">People | Collabs</a>
    </li>
    <li>
      <a href="news.html">News</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    EVENTS
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="workshops.html">Workshops</a>
    </li>
    <li>
      <a href="compthinking.html">Computational Thinking in HASS</a>
    </li>
    <li>
      <a href="webinars2022.html">LADAL Webinar Series 2022</a>
    </li>
    <li>
      <a href="opening.html">LADAL Webinar Series 2021</a>
    </li>
    <li>
      <a href="atapevents.html">ATAP Events</a>
    </li>
  </ul>
</li>
<li>
  <a href="tutorials.html">TUTORIALS</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    RESOURCES
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="links.html">Links</a>
    </li>
    <li>
      <a href="base.html">Tutorial stylesheet</a>
    </li>
  </ul>
</li>
<li>
  <a href="contact.html">CONTACT</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">



<h1 class="title toc-ignore">Topic Modelling of Charles Dickens’
novels</h1>
<h4 class="author">Gerold Schneider, Max Lauber</h4>
<h4 class="date">2022-11-15</h4>

</div>


<div id="introduction" class="section level1" number="1">
<h1><span class="header-section-number">1</span> Introduction</h1>
<p>This tutorial shows how to perform topic modelling using R. The
entire R markdown document for the tutorial can be downloaded <a
href="%22https://slcladal.github.io/content/ATAP_TopMod_Markdown.Rmd%22">here</a>.
The tutorial requires you to install and load a couple of packages (also
called libraries) to analyze linguistic data. To help you, we directly
include the commands to do so in the script and walk you through it step
by step.</p>
<div class="warning"
style="padding:0.1em; background-color:rgba(215,209,204,.3); color:#51247a">
<span>
<p style="margin-top:1em; text-align:center">
To be able to follow this tutorial, we suggest you check out and
familiarize yourself with the content of the following tutorials:<br>
</p>
<p style="margin-top:1em; text-align:left">
<ul>
<li>
<a href="https://ladal.edu.au/intror.html">Getting started with R</a>
</li>
<li>
<a href="https://ladal.edu.au/load.html">Loading, saving, and generating
data in R</a>
</li>
<li>
<a href="https://ladal.edu.au/string.html">String Processing in R</a>
</li>
<li>
<a href="https://ladal.edu.au/regex.html">Regular Expressions in R</a>
</li>
<li>
<a href="https://ladal.edu.au/topicmodels.html">Topic modeling with
R</a>
</li>
</ul>
</p>
<p style="margin-top:1em; text-align:center">
Click <a
href="https://ladal.edu.au/content/topmod_dickens.Rmd"><strong>here</strong></a><a
href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> to
download the <strong>entire R Notebook</strong> for this
tutorial.<br><br> <a
href="https://mybinder.org/v2/gh/SLCLADAL/interactive-notebooks-environment/main?urlpath=git-pull%3Frepo%3Dhttps%253A%252F%252Fgithub.com%252FSLCLADAL%252Finteractive-notebooks%26urlpath%3Dlab%252Ftree%252Finteractive-notebooks%252Fnotebooks%252F_dickens_cb.ipynb%26branch%3Dmain"><img
src="https://mybinder.org/badge_logo.svg" alt="Binder" /></a><br> Click
<a
href="https://mybinder.org/v2/gh/SLCLADAL/interactive-notebooks-environment/main?urlpath=git-pull%3Frepo%3Dhttps%253A%252F%252Fgithub.com%252FSLCLADAL%252Finteractive-notebooks%26urlpath%3Dlab%252Ftree%252Finteractive-notebooks%252Fnotebooks%252F_dickens_cb.ipynb%26branch%3Dmain"><strong>here</strong></a>
to open an interactive Jupyter notebook that allows you to execute,
change, and edit the code as well as to upload your own data. <br>
</p>
<p style="margin-left:1em;">
</p>
<p></span></p>
</div>
<p><br></p>
<div id="motivation" class="section level2" number="1.1">
<h2><span class="header-section-number">1.1</span> Motivation</h2>
<p>There is an incredible amount of text that has been archived all over
the world. Probably, there is more text being produced on any given day
than a single person can ever hope to read. What are linguists and other
language-oriented scholars - be that from perspectives historical,
literary, sociological or beyond - going to do with this embarrassment
of riches? Change field, focus on a particular niche of language or
begin using automated language methods, most likely. If you belong to
the latter category, and have yet to acquaint yourself with topic
modelling, welcome.</p>
<p>So how does topic modelling help us get to grips with large
quantities of texts (or the Great Unread, as Tangherlini and Leonard
appropriately call it)? What it certainly does not do, is read or
analyze a single sentence, let alone a corpus of texts for you. It is
more useful to think of it as “a lens that allows researchers working on
a problem to view a relevant textual corpus in a different light and at
a different scale” <span class="citation">(<a
href="#ref-mohr2013introduction">Mohr and Bogdanov 2013,
560</a>)</span>.</p>
</div>
<div id="basic-idea-of-topic-modelling" class="section level2"
number="1.2">
<h2><span class="header-section-number">1.2</span> Basic Idea of Topic
Modelling</h2>
<p>The way topic modelling allows us to engage with large corpora of
text by identifying co-occurrence patterns, which, when done right, can
yield new perspectives on a set of texts. As such, the methodology is
one implementation of the Firthian hypothesis which states that “you
shall know a word by the company it keeps” <span class="citation">(<a
href="#ref-firth1957ling">Firth 1957, 11</a>)</span>. Basically, the
method exploits the fact that words which frequently appear in a similar
context are often representative of the same topic. To arrive at a point
where a model allows us to interpret anything meaningful about the
topics it captures, we need to mangle the text ever so slightly (to
downplay things somewhat): removing proper names, cutting out a lot of
fluff, perhaps some chopping up of text.</p>
<p>Sounds like fun, doesn’t it? There is a rationale to all of this, and
we will walk through it step by step. Once we arrive at the destination,
we will have a list of keywords which represent the topics in some of
Charles Dickens’ most lauded works and discuss how they allow us to
interpret specific aspects of these novels. This is not the most
technical or comprehensive introduction to topic modelling out there,
but it provides an actionable instruction to modelling topics in R and
points to further resources for those who want to dive in deeper.</p>
</div>
<div id="preparation-and-session-setup" class="section level2"
number="1.3">
<h2><span class="header-section-number">1.3</span> Preparation and
Session Setup</h2>
<p>As mentioned at the outset, this is an introduction to Topic
Modelling based on R. A rudimentary familiarity with R and RStudio are
helpful for getting the most out of this. If you have yet to install R
or are new to it, we can recommend <a
href="https://ladal.edu.au/intror.html">this introductory tutorial to
R</a> or <a
href="https://dlf.uzh.ch/openbooks/statisticsforlinguists/chapter/first-steps-in-r-importing-and-retrieving-corpus-data/">these</a>
<a
href="https://dlf.uzh.ch/openbooks/statisticsforlinguists/chapter/first-steps-in-r-importing-and-retrieving-corpus-data/">two</a>
chapters from our slow-paced introduction to Statistics for Linguists,
which walks you through the installation and shows a range of its
functionalities. In the following, we assume that you have downloaded
and installed both R and RStudio and take it from there.</p>
</div>
<div id="packages" class="section level2" number="1.4">
<h2><span class="header-section-number">1.4</span> Packages</h2>
<p>Topic modelling is pretty package-intense. We won’t go into the
details of what each in this bouquet of packages does, except that each
of them is necessary for topic modelling. If you already have some of
these installed, just skip to the ones that you don’t have yet. Chances
are that you have tried our tutorial on document classification, and are
therefore already familiar with <code>quanteda</code> and
<code>readtext</code>.</p>
<p>If all of these look new, run the following lines of code:</p>
<pre class="r"><code>#install.packages(&quot;gutenbergr&quot;, repos = &quot;http://cran.us.r-project.org&quot;)
#install.packages(&quot;readtext&quot;, repos = &quot;http://cran.us.r-project.org&quot;)
#install.packages(&quot;quanteda&quot;, repos = &quot;http://cran.us.r-project.org&quot;)
#install.packages(&quot;quanteda.textmodels&quot;, repos = &quot;http://cran.us.r-project.org&quot;)
#install.packages(&quot;tidytext&quot;, repos = &quot;http://cran.us.r-project.org&quot;)
#install.packages(&quot;stm&quot;, repos = &quot;http://cran.us.r-project.org&quot;)
#install.packages(&quot;dplyr&quot;, repos = &quot;http://cran.us.r-project.org&quot;)
#install.packages(&quot;tm&quot;, repos = &quot;http://cran.us.r-project.org&quot;)
#install.packages(&quot;udpipe&quot;, repos = &quot;http://cran.us.r-project.org&quot;)
#install.packages(&quot;data.table&quot;, repos = &quot;http://cran.us.r-project.org&quot;)
#install.packages(&quot;future.apply&quot;, repos = &quot;http://cran.us.r-project.org&quot;)</code></pre>
<p>This may take a minute or three. Installing packages is only
necessary once, so if you are working with this script, or your own, you
won’t need to run these installation commands more than once. In your
script, you can <em>mute</em> a command - basically telling R not to
execute it - by placing a #-symbol in front of it, like so:</p>
<pre class="r"><code>#install.packages</code></pre>
<p>In most cases, the installations will work without any issues. If you
should get an error message, we recommend taking a moment to read what
it says, and, if it does not make any sense to you, to google it. If an
issue comes up for you, chances are that this has already happened to
someone else - and, fortunately, the R community has a pretty good track
record of responding to questions about technical issues. Generally, it
is also a good idea to use a relatively new version of R. If you have
last used R two years ago, do update it.</p>
<p>Once you have installed the packages, you’ll need to load them in the
current session. This is done with the following lines of code:</p>
<pre class="r"><code>library(gutenbergr)
library(readtext)
library(quanteda) 
library(quanteda.textmodels)
library(tidytext)
library(stm)
library(dplyr)
library(tm)
library(udpipe)
library(data.table)
library(future.apply)</code></pre>
</div>
<div id="installing-a-language-model" class="section level2"
number="1.5">
<h2><span class="header-section-number">1.5</span> Installing a language
model</h2>
<p>Now that all of the packages are loaded, we need to download and then
activate a language model so that we can part-of-speech tag the data.
Downloading the model only takes a single line of code:</p>
<pre class="r"><code>meng &lt;- udpipe::udpipe_download_model(language = &quot;english-ewt&quot;)</code></pre>
<p>Once the installation process is finished, we can initialize the
model which will allow R to use the language model. For this, we
use:</p>
<pre class="r"><code>m_eng &lt;- udpipe_load_model(file = here::here(&quot;udpipemodels&quot;, &quot;english-ewt-ud-2.5-191206.udpipe&quot;))</code></pre>
<p>With that, we have the computational prerequisites to generate topic
models in R. All we need now is data.</p>
</div>
<div id="research-questions" class="section level2" number="1.6">
<h2><span class="header-section-number">1.6</span> Research
Questions</h2>
<p>For this, we turn to the literary vaults to dust off one of the
greats: Charles Dickens. Beyond and within Christmas fables, Dickens is
famous for his social criticism, especially the way he treated the topic
of poverty and developed visions for including the poor inside society
[see for example <span class="citation">Kailash (<a
href="#ref-kailash2012dickens">2012</a>)</span> or <span
class="citation">Mahlberg (<a
href="#ref-mahlberg2013corpus">2013</a>)</span>). Dickens is generally
considered exemplary for his literary realism, which he employed with no
small success to depict the plight of inequality and poverty. There is
more to Dickens than that, but it gives us a duplette of research
questions:</p>
<ol style="list-style-type: decimal">
<li><p>Can we use topic modelling to bring Dickens’ social criticism to
the fore, without the heavy lifting of actually reading his
books?</p></li>
<li><p>Can we use topic modelling to explore the rich imagery that
Dickens constructs with his literary realism?</p></li>
</ol>
<p>To explore these questions, we will construct a small corpus,
consisting of eight Dickens novels. These are:</p>
<ul>
<li>Christmas Carol<br />
</li>
<li>Tale of Two Cities<br />
</li>
<li>The Pickwick Papers<br />
</li>
<li>Oliver Twist<br />
</li>
<li>David Copperfield<br />
</li>
<li>Hard Times<br />
</li>
<li>Nicholas Nickleby<br />
</li>
<li>Great Expectations</li>
</ul>
</div>
</div>
<div id="data-gutenberg" class="section level1" number="2">
<h1><span class="header-section-number">2</span> Data: Gutenberg</h1>
<p>Beyond the fact that his is a renowned style and perspective, Dickens
is convenient to work with because his work is old enough to be part of
the public domain. This means that the eight novels we are looking at
can be downloaded entirely legally from <a
href="https://www.gutenberg.org/">Project Gutenberg</a>. There is a <a
href="https://ladal.edu.au/gutenberg.html">specific tutorial</a> for
different ways of integrating Gutenberg into R, but we’ll walk you
through one approach anyway.</p>
<p>First, we can check out which of Dickens’ texts are available on
Gutenberg:</p>
<pre class="r"><code>dickens &lt;- gutenberg_works(author == &quot;Dickens, Charles&quot;)</code></pre>
<p>The command accesses a table of Gutenberg work metadata, in this case
specifically only for works authored by one <em>Dickens, Charles”. We
save this table to a variable called, tellingly, </em>dickens”. Let’s
take a look at it:</p>
<pre class="r"><code>dickens</code></pre>
<pre><code>## # A tibble: 78 × 8
##    gutenberg_id title    author gutenberg_author_id language gutenberg_bookshelf
##           &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;                &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;              
##  1           46 &quot;A Chri… Dicke…                  37 en       Children&#39;s Literat…
##  2           98 &quot;A Tale… Dicke…                  37 en       Historical Fiction 
##  3          564 &quot;The My… Dicke…                  37 en       Mystery Fiction    
##  4          580 &quot;The Pi… Dicke…                  37 en       Best Books Ever Li…
##  5          588 &quot;Master… Dicke…                  37 en       &lt;NA&gt;               
##  6          644 &quot;The Ha… Dicke…                  37 en       Christmas          
##  7          650 &quot;Pictur… Dicke…                  37 en       &lt;NA&gt;               
##  8          653 &quot;The Ch… Dicke…                  37 en       &lt;NA&gt;               
##  9          675 &quot;Americ… Dicke…                  37 en       &lt;NA&gt;               
## 10          676 &quot;The Ba… Dicke…                  37 en       Christmas          
## # ℹ 68 more rows
## # ℹ 2 more variables: rights &lt;chr&gt;, has_text &lt;lgl&gt;</code></pre>
<p>At first glance, we can see that there are 74 texts by Dickens on
Gutenberg, and that to each of these texts there is some metadate like
the Gutenberg ID, the language, the rights. This information is stored
in a tibble, an object that does not cram the console with its entire
length, but actually stops itself at ten rows. While this is helpful in
situations where you have millions of rows, here it is more hindrance
than help - we see some of the texts we’re looking for, but not all of
them. To display all of the rows and find our texts, we need the output
to display all 74 rows. This can be achieved like so:</p>
<pre class="r"><code>print(dickens, n=74)</code></pre>
<pre><code>## # A tibble: 78 × 8
##    gutenberg_id title    author gutenberg_author_id language gutenberg_bookshelf
##           &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;                &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;              
##  1           46 &quot;A Chri… Dicke…                  37 en       Children&#39;s Literat…
##  2           98 &quot;A Tale… Dicke…                  37 en       Historical Fiction 
##  3          564 &quot;The My… Dicke…                  37 en       Mystery Fiction    
##  4          580 &quot;The Pi… Dicke…                  37 en       Best Books Ever Li…
##  5          588 &quot;Master… Dicke…                  37 en       &lt;NA&gt;               
##  6          644 &quot;The Ha… Dicke…                  37 en       Christmas          
##  7          650 &quot;Pictur… Dicke…                  37 en       &lt;NA&gt;               
##  8          653 &quot;The Ch… Dicke…                  37 en       &lt;NA&gt;               
##  9          675 &quot;Americ… Dicke…                  37 en       &lt;NA&gt;               
## 10          676 &quot;The Ba… Dicke…                  37 en       Christmas          
## 11          678 &quot;The Cr… Dicke…                  37 en       Children&#39;s Literat…
## 12          699 &quot;A Chil… Dicke…                  37 en       Children&#39;s History…
## 13          700 &quot;The Ol… Dicke…                  37 en       &lt;NA&gt;               
## 14          730 &quot;Oliver… Dicke…                  37 en       &lt;NA&gt;               
## 15          766 &quot;David … Dicke…                  37 en       Harvard Classics   
## 16          786 &quot;Hard T… Dicke…                  37 en       &lt;NA&gt;               
## 17          807 &quot;Hunted… Dicke…                  37 en       Detective Fiction  
## 18          809 &quot;Holida… Dicke…                  37 en       Children&#39;s Picture…
## 19          810 &quot;George… Dicke…                  37 en       &lt;NA&gt;               
## 20          821 &quot;Dombey… Dicke…                  37 en       &lt;NA&gt;               
## 21          824 &quot;Speech… Dicke…                  37 en       &lt;NA&gt;               
## 22          872 &quot;Reprin… Dicke…                  37 en       &lt;NA&gt;               
## 23          882 &quot;Sketch… Dicke…                  37 en       &lt;NA&gt;               
## 24          883 &quot;Our Mu… Dicke…                  37 en       &lt;NA&gt;               
## 25          912 &quot;Mudfog… Dicke…                  37 en       &lt;NA&gt;               
## 26          914 &quot;The Un… Dicke…                  37 en       &lt;NA&gt;               
## 27          916 &quot;Sketch… Dicke…                  37 en       &lt;NA&gt;               
## 28          917 &quot;Barnab… Dicke…                  37 en       Historical Fiction 
## 29          918 &quot;Sketch… Dicke…                  37 en       &lt;NA&gt;               
## 30          922 &quot;Sunday… Dicke…                  37 en       &lt;NA&gt;               
## 31          924 &quot;To Be … Dicke…                  37 en       &lt;NA&gt;               
## 32          927 &quot;The La… Dicke…                  37 en       &lt;NA&gt;               
## 33          963 &quot;Little… Dicke…                  37 en       &lt;NA&gt;               
## 34          967 &quot;Nichol… Dicke…                  37 en       &lt;NA&gt;               
## 35          968 &quot;Martin… Dicke…                  37 en       Best Books Ever Li…
## 36         1023 &quot;Bleak … Dicke…                  37 en       &lt;NA&gt;               
## 37         1289 &quot;Three … Dicke…                  37 en       &lt;NA&gt;               
## 38         1392 &quot;The Se… Dicke…                  37 en       &lt;NA&gt;               
## 39         1394 &quot;The Ho… Dicke…                  37 en       &lt;NA&gt;               
## 40         1400 &quot;Great … Dicke…                  37 en       Best Books Ever Li…
## 41         1406 &quot;The Pe… Dicke…                  37 en       &lt;NA&gt;               
## 42         1407 &quot;A Mess… Dicke…                  37 en       &lt;NA&gt;               
## 43         1413 &quot;Tom Ti… Dicke…                  37 en       &lt;NA&gt;               
## 44         1414 &quot;Somebo… Dicke…                  37 en       &lt;NA&gt;               
## 45         1415 &quot;Doctor… Dicke…                  37 en       &lt;NA&gt;               
## 46         1416 &quot;Mrs. L… Dicke…                  37 en       &lt;NA&gt;               
## 47         1419 &quot;Mugby … Dicke…                  37 en       &lt;NA&gt;               
## 48         1421 &quot;Mrs. L… Dicke…                  37 en       &lt;NA&gt;               
## 49         1422 &quot;Going … Dicke…                  37 en       &lt;NA&gt;               
## 50         1435 &quot;Miscel… Dicke…                  37 en       &lt;NA&gt;               
## 51         1465 &quot;The Wr… Dicke…                  37 en       &lt;NA&gt;               
## 52         1467 &quot;Some C… Dicke…                  37 en       Christmas          
## 53        19337 &quot;A Chri… Dicke…                  37 en       Children&#39;s Literat…
## 54        20795 &quot;The Cr… Dicke…                  37 en       Children&#39;s Literat…
## 55        23344 &quot;The Ma… Dicke…                  37 en       Children&#39;s Picture…
## 56        23452 &quot;The Tr… Dicke…                  37 en       Children&#39;s Picture…
## 57        23765 &quot;Captai… Dicke…                  37 en       Children&#39;s Picture…
## 58        25852 &quot;The Le… Dicke…                  37 en       &lt;NA&gt;               
## 59        25853 &quot;The Le… Dicke…                  37 en       &lt;NA&gt;               
## 60        25854 &quot;The Le… Dicke…                  37 en       &lt;NA&gt;               
## 61        30368 &quot;A Chri… Dicke…                  37 en       &lt;NA&gt;               
## 62        32241 &quot;Dicken… Dicke…                  37 en       &lt;NA&gt;               
## 63        35536 &quot;The Po… Dicke…                  37 en       &lt;NA&gt;               
## 64        37121 &quot;Charle… Dicke…                  37 en       &lt;NA&gt;               
## 65        40723 &quot;The Ba… Dicke…                  37 en       &lt;NA&gt;               
## 66        42232 &quot;A Chil… Dicke…                  37 en       &lt;NA&gt;               
## 67        43111 &quot;The Pe… Dicke…                  37 en       &lt;NA&gt;               
## 68        43207 &quot;Scenes… Dicke…                  37 en       &lt;NA&gt;               
## 69        46675 &quot;Oliver… Dicke…                  37 en       &lt;NA&gt;               
## 70        47529 &quot;Oliver… Dicke…                  37 en       &lt;NA&gt;               
## 71        47530 &quot;Oliver… Dicke…                  37 en       &lt;NA&gt;               
## 72        47531 &quot;Oliver… Dicke…                  37 en       &lt;NA&gt;               
## 73        47534 &quot;The Po… Dicke…                  37 en       &lt;NA&gt;               
## 74        47535 &quot;The Po… Dicke…                  37 en       &lt;NA&gt;               
## # ℹ 4 more rows
## # ℹ 2 more variables: rights &lt;chr&gt;, has_text &lt;lgl&gt;</code></pre>
<p>Now we have the complete output, we can go and identify the IDs of
the works we’re interested in. We find that:</p>
<ul>
<li>46 = A Christmas Carol<br />
</li>
<li>98 = A Tale of Two Cities<br />
</li>
<li>580 = The Pickwick Papers<br />
</li>
<li>730 = Oliver Twist<br />
</li>
<li>766 = David Copperfield<br />
</li>
<li>786 = Hard Times<br />
</li>
<li>967 = Nicholas Nickleby<br />
</li>
<li>1400 = Great Expectations</li>
</ul>
<p>We could either download each of the texts individually, but that
would be a hassle. Instead, we can save all of the ID numbers in a new
variable, which we’ll call <code>list_dickens</code>:</p>
<pre class="r"><code>list_dickens &lt;- c(46, 98, 580, 730, 766, 786, 967, 1400)</code></pre>
<p>This allows us to download all of the texts in one go:</p>
<pre class="r"><code>dickens_corpus &lt;- gutenberg_download(list_dickens, meta_fields = &quot;title&quot;)</code></pre>
<pre><code>## Determining mirror for Project Gutenberg from https://www.gutenberg.org/robot/harvest</code></pre>
<pre><code>## Using mirror http://aleph.gutenberg.org</code></pre>
<p>With this, we download the eight novels, saving them to a data frame
with one row per line per work, as well as the additional metadata of
the title of each work. If one were working with works by different
authors, it might make sense to include the author names - but since we
don’t, we won’t.</p>
<p>Taking a look at the <code>dickens_corpus</code> object is not yet
very meaningful:</p>
<pre class="r"><code>dickens_corpus</code></pre>
<pre><code>## # A tibble: 182,668 × 3
##    gutenberg_id text                         title                              
##           &lt;int&gt; &lt;chr&gt;                        &lt;chr&gt;                              
##  1           46 &quot;A CHRISTMAS CAROL&quot;          A Christmas Carol in Prose; Being …
##  2           46 &quot;&quot;                           A Christmas Carol in Prose; Being …
##  3           46 &quot;IN PROSE&quot;                   A Christmas Carol in Prose; Being …
##  4           46 &quot;BEING&quot;                      A Christmas Carol in Prose; Being …
##  5           46 &quot;A Ghost Story of Christmas&quot; A Christmas Carol in Prose; Being …
##  6           46 &quot;&quot;                           A Christmas Carol in Prose; Being …
##  7           46 &quot;by Charles Dickens&quot;         A Christmas Carol in Prose; Being …
##  8           46 &quot;&quot;                           A Christmas Carol in Prose; Being …
##  9           46 &quot;&quot;                           A Christmas Carol in Prose; Being …
## 10           46 &quot;&quot;                           A Christmas Carol in Prose; Being …
## # ℹ 182,658 more rows</code></pre>
<p>We can see the title of <em>A Christmas Carol</em>, with one line per
row, but beyond that, not yet too much. So let’s get to the data
processing that will allow us to do a bit of meaningful work with these
texts.</p>
</div>
<div id="data-processing" class="section level1" number="3">
<h1><span class="header-section-number">3</span> Data Processing</h1>
<p>Now that we have loaded the data, we need to prepare it for the
analysis (the topic modelling). The following section(s) describe and go
through the various data processing steps such as cleaning and
transformation.</p>
<div id="pre-processing-part-1" class="section level2" number="3.1">
<h2><span class="header-section-number">3.1</span> Pre-Processing, Part
1</h2>
<p>The first step of the pre-processing requires us to transform the
texts we stored under <code>dickens_corpus</code> and clean it by
removing empty text elements using the <code>filter()</code> function
from the <code>dplyr</code> package. Then, we replace sentence
punctuation marks (.?!) with the sequence “qwerty”. Next, we group the
data by title using <code>group_by()</code> (also a function from the
<code>dplyr</code> package) and we then we reframe the data so that each
row represents one sentence. During the reframing, we split the text
into sentences by splitting the text by the “qwerty” sequence we just
added, we remove upper case sequences, and we remove upper case letters
except for capital <em>i</em> (I). This should result in a table where
each sentence represents one row.</p>
<pre class="r"><code>dickens_corpus %&gt;%
  dplyr::filter(text != &quot;&quot;) %&gt;%
  dplyr::mutate(text = stringr::str_replace_all(text, &quot;([[:digit:][:lower:]])((\\.|\\?\\!))&quot;, &quot;\\1\\2qwerty&quot;)) %&gt;%
  dplyr::group_by(title) %&gt;%
  dplyr::reframe(text = paste0(text, collapse = &quot; &quot;),
                 text = unlist(strsplit(text, &quot;qwerty&quot;)),
                 text = stringr::str_remove_all(text, &quot;[:upper:]{2,}&quot;),
                 text = stringr::str_replace_all(text, &quot; [A:H,J:Z] &quot;, &quot; &quot;),
                 text = stringr::str_squish(text)) -&gt; dickens_corpus</code></pre>
<p>To check if the splitting into sentences has worked, let’s have a
look at the transformed <code>dickens_corpus</code>:</p>
<pre class="r"><code>head(dickens_corpus$text, 10)</code></pre>
<pre><code>##  [1] &quot;A Ghost Story of Christmas by Charles Dickens I endeavoured in this Ghostly little book, to raise the Ghost of an Idea, which shall not put my readers out of humour with themselves, with each other, with the season, or with me.&quot;
##  [2] &quot;May it haunt their houses pleasantly, and no one wish to lay it.&quot;                                                                                                                                                                   
##  [3] &quot;Their faithful Friend and Servant, C. D. December, 1843.&quot;                                                                                                                                                                           
##  [4] &quot;Stave I: Marley&#39;s Ghost Stave The First of the Three Spirits Stave The Second of the Three Spirits Stave The Last of the Spirits Stave V: The End of It I: &#39;S was dead: to begin with.&quot;                                             
##  [5] &quot;There is no doubt whatever about that.&quot;                                                                                                                                                                                             
##  [6] &quot;The register of his burial was signed by the clergyman, the clerk, the undertaker, and the chief mourner.&quot;                                                                                                                          
##  [7] &quot;Scrooge signed it: and Scrooge&#39;s name was good upon &#39;Change, for anything he chose to put his hand to.&quot;                                                                                                                             
##  [8] &quot;Old Marley was as dead as a door-nail.&quot;                                                                                                                                                                                             
##  [9] &quot;Mind! I don&#39;t mean to say that I know, of my own knowledge, what there is particularly dead about a door-nail.&quot;                                                                                                                     
## [10] &quot;I might have been inclined, myself, to regard a coffin-nail as the deadest piece of ironmongery in the trade.&quot;</code></pre>
<p>As we can see, we are left with a data frame consisting of many rows
and 2 columns with each row representing one sentence. Unfortunately,
the sentence tokenisation has not worked perfectly as we see that in the
first row, the title and author (<em>A Ghost Story of Christmas by
Charles Dickens</em>) it still attached to the first sentence.</p>
<p>To get a sense of the current structure of the corpus, we can take a
look at the titles, in the form of a table:</p>
<pre class="r"><code>table(dickens_corpus$title)</code></pre>
<pre><code>## 
## A Christmas Carol in Prose; Being a Ghost Story of Christmas 
##                                                         1419 
##                                         A Tale of Two Cities 
##                                                         6525 
##                                            David Copperfield 
##                                                        17792 
##                                           Great Expectations 
##                                                         8363 
##                                                   Hard Times 
##                                                         5778 
##                                            Nicholas Nickleby 
##                                                        14763 
##                                                 Oliver Twist 
##                                                         8017 
##                                          The Pickwick Papers 
##                                                        19120</code></pre>
<p>This command builds a contingency table, basically counting how many
documents in the corpus are associated with each title. This shows us,
on the one hand, how different in length the different novels are. On
the other hand, it allows us to see what we already caught a hint of
above: each sentence gets its own row in the corpus. While this is not
an issue for some approaches of text analysis, for topic modelling it
will present a problem - one that we’ll root out, but only at a later
stage of the pre-processing.</p>
</div>
<div id="parsing-the-corpus" class="section level2" number="3.2">
<h2><span class="header-section-number">3.2</span> Parsing the
Corpus</h2>
<p>The next step is an essential, but also a computationally intense
one: we are going to parse the corpus. This is where we get the corpus
ready for topic modelling.</p>
<p>Before we go into the what and why of parsing, we suggest you
initiate the process - it can take a while (depending on how good your
device’s computing power is - it takes 1 hour and 6 seconds on my
laptop):</p>
<div class="warning"
style="padding:0.1em; background-color:#f2f2f2; color:#51247a">
<span>
<p style="margin-top:1em; text-align:center">
<strong>Because it takes so long, we suggest you do not execute the
following chunk but load the already processed data (as shown in the
<em>loading</em> chunk following the <em>parsing</em> chunk).</strong>
<br>
</p>
<p style="margin-left:1em;">
</p>
<p></span></p>
</div>
<p><br></p>
<pre class="r"><code># parsing
dickens_corpus$text %&gt;%
  # pos-tagging
  udpipe::udpipe_annotate(m_eng, x = .) %&gt;%
  # convert into data frame
  as.data.frame() %&gt;%
  # remove sentence variable and save
  dplyr::select(-sentence) -&gt; dickens_parsed</code></pre>
<blockquote>
<p><strong>Execute the following chunk to load the already parsed
data!</strong></p>
</blockquote>
<pre class="r"><code># loading
# you can directly load the parsed dickens data by executing this chunk
dickens_parsed  &lt;- base::readRDS(url(&quot;https://slcladal.github.io/data/dickens_parsed.rda&quot;, &quot;rb&quot;))</code></pre>
<p>As you see, we take the <em>text</em> variable of the
<code>dickens_corpus</code>, split it into sentences using the
<code>tokenize_sentence()</code> function from the <code>quanteda</code>
transform it with the command <code>spacy_parse</code> and assign it to
a new variable, <code>dickens_parsed</code>. What the command does is
tokenize and tag the texts, returning a data-table of the result. The
corpus is thus transformed from an object that has a sentence in each
row, to one where each word has its own row. More importantly, though,
each word is assigned a part of speech tag. Parsing text with udpipe
also include lemmatization and more complex tags like noun phrases, but
for our purposes, part of speech tags give enough information.</p>
<p>Once the parsing process is finished, we can terminate the Python
process that we initiated above, since it uses up a lot of memory in the
background:</p>
<p>Now, let’s take a look at the structure of our new variable,
<code>dickens_parsed</code>:</p>
<pre class="r"><code>str(dickens_parsed)</code></pre>
<pre><code>## &#39;data.frame&#39;:    1990092 obs. of  13 variables:
##  $ doc_id       : chr  &quot;doc1&quot; &quot;doc1&quot; &quot;doc1&quot; &quot;doc1&quot; ...
##  $ paragraph_id : int  1 1 1 1 1 1 1 1 1 1 ...
##  $ sentence_id  : int  1 1 1 1 1 1 1 1 1 1 ...
##  $ token_id     : chr  &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ...
##  $ token        : chr  &quot;A&quot; &quot;Ghost&quot; &quot;Story&quot; &quot;of&quot; ...
##  $ lemma        : chr  &quot;a&quot; &quot;Ghost&quot; &quot;story&quot; &quot;of&quot; ...
##  $ upos         : chr  &quot;DET&quot; &quot;ADJ&quot; &quot;NOUN&quot; &quot;ADP&quot; ...
##  $ xpos         : chr  &quot;DT&quot; &quot;JJ&quot; &quot;NN&quot; &quot;IN&quot; ...
##  $ feats        : chr  &quot;Definite=Ind|PronType=Art&quot; &quot;Degree=Pos&quot; &quot;Number=Sing&quot; NA ...
##  $ head_token_id: chr  &quot;3&quot; &quot;3&quot; &quot;10&quot; &quot;5&quot; ...
##  $ dep_rel      : chr  &quot;det&quot; &quot;amod&quot; &quot;nsubj&quot; &quot;case&quot; ...
##  $ deps         : chr  NA NA NA NA ...
##  $ misc         : chr  NA NA NA NA ...</code></pre>
<p>We are now dealing with a data frame that contains 1,996,275
observations (rows) of 7 variables (columns). Roughly two million words
for eight novels sounds about right. Of course, if we want to be sure
that we didn’t lose anything on the way thus far, we could go back to
the Gutenberg information to see whether we have the correct amount of
words. But we’ll carry on with the assumption that everything worked
fine so far.</p>
<p>The structure of the parsed corpus is very different from the
structure of the corpus as it was before. Among the columns, we find the
doc-, sentence- and token-ids, the tokens and their lemma, the part of
speech tag and the entity. Most of this, we will ignore for the current
purpose. What we need to get the corpus into processable form are the
part of speech tags.</p>
<p>So let’s look at the first few entries:</p>
<pre class="r"><code>head(dickens_parsed, n=15)</code></pre>
<pre><code>##    doc_id paragraph_id sentence_id token_id       token      lemma  upos xpos
## 1    doc1            1           1        1           A          a   DET   DT
## 2    doc1            1           1        2       Ghost      Ghost   ADJ   JJ
## 3    doc1            1           1        3       Story      story  NOUN   NN
## 4    doc1            1           1        4          of         of   ADP   IN
## 5    doc1            1           1        5   Christmas  Christmas PROPN  NNP
## 6    doc1            1           1        6          by         by   ADP   IN
## 7    doc1            1           1        7     Charles    Charles PROPN  NNP
## 8    doc1            1           1        8     Dickens    Dickens PROPN NNPS
## 9    doc1            1           1        9           I          I  PRON  PRP
## 10   doc1            1           1       10 endeavoured endeavoure  VERB  VBD
## 11   doc1            1           1       11          in         in   ADP   IN
## 12   doc1            1           1       12        this       this  PRON   DT
## 13   doc1            1           1       13     Ghostly    ghostly   ADV   RB
## 14   doc1            1           1       14      little     little   ADJ   JJ
## 15   doc1            1           1       15        book       book  NOUN   NN
##                                         feats head_token_id dep_rel deps
## 1                   Definite=Ind|PronType=Art             3     det &lt;NA&gt;
## 2                                  Degree=Pos             3    amod &lt;NA&gt;
## 3                                 Number=Sing            10   nsubj &lt;NA&gt;
## 4                                        &lt;NA&gt;             5    case &lt;NA&gt;
## 5                                 Number=Sing             3    nmod &lt;NA&gt;
## 6                                        &lt;NA&gt;             7    case &lt;NA&gt;
## 7                                 Number=Sing             5    nmod &lt;NA&gt;
## 8                                 Number=Plur             7    flat &lt;NA&gt;
## 9  Case=Nom|Number=Sing|Person=1|PronType=Prs            10   nsubj &lt;NA&gt;
## 10           Mood=Ind|Tense=Past|VerbForm=Fin             0    root &lt;NA&gt;
## 11                                       &lt;NA&gt;            12    case &lt;NA&gt;
## 12                   Number=Sing|PronType=Dem            10     obl &lt;NA&gt;
## 13                                       &lt;NA&gt;            14  advmod &lt;NA&gt;
## 14                                 Degree=Pos            15    amod &lt;NA&gt;
## 15                                Number=Sing            10     obj &lt;NA&gt;
##             misc
## 1           &lt;NA&gt;
## 2           &lt;NA&gt;
## 3           &lt;NA&gt;
## 4           &lt;NA&gt;
## 5           &lt;NA&gt;
## 6           &lt;NA&gt;
## 7           &lt;NA&gt;
## 8           &lt;NA&gt;
## 9           &lt;NA&gt;
## 10          &lt;NA&gt;
## 11          &lt;NA&gt;
## 12          &lt;NA&gt;
## 13          &lt;NA&gt;
## 14          &lt;NA&gt;
## 15 SpaceAfter=No</code></pre>
<p>In these 15 rows we already see both the uses and the drawbacks of
what we are about to do. In rows 13 and 14 we find the words
<em>Charles</em> and <em>Dickens</em>, respectively. These get
identified correctly as proper nouns. Looking at rows 3 and 5, however,
we see that also <em>Christmas</em> and <em>Carol</em> get identified as
proper nouns (PROPN in the upos column). While Carol is a name, here it
clearly does not refer to a person, but to a type of music that was
typically sung around Christmas in the olden days. The reason for the
mislabelling of these words is rather, ahem, prosaic: as part of the
title, they are both spelt with capital letters, which to the parser
indicates that they are proper nouns. And the reason this presents a
problem is because we are going to remove all proper nouns.</p>
<p>If you respect the integrity of books (as we of course do too, under
most circumstances), you might be cringing at this point. Why would we
ruthlessly mangle texts like that?</p>
</div>
<div id="topic-modelling-and-the-purposes-of-parsing"
class="section level2" number="3.3">
<h2><span class="header-section-number">3.3</span> Topic Modelling and
the Purposes of Parsing</h2>
<p>The concept of topic modelling is based on the Firthian hypothesis
that “you shall know a word by the company it keeps” <span
class="citation">(<a href="#ref-firth1957ling">Firth 1957,
11</a>)</span>. When applying topic modelling to a corpus of text, we
exploit the fact that words which frequently occur in similar contexts
are representative of the same topic. One of the most common techniques
to construct topic models is the latent Dirichlet allocation (LDA). The
LDA</p>
<blockquote>
<p>derives word clusters using a generative statistical process that
begins by assuming that each document in a collection of documents is
constructed from a mix of some set of possible topics. The model then
assigns high probabilities to words and sets of words that tend to
co-occur in multiple contexts across the corpus. <span
class="citation">(<a href="#ref-jockers2013macroanalysis">Jockers 2013,
123</a>)</span></p>
</blockquote>
<p>The algorithm we will use in a minute is called STM, which stands for
Structural Topic Model. STM is very similar to LDA, but in addition to
what the LDA does, STM is capable of processing meta-information about
texts, such as the author or the date on which a document was produced.
More details about STM are available in <span class="citation">Roberts,
Stewart, and Tingley (<a
href="#ref-roberts2019stm">2019</a>)</span>.</p>
<p>The output of a topic modelling process most frequently comes as a
set of words that co-occur in multiple contexts across the corpus. This
understanding of the process reveals why, in this case, we choose to
remove the proper nouns: in the context of novels, proper nouns are
mostly names of characters and locations - and these names and locations
tend to co-occur mostly within novels. The upshot of which is that the
model latches onto these distinct proper nouns, which display a pattern
that is much more consistent than any other co-occurrence patterns in
the corpus, thus crowding out other patterns that are actually helpful
when it comes to answering our research questions on social criticism
and literary realism in Dickens.</p>
<p>Picking up where we left off with the pre-processing, there is
perhaps a question to be answered regarding the misidentification of
<em>Carol</em> as a proper noun. If we see something like that,
shouldn’t we make sure that we at least keep these terms in, since this
is obviously not the type of word we want to get rid of when we remove
the proper nouns from the corpus?</p>
<p>Well, no. For two reasons: firstly, when pre-processing large
quantities of text, there are always going to be some inaccuracies. The
relative merits of keeping in individual tokens that we see are
misidentified are, for all intents and purposes, irrelevant when we are
dealing with corpora that contain millions of words. Secondly, and
probably more importantly, going in and manually adjusting for
individual tokens make the process a lot less reproducible. Obviously,
it depends on the purpose of a project, but generally we don’t only want
to produce research, but produce it in such a way that anyone can
reproduce it (if they set their mind to it). Instead of going in and
fixing individual mislabellings, it’s a lot more useful to make sure
that each step of the research project is comprehensible to someone not
involved in the process.</p>
<p>That being said, if you end up with a topic model that is hard to
make sense of, and you have reason to believe that this is the result of
unsatisfactory pre-processing, it’s generally worthwhile to reflect on
each of the pre-processing steps, rather than going in and fixing
individual instances. We actually ended up doing some of the former for
this project, as we will detail below.</p>
</div>
<div id="pre-processing-part-2" class="section level2" number="3.4">
<h2><span class="header-section-number">3.4</span> Pre-Processing, Part
2</h2>
<p>With the parsed corpus, as well as an understanding of how topic
modelling works, let’s proceed with the pre-processing.</p>
</div>
<div id="removing-proper-nouns" class="section level2" number="3.5">
<h2><span class="header-section-number">3.5</span> Removing Proper
Nouns</h2>
<p>First off, we remove the proper nouns:</p>
<pre class="r"><code>no_prop_dickens &lt;- dickens_parsed %&gt;%
  dplyr::filter( upos != &quot;PROPN&quot;)</code></pre>
<p>We define a new variable, <code>no_prop_dickens</code>, and assign to
it the parsed Dickens corpus without the proper nouns. Specifically, we
assign all tokens which have not been tagged as proper nouns to the new
variable. The logical operator <code>!=</code> stands for
<code>not equal to</code>, thus allowing us to retain everything that is
not a proper noun.</p>
<p>Which, if we look at the number of rows the new variable contains,
turns out be most of the corpus:</p>
<pre class="r"><code>nrow(no_prop_dickens)</code></pre>
<pre><code>## [1] 1916359</code></pre>
<p>We get 1,916,359 rows, which means that only roughly 88,000 tokens
had been classified as proper nouns. While this sounds like a lot in
absolute terms, it only amounts to about 4.4% of the corpus. Considering
how much of an improvement this is for the final model, it’s an
absolutely acceptable price to pay.</p>
<p>It’s also worth checking briefly whether it was actually successful
at removing what we wanted to remove:</p>
<pre class="r"><code>head(no_prop_dickens, n=15)</code></pre>
<pre><code>##    doc_id paragraph_id sentence_id token_id       token      lemma  upos xpos
## 1    doc1            1           1        1           A          a   DET   DT
## 2    doc1            1           1        2       Ghost      Ghost   ADJ   JJ
## 3    doc1            1           1        3       Story      story  NOUN   NN
## 4    doc1            1           1        4          of         of   ADP   IN
## 5    doc1            1           1        6          by         by   ADP   IN
## 6    doc1            1           1        9           I          I  PRON  PRP
## 7    doc1            1           1       10 endeavoured endeavoure  VERB  VBD
## 8    doc1            1           1       11          in         in   ADP   IN
## 9    doc1            1           1       12        this       this  PRON   DT
## 10   doc1            1           1       13     Ghostly    ghostly   ADV   RB
## 11   doc1            1           1       14      little     little   ADJ   JJ
## 12   doc1            1           1       15        book       book  NOUN   NN
## 13   doc1            1           1       16           ,          , PUNCT    ,
## 14   doc1            1           1       17          to         to  PART   TO
## 15   doc1            1           1       18       raise      raise  VERB   VB
##                                         feats head_token_id dep_rel deps
## 1                   Definite=Ind|PronType=Art             3     det &lt;NA&gt;
## 2                                  Degree=Pos             3    amod &lt;NA&gt;
## 3                                 Number=Sing            10   nsubj &lt;NA&gt;
## 4                                        &lt;NA&gt;             5    case &lt;NA&gt;
## 5                                        &lt;NA&gt;             7    case &lt;NA&gt;
## 6  Case=Nom|Number=Sing|Person=1|PronType=Prs            10   nsubj &lt;NA&gt;
## 7            Mood=Ind|Tense=Past|VerbForm=Fin             0    root &lt;NA&gt;
## 8                                        &lt;NA&gt;            12    case &lt;NA&gt;
## 9                    Number=Sing|PronType=Dem            10     obl &lt;NA&gt;
## 10                                       &lt;NA&gt;            14  advmod &lt;NA&gt;
## 11                                 Degree=Pos            15    amod &lt;NA&gt;
## 12                                Number=Sing            10     obj &lt;NA&gt;
## 13                                       &lt;NA&gt;            10   punct &lt;NA&gt;
## 14                                       &lt;NA&gt;            18    mark &lt;NA&gt;
## 15                               VerbForm=Inf            10   advcl &lt;NA&gt;
##             misc
## 1           &lt;NA&gt;
## 2           &lt;NA&gt;
## 3           &lt;NA&gt;
## 4           &lt;NA&gt;
## 5           &lt;NA&gt;
## 6           &lt;NA&gt;
## 7           &lt;NA&gt;
## 8           &lt;NA&gt;
## 9           &lt;NA&gt;
## 10          &lt;NA&gt;
## 11          &lt;NA&gt;
## 12 SpaceAfter=No
## 13          &lt;NA&gt;
## 14          &lt;NA&gt;
## 15          &lt;NA&gt;</code></pre>
<p>The first fifteen rows certainly indicate a success: gone is
<em>Christmas</em> and <em>Carol</em>, gone is <em>Charles</em> and
<em>Dickens</em>. Certain other features that are also not helpful for
interpreting the content of the corpus currently remain, most
prominently punctuation. Let’s take care of this.</p>
</div>
<div id="tokenizing-the-text" class="section level2" number="3.6">
<h2><span class="header-section-number">3.6</span> Tokenizing the
Text</h2>
<p>The first step is tokenizing the <code>no_prop_dickens</code> and
removing punctuation and non-alphanumeric characters:</p>
<pre class="r"><code>no_prop_dickens %&gt;%
  dplyr::group_by(sentence_id) %&gt;%
  dplyr::summarise(sen = paste0(lemma, collapse = &quot; &quot;)) %&gt;%
  dplyr::pull(sen) %&gt;%
  #tokenisation
  quanteda::tokenize_sentence() %&gt;%
  unlist() %&gt;% 
  # remove non-word alpha-numeric characters
  stringr::str_remove_all(&quot;[^[:alnum:] ]&quot;) %&gt;%
  # remiove possessive s
  stringr::str_replace_all(&quot; s &quot;, &quot; &quot;) %&gt;%
  # convert to lower case 
  tolower() -&gt; toks</code></pre>
<p>We assign the contents of the <code>no_prop_dickens</code> object a
new variable <code>toks</code>, coercing the contents into a token
object. With this, we get rid of a lot of excess information that is not
relevant for the next steps. Let’s take a look:</p>
<pre class="r"><code>head(toks, n=10)</code></pre>
<pre><code>##  [1] &quot;a ghost story of by i endeavoure in this ghostly little book  to raise the ghost of a idea  which shall not put my reader out of humour with themselves  with each other  with the season  or with i  may it haunt they house pleasantly  and no one wish to lay it  they faithful friend and   1843 &quot;                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    
##  [2] &quot;stave i  ghost there be no doubt whatever about that  the register of he burial be sign by the clergyman  the clerk  the undertaker  and the chief mourner  sign it  and name be good upon   for anything he choose to put he hand to  be as dead as a door nail  mind &quot;                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  
##  [3] &quot;i might have be inclin  myself  to regard a coffin nail as the deadest piece of ironmongery in the trade  but the wisdom of we ancestor be in the simile  and my unhallowed hand shall not disturb it  or the country do for  you will therefore permit i to repeat  emphatically  that be as dead as a door  nail  know he be dead &quot;                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     
##  [4] &quot;how could it be otherwise &quot;                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               
##  [5] &quot;be he sole executor  he sole administrator  he sole assign  he sole residuary legatee  he sole friend  and sole mourner  and even be not so dreadfully cut up by the sad event  but that he be a excellent man of business on the very day of the funeral  and solemnise it with a undoubted bargain  the mention of funeral bring i back to the point i start from  there be no doubt that be dead  this must be distinctly understand  or nothing wonderful can come of the story i be go to relate  if we be not perfectly convince that die before the play begin  there would be nothing more remarkable in he take a stroll at night  in a easterly wind  upon he own rampart  than there would be in any other middle  a gentleman rashly turn out after dark in a breezy spot  say for instance  literally to astonish he son weak mind  never paint out name  there it stand  year afterwards  above the warehouse door  and  the firm be know as and  sometimes people new to the business call  and sometimes  but he answer to both name  it be all the same to he  oh &quot;                                                                                                                                                                                                                                                                                                                      
##  [6] &quot;the cold within he froze he old feature  nip he point nose  shrivell he cheek  stiffen he gait  make he eye red  he thin lip blue  and speak out shrewdly in he grating voice  frosty rime be on he head  and on he eyebrow  and he wiry ching  he carry he own low temperature always about with he  he ice he office in the dog  day  and do not thaw it one degree at  external heat and could have little influence on  no warmth could warm  no wintry weather chill he  no wind that blew be bitterer than he  no falling snow be more intent upon its purpose  no pelting rain less open to entreaty &quot;                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             
##  [7] &quot;foul weather do not know where to have he  the heaviest rain  and snow  and hail  and sleet  could boast of the advantage over he in only one respect  they often  come down  handsomely  and never do  nobody ever stop he in the street to say  with gladsome look   my dear scrooge  how be you &quot;                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      
##  [8] &quot;even the blind man dog appear to know he  and when they see he come on  would tug they owner into doorway and up court  and then would wag they tail as though they say   no eye at all be better than a evil eye  dark master &quot;                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          
##  [9] &quot; to edge he way along the crowded path of life  warn all human sympathy to keep its distance  be what the know one call  nut  to  once upon a time  of all the good day in the year  on old sit busy in he counting  house  it be cold  bleak  biting weather  foggy withal  and he could hear the people in the court outside  go wheeze up and down  beat they hand upon they breast  and stampe they foot upon the pavement stone to warm they  the city clock have only just go three  but it be quite dark already  it have not be light all day  and candle be flar in the window of the neighbour office  like ruddy smear upon the palpable brown air  the fog come pour in at every chink and keyhole  and be so dense without  that although the court be of the narrowest  the house opposite be mere phantoms  to see the dingy clould come droope down  obscur everything  one might have think that liv hard by  and be brew on a large scale  the door of counting  house be open that he might keep he eye upon he clerk  who in a dismal little cell beyond  a sort of tank  be copying letter  have a very small fire  but the clerk fire be so very much smaller that it look like one coal  but he could not replenish it  for keep the coal  box in he own room  and so surely as the clerk come in with the shovel  the master predict that it would be necessary for they to part &quot;
## [10] &quot;wherefore the clerk put on he white comforter  and try to warm himself at the candle  in which effort  not be a man of a strong imagination  he fail   a merry  uncle &quot;</code></pre>
<p>The corpus now looks a lot like it did before we started with the
pre-processing: we see that again each sentence occupies one document.
The obvious difference is that all the proper nouns are gone - as we
intended.</p>
</div>
<div id="punctuation" class="section level2" number="3.7">
<h2><span class="header-section-number">3.7</span> Punctuation</h2>
<p>Our next target is to transform the <code>toks</code> object into a
tokens object again (yes, it already is one), but this time we add
logical conditions which state <code>remove_punct = TRUE</code>,
<code>remove_symbols = TRUE</code>, and
<code>remove_numbers = TRUE</code>:</p>
<pre class="r"><code>toks &lt;- tokens(toks, remove_punct = TRUE, remove_symbols = TRUE, remove_numbers = TRUE)</code></pre>
<p>Again, a quick check to see if it worked:</p>
<pre class="r"><code>head(toks, n=15)</code></pre>
<pre><code>## Tokens consisting of 15 documents.
## text1 :
##  [1] &quot;a&quot;          &quot;ghost&quot;      &quot;story&quot;      &quot;of&quot;         &quot;by&quot;        
##  [6] &quot;i&quot;          &quot;endeavoure&quot; &quot;in&quot;         &quot;this&quot;       &quot;ghostly&quot;   
## [11] &quot;little&quot;     &quot;book&quot;      
## [ ... and 44 more ]
## 
## text2 :
##  [1] &quot;stave&quot;    &quot;i&quot;        &quot;ghost&quot;    &quot;there&quot;    &quot;be&quot;       &quot;no&quot;      
##  [7] &quot;doubt&quot;    &quot;whatever&quot; &quot;about&quot;    &quot;that&quot;     &quot;the&quot;      &quot;register&quot;
## [ ... and 40 more ]
## 
## text3 :
##  [1] &quot;i&quot;      &quot;might&quot;  &quot;have&quot;   &quot;be&quot;     &quot;inclin&quot; &quot;myself&quot; &quot;to&quot;     &quot;regard&quot;
##  [9] &quot;a&quot;      &quot;coffin&quot; &quot;nail&quot;   &quot;as&quot;    
## [ ... and 51 more ]
## 
## text4 :
## [1] &quot;how&quot;       &quot;could&quot;     &quot;it&quot;        &quot;be&quot;        &quot;otherwise&quot;
## 
## text5 :
##  [1] &quot;be&quot;            &quot;he&quot;            &quot;sole&quot;          &quot;executor&quot;     
##  [5] &quot;he&quot;            &quot;sole&quot;          &quot;administrator&quot; &quot;he&quot;           
##  [9] &quot;sole&quot;          &quot;assign&quot;        &quot;he&quot;            &quot;sole&quot;         
## [ ... and 186 more ]
## 
## text6 :
##  [1] &quot;the&quot;     &quot;cold&quot;    &quot;within&quot;  &quot;he&quot;      &quot;froze&quot;   &quot;he&quot;      &quot;old&quot;    
##  [8] &quot;feature&quot; &quot;nip&quot;     &quot;he&quot;      &quot;point&quot;   &quot;nose&quot;   
## [ ... and 103 more ]
## 
## [ reached max_ndoc ... 9 more documents ]</code></pre>
<p>Comparing this latest output to the one preceding it, we find that
the comma, which was in the last position, has disappeared. We are
getting closer.</p>
</div>
<div id="stopwords" class="section level2" number="3.8">
<h2><span class="header-section-number">3.8</span> Stopwords</h2>
<p>The final of these smaller pre-processing steps, which are all
commonly applied with a lot of automated text analysis, is the removal
of stopwords. These are words which are very common, to a degree that
one can assume that they will show up on practically every page of a
book, thus making it harder for our model to identify more salient and
content-related co-occurrence patterns.</p>
<p>Stopwords are typically also words that have no little semantic
meaning on their own, for example <em>the</em>, <em>in</em>, etc. As
they rather regulate the relations between words, they are also called
function words (as opposed to content words, which we want to keep).
There is no universal list of stopwords - and some methods make do
without removing stopwords - but for our purposes, it makes sense to
just work with the default list supplied in the quanteda package. The
command is as follows:</p>
<pre class="r"><code>toks &lt;- toks %&gt;%
   # remove stopwords
  tokens_remove(pattern = stopwords(&quot;english&quot;)) </code></pre>
<p>Looking at the first few lines shows that this, too, worked:</p>
<pre class="r"><code>head(toks, n=15)</code></pre>
<pre><code>## Tokens consisting of 15 documents.
## text1 :
##  [1] &quot;ghost&quot;      &quot;story&quot;      &quot;endeavoure&quot; &quot;ghostly&quot;    &quot;little&quot;    
##  [6] &quot;book&quot;       &quot;raise&quot;      &quot;ghost&quot;      &quot;idea&quot;       &quot;shall&quot;     
## [11] &quot;put&quot;        &quot;reader&quot;    
## [ ... and 11 more ]
## 
## text2 :
##  [1] &quot;stave&quot;      &quot;ghost&quot;      &quot;doubt&quot;      &quot;whatever&quot;   &quot;register&quot;  
##  [6] &quot;burial&quot;     &quot;sign&quot;       &quot;clergyman&quot;  &quot;clerk&quot;      &quot;undertaker&quot;
## [11] &quot;chief&quot;      &quot;mourner&quot;   
## [ ... and 12 more ]
## 
## text3 :
##  [1] &quot;might&quot;       &quot;inclin&quot;      &quot;regard&quot;      &quot;coffin&quot;      &quot;nail&quot;       
##  [6] &quot;deadest&quot;     &quot;piece&quot;       &quot;ironmongery&quot; &quot;trade&quot;       &quot;wisdom&quot;     
## [11] &quot;ancestor&quot;    &quot;simile&quot;     
## [ ... and 15 more ]
## 
## text4 :
## [1] &quot;otherwise&quot;
## 
## text5 :
##  [1] &quot;sole&quot;          &quot;executor&quot;      &quot;sole&quot;          &quot;administrator&quot;
##  [5] &quot;sole&quot;          &quot;assign&quot;        &quot;sole&quot;          &quot;residuary&quot;    
##  [9] &quot;legatee&quot;       &quot;sole&quot;          &quot;friend&quot;        &quot;sole&quot;         
## [ ... and 79 more ]
## 
## text6 :
##  [1] &quot;cold&quot;     &quot;within&quot;   &quot;froze&quot;    &quot;old&quot;      &quot;feature&quot;  &quot;nip&quot;     
##  [7] &quot;point&quot;    &quot;nose&quot;     &quot;shrivell&quot; &quot;cheek&quot;    &quot;stiffen&quot;  &quot;gait&quot;    
## [ ... and 49 more ]
## 
## [ reached max_ndoc ... 9 more documents ]</code></pre>
<p>We are left with rather fewer words than before, but the ones that
remain are the ones which could be relevant for our research
question.</p>
</div>
<div id="chunks" class="section level2" number="3.9">
<h2><span class="header-section-number">3.9</span> Chunks</h2>
<p>The next challenge is the structure in which Gutenberg supplies its
texts: each sentence occupies one row. In order to arrive at
interpretable results, we need to get chunks of text that are long
enough to contain co-occurrences of words, and we need to get enough of
them in order for the LDA to be able identify patterns of co-occurrences
throughout the corpus. Sentences are too short for this, and with only
eight novels, dividing the corpus into books would not yield good
results because there are not enough different texts in which to observe
co-occurrence patterns.</p>
<p>This leaves two options: dividing the corpus along chapters, or
simply creating chunks of a certain length. It is the latter approach we
are pursuing here, for three reasons: reconstructing the corpus along
chapters is technically somewhat more involved; on top of that, a
chapter may still be so long that the topic changes considerably for the
beginning of the chapter to the end; finally, you might be interested in
working with texts from a different source than Gutenberg, and the
approach to chunking that we use here will work for most textual data,
regardless of the source.</p>
<p>We begin by unraveling these documents into an object that contains a
single token per row. The command <code>unlist()</code> allows us to do
this, as it produces a vector which contains all the atomic components
that occur in an object.</p>
<pre class="r"><code>list_toks &lt;- unlist(toks, use.names = F)</code></pre>
<p>Now, we basically have Dickens novels as a long list of individual
words, without a proper name:</p>
<pre class="r"><code>head(list_toks, n=15)</code></pre>
<pre><code>##  [1] &quot;ghost&quot;      &quot;story&quot;      &quot;endeavoure&quot; &quot;ghostly&quot;    &quot;little&quot;    
##  [6] &quot;book&quot;       &quot;raise&quot;      &quot;ghost&quot;      &quot;idea&quot;       &quot;shall&quot;     
## [11] &quot;put&quot;        &quot;reader&quot;     &quot;humour&quot;     &quot;season&quot;     &quot;may&quot;</code></pre>
<p>Now, we are faced with choosing the size of the text chunks we want
to feed into our model. There is no best practice of how to choose the
chunk size, which means that there is some trial and error involved
here. Basically we want chunks that are large enough to allow for
meaningful co-occurrences to be observed, but small enough that
co-occurrences that get identified by the model are meaningful in some
way. One way to get there is to use paragraphs, as they are a meaningful
unit of discourse. But we do not have paragraphs in our corpus.</p>
<p>The heuristic we chose to start from is that two pages of a novel
should contain a bunch of words relating to a similar theme - thinking
for instance of descriptions of a space or person. As the average A5
page contains roughly 500 words, we started out with chunk sizes of
1,000 words.</p>
<p>However, it is very much a process of trial and error as to what
chunk size yields the best results. We played around with a bunch of
plausible sizes - plausible in the sense that, for instance, only half a
page is roughly the lower bound on which to expect co-occurrences. Thus,
we tried different versions between 250 and 5000 words, and we will
explore different chunk sizes in this range together in what
follows.</p>
<p>Once we decide on where to start, we simply define the chunk
size:</p>
<pre class="r"><code>chunk &lt;- 1000</code></pre>
<p>Next, we assign the length of our corpus to a new variable:</p>
<pre class="r"><code>n &lt;- length(list_toks)</code></pre>
<p>We now have two variables, <code>chunk</code> and <code>n</code>
which simply contain a number each. With these, we define a new
object:</p>
<pre class="r"><code>r &lt;- rep(1:ceiling(n/chunk), each = chunk)[1:n]</code></pre>
<p>This takes a bit of unpacking. The <code>rep()</code> function
replicates the values in the brackets. But what are we feeding it there?
The centerpiece is in the fraction <code>n/chunk</code>, a simple
calculation which states that, given the length (n) of our corpus, if we
want chunks of 1,000 words each, there will be 717 chunks. We set this
value (716.412, to be precise) as the ceiling. In effect, we are setting
a range between 1 and 717. The next element, <code>each = chunk</code>,
tells us how frequently each element in the range is to be repeated.
Finally, with the specifications in the square brackets, we say that
this process should be carried out for the whole length of our corpus.
Confused yet?</p>
<p>If we simply were to run this command, instead of assigning it to the
object
<code>r``, as we do above, the output would be a list of 1,000 repetitions of</code>1<code>, followed by 1,000 repetitions of</code>2<code>and so on, until we get to 1,000 repetitions of</code>716<code>and finally some repetitions of</code>717`.</p>
<p>Have a look at the variable r, for instance with the table
function:</p>
<pre class="r"><code>table(r)</code></pre>
<pre><code>## r
##    1    2    3    4    5    6    7    8    9   10   11   12   13   14   15   16 
## 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 
##   17   18   19   20   21   22   23   24   25   26   27   28   29   30   31   32 
## 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 
##   33   34   35   36   37   38   39   40   41   42   43   44   45   46   47   48 
## 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 
##   49   50   51   52   53   54   55   56   57   58   59   60   61   62   63   64 
## 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 
##   65   66   67   68   69   70   71   72   73   74   75   76   77   78   79   80 
## 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 
##   81   82   83   84   85   86   87   88   89   90   91   92   93   94   95   96 
## 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 
##   97   98   99  100  101  102  103  104  105  106  107  108  109  110  111  112 
## 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 
##  113  114  115  116  117  118  119  120  121  122  123  124  125  126  127  128 
## 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 
##  129  130  131  132  133  134  135  136  137  138  139  140  141  142  143  144 
## 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 
##  145  146  147  148  149  150  151  152  153  154  155  156  157  158  159  160 
## 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 
##  161  162  163  164  165  166  167  168  169  170  171  172  173  174  175  176 
## 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 
##  177  178  179  180  181  182  183  184  185  186  187  188  189  190  191  192 
## 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 
##  193  194  195  196  197  198  199  200  201  202  203  204  205  206  207  208 
## 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 
##  209  210  211  212  213  214  215  216  217  218  219  220  221  222  223  224 
## 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 
##  225  226  227  228  229  230  231  232  233  234  235  236  237  238  239  240 
## 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 
##  241  242  243  244  245  246  247  248  249  250  251  252  253  254  255  256 
## 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 
##  257  258  259  260  261  262  263  264  265  266  267  268  269  270  271  272 
## 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 
##  273  274  275  276  277  278  279  280  281  282  283  284  285  286  287  288 
## 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 
##  289  290  291  292  293  294  295  296  297  298  299  300  301  302  303  304 
## 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 
##  305  306  307  308  309  310  311  312  313  314  315  316  317  318  319  320 
## 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 
##  321  322  323  324  325  326  327  328  329  330  331  332  333  334  335  336 
## 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 
##  337  338  339  340  341  342  343  344  345  346  347  348  349  350  351  352 
## 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 
##  353  354  355  356  357  358  359  360  361  362  363  364  365  366  367  368 
## 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 
##  369  370  371  372  373  374  375  376  377  378  379  380  381  382  383  384 
## 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 
##  385  386  387  388  389  390  391  392  393  394  395  396  397  398  399  400 
## 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 
##  401  402  403  404  405  406  407  408  409  410  411  412  413  414  415  416 
## 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 
##  417  418  419  420  421  422  423  424  425  426  427  428  429  430  431  432 
## 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 
##  433  434  435  436  437  438  439  440  441  442  443  444  445  446  447  448 
## 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 
##  449  450  451  452  453  454  455  456  457  458  459  460  461  462  463  464 
## 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 
##  465  466  467  468  469  470  471  472  473  474  475  476  477  478  479  480 
## 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 
##  481  482  483  484  485  486  487  488  489  490  491  492  493  494  495  496 
## 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 
##  497  498  499  500  501  502  503  504  505  506  507  508  509  510  511  512 
## 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 
##  513  514  515  516  517  518  519  520  521  522  523  524  525  526  527  528 
## 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 
##  529  530  531  532  533  534  535  536  537  538  539  540  541  542  543  544 
## 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 
##  545  546  547  548  549  550  551  552  553  554  555  556  557  558  559  560 
## 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 
##  561  562  563  564  565  566  567  568  569  570  571  572  573  574  575  576 
## 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 
##  577  578  579  580  581  582  583  584  585  586  587  588  589  590  591  592 
## 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 
##  593  594  595  596  597  598  599  600  601  602  603  604  605  606  607  608 
## 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 
##  609  610  611  612  613  614  615  616  617  618  619  620  621  622  623  624 
## 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 
##  625  626  627  628  629  630  631  632  633  634  635  636  637  638  639  640 
## 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 
##  641  642  643  644  645  646  647  648  649  650  651  652  653  654  655  656 
## 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 
##  657  658  659  660  661  662  663  664  665  666  667  668  669  670  671  672 
## 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 
##  673  674  675  676  677  678  679  680  681  682  683  684  685  686  687  688 
## 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 
##  689  690  691  692  693  694  695 
## 1000 1000 1000 1000 1000 1000  869</code></pre>
<p>The table shows you the number of repeated items. The number
corresponds to what we describe above. <code>1</code> is repeated 1,000
times, as is <code>2</code>, as is <code>3</code> and so on, until we
get to <code>702</code>, which is only repeated 412 times. The purpose
to this madness comes with the next command, where we split the corpus
in its current <code>list_toks</code> form into chunks of 1,000 words
each:</p>
<pre class="r"><code>chunky_dickens &lt;- split(list_toks, r)</code></pre>
<p>Basically, the first 1,000 words are assigned to the first
pseudo-document, simply labeled <code>1</code>, the second 1,000 words
are assigned to a second pseudo-document, labeled <code>2</code>, and so
on and so forth. If this does not seem to make heads or tails, the
description of the split function in R is actually quite clear.</p>
<pre class="r"><code>help(split)</code></pre>
<p>This tells us that the command <code>split(x, f)</code> divides the
data in the vector x into the groups defined by f. Which is how we end
up with 717 chunks, 716 of which contain 1,000 words and the last of
which contains the end of the tail, so to speak.</p>
<p>Before looking at the contents of the <code>chunky_dickens</code>
object, we can get an indication for whether the process was a success
by looking at the length of the object:</p>
<pre class="r"><code>length(chunky_dickens)</code></pre>
<pre><code>## [1] 695</code></pre>
<p>The length of 695, indicating 695 pseudo-documents in the corpus, is
already a good sign. We term these chunks pseudo-documents since they
are artificially constructed and not exactly representative of a
structure of the novels (the way chapters would be, for instance). Now,
let’s take a look at only the structure of our new object
<code>chunky_dickens</code>:</p>
<pre class="r"><code>str(chunky_dickens)</code></pre>
<pre><code>## List of 695
##  $ 1  : chr [1:1000] &quot;ghost&quot; &quot;story&quot; &quot;endeavoure&quot; &quot;ghostly&quot; ...
##  $ 2  : chr [1:1000] &quot;every&quot; &quot;twenty&quot; &quot;fifth&quot; &quot;say&quot; ...
##  $ 3  : chr [1:1000] &quot;seven&quot; &quot;year&quot; &quot;say&quot; &quot;ghost&quot; ...
##  $ 4  : chr [1:1000] &quot;squeak&quot; &quot;scuffle&quot; &quot;mouse&quot; &quot;behind&quot; ...
##  $ 5  : chr [1:1000] &quot;sit&quot; &quot;beautiful&quot; &quot;young&quot; &quot;girl&quot; ...
##  $ 6  : chr [1:1000] &quot;lemon&quot; &quot;great&quot; &quot;compactness&quot; &quot;juicy&quot; ...
##  $ 7  : chr [1:1000] &quot;round&quot; &quot;bye&quot; &quot;song&quot; &quot;lost&quot; ...
##  $ 8  : chr [1:1000] &quot;last&quot; &quot;plump&quot; &quot;sister&quot; &quot;fall&quot; ...
##  $ 9  : chr [1:1000] &quot;speak&quot; &quot;throw&quot; &quot;bundle&quot; &quot;floor&quot; ...
##  $ 10 : chr [1:1000] &quot;name&quot; &quot;finger&quot; &quot;point&quot; &quot;grave&quot; ...
##  $ 11 : chr [1:1000] &quot;fire&quot; &quot;blunderbuss&quot; &quot;among&quot; &quot;load&quot; ...
##  $ 12 : chr [1:1000] &quot;message&quot; &quot;deliver&quot; &quot;night&quot; &quot;watchman&quot; ...
##  $ 13 : chr [1:1000] &quot;negro&quot; &quot;cupid&quot; &quot;several&quot; &quot;headless&quot; ...
##  $ 14 : chr [1:1000] &quot;door&quot; &quot;step&quot; &quot;little&quot; &quot;pot&quot; ...
##  $ 15 : chr [1:1000] &quot;crowded&quot; &quot;part&quot; &quot;bad&quot; &quot;enough&quot; ...
##  $ 16 : chr [1:1000] &quot;resume&quot; &quot;work&quot; &quot;recognise&quot; &quot;monsieur&quot; ...
##  $ 17 : chr [1:1000] &quot;least&quot; &quot;death&quot; &quot;remedy&quot; &quot;thing&quot; ...
##  $ 18 : chr [1:1000] &quot;look&quot; &quot;ceiling&quot; &quot;stare&quot; &quot;human&quot; ...
##  $ 19 : chr [1:1000] &quot;anything&quot; &quot;bulk&quot; &quot;stature&quot; &quot;render&quot; ...
##  $ 20 : chr [1:1000] &quot;shoemaker&quot; &quot;garret&quot; &quot;yet&quot; &quot;one&quot; ...
##  $ 21 : chr [1:1000] &quot;remove&quot; &quot;towel&quot; &quot;head&quot; &quot;steam&quot; ...
##  $ 22 : chr [1:1000] &quot;recover&quot; &quot;may&quot; &quot;never&quot; &quot;feel&quot; ...
##  $ 23 : chr [1:1000] &quot;business&quot; &quot;let&quot; &quot;everything&quot; &quot;go&quot; ...
##  $ 24 : chr [1:1000] &quot;hold&quot; &quot;say&quot; &quot;hold&quot; &quot;horse&quot; ...
##  $ 25 : chr [1:1000] &quot;come&quot; &quot;know&quot; &quot;yomonseigneu&quot; &quot;receive&quot; ...
##  $ 26 : chr [1:1000] &quot;penetrat&quot; &quot;midst&quot; &quot;group&quot; &quot;fifty&quot; ...
##  $ 27 : chr [1:1000] &quot;agreeable&quot; &quot;woman&quot; &quot;society&quot; &quot;go&quot; ...
##  $ 28 : chr [1:1000] &quot;final&quot; &quot;way&quot; &quot;matter&quot; &quot;matter&quot; ...
##  $ 29 : chr [1:1000] &quot;get&quot; &quot;roof&quot; &quot;hearse&quot; &quot;exercise&quot; ...
##  $ 30 : chr [1:1000] &quot;fine&quot; &quot;morning&quot; &quot;say&quot; &quot;yong&quot; ...
##  $ 31 : chr [1:1000] &quot;next&quot; &quot;day&quot; &quot;meet&quot; &quot;warn&quot; ...
##  $ 32 : chr [1:1000] &quot;detect&quot; &quot;sign&quot; &quot;lounge&quot; &quot;away&quot; ...
##  $ 33 : chr [1:1000] &quot;consideration&quot; &quot;brother&quot; &quot;bridegroom&quot; &quot;say&quot; ...
##  $ 34 : chr [1:1000] &quot;say&quot; &quot;curious&quot; &quot;perhaps&quot; &quot;better&quot; ...
##  $ 35 : chr [1:1000] &quot;first&quot; &quot;time&quot; &quot;though&quot; &quot;perfectly&quot; ...
##  $ 36 : chr [1:1000] &quot;noise&quot; &quot;living&quot; &quot;ocean&quot; &quot;irruption&quot; ...
##  $ 37 : chr [1:1000] &quot;equal&quot; &quot;purpose&quot; &quot;nevertheless&quot; &quot;class&quot; ...
##  $ 38 : chr [1:1000] &quot;many&quot; &quot;day&quot; &quot;inmate&quot; &quot;listen&quot; ...
##  $ 39 : chr [1:1000] &quot;bread&quot; &quot;hold&quot; &quot;impoverished&quot; &quot;involved&quot; ...
##  $ 40 : chr [1:1000] &quot;identification&quot; &quot;strict&quot; &quot;filter&quot; &quot;barrier&quot; ...
##  $ 41 : chr [1:1000] &quot;dear&quot; &quot;dreadful&quot; &quot;town&quot; &quot;night&quot; ...
##  $ 42 : chr [1:1000] &quot;turn&quot; &quot;eye&quot; &quot;upon&quot; &quot;judge&quot; ...
##  $ 43 : chr [1:1000] &quot;everything&quot; &quot;appointed&quot; &quot;place&quot; &quot;appointed&quot; ...
##  $ 44 : chr [1:1000] &quot;whisper&quot; &quot;ear&quot; &quot;seem&quot; &quot;wife&quot; ...
##  $ 45 : chr [1:1000] &quot;good&quot; &quot;creature&quot; &quot;way&quot; &quot;affect&quot; ...
##  $ 46 : chr [1:1000] &quot;see&quot; &quot;lose&quot; &quot;card&quot; &quot;kknow&quot; ...
##  $ 47 : chr [1:1000] &quot;first&quot; &quot;come&quot; &quot;better&quot; &quot;see&quot; ...
##  $ 48 : chr [1:1000] &quot;hand&quot; &quot;mouth&quot; &quot;produce&quot; &quot;court&quot; ...
##  $ 49 : chr [1:1000] &quot;clenched&quot; &quot;cover&quot; &quot;wound&quot; &quot;man&quot; ...
##  $ 50 : chr [1:1000] &quot;shoulders&quot; &quot;good&quot; &quot;physician&quot; &quot;deserve&quot; ...
##  $ 51 : chr [1:1000] &quot;tone&quot; &quot;use&quot; &quot;watch&quot; &quot;sick&quot; ...
##  $ 52 : chr [1:1000] &quot;already&quot; &quot;speed&quot; &quot;lightning&quot; &quot;get&quot; ...
##  $ 53 : chr [1:1000] &quot;death&quot; &quot;will&quot; &quot;mourn&quot; &quot;grieve&quot; ...
##  $ 54 : chr [1:1000] &quot;air&quot; &quot;like&quot; &quot;soul&quot; &quot;furious&quot; ...
##  $ 55 : chr [1:1000] &quot;go&quot; &quot;ever&quot; &quot;yet&quot; &quot;nothing&quot; ...
##  $ 56 : chr [1:1000] &quot;dimly&quot; &quot;see&quot; &quot;without&quot; &quot;aid&quot; ...
##  $ 57 : chr [1:1000] &quot;place&quot; &quot;run&quot; &quot;past&quot; &quot;night&quot; ...
##  $ 58 : chr [1:1000] &quot;quite&quot; &quot;fool&quot; &quot;know&quot; &quot;fall&quot; ...
##  $ 59 : chr [1:1000] &quot;little&quot; &quot;nearer&quot; &quot;see&quot; &quot;whole&quot; ...
##  $ 60 : chr [1:1000] &quot;peggotty&quot; &quot;well&quot; &quot;deserve&quot; &quot;treasure&quot; ...
##  $ 61 : chr [1:1000] &quot;raising&quot; &quot;dead&quot; &quot;seem&quot; &quot;strike&quot; ...
##  $ 62 : chr [1:1000] &quot;magnanimous&quot; &quot;word&quot; &quot;fit&quot; &quot;scene&quot; ...
##  $ 63 : chr [1:1000] &quot;skate&quot; &quot;skim&quot; &quot;away&quot; &quot;smoothness&quot; ...
##  $ 64 : chr [1:1000] &quot;peggotty&quot; &quot;come&quot; &quot;safe&quot; &quot;barkis&quot; ...
##  $ 65 : chr [1:1000] &quot;begin&quot; &quot;march&quot; &quot;mind&quot; &quot;suppose&quot; ...
##  $ 66 : chr [1:1000] &quot;see&quot; &quot;lean&quot; &quot;tree&quot; &quot;wall&quot; ...
##  $ 67 : chr [1:1000] &quot;rest&quot; &quot;group&quot; &quot;nearest&quot; &quot;bed&quot; ...
##  $ 68 : chr [1:1000] &quot;like&quot; &quot;something&quot; &quot;somebody&quot; &quot;say&quot; ...
##  $ 69 : chr [1:1000] &quot;eye&quot; &quot;fix&quot; &quot;continue&quot; &quot;pat&quot; ...
##  $ 70 : chr [1:1000] &quot;distant&quot; &quot;idea&quot; &quot;holiday&quot; &quot;see&quot; ...
##  $ 71 : chr [1:1000] &quot;work&quot; &quot;always&quot; &quot;insinuat&quot; &quot;revel&quot; ...
##  $ 72 : chr [1:1000] &quot;breakfast&quot; &quot;summon&quot; &quot;playground&quot; &quot;enter&quot; ...
##  $ 73 : chr [1:1000] &quot;ask&quot; &quot;iron&quot; &quot;whisper&quot; &quot;measure&quot; ...
##  $ 74 : chr [1:1000] &quot;box&quot; &quot;go&quot; &quot;think&quot; &quot;meaning&quot; ...
##  $ 75 : chr [1:1000] &quot;wooden&quot; &quot;step&quot; &quot;foot&quot; &quot;read&quot; ...
##  $ 76 : chr [1:1000] &quot;dislike&quot; &quot;seldom&quot; &quot;allow&quot; &quot;visit&quot; ...
##  $ 77 : chr [1:1000] &quot;honour&quot; &quot;gentility&quot; &quot;walk&quot; &quot;house&quot; ...
##  $ 78 : chr [1:1000] &quot;often&quot; &quot;much&quot; &quot;take&quot; &quot;entire&quot; ...
##  $ 79 : chr [1:1000] &quot;know&quot; &quot;wear&quot; &quot;moment&quot; &quot;childish&quot; ...
##  $ 80 : chr [1:1000] &quot;love&quot; &quot;much&quot; &quot;soften&quot; &quot;whole&quot; ...
##  $ 81 : chr [1:1000] &quot;ark&quot; &quot;creep&quot; &quot;last&quot; &quot;upon&quot; ...
##  $ 82 : chr [1:1000] &quot;second&quot; &quot;permission&quot; &quot;though&quot; &quot;time&quot; ...
##  $ 83 : chr [1:1000] &quot;pleasure&quot; &quot;poor&quot; &quot;baby&quot; &quot;fix&quot; ...
##  $ 84 : chr [1:1000] &quot;eccentric&quot; &quot;good&quot; &quot;many&quot; &quot;people&quot; ...
##  $ 85 : chr [1:1000] &quot;measure&quot; &quot;suit&quot; &quot;clothes&quot; &quot;directly&quot; ...
##  $ 86 : chr [1:1000] &quot;door&quot; &quot;farther&quot; &quot;end&quot; &quot;room&quot; ...
##  $ 87 : chr [1:1000] &quot;schoolroom&quot; &quot;much&quot; &quot;surprised&quot; &quot;hear&quot; ...
##  $ 88 : chr [1:1000] &quot;kind&quot; &quot;intention&quot; &quot;give&quot; &quot;article&quot; ...
##  $ 89 : chr [1:1000] &quot;declaration&quot; &quot;offer&quot; &quot;say&quot; &quot;now&quot; ...
##  $ 90 : chr [1:1000] &quot;letter&quot; &quot;excellent&quot; &quot;husband&quot; &quot;say&quot; ...
##  $ 91 : chr [1:1000] &quot;umbleness&quot; &quot;stood&quot; &quot;way&quot; &quot;join&quot; ...
##  $ 92 : chr [1:1000] &quot;say&quot; &quot;believe&quot; &quot;creditor&quot; &quot;great&quot; ...
##  $ 93 : chr [1:1000] &quot;say&quot; &quot;seventeen&quot; &quot;young&quot; &quot;eldest&quot; ...
##  $ 94 : chr [1:1000] &quot;look&quot; &quot;fervent&quot; &quot;appeal&quot; &quot;deal&quot; ...
##  $ 95 : chr [1:1000] &quot;journey&quot; &quot;feel&quot; &quot;completely&quot; &quot;extinguished&quot; ...
##  $ 96 : chr [1:1000] &quot;opposite&quot; &quot;perhaps&quot; &quot;something&quot; &quot;really&quot; ...
##  $ 97 : chr [1:1000] &quot;sir&quot; &quot;steerforth&quot; &quot;quite&quot; &quot;well&quot; ...
##  $ 98 : chr [1:1000] &quot;easier&quot; &quot;go&quot; &quot;upstairs&quot; &quot;wait&quot; ...
##  $ 99 : chr [1:1000] &quot;sudden&quot; &quot;one&quot; &quot;evening&quot; &quot;might&quot; ...
##   [list output truncated]</code></pre>
</div>
<div id="document-term-matrix" class="section level2" number="3.10">
<h2><span class="header-section-number">3.10</span> Document-Term
Matrix</h2>
<p>Now, the corpus is almost in the shape it needs to be in for the
topic modelling to proceed. What we require for the modelling process is
a document-term matrix (have a look at the tutorial on document
classification if you do not know what a document-term matrix is). This
is essentially a table which counts how often each word occurs in each
of the pseudo-documents. We’ll get into the structure of the
document-term matrix in a second. First, however, we need to turn it
into a tokens object, as the current object cannot be transformed into a
document-term matrix. The different types of objects that we use are
created with the quanteda library, which means you can find further
details on any of these objects in the documentation of quanteda
library.</p>
<pre class="r"><code>chunky_toks &lt;- tokens(chunky_dickens)</code></pre>
<p>The contents of the <code>chunky_dickens</code> object are
transformed into a tokens object and assigned to a new object,
<code>chunky_toks</code>. Looking at the new object, it displays a
different behavior:</p>
<pre class="r"><code>head(chunky_toks)</code></pre>
<pre><code>## Tokens consisting of 6 documents.
## 1 :
##  [1] &quot;ghost&quot;      &quot;story&quot;      &quot;endeavoure&quot; &quot;ghostly&quot;    &quot;little&quot;    
##  [6] &quot;book&quot;       &quot;raise&quot;      &quot;ghost&quot;      &quot;idea&quot;       &quot;shall&quot;     
## [11] &quot;put&quot;        &quot;reader&quot;    
## [ ... and 988 more ]
## 
## 2 :
##  [1] &quot;every&quot;   &quot;twenty&quot;  &quot;fifth&quot;   &quot;say&quot;     &quot;button&quot;  &quot;great&quot;   &quot;coat&quot;   
##  [8] &quot;ching&quot;   &quot;suppose&quot; &quot;must&quot;    &quot;whole&quot;   &quot;day&quot;    
## [ ... and 988 more ]
## 
## 3 :
##  [1] &quot;seven&quot;     &quot;year&quot;      &quot;say&quot;       &quot;ghost&quot;     &quot;hear&quot;      &quot;set&quot;      
##  [7] &quot;another&quot;   &quot;cry&quot;       &quot;clank&quot;     &quot;chain&quot;     &quot;hideously&quot; &quot;dead&quot;     
## [ ... and 988 more ]
## 
## 4 :
##  [1] &quot;squeak&quot;    &quot;scuffle&quot;   &quot;mouse&quot;     &quot;behind&quot;    &quot;panelling&quot; &quot;drip&quot;     
##  [7] &quot;half&quot;      &quot;thaw&quot;      &quot;water&quot;     &quot;spout&quot;     &quot;dull&quot;      &quot;yard&quot;     
## [ ... and 988 more ]
## 
## 5 :
##  [1] &quot;sit&quot;       &quot;beautiful&quot; &quot;young&quot;     &quot;girl&quot;      &quot;like&quot;      &quot;last&quot;     
##  [7] &quot;believe&quot;   &quot;see&quot;       &quot;now&quot;       &quot;comely&quot;    &quot;matron&quot;    &quot;sit&quot;      
## [ ... and 988 more ]
## 
## 6 :
##  [1] &quot;lemon&quot;       &quot;great&quot;       &quot;compactness&quot; &quot;juicy&quot;       &quot;person&quot;     
##  [6] &quot;urgently&quot;    &quot;entreat&quot;     &quot;beseech&quot;     &quot;carry&quot;       &quot;home&quot;       
## [11] &quot;paper&quot;       &quot;bag&quot;        
## [ ... and 988 more ]</code></pre>
<p>The structure with pseudo-documents of 1,000 tokens remains, but now
the output doesn’t overflow as hard as it did before. Moreover, the
<code>chunky_toks</code> object can be turned into a document-term
matrix. For this, we use the command <code>dfm()</code>, which
transforms token (and other) objects into a sparse document-feature
matrix:</p>
<pre class="r"><code>dtm &lt;- dfm(chunky_toks)</code></pre>
<p>We create a new object <code>dtm</code>, to which we assign the
transformed contents of the <code>chunky_toks</code> object.</p>
<p>Taking a look at the <code>dtm</code> object gives a sense for its
structure:</p>
<pre class="r"><code>dtm</code></pre>
<pre><code>## Document-feature matrix of: 695 documents, 21,874 features (97.22% sparse) and 0 docvars.
##     features
## docs ghost story endeavoure ghostly little book raise idea shall put
##    1     3     2          1       1      4    2     1    1     3   6
##    2    11     1          0       2      4    1     0    0     0   3
##    3    15     0          1       0      6    0     3    0     0   1
##    4     9     0          0       0      5    1     0    0     0   2
##    5     4     0          1       1      2    0     1    1     0   1
##    6     2     0          0       0      7    0     1    1     2   2
## [ reached max_ndoc ... 689 more documents, reached max_nfeat ... 21,864 more features ]</code></pre>
<p>The corpus is now structured as a table, and we see that for each of
the pseudo-documents, there is a count for how often each word occurs.
We also see that the <code>dtm</code> object retains the 717
pseudo-documents, and contains 27,695 features.</p>
</div>
</div>
<div id="exploring-the-data" class="section level1" number="4">
<h1><span class="header-section-number">4</span> Exploring the Data</h1>
<div id="document-frequencies" class="section level2" number="4.1">
<h2><span class="header-section-number">4.1</span> Document
Frequencies</h2>
<p>It makes sense to investigate the features some more before
proceeding to the topic modelling. Specifically, it’s worth looking at
the document frequency of the different features in the corpus:</p>
<pre class="r"><code>doc_freq = docfreq(dtm)</code></pre>
<p>The command <code>doc_freq()</code> by default simply counts how many
documents each feature occurs in. We assign this information to a new
variable, <code>doc_freq</code>. This is useful information for topic
modelling since it presents us with a chance to see which features are,
on the basis of their overall frequency, likely to co-occur with a lot
of other features. Also, like in document classification, it typically
makes sense to get rid of very rare features, which would lead to large
and overfitted models. It also gives us the opportunity to get a sense
for just how frequent the most frequent features are.</p>
<p>Let’s take a look at the first twenty features:</p>
<pre class="r"><code>head(doc_freq, n=20)</code></pre>
<pre><code>##      ghost      story endeavoure    ghostly     little       book      raise 
##         76        187         39         22        680        270        310 
##       idea      shall        put     reader     humour     season        may 
##        229        512        577         51        101         52        555 
##      haunt      house pleasantly        one       wish        lay 
##         49        585         30        690        391        451</code></pre>
<p>This already gives us a sense for the range: from <code>1843</code>
which occurs in only one of our pseudo-documents, whereas
<code>one</code> occurs in 715 pseudo-documents.</p>
<p>We can also take a look at the twenty five most common features. For
this, we sort the features in <code>doc_freq</code> in decreasing
order:</p>
<pre class="r"><code>head(sort(doc_freq, decreasing=T), n=25)</code></pre>
<pre><code>##    say   take    see   know     go   look    one   upon   make   time   come 
##    694    693    693    692    691    691    690    690    690    687    684 
## little   hand  think    now    man   good   will   like  great   well   head 
##    680    678    675    674    664    659    657    654    653    653    652 
##    way    eye   much 
##    648    645    645</code></pre>
<p>This list already gives a slight pointer towards Dickens style, if
not yet too much toward literary realism. The most common feature
<em>said</em>, which occurs in all but one of our pseudo-documents, as
well as its present equivalent <em>say</em>, which occurs in 667
pseudo-documents, indicate that there is a lot of speech and dialogue in
these novels. There are also several words related to time,
<em>time</em>, <em>now</em>, <em>never</em>, <em>first</em>, and in some
way also <em>old</em>, indicating that there is likely some development
of characters and states.</p>
<p>We are also seeing the rewards of our pre-processing: had we not
removed punctuation and stopwords, this list would be chock-full of
<em>the</em>, <em>a</em>, <em>I</em>, commas, fullstops and so on.</p>
<p>Moreover, we can see that the twenty fifth most frequent word,
<em>first</em>, has a document-frequency of 627, which is to say it
occurs in 87% of all pseudo-documents. This is still quite a lot and
would lead to many uninterpretable co-occurrences if we weren’t to trim
away the most frequent of the remaining features, as we will in a later
step.</p>
<p>But first, let’s take a brief look at the other end of the document
frequencies. Again, we sort the frequencies in decreasing order, but
this time we look at the twenty five entries with the lowest count,
using the <code>tail()</code> command:</p>
<pre class="r"><code>tail(sort(doc_freq, decreasing=T), n=25)</code></pre>
<pre><code>##       quibble         neame      inspired    streetdoor       vampire 
##             1             1             1             1             1 
##   incoherence           lik    astonishes     penitents        stigma 
##             1             1             1             1             1 
##        browdy     wackfords         spree         writo       fourtee 
##             1             1             1             1             1 
## suffocatingly       conclud    screamings         belle         croon 
##             1             1             1             1             1 
## spasmodically   foolishness      noisiest       insiste         turns 
##             1             1             1             1             1</code></pre>
<p>Unsurprisingly, we see only features that occur in one single
pseudo-document each. Some of these, specifically, all the ones which
include commas and dashes that stand between words without space, can be
attributed to erroneous processing. Others just seem to be rare words,
like <em>limes</em>, <em>pretender</em> or <em>schoolhouse</em>.</p>
<p>More interesting, and indicative of Dickens style and perspective,
are the words <em>stimilated</em> and <em>olesome</em>. Deriving from
<em>stimulated</em> and <em>wholesome</em>, both are clearly legible,
but clearly deviate from the standard spelling. These two words give us
a first flavour of Dickens’ literary realism, which entails the use of
non-standard spelling to render the dialogue of characters who speak
dialects other than the Queen’s English.</p>
<p>With a quick search, we can also identify <em>untoe</em> as an
alternate spelling of <em>unto</em>, as spoken by Pumblechook in Great
Expectations. Looking at the source material can certainly help to
identify features which are otherwise hardly legible. On that same page
as we find <em>untoe</em>, we also see that Pumblechook is the character
who utters <em>m’ria</em>, which turns out to be a contracted form of
the name <em>Maria</em>. A few sentences further, we also see that in
Pumblechook’s dialogue <em>are</em> is spelt as <em>air</em> - thus
showing that we won’t be able to identify all alternate spellings when
they are taken out of context, the way they are presented here.</p>
</div>
</div>
<div id="topic-modellings" class="section level1" number="5">
<h1><span class="header-section-number">5</span> Topic Modellings</h1>
<p>After this substantial bit of pre-processing, we finally get to the
juicy bits. One last step of pre-processing remains, however. We are
going to trim the document-feature matrix, so as to remove the most
frequent words, as well as the rarest ones. The reason we include this
step as part of the topic modelling section rather than the
pre-processing is that this step is closer to fiddling with the
parameters of the model than most of the preceding steps were.</p>
<div id="trimming" class="section level2" number="5.1">
<h2><span class="header-section-number">5.1</span> Trimming</h2>
<p>Trimming is used to return a document-feature matrix reduced in size
based on the document and term frequency. That is to say, we can set a
range for how many individual occurrences of a feature are acceptable,
and set a range for the percentage of documents a feature can occur in.
Although, the latter part we need to define specifically. Let’s take a
look at the command:</p>
<pre class="r"><code>dtm_trimmed &lt;- dfm_trim(dtm, min_termfreq=2, min_docfreq=0.0001, max_docfreq=0.15, docfreq_type=&quot;prop&quot;)</code></pre>
<p>We apply the command <code>dfm_trim()</code> to the <code>dtm</code>
object, and assign the output to a new object, <code>dtm_trimmed</code>.
We further specify:</p>
<p>*That the minimum term frequency should be 2. So only terms which
occur at least twice in the corpus are to be included in
<code>dtm_trimmed</code>. This makes sense if you consider that the
topic model is seeking co-occurrence patterns that appear throughout the
corpus. If a word appears only once, it will have some co-occurrences
with other words, but it will not yield an identifiable pattern, since,
well, it doesn’t occur in any other pseudo-documents.</p>
<p>*That the minimum document frequency should be 0.005. That is to say
that only features which appear in more than 0.5% of all
pseudo-documents are included in <code>dtm_trimmed</code>. So only
features which occur in more than three pseudo-documents are
included.</p>
<p>*That the maximum document frequency should be 0.25. That is to say
that only features which appear in less than a quarter, in less than
179, of all pseudo-documents are included in
<code>dtm_trimmed</code>.</p>
<p>*We define this with the specification
<code>docfreq_type="prop"</code>, which gives us a proportion by
dividing the document frequencies of all features by the total sum of
pseudo-documents.</p>
<p>If you are wondering why we went to the trouble of plucking out all
the stopwords and proper nouns, if we have this much simpler way of
cutting the corpus down in size, you have a point. The answer is as
straightforward as the question: topic modelling works better if you
have less features that are not interpretable (stopwords) or the mess up
the modelling process (proper nouns). If we simply relied on trimming to
get out the most frequent words, the chance of landing in the sweet spot
where meaningfully interpretable topics arise are a lot slimmer. Which
is why we do all of the pre-processing, and then still trim the corpus
down to an even more manageable size - which is where we are now.</p>
<p>So the pre-processed and trimmed version of the corpus,
<code>dtm_trimmed</code>, provides the basis for the first model. And
the first model is always a first model, since all of the parameters
will have to be tweaked later on, in order to arrive at better models
that are easier to interpret. But before we get into that, let’s create
the first model on the basis of the above parameters.</p>
</div>
<div id="first-model" class="section level2" number="5.2">
<h2><span class="header-section-number">5.2</span> First Model</h2>
<p>Actually creating a topic model is a question of a single line of
code, which looks like this:</p>
<pre class="r"><code>stmOut_1 &lt;- stm(documents=dtm_trimmed, K=10, max.em.its=200, verbose = F)</code></pre>
<p>Before checking out the results, let’s look at what this command
does. We create a new object, <code>stmOut_1</code> and assign a
structural topic model to it with the <code>stm()</code> command. We set
<code>dtm_trimmed</code> as the document term matrix we want modelled,
and tell the algorithm to create a model with 10 topics, which is
represented by <code>K=10</code>. The final specification,
<code>max.em.its=200</code> basically tells R to stop improving the
model after 200 iterations, even if it does not reach convergence. What
does this mean?</p>
<p>Basically, the model starts with the 10,000 most frequent words. From
these, it identifies some anchor words to build co-occurrence patterns
on, and then it begins iterating over the model. With each iteration,
the co-occurrences patterns are refined, with the new model being
compared to the preceding one. In the output, you can see the relative
change from model to model. Once the marginal improvements from further
iterations flattens out, the model is considered converged, i.e. as good
as it gets. So, with the parameter above, the improvement of the model
is terminated after 200 iterations even if there are still marginal
improvements to be had. For a more technical description of how this
works, we refer you to <span class="citation">Blei, Ng, and Jordan (<a
href="#ref-blei2003lda">2003</a>)</span>.</p>
<p>Let’s see what our latest object, <code>stmOut_1</code> holds:</p>
<pre class="r"><code>stmOut_1</code></pre>
<pre><code>## A topic model with 10 topics, 695 documents and a 14152 word dictionary.</code></pre>
<p>Not very informative yet, is it? For interpretable results, we want
to see the keywords of the ten topics in the model, which we can do by
plotting the contents of the model. For this, we use the
<code>plot()</code> command, feed it our model and tell it to display
the top ten keywords for each model:</p>
<pre class="r"><code>plot(stmOut_1, n=10, cex = .5, xlim = c(0, .8))</code></pre>
<p><img src="topmod_dickens_files/figure-html/unnamed-chunk-48-1.png" width="672" /></p>
<p>If your model is anything like ours, this is not yet what you’d call
revelatory. There are some word pairs throughout the topics which go
well together. For instance, we get <em>prisoner</em> and
<em>prison</em> in one topic; <em>sea</em>, <em>river</em>, and
<em>ship</em> in another; <em>houses</em> and <em>windows</em>;
<em>dog</em> and <em>guardian</em>; <em>magistrate</em> and
<em>collector</em>.</p>
<p>Additionally, we get one topic with a lot of contractions: ’ll - ’em
- ’re - ’ve; and another one with alternate spellings like
<em>wery</em>, <em>wot</em>, <em>ai</em>, <em>wos</em>. The former
represents informal speech settings, and the latter points to dialogue
in dialect.</p>
<p>The topics showing up in your model will in all likelihood look
similar, but somewhat differently. This has to do with the fact that the
<code>stm()</code> function uses a random seed, which is to say that
each time you run the command it starts at a different point in the
corpus, leading to some variance in the results each time. There is, in
theory, the possibility of setting the seed, so as to guarantee that
with the same input, you will get the same results every time you run
the command. We refrain from doing so here, since the results are robust
enough that you will find ample similarities between our descriptions
and your own models.</p>
<p>With this disclaimer out of the way, let’s briefly recall the initial
research questions: 1. Can we use topic modelling to bring Dickens’
social criticism to the fore, without the heavy lifting of actually
reading his books? 2. Can we use topic modelling to explore the rich
imagery that Dickens constructs with his literary realism?</p>
<p>Despite this first model only showing hints of meaning, we can relate
the two distinct topics consisting of contractions and alternate
spellings to both research questions. On the one hand, the inclusion of
informal and dialectal language points towards Dickens’ literary
realism, which incorporates realistic depictions of spoken language. On
the other hand, Dickens’ choice to represent characters which speak a
dialect other than received pronunciation is clearly predicated on his
progressive attitudes towards poverty and the people living in it.</p>
<p>So we have some indication that the topic modelling is a) working, by
making visible non-standard speech, and b) that we can actually glean
some insights from it. However, it’s also clear that there is a lot of
room for improvement. The question is: where do we start?</p>
</div>
<div id="wild-goose-chase" class="section level2" number="5.3">
<h2><span class="header-section-number">5.3</span> Wild Goose Chase</h2>
<p>There is a whole bunch of parameters that determine what the model
looks like. These are: * As part of the <code>stm()</code>
function:<br />
+ Number of topics<br />
+ Maximum number of iterations (fairly irrelevant in this case)<br />
* As part of the <code>dfm_trim()</code> function:<br />
+ Minimum term frequency<br />
+ Minimum document frequency<br />
+ Maximum document frequency<br />
* As part of the pre-processing:<br />
+ The chunk size</p>
<p>Getting from a first model to a good model is typically something of
a wild goose chase, involving a lot of trial and error. In the
following, we walk backwards through our options, starting with the
number of topics, tweaking as we go along.</p>
</div>
<div id="second-model-more-topics" class="section level2" number="5.4">
<h2><span class="header-section-number">5.4</span> Second Model: More
Topics</h2>
<p>Sometimes, the number of topics is a constraint that prevents the
model from displaying its found co-occurrences in the level of
granularity that is required. To remedy this, we give it space for 20
topics instead of 10, and see where this takes us:</p>
<pre class="r"><code>stmOut_2 &lt;- stm(documents=dtm_trimmed, K=20, seed = 123, max.em.its=200, verbose = F)
plot(stmOut_2, n=10, cex = .5, xlim = c(0, .4))</code></pre>
<p><img src="topmod_dickens_files/figure-html/unnamed-chunk-49-1.png" width="672" /></p>
<p>This already presents quite an improvement over the first model.
There is a degree of clarity to the topics that gives us more cause for
hope. One of the topics pertains to water, with words like
<em>boat</em>, <em>river</em>, <em>wind</em>, <em>tide</em>,
<em>sea</em> and <em>marshes</em>. Another topic reflects a young man’s
education, with the words like <em>boys</em>, <em>brothers</em>,
<em>school</em>, <em>schoolmaster</em>, <em>son</em> and <em>desk</em>.
Giving further indication of Dickens’ interests in justice, there is a
topic containing words like <em>prisoner</em>, <em>prison</em>,
<em>jury</em>, <em>court</em>, <em>citizen</em> and <em>witness</em>.
Closely related, there is a topic that reflects the institutional aspect
more strongly, featuring words like <em>magistrate</em>,
<em>office</em>, <em>clerk</em>, <em>attorney</em>, <em>officer</em> and
<em>judge</em>. Pointing towards an interest in death and the
otherworldly, there is a topic showing <em>beneath</em>, <em>grave</em>,
<em>goblin</em>, <em>churchyard</em>, <em>earth</em>, <em>church</em>
and <em>wind</em>. This also picks up on the ambiance Dickens is capable
of conjuring, as he for instance does at the beginning of Great
Expectations. Additionally, there are now several topics that capture
contractions and alternate spellings, as discussed above. Although you
won’t see the exact same topics, you should be able to see some
improvements in the model.</p>
<p>Despite this increase in meaningful topics, we also get several
topics that don’t have a clear line, which are somewhere between hard
and impossible to interpret. But overall, with this simple adjustment of
a single parameter, we have made a big step away from poking around in
tea-leaves, and towards an interpretable means of automated content
analysis.</p>
</div>
<div id="third-model-more-rare-words" class="section level2"
number="5.5">
<h2><span class="header-section-number">5.5</span> Third Model: More
Rare Words</h2>
<p>Taking a step back towards pre-processing, we can amplify the role
that rare words play, which might or might not lead us towards a more
interpretable model. To do this, we trim the <code>dtm</code> object in
such a way that only words that occur in no more than 15% of the
pseudo-documents are included:</p>
<pre class="r"><code>dtm_trimmed &lt;- dfm_trim(dtm, min_termfreq=2, min_docfreq=0.005, max_docfreq=0.15, docfreq_type=&quot;prop&quot;)</code></pre>
<p>And then we create a new model:</p>
<pre class="r"><code>stmOut_3 &lt;- stm(documents=dtm_trimmed, K=20, seed = 123, max.em.its=200, verbose = F)
stmOut_3</code></pre>
<pre><code>## A topic model with 20 topics, 695 documents and a 9306 word dictionary.</code></pre>
<pre class="r"><code>stmOut_1</code></pre>
<pre><code>## A topic model with 10 topics, 695 documents and a 14152 word dictionary.</code></pre>
<p>Comparing this new model <code>stmOut_3</code> to the first one,
<code>stmOut_1</code>, we see that the dictionary is smaller, as we
would expect, limiting the range of included words as we did. With the
latest trimming step, we lost some 397 words. Let’s see where that gets
us in terms of the model:</p>
<pre class="r"><code>plot(stmOut_3, n=10, cex = .5, xlim = c(0, .4))</code></pre>
<p><img src="topmod_dickens_files/figure-html/unnamed-chunk-52-1.png" width="672" /></p>
<p>If your model is anything like ours, you will find some rather
intriguing new words in some of the topics, for instance <em>monks</em>
and a <em>lion</em> show up for us, but in general this does not seem to
have been an improvement. Indeed, we also get words like
<em>murdstone</em> and <em>peggotty</em>, which in addition to
<em>copperfield</em> and <em>squeers</em> that we already had before,
indicate that the removal of proper nouns in the pre-processing did not
capture all of the characters. Depending on how rigorous a research
project is intended to be, this might be the point at which to revisit
the pre-processing, and finding alternate means of identifying character
names. For the current purposes we continue working with this rather
imperfect data (although, for expectation management: your data is
likely to always have some flaws or quirks).</p>
</div>
<div id="fourth-model-more-common-words" class="section level2"
number="5.6">
<h2><span class="header-section-number">5.6</span> Fourth Model: More
Common Words</h2>
<p>We can also explore what happens when increasing the minimum document
frequency to 1%. This means that only words occurring in at least seven
pseudo-documents:</p>
<pre class="r"><code>dtm_trimmed &lt;- dfm_trim(dtm, min_termfreq=2, min_docfreq=0.01, max_docfreq=0.25, docfreq_type=&quot;prop&quot;)
stmOut_4 &lt;- stm(documents=dtm_trimmed, K=30, seed = 123, max.em.its=200, verbose = F)</code></pre>
<p>You’ll notice that we jumped directly to <code>K=30</code>, sparing
you at least one step between the previous model and this one. The
reason is that the model in between did not yield anything exciting. We
would very much encourage you to experiment with different parameters
here to get a sense for what changes when you shift a single parameter.
Or, to at the very least do so when you are working on a project of your
own. For better or worse, at this stage in its life cycle (and
improvements in the method are sure to come), topic modelling takes a
lot of tinkering.</p>
<p>Let’s see how the most recent adjustment, tightening the range of
document frequencies, changed our model:</p>
<pre class="r"><code>plot(stmOut_4, n=10, cex = .5, xlim = c(0, .3))</code></pre>
<p><img src="topmod_dickens_files/figure-html/unnamed-chunk-54-1.png" width="672" /></p>
<p>Again, if your model is anything like ours, there will be a remnant
of some of the clearest topics from the second model (we smell a vaguely
salty breeze from a somewhat diluted water-related topic and hear the
rustling of papers in court from another, less coherent legal topic),
but no overall improvement - which may be down to the fact that we need
to backtrack a bit further.</p>
</div>
<div id="fifth-model-smaller-chunks" class="section level2"
number="5.7">
<h2><span class="header-section-number">5.7</span> Fifth Model: Smaller
Chunks</h2>
<p>Since we already walked through each line of code step by step above,
we take the shortcut and copy the relevant steps here:</p>
<pre class="r"><code>chunk &lt;- 500</code></pre>
<p>We adjust the size of chunks downward, from 1,000 to 500 words. This
will by definition limit the number of co-occurrences each word has,
which should be especially relevant for rare and/or very scene-specific
words. Then we proceed:</p>
<pre class="r"><code>n &lt;- length(list_toks)
r &lt;- rep(1:ceiling(n/chunk), each = chunk)[1:n]
chunky_dickens &lt;- split(list_toks, r)
chunky_toks &lt;- tokens(chunky_dickens)
dtm &lt;- dfm(chunky_toks)</code></pre>
<p>For this first model derived from smaller chunks, we use the
specifications that have worked best thus far, namely the ones we had on
the second model:</p>
<pre class="r"><code>dtm_trimmed &lt;- dfm_trim(dtm, min_termfreq=2, min_docfreq=0.005, max_docfreq=0.25, docfreq_type=&quot;prop&quot;)
stmOut_5 &lt;- stm(documents=dtm_trimmed, K=25, seed = 123, max.em.its=200, verbose = F)</code></pre>
<p>Again, we opt to go for another number of topics, this time twenty
five, for identical reasons as above.</p>
<p>While the model is being constructed, we can quickly think through
how working with smaller chunks affects the other parameters. We still
simply select a number of topics, and since we do not process the
content of the corpus any further, the minimum term frequency will
remain the same. However, the minimum and maximum document frequency
will change. Since we are using relative document frequencies, and
increasing the number of documents, rare words that made the
trimming-cut before might now not be included anymore. Conversely, more
frequent words that made the cut before (because they appeared in, let’s
say, 23% of all documents) might not be included anymore, since they
might now be present in 26% of documents, or even more. Let’s see how
that changes our model:</p>
<pre class="r"><code>plot(stmOut_5, n=10, cex = .5, xlim = c(0, .3))</code></pre>
<p><img src="topmod_dickens_files/figure-html/unnamed-chunk-58-1.png" width="672" /></p>
<p>This looks rather different than before, and the glimpses of Dickens’
literary realism and perspective on poverty that we saw in earlier
models are beginning to consolidate. The most salient topics in our
model are still the ones that capture informal language usage:
Considering that <em>said</em>’ was the term with the highest document
frequency, we have a good indication that there is a lot of dialogue -
and we could tell so even if we had never read a single un-processed
word of these novels. Now we see that seven out of twenty five topics
contain informal or dialectal language. This sheer presence of informal
language topics shows how much space Dickens is willing to give to
people who do not speak received pronunciation, which can be a marker
for rural and working class folks, as well as children - all of whom
have a high chance of living in poverty. So Dickens’ willingness to
represent people in poverty becomes even more salient than it was
before.</p>
<p>We also get some more distinct topics than we did before. One of them
points to graveyards, with words like <em>ground</em>, <em>cold</em>,
<em>beneath</em>, <em>lay</em>, <em>spot</em>, <em>earth</em> and
notably <em>grave</em>. Another one points to comfortable evenings, with
words including <em>evening</em>, <em>dinner</em>, <em>remember</em>,
<em>parlour</em>, <em>glad</em> and <em>sitting</em>. Two topics again
indicate an engagement with justice, one of them containing the likes of
<em>prisoner</em>, <em>prison</em> and <em>death</em>, while the other
harbors words like <em>case</em>, <em>question</em>, <em>judge</em>,
<em>clerk</em>, <em>jury</em> and <em>attorney</em>.</p>
<p>By decreasing the chunk size from 1,000 to 500, we are coming closer
to where we want to get, but there is still room for improvement. To
answer the question whether we should go even smaller, it’s perhaps
worthwhile to reflect on why 500 word chunks work better than 1,000 word
chunks: Basically, the whole process is intended to find a sweet spot
between words that are rare enough to be semantically representative of
a given topic, but frequent enough to not be singular or hyper-specific.
So, in order to arrive at a model that is meaningfully interpretable, we
need to find a chunk size that allows for thematically relevant
co-occurrence patterns to emerge, while shifting the thresholds for the
minimum and maximum document frequencies to a range that reveals the
thematically salient co-occurrence patterns.</p>
<p>A further factor is data sparseness versus topic development: if our
chunks are too short, only few words, that is features, remain. As a
result, the detection of topics, which typically hinges on several words
in collaboration, suffers. But if our chunks are too large, they cannot
detect changes in topics as they happen in the course of the development
of the document, and the resulting co-occurrence patterns become
impossible to interpret.</p>
<p>The direction that follows from the latest adjustment of the model is
clear: we should try smaller chunks to see whether they can even better
capture meaningful co-occurrence patterns.</p>
</div>
<div id="sixth-model-even-smaller-chunks" class="section level2"
number="5.8">
<h2><span class="header-section-number">5.8</span> Sixth Model: Even
Smaller Chunks</h2>
<p>For our next model, we will work with chunks of 200 words each, and
use the same specifications on the trimming as before:</p>
<pre class="r"><code>chunk &lt;- 200
n &lt;- length(list_toks)
r &lt;- rep(1:ceiling(n/chunk), each = chunk)[1:n]
chunky_dickens &lt;- split(list_toks, r)
chunky_toks &lt;- tokens(chunky_dickens)
dtm_3 &lt;- dfm(chunky_toks)
dtm_trimmed_3 &lt;- dfm_trim(dtm_3, min_termfreq=2, min_docfreq=0.005, max_docfreq=0.25, docfreq_type=&quot;prop&quot;)
stmOut_6 &lt;- stm(documents=dtm_trimmed_3, K=25, seed = 123, max.em.its=200, verbose = F)
plot(stmOut_6, n=10, cex = .5, xlim = c(0, .3))</code></pre>
<p><img src="topmod_dickens_files/figure-html/unnamed-chunk-59-1.png" width="672" /></p>
<p>Decreasing the chunk size gets us much closer to where we want to
get. Across the twenty five topics, themes begin to emerge. Reliably
emerging - and since the very first model, too - are alternate spellings
and contractions, indicative of non-RP spoken language. In our version,
we get four different topics relating to this theme, with some distinct
flavors. One of them is reflective of dialects or sociolects -
containing <em>wery, </em>wot<em>, </em>ere<em>, </em>ai* - while
another one contains contractions like <em>’ll</em> and <em>n’t</em>
alongside words that detail interactions, like <em>asked</em>,
<em>hear</em> and <em>tell</em>. We have four topics of this kind, with
some variation and distinction but a lot of consistency.</p>
<p>Another set of topics more clearly points to Dickens literary
realism. One of them - displaying a high degree of internal consistency
- pertains to travel, with the words <em>coach</em>, <em>road</em>,
<em>horses</em>, <em>chaise</em> and <em>guard</em>. Another topic
captures descriptions of outdoor spaces, containing <em>light</em>,
<em>wind</em>, <em>people</em>, <em>water</em>, <em>windows</em>,
<em>dark</em>, <em>streets</em> and <em>sea</em>. The third one in this
group describes a comfortable social setting with <em>glass</em>,
<em>table</em>, <em>gentlemen</em>, <em>wine</em>, <em>company</em>,
<em>bottle</em>, <em>chair</em> and <em>water</em>. The fact that these
topics contain words which describe different spaces with high degree of
granularity reflects Dickens’ interest in including mundane-ish
experiences in some level of detail, which is pretty much the definition
of literary realism. This is also reflected in other topics, which
contain words like <em>hat</em> and <em>coat</em>. Although these are
less salient than the ones described above, they contain aspects of the
mundane and, thus, of literary realism.</p>
<p>A further theme that arises in various topics are positive emotions,
which could also be put under the label care. There are two topics that
reflect this theme in terms of family, with words like <em>child</em>,
<em>home</em>, <em>shall</em>, <em>happy</em>, <em>loved</em>,
<em>love</em> and <em>heart</em> in one, and <em>mother</em>,
<em>home</em>, <em>always</em>, <em>pretty</em>, <em>laughing</em>,
<em>remember</em> and <em>sure</em> in the other. In the third topic of
this theme, things get really interesting, as the scope opens up: we get
<em>heart</em>, <em>love</em>, <em>beautiful</em>, <em>happiness</em>,
<em>happy</em> and in addition we get <em>poor</em>, <em>people</em> and
<em>world</em>. Of course, we should not jump to conclusions based on
topic models alone, but this topic certainly gives us an indication that
Dickens’ might just have a broader conception of who is worthy of
happiness, beauty, and love, than some of his contemporaries.</p>
<p>Beyond these themes which are directly pertinent to our research
questions, there are some other topics worth mentioning. With the words
<em>father</em>, <em>business</em>, <em>money</em>, <em>brother</em>,
<em>tell</em> and <em>hope</em>, which refers to the prospects Dickens’
characters tend to have, before their lives get sidetracked by chance
and circumstance. There are also topics pertaining to remembrances -
with <em>saw</em>, <em>seen</em>, <em>sat</em>, <em>home</em>,
<em>knew</em>, <em>gone</em> and <em>left</em> - as well as loss:
<em>child</em>, <em>hands</em>, <em>let</em>, <em>heart</em>,
<em>cried</em>, <em>moment</em>, <em>arm</em>, <em>arms</em> and
<em>death</em>.</p>
<p>Finally, we get some topics that that we’ve seen in previous models,
related for example to school - <em>boys</em>, <em>school</em>,
<em>morning</em> - or jurisdiction, with <em>gentlemen</em>,
<em>case</em>, <em>name</em>, <em>shall</em>, <em>judge</em>,
<em>magistrate</em> and <em>court</em>. However, these are not as clear
cut as in some of our other models. Then of course, there remain some
topics which are barely interpretable, but that is really not
uncommon.</p>
<p>Still, we can see (and you should be able to as well, albeit with
slight differences in what arises exactly) that this model with smaller
chunks allows us to answer our research questions rather well.</p>
</div>
<div id="seventh-model-small-chunks-narrower-range"
class="section level2" number="5.9">
<h2><span class="header-section-number">5.9</span> Seventh Model: Small
Chunks, Narrower Range</h2>
<p>Since the trajectory of smaller chunks has thus far led to
improvements with our results, we tried a model based on even smaller
chunks, but the results got worse. So instead of displaying that, we’ll
look at another model based on 200-word chunks, but this time we narrow
the range of included words by setting the maximum document frequency at
15%.</p>
<pre class="r"><code>dtm_trimmed_4 &lt;- dfm_trim(dtm_3, min_termfreq=2, min_docfreq=0.005, max_docfreq=0.15, docfreq_type=&quot;prop&quot;)
stmOut_7 &lt;- stm(documents=dtm_trimmed_4, K=25, seed = 123, max.em.its=200, verbose = F)
plot(stmOut_7, n=10, cex = .5, xlim = c(0, .3))</code></pre>
<p><img src="topmod_dickens_files/figure-html/unnamed-chunk-60-1.png" width="672" /></p>
<p>In comparison to the improvements between earlier models, these might
only be a few degrees, but some of them are worth getting into in a bit
of detail.</p>
<p>What is especially striking is that there are more topics containing
relatively fine-grained words indicative of literary realism. In
addition to the outdoor spaces described above, this model contains a
topic that more clearly details people’s appearance - with
<em>coat</em>, <em>black</em>, <em>hat</em>, <em>small</em>,
<em>white</em>, <em>large</em>, <em>pretty</em>, and, yes,
<em>appearance</em> - as well as one that describes social settings,
with words like <em>gentlemen</em>, <em>company</em>, <em>ladies</em>,
<em>fat</em>, <em>chair</em>, <em>party</em>, <em>honourable</em>,
<em>loud</em>, <em>men</em> and <em>crowd</em>. The topic of loss is
also complemented by more specific words like <em>bed</em>,
<em>fell</em>, <em>lay</em> and <em>hair</em>. In addition, there is a
topic that captures the passing of time in a very evocative but specific
way, containing words like <em>evening</em>, <em>often</em>,
<em>walked</em>, <em>quiet</em>, <em>hour</em>, <em>passed</em>,
<em>days</em>, <em>window</em>, <em>thoughts</em> and
<em>hours</em>.</p>
<p>The theme of the legal system also gains some specificity, with words
like <em>paper</em> and <em>read</em> finding their way into the same
topics as <em>case</em>, <em>prisoner</em>, <em>court</em> and
<em>judge</em>. Somewhat related, there is a topic that seems to capture
formal interactions in an institutional setting, with words like
<em>matter</em>, <em>beg</em>, <em>magistrate</em>, <em>fellow</em>,
<em>person</em>, <em>pray</em>, <em>certainly</em>, <em>ma’am</em>,
<em>immediately</em> and <em>inquired</em>.</p>
<p>We describe a topic on prospects above, which is present in this
model as well. In this model, there is a second topic that relates to
the prospects deriving from the family situation, with words like
<em>family</em>, <em>really</em>, <em>present</em>, <em>letter</em>,
<em>subject</em>, <em>opinion</em>, <em>state</em> and <em>find</em>.
This topic seems to capture the gap between the prospects that could be
expected, and the actual situation.</p>
<p>On the subject of family, there is again a topic on love - with
<em>love</em>, <em>child</em>, <em>happy</em>, <em>woman</em>,
<em>speak</em>, <em>tears</em>, <em>loved</em> and <em>feel</em> - and a
second one that captures heritage, with words like <em>mother</em>,
<em>gave</em>, <em>mine</em>, <em>remember</em>, <em>pretty</em>,
<em>wonder</em>, <em>father</em>, <em>suppose</em> and <em>baby</em>.
Especially the <em>wonder</em> and <em>suppose</em> bring in a quality
of speculation that could well refer to the orphan Pip in Great
Expectations. There are two more topics which reflect family relations,
one containing <em>uncle</em>, <em>ma’am</em>, <em>widow</em>,
<em>nephew</em>, <em>married</em>, <em>husband</em> and <em>wife</em>,
and the other containing <em>ladies</em>, <em>married</em>,
<em>papa</em>, <em>rejoined</em>, <em>children</em> and
<em>daughter</em>.</p>
<p>The obligatory topics containing contractions and alternate spellings
are represented again. The only thing to note here is that, because of
the different trimming, some words appear that were previously not
captured, with <em>coom</em> and <em>gen’l’m’n</em> standing out
particularly. Finally, there are still some topics that are challenging
to interpret.</p>
<p>An interesting sidenote is that the word <em>poor</em> no longer
appears, with this specification. This indicates that it occurs in
somewhere between 15- and 25% of all documents. As such, it is rather
frequent. Of course, the word <em>poor</em> is not necessarily related
to poverty per se, as it can express sympathy or pity for others, but
Dickens’ use of the word <em>poor</em> could be an interesting avenue
for more qualitative linguistic investigation.</p>
<p>As mentioned numerous times, there is of course going to be some
deviation between what we describe and what you see, since the models
are constructed with a random component. But you will certainly have
seen how tweaking the parameters can change the results, and also have
gotten a taste of how topic models can be interpreted. Which almost
brings this introduction to topic modelling to a close.</p>
</div>
</div>
<div id="final-comments" class="section level1" number="6">
<h1><span class="header-section-number">6</span> Final Comments</h1>
<p>The biggest disclaimer is: many roads lead to Rome. This goes for
each step of the way, beginning with the choice of programming language
(there are dedicated tools for topic modelling, like MALLET, but also
ways of doing it in Python, for instance) and the packages, through the
pre-processing, to the precise specification and interpretation of the
models. There is no one-size fits all approach. The question is rather,
whether you find an approach that allows you to reach meaningful
results. Probably the best frame for thinking about topic modelling is
provided by <span class="citation">Tangherlini and Leonard (<a
href="#ref-tangherlini2013trawling">2013</a>)</span> who discuss topic
modelling in terms of a division of labour: “the computer algorithm is
given the task of doing what it does best: counting words and
calculating probabilities of term co-occurrence” and the *researcher is
given the task of doing what he or she does best: applying domain
expertise and experience for labelling and curating the topics” <span
class="citation">(<a href="#ref-tangherlini2013trawling">Tangherlini and
Leonard 2013, 728</a>)</span>. There is a vast literature on topic
modelling out there that will give more technical analyses, further
information on how to go from the results of your models back to the
text, different perspectives on the pros and cons of the method, and
more. However, after working through this humble course of ours, you
should be well equipped to get to decent topic models using R and be
able to begin investigating your own research questions. Let’s get on
it!</p>
<hr />
<p><a href="#introduction">Back to top</a></p>
<p><a href="https://ladal.edu.au">Back to HOME</a></p>
<hr />
</div>
<div id="references" class="section level1 unnumbered">
<h1 class="unnumbered">References</h1>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-blei2003lda" class="csl-entry">
Blei, David M., Andrew Y. Ng, and Michael I. Jordan. 2003. <span>“Latent
Dirichlet Allocation.”</span> <em>Journal of Machine Learning
Research</em> 3 (3): 993–1022.
</div>
<div id="ref-firth1957ling" class="csl-entry">
Firth, John Rupert. 1957. <span>“Studies in Linguistic Analysis.”</span>
In <em>A Synopsis of Linguistic Theory 1930–1955</em>, edited by John
Rupert Firth, 1–32. Oxford: Blackwell.
</div>
<div id="ref-jockers2013macroanalysis" class="csl-entry">
Jockers, Matthew L. 2013. <em>Macroanalysis: Digital Methods and
Literary History</em>. Urbana, Chicago; Springfield: University of
Illinois Press.
</div>
<div id="ref-kailash2012dickens" class="csl-entry">
Kailash, Sudha. 2012. <span>“Charles Dickens as a Social Critic.”</span>
<em>International Journal of Research in Economics &amp; Social
Sciences</em> 2 (8): 1–51.
</div>
<div id="ref-mahlberg2013corpus" class="csl-entry">
Mahlberg, Michaela. 2013. <em>Corpus Stylistics and Dickens’s
Fiction</em>. Routledge.
</div>
<div id="ref-mohr2013introduction" class="csl-entry">
Mohr, John W, and Petko Bogdanov. 2013. <span>“Introduction—Topic
Models: What They Are and Why They Matter.”</span> <em>Poetics</em> 41
(6): 545–69.
</div>
<div id="ref-roberts2019stm" class="csl-entry">
Roberts, Margaret E, Brandon M Stewart, and Dustin Tingley. 2019.
<span>“Stm: An r Package for Structural Topic Models.”</span>
<em>Journal of Statistical Software</em> 91: 1–40.
</div>
<div id="ref-tangherlini2013trawling" class="csl-entry">
Tangherlini, Timothy R, and Peter Leonard. 2013. <span>“Trawling in the
Sea of the Great Unread: Sub-Corpus Topic Modeling and Humanities
Research.”</span> <em>Poetics</em> 41 (6): 725–49.
</div>
</div>
</div>
<div class="footnotes footnotes-end-of-document">
<hr />
<ol>
<li id="fn1"><p>If you want to render the R Notebook on your machine,
i.e. knitting the document to html or a pdf, you need to make sure that
you have R and RStudio installed and you also need to download the <a
href="https://slcladal.github.io/content/bibliography.bib"><strong>bibliography
file</strong></a> and store it in the same folder where you store the
Rmd file.<a href="#fnref1" class="footnote-back">↩︎</a></p></li>
</ol>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // temporarily add toc-ignore selector to headers for the consistency with Pandoc
    $('.unlisted.unnumbered').addClass('toc-ignore')

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
