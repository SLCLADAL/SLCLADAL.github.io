<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />


<meta name="author" content="Martin Schweinberger" />

<meta name="date" content="2023-02-05" />

<title>Topic Modeling with R</title>

<script src="site_libs/header-attrs-2.16/header-attrs.js"></script>
<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cosmo.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<script src="site_libs/clipboard-1.7.1/clipboard.min.js"></script>
<link href="site_libs/primer-tooltips-1.4.0/build.css" rel="stylesheet" />
<link href="site_libs/klippy-0.0.0.9500/css/klippy.min.css" rel="stylesheet" />
<script src="site_libs/klippy-0.0.0.9500/js/klippy.min.js"></script>
<link rel="stylesheet" href="styles.css" />

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-VSGK4KYDQZ"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-VSGK4KYDQZ');
</script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>





<style type="text/css">
/* for pandoc --citeproc since 2.11 */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>




<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark the anchor link active (and if it's in a dropdown, also mark that active)
  var dropdown = menuAnchor.closest('li.dropdown');
  if (window.bootstrap) { // Bootstrap 4+
    menuAnchor.addClass('active');
    dropdown.find('> .dropdown-toggle').addClass('active');
  } else { // Bootstrap 3
    menuAnchor.parent().addClass('active');
    dropdown.addClass('active');
  }

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">



<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  
  <!-- Added by SKC - LADAL image and thicker top with   -->
  <div class="container-fluid navbar-top" >
    <a href="index.html"> <!-- Make entire top row and text clickable home link  -->
        <div class="row">
            <div class="navbar-brand col-md-12">
              <img src="/content/ladal_icon_cas_tran_white_trimed.png" class="navbar-icon" alt="LADAL"/>
              <span class="navbar-title-note navbar-collapse collapse" >Language Technology and Data Analysis Laboratory</span>
            </div>
        </div>
    </a>
  </div>
  
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <!-- SKC removed  navbar brand -->
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">HOME</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    ABOUT LADAL
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="people.html">People | Collabs</a>
    </li>
    <li>
      <a href="news.html">News</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    EVENTS
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="workshops.html">Workshops</a>
    </li>
    <li>
      <a href="compthinking.html">Computational Thinking in HASS</a>
    </li>
    <li>
      <a href="webinars2022.html">LADAL Webinar Series 2022</a>
    </li>
    <li>
      <a href="opening.html">LADAL Webinar Series 2021</a>
    </li>
    <li>
      <a href="atapevents.html">ATAP Events</a>
    </li>
  </ul>
</li>
<li>
  <a href="tutorials.html">TUTORIALS</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    RESOURCES
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="links.html">Links</a>
    </li>
    <li>
      <a href="base.html">Tutorial stylesheet</a>
    </li>
  </ul>
</li>
<li>
  <a href="contact.html">CONTACT</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">



<h1 class="title toc-ignore">Topic Modeling with R</h1>
<h4 class="author">Martin Schweinberger</h4>
<h4 class="date">2023-02-05</h4>

</div>


<p><img src="https://slcladal.github.io/images/uq1.jpg" width="100%" /></p>
<div id="introduction" class="section level1 unnumbered">
<h1 class="unnumbered">Introduction</h1>
<p>This tutorial introduces topic modeling using R.</p>
<p><img src="https://slcladal.github.io/images/gy_chili.jpg" width="15%" style="float:right; padding:10px" /></p>
<p>This tutorial is aimed at beginners and intermediate users of R with
the aim of showcasing how to perform basic topic modeling on textual
data using R and how to visualize the results of such a model. The aim
is not to provide a fully-fledged analysis but rather to show and
exemplify selected useful methods associated with topic modeling.</p>
<div class="warning"
style="padding:0.1em; background-color:#f2f2f2; color:#51247a">
<span>
<p style="margin-top:1em; text-align:center">
The entire R Notebook for the tutorial can be downloaded <a
href="https://slcladal.github.io/content/topicmodels.Rmd"><strong>here</strong></a>.
If you want to render the R Notebook on your machine, i.e. knitting the
document to html or a pdf, you need to make sure that you have R and
RStudio installed and you also need to download the <a
href="https://slcladal.github.io/content/bibliography.bib"><strong>bibliography
file</strong></a> and store it in the same folder where you store the
Rmd file. <br><br> <a
href="https://mybinder.org/v2/gh/SLCLADAL/interactive-notebooks/main?labpath=notebooks%2Ftopicmodels_cb.ipynb"><img
src="https://mybinder.org/badge_logo.svg" alt="Binder" /></a><br> <a
href="https://mybinder.org/v2/gh/SLCLADAL/interactive-notebooks/main?labpath=notebooks%2Ftopicmodels_cb.ipynb"><strong>Click
this link to open an interactive version of this tutorial on
MyBinder.org</strong></a>. <br> This interactive Jupyter notebook allows
you to execute code yourself and you can also change and edit the
notebook, e.g. you can change code and upload your own data. <br>
</p>
<p style="margin-left:1em;">
</p>
<p></span></p>
</div>
<p><br></p>
<p>This tutorial builds heavily on and uses materials from <a
href="https://tm4ss.github.io/docs/Tutorial_6_Topic_Models.html">this
tutorial</a> on web crawling and scraping using R by Andreas Niekler and
Gregor Wiedemann <span class="citation">(see <a href="#ref-WN17"
role="doc-biblioref">Wiedemann and Niekler 2017</a>)</span>. <a
href="https://tm4ss.github.io/docs/index.html">The tutorial</a> by
Andreas Niekler and Gregor Wiedemann is more thorough, goes into more
detail than this tutorial, and covers many more very useful text mining
methods. an alternative and equally recommendable introduction to topic
modeling with R is, of course, <span class="citation">Silge and Robinson
(<a href="#ref-silge2017text" role="doc-biblioref">2017</a>)</span>.</p>
<div class="warning"
style="padding:0.1em; background-color:#f2f2f2; color:#51247a">
<span>
<p style="margin-top:1em; text-align:center">
<strong>Topic models aim to find topics (which are operationalized as
bundles of correlating terms) in documents to see what the texts are
about.</strong>
</p>
<p style="margin-left:1em;">
</p>
<p></span></p>
</div>
<p><br></p>
<p>Topic models are a common procedure in In machine learning and
natural language processing. Topic models represent a type of
statistical model that is use to discover more or less abstract
<em>topics</em> in a given selection of documents. Topic models are
particularly common in text mining to unearth hidden semantic structures
in textual data. Topics can be conceived of as networks of collocation
terms that, because of the co-occurrence across documents, can be
assumed to refer to the same semantic domain (or topic). This assumes
that, if a document is about a certain topic, one would expect words,
that are related to that topic, to appear in the document more often
than in documents that deal with other topics. For instance,
<em>dog</em> and <em>bone</em> will appear more often in documents about
dogs whereas <em>cat</em> and <em>meow</em> will appear in documents
about cats. Terms like <em>the</em> and <em>is</em> will, however,
appear approximately equally in both.</p>
<p>Topic models are also referred to as probabilistic topic models,
which refers to statistical algorithms for discovering the latent
semantic structures of an extensive text body. Given the availability of
vast amounts of textual data, topic models can help to organize and
offer insights and assist in understanding large collections of
unstructured text.</p>
<div id="preparation-and-session-set-up"
class="section level2 unnumbered">
<h2 class="unnumbered">Preparation and session set up</h2>
<p>This tutorial is based on R. If you have not installed R or are new
to it, you will find an introduction to and more information how to use
R <a href="https://slcladal.github.io/intror.html">here</a>. For this
tutorials, we need to install certain <em>packages</em> from an R
<em>library</em> so that the scripts shown below are executed without
errors. Before turning to the code below, please install the packages by
running the code below this paragraph. If you have already installed the
packages mentioned below, then you can skip ahead and ignore this
section. To install the necessary packages, simply run the following
code - it may take some time (between 1 and 5 minutes to install all of
the packages so you do not need to worry if it takes some time).</p>
<pre class="r"><code># install packages
install.packages(&quot;tm&quot;)
install.packages(&quot;topicmodels&quot;)
install.packages(&quot;reshape2&quot;)
install.packages(&quot;ggplot2&quot;)
install.packages(&quot;wordcloud&quot;)
install.packages(&quot;pals&quot;)
install.packages(&quot;SnowballC&quot;)
install.packages(&quot;lda&quot;)
install.packages(&quot;ldatuning&quot;)
install.packages(&quot;kableExtra&quot;)
install.packages(&quot;DT&quot;)
install.packages(&quot;flextable&quot;)
# install klippy for copy-to-clipboard button in code chunks
install.packages(&quot;remotes&quot;)
remotes::install_github(&quot;rlesur/klippy&quot;)</code></pre>
<p>Next, we activate the packages.</p>
<pre class="r"><code># set options
options(stringsAsFactors = F)         # no automatic data transformation
options(&quot;scipen&quot; = 100, &quot;digits&quot; = 4) # suppress math annotation
# load packages
library(knitr) 
library(kableExtra) 
library(DT)
library(tm)
library(topicmodels)
library(reshape2)
library(ggplot2)
library(wordcloud)
library(pals)
library(SnowballC)
library(lda)
library(ldatuning)
library(flextable)
# activate klippy for copy-to-clipboard button
klippy::klippy()</code></pre>
<script>
  addClassKlippyTo("pre.r, pre.markdown");
  addKlippy('left', 'top', 'auto', '1', 'Copy code', 'Copied!');
</script>
<p>Once you have installed R and RStudio and once you have initiated the
session by executing the code shown above, you are good to go.</p>
</div>
</div>
<div id="topic-modelling" class="section level1 unnumbered">
<h1 class="unnumbered">Topic Modelling</h1>
<p>The process starts as usual with the reading of the corpus data. For
this tutorial we will analyze <em>State of the Union Addresses</em>
(SOTU) by US presidents and investigate how the topics that were
addressed in the SOTU speeches changeover time. The 231 SOTU addresses
are rather long documents. Documents lengths clearly affects the results
of topic modeling. For very short texts (e.g. Twitter posts) or very
long texts (e.g. books), it can make sense to concatenate/split single
documents to receive longer/shorter textual units for modeling.</p>
<p>For the SOTU speeches for instance, we infer the model based on
paragraphs instead of entire speeches. By manual inspection /
qualitative inspection of the results you can check if this procedure
yields better (interpretable) topics. In
<code>sotu_paragraphs.csv</code>, we provide a paragraph separated
version of the speeches.</p>
<p>For text preprocessing, we remove stopwords, since they tend to occur
as “noise” in the estimated topics of the LDA model.</p>
<pre class="r"><code># load data
textdata &lt;- base::readRDS(url(&quot;https://slcladal.github.io/data/sotu_paragraphs.rda&quot;, &quot;rb&quot;))
# load stopwords
english_stopwords &lt;- readLines(&quot;https://slcladal.github.io/resources/stopwords_en.txt&quot;, encoding = &quot;UTF-8&quot;)
# create corpus object
corpus &lt;- Corpus(DataframeSource(textdata))
# Preprocessing chain
processedCorpus &lt;- tm_map(corpus, content_transformer(tolower))
processedCorpus &lt;- tm_map(processedCorpus, removeWords, english_stopwords)
processedCorpus &lt;- tm_map(processedCorpus, removePunctuation, preserve_intra_word_dashes = TRUE)
processedCorpus &lt;- tm_map(processedCorpus, removeNumbers)
processedCorpus &lt;- tm_map(processedCorpus, stemDocument, language = &quot;en&quot;)
processedCorpus &lt;- tm_map(processedCorpus, stripWhitespace)</code></pre>
<div id="model-calculation" class="section level2 unnumbered">
<h2 class="unnumbered">Model calculation</h2>
<p>After the preprocessing, we have two corpus objects:
<code>processedCorpus</code>, on which we calculate an LDA topic model
<span class="citation">(<a href="#ref-blei2003lda"
role="doc-biblioref">Blei, Ng, and Jordan 2003</a>)</span>. To this end,
<em>stopwords</em>, i.e. function words that have relational rather than
content meaning, were removed, words were stemmed and converted to
lowercase letters and special characters were removed. The second corpus
object <code>corpus</code> serves to be able to view the original texts
and thus to facilitate a qualitative control of the topic model
results.</p>
<p>We now calculate a topic model on the <code>processedCorpus</code>.
For this purpose, a DTM of the corpus is created. In this case, we only
want to consider terms that occur with a certain minimum frequency in
the body. This is primarily used to speed up the model calculation.</p>
<pre class="r"><code># compute document term matrix with terms &gt;= minimumFrequency
minimumFrequency &lt;- 5
DTM &lt;- DocumentTermMatrix(processedCorpus, control = list(bounds = list(global = c(minimumFrequency, Inf))))
# have a look at the number of documents and terms in the matrix
dim(DTM)</code></pre>
<pre><code>## [1] 8833 4278</code></pre>
<pre class="r"><code># due to vocabulary pruning, we have empty rows in our DTM
# LDA does not like this. So we remove those docs from the
# DTM and the metadata
sel_idx &lt;- slam::row_sums(DTM) &gt; 0
DTM &lt;- DTM[sel_idx, ]
textdata &lt;- textdata[sel_idx, ]</code></pre>
<p>As an unsupervised machine learning method, topic models are suitable
for the exploration of data. The calculation of topic models aims to
determine the proportionate composition of a fixed number of topics in
the documents of a collection. It is useful to experiment with different
parameters in order to find the most suitable parameters for your own
analysis needs.</p>
<p>For parameterized models such as Latent Dirichlet Allocation (LDA),
the number of topics <code>K</code> is the most important parameter to
define in advance. How an optimal <code>K</code> should be selected
depends on various factors. If <code>K</code> is too small, the
collection is divided into a few very general semantic contexts. If
<code>K</code> is too large, the collection is divided into too many
topics of which some may overlap and others are hardly
interpretable.</p>
<p>An alternative to deciding on a set number of topics is to extract
parameters form a models using a rage of number of topics. This approach
can be useful when the number of topics is not theoretically motivated
or based on closer, qualitative inspection of the data. In the example
below, the determination of the optimal number of topics follows <span
class="citation">Murzintcev (<a href="#ref-murzintcev2020idealtopics"
role="doc-biblioref">n.d.</a>)</span>, but we only use two metrics
(<em>CaoJuan2009</em> and <em>Deveaud2014</em>) - it is highly
recommendable to inspect the results of the four metrics available for
the <code>FindTopicsNumber</code> function (<em>Griffiths2004</em>,
<em>CaoJuan2009</em>, <em>Arun2010</em>, and <em>Deveaud2014</em>).</p>
<pre class="r"><code># create models with different number of topics
result &lt;- ldatuning::FindTopicsNumber(
  DTM,
  topics = seq(from = 2, to = 20, by = 1),
  metrics = c(&quot;CaoJuan2009&quot;,  &quot;Deveaud2014&quot;),
  method = &quot;Gibbs&quot;,
  control = list(seed = 77),
  verbose = TRUE
)</code></pre>
<pre><code>## fit models... done.
## calculate metrics:
##   CaoJuan2009... done.
##   Deveaud2014... done.</code></pre>
<p>We can now plot the results. In this case, we have only use two
methods <em>CaoJuan2009</em> and <em>Griffith2004</em>. The best number
of topics shows low values for <em>CaoJuan2009</em> and high values for
<em>Griffith2004</em> (optimally, several methods should converge and
show peaks and dips respectively for a certain number of topics).</p>
<pre class="r"><code>FindTopicsNumber_plot(result)</code></pre>
<p><img src="topicmodels_files/figure-html/tm3c-1.png" width="672" /></p>
<p>For our first analysis, however, we choose a thematic “resolution” of
<code>K = 20</code> topics. In contrast to a resolution of 100 or more,
this number of topics can be evaluated qualitatively very easy.</p>
<pre class="r"><code># number of topics
K &lt;- 20
# set random number generator seed
set.seed(9161)
# compute the LDA model, inference via 1000 iterations of Gibbs sampling
topicModel &lt;- LDA(DTM, K, method=&quot;Gibbs&quot;, control=list(iter = 500, verbose = 25))</code></pre>
<pre><code>## K = 20; V = 4278; M = 8810
## Sampling 500 iterations!
## Iteration 25 ...
## Iteration 50 ...
## Iteration 75 ...
## Iteration 100 ...
## Iteration 125 ...
## Iteration 150 ...
## Iteration 175 ...
## Iteration 200 ...
## Iteration 225 ...
## Iteration 250 ...
## Iteration 275 ...
## Iteration 300 ...
## Iteration 325 ...
## Iteration 350 ...
## Iteration 375 ...
## Iteration 400 ...
## Iteration 425 ...
## Iteration 450 ...
## Iteration 475 ...
## Iteration 500 ...
## Gibbs sampling completed!</code></pre>
<p>Depending on the size of the vocabulary, the collection size and the
number K, the inference of topic models can take a very long time. This
calculation may take several minutes. If it takes too long, reduce the
vocabulary in the DTM by increasing the minimum frequency in the
previous step.</p>
<p>The topic model inference results in two (approximate) posterior
probability distributions: a distribution <code>theta</code> over K
topics within each document and a distribution <code>beta</code> over V
terms within each topic, where V represents the length of the vocabulary
of the collection (V = 4278). Let’s take a closer look at these
results:</p>
<pre class="r"><code># have a look a some of the results (posterior distributions)
tmResult &lt;- posterior(topicModel)
# format of the resulting object
attributes(tmResult)</code></pre>
<pre><code>## $names
## [1] &quot;terms&quot;  &quot;topics&quot;</code></pre>
<pre class="r"><code>nTerms(DTM)              # lengthOfVocab</code></pre>
<pre><code>## [1] 4278</code></pre>
<pre class="r"><code># topics are probability distributions over the entire vocabulary
beta &lt;- tmResult$terms   # get beta from results
dim(beta)                # K distributions over nTerms(DTM) terms</code></pre>
<pre><code>## [1]   20 4278</code></pre>
<pre class="r"><code>rowSums(beta)            # rows in beta sum to 1</code></pre>
<pre><code>##  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 
##  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1</code></pre>
<pre class="r"><code>nDocs(DTM)               # size of collection</code></pre>
<pre><code>## [1] 8810</code></pre>
<pre class="r"><code># for every document we have a probability distribution of its contained topics
theta &lt;- tmResult$topics 
dim(theta)               # nDocs(DTM) distributions over K topics</code></pre>
<pre><code>## [1] 8810   20</code></pre>
<pre class="r"><code>rowSums(theta)[1:10]     # rows in theta sum to 1</code></pre>
<pre><code>##  1  2  3  4  5  6  7  8  9 10 
##  1  1  1  1  1  1  1  1  1  1</code></pre>
<p>Let’s take a look at the 10 most likely terms within the term
probabilities <code>beta</code> of the inferred topics (only the first 8
are shown below).</p>
<pre class="r"><code>terms(topicModel, 10)</code></pre>
<pre><code>##       Topic 1     Topic 2     Topic 3    Topic 4    Topic 5     Topic 6     
##  [1,] &quot;land&quot;      &quot;recommend&quot; &quot;measur&quot;   &quot;citizen&quot;  &quot;great&quot;     &quot;claim&quot;     
##  [2,] &quot;indian&quot;    &quot;report&quot;    &quot;interest&quot; &quot;law&quot;      &quot;line&quot;      &quot;govern&quot;    
##  [3,] &quot;territori&quot; &quot;congress&quot;  &quot;view&quot;     &quot;case&quot;     &quot;part&quot;      &quot;question&quot;  
##  [4,] &quot;larg&quot;      &quot;attent&quot;    &quot;subject&quot;  &quot;person&quot;   &quot;coast&quot;     &quot;commiss&quot;   
##  [5,] &quot;tribe&quot;     &quot;secretari&quot; &quot;time&quot;     &quot;court&quot;    &quot;pacif&quot;     &quot;spain&quot;     
##  [6,] &quot;limit&quot;     &quot;depart&quot;    &quot;present&quot;  &quot;properti&quot; &quot;construct&quot; &quot;island&quot;    
##  [7,] &quot;popul&quot;     &quot;subject&quot;   &quot;object&quot;   &quot;protect&quot;  &quot;import&quot;    &quot;made&quot;      
##  [8,] &quot;portion&quot;   &quot;consider&quot;  &quot;reason&quot;   &quot;natur&quot;    &quot;river&quot;     &quot;adjust&quot;    
##  [9,] &quot;general&quot;   &quot;present&quot;   &quot;adopt&quot;    &quot;justic&quot;   &quot;complet&quot;   &quot;commission&quot;
## [10,] &quot;public&quot;    &quot;import&quot;    &quot;regard&quot;   &quot;demand&quot;   &quot;south&quot;     &quot;final&quot;     
##       Topic 7     Topic 8     Topic 9      Topic 10     Topic 11   Topic 12   
##  [1,] &quot;public&quot;    &quot;state&quot;     &quot;govern&quot;     &quot;year&quot;       &quot;nation&quot;   &quot;constitut&quot;
##  [2,] &quot;offic&quot;     &quot;unit&quot;      &quot;relat&quot;      &quot;amount&quot;     &quot;power&quot;    &quot;power&quot;    
##  [3,] &quot;duti&quot;      &quot;govern&quot;    &quot;receiv&quot;     &quot;expenditur&quot; &quot;peac&quot;     &quot;state&quot;    
##  [4,] &quot;execut&quot;    &quot;mexico&quot;    &quot;minist&quot;     &quot;increas&quot;    &quot;govern&quot;   &quot;peopl&quot;    
##  [5,] &quot;general&quot;   &quot;part&quot;      &quot;friend&quot;     &quot;treasuri&quot;   &quot;war&quot;      &quot;union&quot;    
##  [6,] &quot;administr&quot; &quot;territori&quot; &quot;republ&quot;     &quot;end&quot;        &quot;foreign&quot;  &quot;repres&quot;   
##  [7,] &quot;give&quot;      &quot;texa&quot;      &quot;continu&quot;    &quot;estim&quot;      &quot;independ&quot; &quot;govern&quot;   
##  [8,] &quot;respect&quot;   &quot;mexican&quot;   &quot;intercours&quot; &quot;fiscal&quot;     &quot;maintain&quot; &quot;presid&quot;   
##  [9,] &quot;direct&quot;    &quot;republ&quot;    &quot;hope&quot;       &quot;revenu&quot;     &quot;polici&quot;   &quot;hous&quot;     
## [10,] &quot;proper&quot;    &quot;author&quot;    &quot;inform&quot;     &quot;june&quot;       &quot;intern&quot;   &quot;elect&quot;    
##       Topic 13   Topic 14   Topic 15    Topic 16   Topic 17     Topic 18  
##  [1,] &quot;great&quot;    &quot;treati&quot;   &quot;made&quot;      &quot;congress&quot; &quot;duti&quot;       &quot;war&quot;     
##  [2,] &quot;countri&quot;  &quot;great&quot;    &quot;appropri&quot;  &quot;act&quot;      &quot;import&quot;     &quot;forc&quot;    
##  [3,] &quot;peopl&quot;    &quot;british&quot;  &quot;improv&quot;    &quot;law&quot;      &quot;increas&quot;    &quot;servic&quot;  
##  [4,] &quot;labor&quot;    &quot;britain&quot;  &quot;work&quot;      &quot;author&quot;   &quot;countri&quot;    &quot;militari&quot;
##  [5,] &quot;interest&quot; &quot;convent&quot;  &quot;purpos&quot;    &quot;provis&quot;   &quot;foreign&quot;    &quot;armi&quot;    
##  [6,] &quot;condit&quot;   &quot;trade&quot;    &quot;provid&quot;    &quot;session&quot;  &quot;product&quot;    &quot;navi&quot;    
##  [7,] &quot;good&quot;     &quot;vessel&quot;   &quot;make&quot;      &quot;legisl&quot;   &quot;produc&quot;     &quot;men&quot;     
##  [8,] &quot;system&quot;   &quot;port&quot;     &quot;establish&quot; &quot;execut&quot;   &quot;manufactur&quot; &quot;offic&quot;   
##  [9,] &quot;busi&quot;     &quot;negoti&quot;   &quot;secur&quot;     &quot;effect&quot;   &quot;revenu&quot;     &quot;ship&quot;    
## [10,] &quot;individu&quot; &quot;american&quot; &quot;object&quot;    &quot;pass&quot;     &quot;larg&quot;       &quot;command&quot; 
##       Topic 19   Topic 20  
##  [1,] &quot;nation&quot;   &quot;public&quot;  
##  [2,] &quot;countri&quot;  &quot;bank&quot;    
##  [3,] &quot;peopl&quot;    &quot;govern&quot;  
##  [4,] &quot;prosper&quot;  &quot;money&quot;   
##  [5,] &quot;great&quot;    &quot;issu&quot;    
##  [6,] &quot;institut&quot; &quot;treasuri&quot;
##  [7,] &quot;preserv&quot;  &quot;gold&quot;    
##  [8,] &quot;honor&quot;    &quot;note&quot;    
##  [9,] &quot;happi&quot;    &quot;debt&quot;    
## [10,] &quot;spirit&quot;   &quot;interest&quot;</code></pre>
<pre class="r"><code>exampleTermData &lt;- terms(topicModel, 10)
exampleTermData[, 1:8]</code></pre>
<pre><code>##       Topic 1     Topic 2     Topic 3    Topic 4    Topic 5     Topic 6     
##  [1,] &quot;land&quot;      &quot;recommend&quot; &quot;measur&quot;   &quot;citizen&quot;  &quot;great&quot;     &quot;claim&quot;     
##  [2,] &quot;indian&quot;    &quot;report&quot;    &quot;interest&quot; &quot;law&quot;      &quot;line&quot;      &quot;govern&quot;    
##  [3,] &quot;territori&quot; &quot;congress&quot;  &quot;view&quot;     &quot;case&quot;     &quot;part&quot;      &quot;question&quot;  
##  [4,] &quot;larg&quot;      &quot;attent&quot;    &quot;subject&quot;  &quot;person&quot;   &quot;coast&quot;     &quot;commiss&quot;   
##  [5,] &quot;tribe&quot;     &quot;secretari&quot; &quot;time&quot;     &quot;court&quot;    &quot;pacif&quot;     &quot;spain&quot;     
##  [6,] &quot;limit&quot;     &quot;depart&quot;    &quot;present&quot;  &quot;properti&quot; &quot;construct&quot; &quot;island&quot;    
##  [7,] &quot;popul&quot;     &quot;subject&quot;   &quot;object&quot;   &quot;protect&quot;  &quot;import&quot;    &quot;made&quot;      
##  [8,] &quot;portion&quot;   &quot;consider&quot;  &quot;reason&quot;   &quot;natur&quot;    &quot;river&quot;     &quot;adjust&quot;    
##  [9,] &quot;general&quot;   &quot;present&quot;   &quot;adopt&quot;    &quot;justic&quot;   &quot;complet&quot;   &quot;commission&quot;
## [10,] &quot;public&quot;    &quot;import&quot;    &quot;regard&quot;   &quot;demand&quot;   &quot;south&quot;     &quot;final&quot;     
##       Topic 7     Topic 8    
##  [1,] &quot;public&quot;    &quot;state&quot;    
##  [2,] &quot;offic&quot;     &quot;unit&quot;     
##  [3,] &quot;duti&quot;      &quot;govern&quot;   
##  [4,] &quot;execut&quot;    &quot;mexico&quot;   
##  [5,] &quot;general&quot;   &quot;part&quot;     
##  [6,] &quot;administr&quot; &quot;territori&quot;
##  [7,] &quot;give&quot;      &quot;texa&quot;     
##  [8,] &quot;respect&quot;   &quot;mexican&quot;  
##  [9,] &quot;direct&quot;    &quot;republ&quot;   
## [10,] &quot;proper&quot;    &quot;author&quot;</code></pre>
<p>For the next steps, we want to give the topics more descriptive names
than just numbers. Therefore, we simply concatenate the five most likely
terms of each topic to a string that represents a pseudo-name for each
topic.</p>
<pre class="r"><code>top5termsPerTopic &lt;- terms(topicModel, 5)
topicNames &lt;- apply(top5termsPerTopic, 2, paste, collapse=&quot; &quot;)</code></pre>
</div>
<div id="visualization-of-words-and-topics"
class="section level2 unnumbered">
<h2 class="unnumbered">Visualization of Words and Topics</h2>
<p>Although wordclouds may not be optimal for scientific purposes they
can provide a quick visual overview of a set of terms. Let’s look at
some topics as wordcloud.</p>
<p>In the following code, you can change the variable
<strong>topicToViz</strong> with values between 1 and 20 to display
other topics.</p>
<pre class="r"><code># visualize topics as word cloud
topicToViz &lt;- 11 # change for your own topic of interest
topicToViz &lt;- grep(&#39;mexico&#39;, topicNames)[1] # Or select a topic by a term contained in its name
# select to 40 most probable terms from the topic by sorting the term-topic-probability vector in decreasing order
top40terms &lt;- sort(tmResult$terms[topicToViz,], decreasing=TRUE)[1:40]
words &lt;- names(top40terms)
# extract the probabilites of each of the 40 terms
probabilities &lt;- sort(tmResult$terms[topicToViz,], decreasing=TRUE)[1:40]
# visualize the terms as wordcloud
mycolors &lt;- brewer.pal(8, &quot;Dark2&quot;)
wordcloud(words, probabilities, random.order = FALSE, color = mycolors)</code></pre>
<p><img src="topicmodels_files/figure-html/unnamed-chunk-1-1.png" width="384" style="display: block; margin: auto;" /></p>
<p>Let us now look more closely at the distribution of topics within
individual documents. To this end, we visualize the distribution in 3
sample documents.</p>
<p>Let us first take a look at the contents of three sample
documents:</p>
<pre class="r"><code>exampleIds &lt;- c(2, 100, 200)
lapply(corpus[exampleIds], as.character)</code></pre>
<pre><code>## $`2`
## [1] &quot;I embrace with great satisfaction the opportunity which now presents itself\nof congratulating you on the present favorable prospects of our public\naffairs. The recent accession of the important state of North Carolina to\nthe Constitution of the United States (of which official information has\nbeen received), the rising credit and respectability of our country, the\ngeneral and increasing good will toward the government of the Union, and\nthe concord, peace, and plenty with which we are blessed are circumstances\nauspicious in an eminent degree to our national prosperity.&quot;
## 
## $`100`
## [1] &quot;Provision is likewise requisite for the reimbursement of the loan which has\nbeen made of the Bank of the United States, pursuant to the eleventh\nsection of the act by which it is incorporated. In fulfilling the public\nstipulations in this particular it is expected a valuable saving will be\nmade.&quot;
## 
## $`200`
## [1] &quot;After many delays and disappointments arising out of the European war, the\nfinal arrangements for fulfilling the engagements made to the Dey and\nRegency of Algiers will in all present appearance be crowned with success,\nbut under great, though inevitable, disadvantages in the pecuniary\ntransactions occasioned by that war, which will render further provision\nnecessary. The actual liberation of all our citizens who were prisoners in\nAlgiers, while it gratifies every feeling of heart, is itself an earnest of\na satisfactory termination of the whole negotiation. Measures are in\noperation for effecting treaties with the Regencies of Tunis and Tripoli.&quot;</code></pre>
<pre class="r"><code>exampleIds &lt;- c(2, 100, 200)
print(paste0(exampleIds[1], &quot;: &quot;, substr(content(corpus[[exampleIds[1]]]), 0, 400), &#39;...&#39;))</code></pre>
<pre><code>## [1] &quot;2: I embrace with great satisfaction the opportunity which now presents itself\nof congratulating you on the present favorable prospects of our public\naffairs. The recent accession of the important state of North Carolina to\nthe Constitution of the United States (of which official information has\nbeen received), the rising credit and respectability of our country, the\ngeneral and increasing good will ...&quot;</code></pre>
<pre class="r"><code>print(paste0(exampleIds[2], &quot;: &quot;, substr(content(corpus[[exampleIds[2]]]), 0, 400), &#39;...&#39;))</code></pre>
<pre><code>## [1] &quot;100: Provision is likewise requisite for the reimbursement of the loan which has\nbeen made of the Bank of the United States, pursuant to the eleventh\nsection of the act by which it is incorporated. In fulfilling the public\nstipulations in this particular it is expected a valuable saving will be\nmade....&quot;</code></pre>
<pre class="r"><code>print(paste0(exampleIds[3], &quot;: &quot;, substr(content(corpus[[exampleIds[3]]]), 0, 400), &#39;...&#39;))</code></pre>
<pre><code>## [1] &quot;200: After many delays and disappointments arising out of the European war, the\nfinal arrangements for fulfilling the engagements made to the Dey and\nRegency of Algiers will in all present appearance be crowned with success,\nbut under great, though inevitable, disadvantages in the pecuniary\ntransactions occasioned by that war, which will render further provision\nnecessary. The actual liberation of all ...&quot;</code></pre>
<p>After looking into the documents, we visualize the topic
distributions within the documents.</p>
<pre class="r"><code>N &lt;- length(exampleIds)
# get topic proportions form example documents
topicProportionExamples &lt;- theta[exampleIds,]
colnames(topicProportionExamples) &lt;- topicNames
vizDataFrame &lt;- melt(cbind(data.frame(topicProportionExamples), document = factor(1:N)), variable.name = &quot;topic&quot;, id.vars = &quot;document&quot;)  
ggplot(data = vizDataFrame, aes(topic, value, fill = document), ylab = &quot;proportion&quot;) + 
  geom_bar(stat=&quot;identity&quot;) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +  
  coord_flip() +
  facet_wrap(~ document, ncol = N)</code></pre>
<p><img src="topicmodels_files/figure-html/unnamed-chunk-2-1.png" width="960" style="display: block; margin: auto;" /></p>
</div>
<div id="topic-distributions" class="section level2 unnumbered">
<h2 class="unnumbered">Topic distributions</h2>
<p>The figure above shows how topics within a document are distributed
according to the model. In the current model all three documents show at
least a small percentage of each topic. However, two to three topics
dominate each document.</p>
<p>The topic distribution within a document can be controlled with the
<em>Alpha</em>-parameter of the model. Higher alpha priors for topics
result in an even distribution of topics within a document. Low alpha
priors ensure that the inference process distributes the probability
mass on a few topics for each document.</p>
<p>In the previous model calculation the alpha-prior was automatically
estimated in order to fit to the data (highest overall probability of
the model). However, this automatic estimate does not necessarily
correspond to the results that one would like to have as an analyst.
Depending on our analysis interest, we might be interested in a more
peaky/more even distribution of topics in the model.</p>
<p>Now let us change the alpha prior to a lower value to see how this
affects the topic distributions in the model.</p>
<pre class="r"><code># see alpha from previous model
attr(topicModel, &quot;alpha&quot;) </code></pre>
<pre><code>## [1] 2.5</code></pre>
<pre class="r"><code>topicModel2 &lt;- LDA(DTM, K, method=&quot;Gibbs&quot;, control=list(iter = 500, verbose = 25, alpha = 0.2))</code></pre>
<pre><code>## K = 20; V = 4278; M = 8810
## Sampling 500 iterations!
## Iteration 25 ...
## Iteration 50 ...
## Iteration 75 ...
## Iteration 100 ...
## Iteration 125 ...
## Iteration 150 ...
## Iteration 175 ...
## Iteration 200 ...
## Iteration 225 ...
## Iteration 250 ...
## Iteration 275 ...
## Iteration 300 ...
## Iteration 325 ...
## Iteration 350 ...
## Iteration 375 ...
## Iteration 400 ...
## Iteration 425 ...
## Iteration 450 ...
## Iteration 475 ...
## Iteration 500 ...
## Gibbs sampling completed!</code></pre>
<pre class="r"><code>tmResult &lt;- posterior(topicModel2)
theta &lt;- tmResult$topics
beta &lt;- tmResult$terms
topicNames &lt;- apply(terms(topicModel2, 5), 2, paste, collapse = &quot; &quot;)  # reset topicnames</code></pre>
<p>Now visualize the topic distributions in the three documents again.
What are the differences in the distribution structure?</p>
<pre class="r"><code># get topic proportions form example documents
topicProportionExamples &lt;- theta[exampleIds,]
colnames(topicProportionExamples) &lt;- topicNames
vizDataFrame &lt;- melt(cbind(data.frame(topicProportionExamples), document = factor(1:N)), variable.name = &quot;topic&quot;, id.vars = &quot;document&quot;)  
ggplot(data = vizDataFrame, aes(topic, value, fill = document), ylab = &quot;proportion&quot;) + 
  geom_bar(stat=&quot;identity&quot;) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +  
  coord_flip() +
  facet_wrap(~ document, ncol = N)</code></pre>
<p><img src="topicmodels_files/figure-html/unnamed-chunk-3-1.png" width="960" style="display: block; margin: auto;" /></p>
</div>
<div id="topic-ranking" class="section level2 unnumbered">
<h2 class="unnumbered">Topic ranking</h2>
<p>First, we try to get a more meaningful order of top terms per topic
by re-ranking them with a specific score <span class="citation">(<a
href="#ref-Chang2009" role="doc-biblioref">Chang et al.
2009</a>)</span>. The idea of re-ranking terms is similar to the idea of
TF-IDF. The more a term appears in top levels w.r.t. its probability,
the less meaningful it is to describe the topic. Hence, the scoring
advanced favors terms to describe a topic.</p>
<pre class="r"><code># re-rank top topic terms for topic names
topicNames &lt;- apply(lda::top.topic.words(beta, 5, by.score = T), 2, paste, collapse = &quot; &quot;)</code></pre>
<p>What are the defining topics within a collection? There are different
approaches to find out which can be used to bring the topics into a
certain order.</p>
<div id="approach-1" class="section level3 unnumbered">
<h3 class="unnumbered">Approach 1</h3>
<p>We sort topics according to their probability within the entire
collection:</p>
<pre class="r"><code># What are the most probable topics in the entire collection?
topicProportions &lt;- colSums(theta) / nDocs(DTM)  # mean probabilities over all paragraphs
names(topicProportions) &lt;- topicNames     # assign the topic names we created before
sort(topicProportions, decreasing = TRUE) # show summed proportions in decreased order</code></pre>
<pre><code>##        public object system consider great 
##                                    0.06485 
##          nation peopl countri prosper peac 
##                                    0.06293 
##          claim govern adjust treati negoti 
##                                    0.05979 
## congress attent report recommend secretari 
##                                    0.05888 
##         congress senat state treati repres 
##                                    0.05527 
##       year amount treasuri expenditur debt 
##                                    0.05500 
##            govern relat state island spain 
##                                    0.05493 
##              war mexico peac state citizen 
##                                    0.05265 
##          constitut state power peopl union 
##                                    0.05128 
##               peopl man labor polit condit 
##                                    0.04965 
##         offic servic depart appoint public 
##                                    0.04889 
##    product manufactur tariff duti industri 
##                                    0.04537 
##               state unit trade vessel port 
##                                    0.04491 
##                law court state unit person 
##                                    0.04416 
##           year increas mail pension number 
##                                    0.04386 
##            bank gold currenc silver circul 
##                                    0.04337 
##           navi vessel ship naval construct 
##                                    0.04236 
##              armi war forc militari servic 
##                                    0.04122 
##           line state territori river pacif 
##                                    0.04117 
##            land indian tribe territori acr 
##                                    0.03949</code></pre>
<pre class="r"><code>soP &lt;- sort(topicProportions, decreasing = TRUE)
paste(round(soP, 5), &quot;:&quot;, names(soP))</code></pre>
<pre><code>##  [1] &quot;0.06485 : public object system consider great&quot;       
##  [2] &quot;0.06293 : nation peopl countri prosper peac&quot;         
##  [3] &quot;0.05979 : claim govern adjust treati negoti&quot;         
##  [4] &quot;0.05888 : congress attent report recommend secretari&quot;
##  [5] &quot;0.05527 : congress senat state treati repres&quot;        
##  [6] &quot;0.055 : year amount treasuri expenditur debt&quot;        
##  [7] &quot;0.05493 : govern relat state island spain&quot;           
##  [8] &quot;0.05265 : war mexico peac state citizen&quot;             
##  [9] &quot;0.05128 : constitut state power peopl union&quot;         
## [10] &quot;0.04965 : peopl man labor polit condit&quot;              
## [11] &quot;0.04889 : offic servic depart appoint public&quot;        
## [12] &quot;0.04537 : product manufactur tariff duti industri&quot;   
## [13] &quot;0.04491 : state unit trade vessel port&quot;              
## [14] &quot;0.04416 : law court state unit person&quot;               
## [15] &quot;0.04386 : year increas mail pension number&quot;          
## [16] &quot;0.04337 : bank gold currenc silver circul&quot;           
## [17] &quot;0.04236 : navi vessel ship naval construct&quot;          
## [18] &quot;0.04122 : armi war forc militari servic&quot;             
## [19] &quot;0.04117 : line state territori river pacif&quot;          
## [20] &quot;0.03949 : land indian tribe territori acr&quot;</code></pre>
<p>We recognize some topics that are way more likely to occur in the
corpus than others. These describe rather general thematic coherence.
Other topics correspond more to specific contents.</p>
</div>
<div id="approach-2" class="section level3 unnumbered">
<h3 class="unnumbered">Approach 2</h3>
<p>We count how often a topic appears as a primary topic within a
paragraph This method is also called Rank-1.</p>
<pre class="r"><code>countsOfPrimaryTopics &lt;- rep(0, K)
names(countsOfPrimaryTopics) &lt;- topicNames
for (i in 1:nDocs(DTM)) {
  topicsPerDoc &lt;- theta[i, ] # select topic distribution for document i
  # get first element position from ordered list
  primaryTopic &lt;- order(topicsPerDoc, decreasing = TRUE)[1] 
  countsOfPrimaryTopics[primaryTopic] &lt;- countsOfPrimaryTopics[primaryTopic] + 1
}
sort(countsOfPrimaryTopics, decreasing = TRUE)</code></pre>
<pre><code>##          claim govern adjust treati negoti 
##                                        623 
##            govern relat state island spain 
##                                        594 
##          nation peopl countri prosper peac 
##                                        576 
##        public object system consider great 
##                                        525 
##       year amount treasuri expenditur debt 
##                                        524 
##         congress senat state treati repres 
##                                        521 
##              war mexico peac state citizen 
##                                        476 
## congress attent report recommend secretari 
##                                        461 
##            bank gold currenc silver circul 
##                                        428 
##         offic servic depart appoint public 
##                                        420 
##          constitut state power peopl union 
##                                        414 
##                law court state unit person 
##                                        383 
##               state unit trade vessel port 
##                                        373 
##    product manufactur tariff duti industri 
##                                        373 
##           navi vessel ship naval construct 
##                                        370 
##               peopl man labor polit condit 
##                                        369 
##            land indian tribe territori acr 
##                                        368 
##           year increas mail pension number 
##                                        357 
##              armi war forc militari servic 
##                                        336 
##           line state territori river pacif 
##                                        319</code></pre>
<pre class="r"><code>so &lt;- sort(countsOfPrimaryTopics, decreasing = TRUE)
paste(so, &quot;:&quot;, names(so))</code></pre>
<pre><code>##  [1] &quot;623 : claim govern adjust treati negoti&quot;         
##  [2] &quot;594 : govern relat state island spain&quot;           
##  [3] &quot;576 : nation peopl countri prosper peac&quot;         
##  [4] &quot;525 : public object system consider great&quot;       
##  [5] &quot;524 : year amount treasuri expenditur debt&quot;      
##  [6] &quot;521 : congress senat state treati repres&quot;        
##  [7] &quot;476 : war mexico peac state citizen&quot;             
##  [8] &quot;461 : congress attent report recommend secretari&quot;
##  [9] &quot;428 : bank gold currenc silver circul&quot;           
## [10] &quot;420 : offic servic depart appoint public&quot;        
## [11] &quot;414 : constitut state power peopl union&quot;         
## [12] &quot;383 : law court state unit person&quot;               
## [13] &quot;373 : state unit trade vessel port&quot;              
## [14] &quot;373 : product manufactur tariff duti industri&quot;   
## [15] &quot;370 : navi vessel ship naval construct&quot;          
## [16] &quot;369 : peopl man labor polit condit&quot;              
## [17] &quot;368 : land indian tribe territori acr&quot;           
## [18] &quot;357 : year increas mail pension number&quot;          
## [19] &quot;336 : armi war forc militari servic&quot;             
## [20] &quot;319 : line state territori river pacif&quot;</code></pre>
<p>We see that sorting topics by the Rank-1 method places topics with
rather specific thematic coherences in upper ranks of the list.</p>
<p>This sorting of topics can be used for further analysis steps such as
the semantic interpretation of topics found in the collection, the
analysis of time series of the most important topics or the filtering of
the original collection based on specific sub-topics.</p>
</div>
</div>
<div id="filtering-documents" class="section level2 unnumbered">
<h2 class="unnumbered">Filtering documents</h2>
<p>The fact that a topic model conveys of topic probabilities for each
document, resp. paragraph in our case, makes it possible to use it for
thematic filtering of a collection. AS filter we select only those
documents which exceed a certain threshold of their probability value
for certain topics (for example, each document which contains topic
<code>X</code> to more than 20 percent).</p>
<p>In the following, we will select documents based on their topic
content and display the resulting document quantity over time.</p>
<pre class="r"><code>topicToFilter &lt;- 6  # you can set this manually ...
# ... or have it selected by a term in the topic name (e.g. &#39;children&#39;)
topicToFilter &lt;- grep(&#39;children&#39;, topicNames)[1] 
topicThreshold &lt;- 0.2
selectedDocumentIndexes &lt;- which(theta[, topicToFilter] &gt;= topicThreshold)
filteredCorpus &lt;- corpus[selectedDocumentIndexes]
# show length of filtered corpus
filteredCorpus</code></pre>
<pre><code>## &lt;&lt;SimpleCorpus&gt;&gt;
## Metadata:  corpus specific: 1, document level (indexed): 4
## Content:  documents: 0</code></pre>
<p>Our filtered corpus contains 0 documents related to the topic NA to
at least 20 %.</p>
</div>
<div id="topic-proportions-over-time" class="section level2 unnumbered">
<h2 class="unnumbered">Topic proportions over time</h2>
<p>In a last step, we provide a distant view on the topics in the data
over time. For this, we aggregate mean topic proportions per decade of
all SOTU speeches. These aggregated topic proportions can then be
visualized, e.g. as a bar plot.</p>
<pre class="r"><code># append decade information for aggregation
textdata$decade &lt;- paste0(substr(textdata$date, 0, 3), &quot;0&quot;)
# get mean topic proportions per decade
topic_proportion_per_decade &lt;- aggregate(theta, by = list(decade = textdata$decade), mean)
# set topic names to aggregated columns
colnames(topic_proportion_per_decade)[2:(K+1)] &lt;- topicNames
# reshape data frame
vizDataFrame &lt;- melt(topic_proportion_per_decade, id.vars = &quot;decade&quot;)
# plot topic proportions per decade as bar plot
ggplot(vizDataFrame, aes(x=decade, y=value, fill=variable)) + 
  geom_bar(stat = &quot;identity&quot;) + ylab(&quot;proportion&quot;) + 
  scale_fill_manual(values = paste0(alphabet(20), &quot;FF&quot;), name = &quot;decade&quot;) + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))</code></pre>
<p><img src="topicmodels_files/figure-html/unnamed-chunk-4-1.png" width="864" style="display: block; margin: auto;" /></p>
<p>The visualization shows that topics around the relation between the
federal government and the states as well as inner conflicts clearly
dominate the first decades. Security issues and the economy are the most
important topics of recent SOTU addresses.</p>
</div>
</div>
<div id="citation-session-info" class="section level1 unnumbered">
<h1 class="unnumbered">Citation &amp; Session Info</h1>
<p>Schweinberger, Martin. 2023. <em>Topic Modeling with R</em>.
Brisbane: The University of Queensland. url: <a
href="https://slcladal.github.io/topicmodels.html"
class="uri">https://slcladal.github.io/topicmodels.html</a> (Version
2023.02.05).</p>
<pre><code>@manual{schweinberger2023topic,
  author = {Schweinberger, Martin},
  title = {Topic Modeling with R},
  note = {https://slcladal.github.io/topicmodels.html},
  year = {2023},
  organization = &quot;The University of Queensland, Australia. School of Languages and Cultures},
  address = {Brisbane},
  edition = {2023.02.05}
}</code></pre>
<pre class="r"><code>sessionInfo()</code></pre>
<pre><code>## R version 4.2.1 RC (2022-06-17 r82510 ucrt)
## Platform: x86_64-w64-mingw32/x64 (64-bit)
## Running under: Windows 10 x64 (build 19045)
## 
## Matrix products: default
## 
## locale:
## [1] LC_COLLATE=German_Germany.utf8  LC_CTYPE=German_Germany.utf8   
## [3] LC_MONETARY=German_Germany.utf8 LC_NUMERIC=C                   
## [5] LC_TIME=German_Germany.utf8    
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] flextable_0.8.3    ldatuning_1.0.2    lda_1.4.2          SnowballC_0.7.0   
##  [5] pals_1.7           wordcloud_2.6      RColorBrewer_1.1-3 ggplot2_3.4.0     
##  [9] reshape2_1.4.4     topicmodels_0.2-13 tm_0.7-10          NLP_0.2-1         
## [13] DT_0.26            kableExtra_1.3.4   knitr_1.40        
## 
## loaded via a namespace (and not attached):
##  [1] httr_1.4.4        sass_0.4.2        maps_3.4.0        jsonlite_1.8.0   
##  [5] viridisLite_0.4.1 bslib_0.4.0       assertthat_0.2.1  highr_0.9        
##  [9] stats4_4.2.1      yaml_2.3.5        slam_0.1-50       gdtools_0.2.4    
## [13] pillar_1.8.1      glue_1.6.2        uuid_1.1-0        digest_0.6.29    
## [17] rvest_1.0.3       colorspace_2.0-3  htmltools_0.5.3   plyr_1.8.7       
## [21] pkgconfig_2.0.3   purrr_0.3.4       scales_1.2.1      webshot_0.5.3    
## [25] svglite_2.1.0     officer_0.4.4     tibble_3.1.8      farver_2.1.1     
## [29] generics_0.1.3    cachem_1.0.6      withr_2.5.0       klippy_0.0.0.9500
## [33] cli_3.6.0         magrittr_2.0.3    evaluate_0.16     fansi_1.0.3      
## [37] xml2_1.3.3        tools_4.2.1       data.table_1.14.2 lifecycle_1.0.3  
## [41] stringr_1.5.0     munsell_0.5.0     zip_2.2.0         compiler_4.2.1   
## [45] jquerylib_0.1.4   systemfonts_1.0.4 rlang_1.0.6       grid_4.2.1       
## [49] dichromat_2.0-0.1 rstudioapi_0.14   htmlwidgets_1.5.4 labeling_0.4.2   
## [53] base64enc_0.1-3   rmarkdown_2.16    gtable_0.3.0      DBI_1.1.3        
## [57] R6_2.5.1          dplyr_1.0.10      fastmap_1.1.0     utf8_1.2.2       
## [61] modeltools_0.2-23 stringi_1.7.8     parallel_4.2.1    Rcpp_1.0.9       
## [65] vctrs_0.5.1       mapproj_1.2.8     tidyselect_1.1.2  xfun_0.32</code></pre>
<hr />
<p><a href="#introduction">Back to top</a></p>
<p><a href="https://slcladal.github.io/index.html">Back to HOME</a></p>
<hr />
</div>
<div id="references" class="section level1 unnumbered">
<h1 class="unnumbered">References</h1>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-blei2003lda" class="csl-entry">
Blei, David M., Andrew Y. Ng, and Michael I. Jordan. 2003. <span>“Latent
Dirichlet Allocation.”</span> <em>Journal of Machine Learning
Research</em> 3 (3): 993–1022.
</div>
<div id="ref-Chang2009" class="csl-entry">
Chang, Jonathan, Sean Gerrish, Chong Wang, Jordan L. Boyd-graber, and
David M. Blei. 2009. <span>“Reading Tea Leaves: How Humans Interpret
Topic Models.”</span> In <em>Advances in Neural Information Processing
Systems 22</em>, edited by Yoshua Bengio, Dale Schuurmans, John D.
Lafferty, Christopher K. Williams, and Aron Culotta, 288–96. Curran. <a
href="http://papers.nips.cc/paper/3700-reading-tea-leaves-how-humans-interpret-topic-models.pdf">http://papers.nips.cc/paper/3700-reading-tea-leaves-how-humans-interpret-topic-models.pdf</a>.
</div>
<div id="ref-murzintcev2020idealtopics" class="csl-entry">
Murzintcev, Nikita. n.d. <span>“Select Number of Topics for LDA
Model.”</span> <a
href="https://cran.r-project.org/web/packages/ldatuning/vignettes/topics.html">https://cran.r-project.org/web/packages/ldatuning/vignettes/topics.html</a>.
</div>
<div id="ref-silge2017text" class="csl-entry">
Silge, Julia, and David Robinson. 2017. <em>Text Mining with r: A Tidy
Approach</em>. " O’Reilly Media, Inc.".
</div>
<div id="ref-WN17" class="csl-entry">
Wiedemann, Gregor, and Andreas Niekler. 2017. <span>“Hands-on:
<span>A</span> Five Day Text Mining Course for Humanists and Social
Scientists in <span>R</span>.”</span> In <em>Proceedings of the Workshop
on Teaching <span>NLP</span> for Digital Humanities
(<span>Teach4DH</span>), Berlin, Germany, September 12, 2017.</em>,
57–65. <a
href="http://ceur-ws.org/Vol-1918/wiedemann.pdf">http://ceur-ws.org/Vol-1918/wiedemann.pdf</a>.
</div>
</div>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // temporarily add toc-ignore selector to headers for the consistency with Pandoc
    $('.unlisted.unnumbered').addClass('toc-ignore')

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
