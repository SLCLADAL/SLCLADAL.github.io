<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />


<meta name="author" content="Martin Schweinberger" />

<meta name="date" content="2024-05-17" />

<title>Topic Modeling with R</title>

<script src="site_libs/header-attrs-2.26/header-attrs.js"></script>
<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cosmo.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/jqueryui-1.13.2/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<script src="site_libs/clipboard-1.7.1/clipboard.min.js"></script>
<link href="site_libs/primer-tooltips-1.4.0/build.css" rel="stylesheet" />
<link href="site_libs/klippy-0.0.0.9500/css/klippy.min.css" rel="stylesheet" />
<script src="site_libs/klippy-0.0.0.9500/js/klippy.min.js"></script>
<link rel="stylesheet" href="styles.css" />

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-VSGK4KYDQZ"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-VSGK4KYDQZ');
</script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>





<style type="text/css">
/* for pandoc --citeproc since 2.11 */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>




<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark the anchor link active (and if it's in a dropdown, also mark that active)
  var dropdown = menuAnchor.closest('li.dropdown');
  if (window.bootstrap) { // Bootstrap 4+
    menuAnchor.addClass('active');
    dropdown.find('> .dropdown-toggle').addClass('active');
  } else { // Bootstrap 3
    menuAnchor.parent().addClass('active');
    dropdown.addClass('active');
  }

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before, .tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "\e259";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "\e258";
  font-family: 'Glyphicons Halflings';
  border: none;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">



<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  
  <!-- Added by SKC - LADAL image and thicker top with   -->
  <div class="container-fluid navbar-top" >
    <a href="index.html"> <!-- Make entire top row and text clickable home link  -->
        <div class="row">
            <div class="navbar-brand col-md-12">
              <img src="/content/ladal_icon_cas_tran_white_trimed.png" class="navbar-icon" alt="LADAL"/>
              <span class="navbar-title-note navbar-collapse collapse" >Language Technology and Data Analysis Laboratory</span>
            </div>
        </div>
    </a>
  </div>
  
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <!-- SKC removed  navbar brand -->
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">HOME</a>
</li>
<li>
  <a href="about.html">ABOUT</a>
</li>
<li>
  <a href="events.html">EVENTS</a>
</li>
<li>
  <a href="tutorials.html">TUTORIALS</a>
</li>
<li>
  <a href="tools.html">TOOLS</a>
</li>
<li>
  <a href="resources.html">RESOURCES</a>
</li>
<li>
  <a href="contact.html">CONTACT</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">



<h1 class="title toc-ignore">Topic Modeling with R</h1>
<h4 class="author">Martin Schweinberger</h4>
<h4 class="date">2024-05-17</h4>

</div>


<p><img src="https://slcladal.github.io/images/uq1.jpg" width="100%" /></p>
<div id="introduction" class="section level1 unnumbered">
<h1 class="unnumbered">Introduction</h1>
<p>This tutorial introduces topic modeling using R.</p>
<p><img src="https://slcladal.github.io/images/gy_chili.jpg" width="15%" style="float:right; padding:10px" /></p>
<p>This tutorial is aimed at beginners and intermediate users of R with
the aim of showcasing how to perform basic topic modeling on textual
data using R and how to visualize the results of such a model. The aim
is not to provide a fully-fledged analysis but rather to show and
exemplify selected useful methods associated with topic modeling.</p>
<div class="warning"
style="padding:0.1em; background-color:rgba(215,209,204,.3); color:#51247a">
<span>
<p style="margin-top:1em; text-align:center">
To be able to follow this tutorial, we suggest you check out and
familiarize yourself with the content of the following <strong>R
Basics</strong> tutorials:<br>
</p>
<p style="margin-top:1em; text-align:left">
<ul>
<li>
<a href="https://ladal.edu.au/intror.html">Getting started with R</a>
</li>
<li>
<a href="https://ladal.edu.au/load.html">Loading, saving, and generating
data in R</a>
</li>
<li>
<a href="https://ladal.edu.au/string.html">String Processing in R</a>
</li>
<li>
<a href="https://ladal.edu.au/regex.html">Regular Expressions in R</a>
</li>
</ul>
</p>
<p style="margin-top:1em; text-align:center">
Click <a
href="https://ladal.edu.au/content/topic.Rmd"><strong>here</strong></a><a
href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> to
download the <strong>entire R Notebook</strong> for this
tutorial.<br><br> <a
href="https://mybinder.org/v2/gh/SLCLADAL/interactive-notebooks-environment/main?urlpath=git-pull%3Frepo%3Dhttps%253A%252F%252Fgithub.com%252FSLCLADAL%252Finteractive-notebooks%26urlpath%3Dlab%252Ftree%252Finteractive-notebooks%252Fnotebooks%252Ftm_cb.ipynb%26branch%3Dmain"><img
src="https://mybinder.org/badge_logo.svg" alt="Binder" /></a><br> Click
<a
href="https://mybinder.org/v2/gh/SLCLADAL/interactive-notebooks-environment/main?urlpath=git-pull%3Frepo%3Dhttps%253A%252F%252Fgithub.com%252FSLCLADAL%252Finteractive-notebooks%26urlpath%3Dlab%252Ftree%252Finteractive-notebooks%252Fnotebooks%252Ftm_cb.ipynb%26branch%3Dmain"><strong>here</strong></a>
to open a Jupyter notebook that allows you to follow this tutorial
interactively. This means that you can execute, change, and edit the
code used in this tutorial to help you better understand how the code
shown here works (make sure you run all code chunks in the order in
which they appear to avoid running into errors).
</p>
<br>
</p>
<p style="margin-left:1em;">
</p>
<p></span></p>
</div>
<p><br></p>
<div class="warning"
style="padding:0.1em; background-color:#51247a; color:#f2f2f2">
<span>
<p style="margin-top:1em; text-align:center">
<strong>TOPIC MODEL TOOL</strong>
</p>
<p style="margin-top:1em; text-align:center">
Click on this <a
href="https://binderhub.atap-binder.cloud.edu.au/v2/gh/SLCLADAL/interactive-notebooks-environment/main?urlpath=git-pull%3Frepo%3Dhttps%253A%252F%252Fgithub.com%252FSLCLADAL%252Finteractive-notebooks%26urlpath%3Dlab%252Ftree%252Finteractive-notebooks%252Fnotebooks%252Ftopictool.ipynb%26branch%3Dmain"><img
src="https://mybinder.org/badge_logo.svg" alt="Binder" /></a> badge to
open an notebook-based tool <br>that allows you to upload your own
text(s), perform topic modelling on them, and download the results.
</p>
<br>
<p style="margin-left:1em;">
</p>
<p></span></p>
</div>
<p><br></p>
<p>This tutorial builds heavily on and uses materials from <span
class="citation">(<a href="#ref-silge2017text">Silge and Robinson 2017,
chap. 6</a>)</span> (see <a
href="https://www.tidytextmining.com/topicmodeling">here</a>) and <a
href="https://tm4ss.github.io/docs/Tutorial_6_Topic_Models.html">this
tutorial</a> on topic modelling using R by Andreas Niekler and Gregor
Wiedemann <span class="citation">(see <a href="#ref-WN17">Wiedemann and
Niekler 2017</a>)</span>. <a
href="https://tm4ss.github.io/docs/index.html">The tutorial</a> by
Andreas Niekler and Gregor Wiedemann is more thorough, goes into more
detail than this tutorial, and covers many more very useful text mining
methods.</p>
<div class="warning"
style="padding:0.1em; background-color:#f2f2f2; color:#51247a">
<span>
<p style="margin-top:1em; text-align:center">
<strong>Topic models aim to find topics (which are operationalized as
bundles of correlating terms) in documents to see what the texts are
about.</strong>
</p>
<p style="margin-left:1em;">
</p>
<p></span></p>
</div>
<p><br></p>
<p>Topic models refers to a suit of methods employed to uncover latent
structures within a corpus of text. These models operate on the premise
of identifying abstract <em>topics</em> that recur across documents. In
essence, topic models sift through the textual data to discern recurring
patterns of word co-occurrence, revealing underlying semantic themes
<span class="citation">(<a href="#ref-busso2022operation">Busso et al.
2022</a>; <a href="#ref-blei2003latent">Blei, Ng, and Jordan
2003a</a>)</span>. This technique is particularly prevalent in text
mining, where it serves to unveil hidden semantic structures in large
volumes of textual data.</p>
<p>Conceptually, topics can be understood as clusters of co-occurring
terms, indicative of shared semantic domains within the text. The
underlying assumption is that if a document pertains to a specific
topic, words related to that topic will exhibit higher frequency
compared to documents addressing other subjects. For example, in
documents discussing dogs, terms like <em>dog</em> and <em>bone</em> are
likely to feature prominently, while in documents focusing on cats,
<em>cat</em> and <em>meow</em> would be more prevalent. Meanwhile,
ubiquitous terms such as <em>the</em> and <em>is</em> are expected to
occur with similar frequency across diverse topics, serving as noise
rather than indicative signals of topic specificity.</p>
<p>Various methods exist for determining topics within topic models. For
instance, <span class="citation">Gerlach, Peixoto, and Altmann (<a
href="#ref-gerlach2018network">2018</a>)</span> and <span
class="citation">Hyland et al. (<a
href="#ref-hyland2021multilayer">2021</a>)</span> advocate for an
approach grounded in stochastic block models. However, most applications
of topic models use Latent Dirichlet Allocation (LDA) <span
class="citation">(<a href="#ref-blei2003latent">Blei, Ng, and Jordan
2003a</a>)</span> or Structural Topic Modeling <span
class="citation">(<a href="#ref-roberts2016navigating">Roberts, Stewart,
and Tingley 2016</a>)</span>.</p>
<p>LDA, in particular, emerges as a widely embraced technique for
fitting topic models. It operates by treating each document as a blend
of topics and each topic as a blend of words. Consequently, documents
can exhibit content overlaps, akin to the fluidity observed in natural
language usage, rather than being strictly segregated into distinct
groups.</p>
<p><span class="citation">Gillings and Hardie (<a
href="#ref-gillings2022interpretation">2022</a>)</span> state that topic
modelling is based on the following key assumptions:</p>
<ul>
<li>The corpus comprises a substantial number of documents.<br />
</li>
<li>A topic is delineated as a set of words with varying probabilities
of occurrence across the documents.<br />
</li>
<li>Each document exhibits diverse degrees of association with multiple
topics.<br />
</li>
<li>The collection is structured by underlying topics, which are finite
in number, organizing the corpus.</li>
</ul>
<p>Given the availability of vast amounts of textual data, topic models
can help to organize and offer insights and assist in understanding
large collections of unstructured text and they are widely used in
natural language processing and computational text analytics. However,
the use of topic modelling in discourse studies has received criticism
<span class="citation">(<a href="#ref-brookes2019utility">Brookes and
McEnery 2019</a>)</span> due to the following issues:</p>
<ol style="list-style-type: decimal">
<li><p><strong>Thematic Coherence</strong>: While topic modeling can
group texts into <em>topics</em>, the degree of thematic coherence
varies. Some topics may be thematically coherent, but others may lack
cohesion or accuracy in capturing the underlying themes present in the
texts.</p></li>
<li><p><strong>Nuanced Perspective</strong>: Compared to more
traditional approaches to discourse analysis, topic modeling often
provides a less nuanced perspective on the data. The automatically
generated topics may overlook subtle nuances and intricacies present in
the texts, leading to a less accurate representation of the
discourse.</p></li>
<li><p><strong>Distance from Reality</strong>: <span
class="citation">Brookes and McEnery (<a
href="#ref-brookes2019utility">2019</a>)</span> suggest that the
insights derived from topic modeling may not fully capture the “reality”
of the texts. The topics generated by the model may not accurately
reflect the complex nature of the discourse, leading to potential
misinterpretations or oversimplifications of the data.</p></li>
<li><p><strong>Utility for Discourse Analysts</strong>: While topic
modeling may offer a method for organizing and studying sizable data
sets, <span class="citation">Brookes and McEnery (<a
href="#ref-brookes2019utility">2019</a>)</span> questions the utility
for discourse analysts and suggests that traditional discourse analysis
methods consistently provide a more nuanced and accurate perspective on
the data compared to topic modeling approaches.</p></li>
</ol>
<p>This criticism is certainly valid if topic modeling is solely reliant
on a purely data-driven approach without human intervention. In this
tutorial, we will demonstrate how to combine data-driven topic modeling
with human-supervised seeded methods to arrive at more reliable and
accurate topics.</p>
<p><strong>Preparation and session set up</strong></p>
<p>This tutorial is conducted within the R environment. If you’re new to
R or haven’t installed it yet, you can find an introduction to R and
further instructions on how to use it <a
href="https://ladal.edu.au/intror.html">here</a>. To ensure smooth
execution of the scripts provided in this tutorial, it’s necessary to
install specific packages from the R library. Before proceeding to the
code examples, please ensure you’ve installed these packages by running
the code provided below. If you’ve already installed the required
packages, feel free to skip ahead and disregard this section. To install
the necessary packages, simply execute the following code. Please note
that installation may take some time (usually between 1 and 5 minutes),
so there’s no need to be concerned if it takes a while.</p>
<pre class="r"><code># install packages
install.packages(&quot;dplyr&quot;)
install.packages(&quot;flextable&quot;)
install.packages(&quot;ggplot2&quot;)
install.packages(&quot;lda&quot;)
install.packages(&quot;ldatuning&quot;)
install.packages(&quot;quanteda&quot;)
install.packages(&quot;RColorBrewer&quot;)
install.packages(&quot;reshape2&quot;)
install.packages(&quot;slam&quot;)
install.packages(&quot;stringr&quot;)
install.packages(&quot;tidyr&quot;)
install.packages(&quot;tidytext&quot;)
install.packages(&quot;tm&quot;)
install.packages(&quot;topicmodels&quot;)
install.packages(&quot;wordcloud&quot;)
# install klippy for copy-to-clipboard button in code chunks
install.packages(&quot;remotes&quot;)
remotes::install_github(&quot;rlesur/klippy&quot;)</code></pre>
<p>Next, we activate the packages.</p>
<pre class="r"><code># set options
options(stringsAsFactors = F)         # no automatic data transformation
options(&quot;scipen&quot; = 100, &quot;digits&quot; = 4) # suppress math annotation
# load packages
library(dplyr)
library(flextable)
library(ggplot2)
library(lda)
library(ldatuning)
library(quanteda)
library(RColorBrewer)
library(reshape2)
library(slam)
library(stringr)
library(tidyr)
library(tidytext)
library(tm)
library(topicmodels)
library(wordcloud)
# activate klippy for copy-to-clipboard button
klippy::klippy()</code></pre>
<script>
  addClassKlippyTo("pre.r, pre.markdown");
  addKlippy('left', 'top', 'auto', '1', 'Copy code', 'Copied!');
</script>
<p>Once you have installed R and RStudio and once you have initiated the
session by executing the code shown above, you are good to go.</p>
</div>
<div id="topic-modelling" class="section level1 unnumbered">
<h1 class="unnumbered">Topic Modelling</h1>
<p>In this tutorial, we’ll explore a two-step approach to topic
modeling. Initially, we’ll employ an unsupervised method to generate a
preliminary topic model, uncovering inherent topics within the data.
Subsequently, we’ll introduce a human-supervised, seeded model, informed
by the outcomes of the initial data-driven approach. Following this
(recommended) procedure, we’ll then delve into an alternative purely
data-driven approach.</p>
<p>Our tutorial begins by gathering the necessary corpus data. We’ll be
focusing on analyzing the <em>State of the Union Addresses</em> (SOTU)
delivered by US presidents, with the aim of understanding how the
addressed topics have evolved over time. Given the length of these
addresses (amounting to 231 in total), it’s important to acknowledge
that document length can influence topic modeling outcomes. In cases
where texts are exceptionally short (like Twitter posts) or long (such
as books), adjusting the document units for modeling purposes can be
beneficial—either by combining or splitting them accordingly.</p>
<p>To tailor our approach to the SOTU speeches, we’ve chosen to model at
the paragraph level instead of analyzing entire speeches at once. This
allows for a more detailed analysis, potentially leading to clearer and
more interpretable topics. We’ve provided a data set named
<code>sotu_paragraphs.rda</code>, which contains the speeches segmented
into paragraphs for easier analysis.</p>
<div id="human-in-the-loop-topic-modelling"
class="section level2 unnumbered">
<h2 class="unnumbered">Human-in-the-loop Topic Modelling</h2>
<p>In this human-in-the-loop approach to topic modelling which mainly
uses and combines the <code>quanteda</code> package <span
class="citation">(<a href="#ref-benoit2018quanteda">Benoit et al.
2018</a>)</span>, the <code>topicmodels</code> package <span
class="citation">(<a href="#ref-topicmodels2024package">Grün and Hornik
2024</a>, <a href="#ref-topicmodels2011pub">2011</a>)</span>, and the
<code>seededlda</code> package <span class="citation">(<a
href="#ref-seededlda2024">Watanabe and Xuan-Hieu 2024</a>)</span>. Now
that we have cleaned the data, we can perform the topic modelling. This
consists of two steps:</p>
<ol style="list-style-type: decimal">
<li><p>First, we perform an unsupervised LDA. We do this to check what
topics are in our corpus.</p></li>
<li><p>Then, we perform a supervised LDA (based on the results of the
unsupervised LDA) to identify meaningful topics in our data. For the
supervised LDA, we define so-called <em>seed terms</em> that help in
generating coherent topics.</p></li>
</ol>
<div id="loading-and-preparing-data" class="section level3 unnumbered">
<h3 class="unnumbered">Loading and preparing data</h3>
<p>When preparing the data for analysis, we employ several preprocessing
steps to ensure its cleanliness and readiness for analysis. Initially,
we load the data and then remove punctuation, symbols, and numerical
characters. Additionally, we eliminate common stop words, such as
<em>the</em> and <em>and</em>, which can introduce noise and hinder the
topic modeling process. To standardize the text, we convert it to
lowercase and, lastly, we apply stemming to reduce words to their base
form.</p>
<pre class="r"><code># load data
txts &lt;- base::readRDS(url(&quot;https://slcladal.github.io/data/sotu_paragraphs.rda&quot;, &quot;rb&quot;)) 
txts$text %&gt;%
  # tokenise
  quanteda::tokens(remove_punct = TRUE,       # remove punctuation 
                   remove_symbols = TRUE,     # remove symbols 
                   remove_number = TRUE) %&gt;%  # remove numbers
  # remove stop words
  quanteda::tokens_select(pattern = stopwords(&quot;en&quot;), selection = &quot;remove&quot;) %&gt;%
  # stemming
  quanteda::tokens_wordstem() %&gt;%
  # convert to document-frequency matrix
  quanteda::dfm(tolower = T) -&gt; ctxts
# add docvars
docvars(ctxts, &quot;president&quot;) &lt;- txts$president
docvars(ctxts, &quot;date&quot;) &lt;- txts$date
docvars(ctxts, &quot;speechid&quot;) &lt;- txts$speech_doc_id
docvars(ctxts, &quot;docid&quot;) &lt;- txts$doc_id
# clean data
ctxts &lt;- dfm_subset(ctxts, ntoken(ctxts) &gt; 0)
# inspect data
ctxts[1:5, 1:5]</code></pre>
<pre><code>## Document-feature matrix of: 5 documents, 5 features (80.00% sparse) and 4 docvars.
##        features
## docs    fellow-citizen senat hous repres embrac
##   text1              1     1    1      1      0
##   text2              0     0    0      0      1
##   text3              0     0    0      0      0
##   text4              0     0    0      0      0
##   text5              0     0    0      0      0</code></pre>
</div>
<div id="initial-unsupervised-topic-model"
class="section level3 unnumbered">
<h3 class="unnumbered">Initial unsupervised topic model</h3>
<p>Now that we have loaded and prepared the data for analysis, we will
follow a two-step approach.</p>
<ol style="list-style-type: decimal">
<li><p>First, we perform an unsupervised topic model using Latent
Dirichlet Allocation (LDA) to identify the topics present in our data.
This initial step helps us understand the broad themes and structure
within the data set.</p></li>
<li><p>Then, based on the results of the unsupervised topic model, we
conduct a supervised topic model using LDA to refine and identify more
meaningful topics in our data.</p></li>
</ol>
<p>This combined approach allows us to leverage both data-driven
insights and expert supervision to enhance the accuracy and
interpretability of the topics.</p>
<p>In the initial step that implements a unsupervised, data-driven topic
model, we vary the number of topics the LDA algorithm looks for until we
identify coherent topics in the data. We use the <code>LDA</code>
function from the <code>topicmodels</code> package instead of the
<code>textmodel_lda</code> function from the <code>seededlda</code>
package because the former allows us to include a seed. Including a seed
ensures that the results of this unsupervised topic model are
reproducible, which is not the case if we do not seed the model, as each
model will produce different results without setting a seed.</p>
<pre class="r"><code># generate model: change k to different numbers, e.g. 10 or 20 and look for consistencies in the keywords for the topics below.
topicmodels::LDA(ctxts, k = 15, control = list(seed = 1234)) -&gt; ddlda</code></pre>
<p>Now that we have generated an initial data-driven model, the next
step is to inspect it to evaluate its performance and understand the
topics it has identified. To do this, we need to examine the terms
associated with each detected topic. By analyzing these terms, we can
gain insights into the themes represented by each topic and assess the
coherence and relevance of the model’s output.</p>
<pre class="r"><code># define number of topics
ntopics = 15
# define number of terms
nterms = 10
# generate table
tidytext::tidy(ddlda, matrix = &quot;beta&quot;) %&gt;%
  dplyr::group_by(topic) %&gt;%
  dplyr::slice_max(beta, n = nterms) %&gt;% 
  dplyr::ungroup() %&gt;%
  dplyr::arrange(topic, -beta) %&gt;%
  dplyr::mutate(term = paste(term, &quot; (&quot;, round(beta, 3), &quot;)&quot;, sep = &quot;&quot;),
                topic = paste(&quot;topic&quot;, topic),
                topic = factor(topic, levels = c(paste(&quot;topic&quot;, 1:ntopics))),
                top = rep(paste(&quot;top&quot;, 1:nterms), nrow(.)/nterms),
                top = factor(top, levels = c(paste(&quot;top&quot;, 1:nterms)))) %&gt;%
  dplyr::select(-beta) %&gt;%
  tidyr::spread(topic, term) -&gt; ddlda_top_terms
ddlda_top_terms</code></pre>
<pre><code>## # A tibble: 10 × 16
##    top    `topic 1`  `topic 2` `topic 3` `topic 4` `topic 5` `topic 6` `topic 7`
##    &lt;fct&gt;  &lt;chr&gt;      &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;    
##  1 top 1  state (0.… countri … state (0… govern (… state (0… state (0… state (0…
##  2 top 2  countri (… upon (0.… unite (0… will (0.… countri … will (0.… upon (0.…
##  3 top 3  will (0.0… present … congress… year (0.… will (0.… unite (0… will (0.…
##  4 top 4  congress … war (0.0… may (0.0… unite (0… congress… govern (… congress…
##  5 top 5  nation (0… can (0.0… treati (… law (0.0… public (… power (0… may (0.0…
##  6 top 6  can (0.00… unite (0… citizen … may (0.0… year (0.… law (0.0… govern (…
##  7 top 7  subject (… nation (… nation (… upon (0.… nation (… peopl (0… citizen …
##  8 top 8  govern (0… author (… great (0… act (0.0… can (0.0… last (0.… nation (…
##  9 top 9  land (0.0… may (0.0… territor… public (… law (0.0… duti (0.… import (…
## 10 top 10 made (0.0… subject … made (0.… last (0.… import (… part (0.… great (0…
## # ℹ 8 more variables: `topic 8` &lt;chr&gt;, `topic 9` &lt;chr&gt;, `topic 10` &lt;chr&gt;,
## #   `topic 11` &lt;chr&gt;, `topic 12` &lt;chr&gt;, `topic 13` &lt;chr&gt;, `topic 14` &lt;chr&gt;,
## #   `topic 15` &lt;chr&gt;</code></pre>
<p>In a real analysis, we would re-run the unsupervised model multiple
times, adjusting the number of topics that the Latent Dirichlet
Allocation (LDA) algorithm “looks for.” For each iteration, we would
inspect the key terms associated with the identified topics to check
their thematic consistency. This evaluation helps us determine whether
the results of the topic model make sense and accurately reflect the
themes present in the data. By varying the number of topics and
examining the corresponding key terms, we can identify the optimal
number of topics that best represent the underlying themes in our data
set. However, we will skip re-running the model here, as this is just a
tutorial intended to showcase the process rather than a comprehensive
analysis.</p>
<p>To obtain a comprehensive table of terms and their association
strengths with topics (the beta values), follow the steps outlined
below. This table can help verify if the data contains thematically
distinct topics. Additionally, visualizations and statistical modeling
can be employed to compare the distinctness of topics and determine the
ideal number of topics. However, I strongly recommend not solely relying
on statistical measures when identifying the optimal number of topics.
In my experience, human intuition is still essential for evaluating
topic coherence and consistency.</p>
<pre class="r"><code># extract topics
ddlda_topics &lt;- tidy(ddlda, matrix = &quot;beta&quot;)
# inspect
head(ddlda_topics, 20)</code></pre>
<pre><code>## # A tibble: 20 × 3
##    topic term                beta
##    &lt;int&gt; &lt;chr&gt;              &lt;dbl&gt;
##  1     1 fellow-citizen 0.000249 
##  2     2 fellow-citizen 0.000351 
##  3     3 fellow-citizen 0.000416 
##  4     4 fellow-citizen 0.0000333
##  5     5 fellow-citizen 0.0000797
##  6     6 fellow-citizen 0.000183 
##  7     7 fellow-citizen 0.000445 
##  8     8 fellow-citizen 0.000306 
##  9     9 fellow-citizen 0.000381 
## 10    10 fellow-citizen 0.000332 
## 11    11 fellow-citizen 0.000187 
## 12    12 fellow-citizen 0.000147 
## 13    13 fellow-citizen 0.000278 
## 14    14 fellow-citizen 0.000336 
## 15    15 fellow-citizen 0.000205 
## 16     1 senat          0.000708 
## 17     2 senat          0.000477 
## 18     3 senat          0.00263  
## 19     4 senat          0.00118  
## 20     5 senat          0.000436</code></pre>
<p>The purpose of this initial step, in which we generate data-driven
unsupervised topic models, is to identify the number of coherent topics
present in the data and to determine the key terms associated with these
topics. These key terms will then be used as seed terms in the next
step: the supervised, seeded topic model. This approach ensures that the
supervised model is grounded in the actual thematic structure of the
data set, enhancing the accuracy and relevance of the identified
topics.</p>
</div>
<div id="supervised-seeded-topic-model"
class="section level3 unnumbered">
<h3 class="unnumbered">Supervised, seeded topic model</h3>
<p>To implement the supervised, seeded topic model, we start by creating
a dictionary containing the seed terms we have identified in the first
step.</p>
<p>To check terms (to see if ), you can use the following code
chunk:</p>
<pre class="r"><code>ddlda_topics %&gt;%  select(term) %&gt;% unique() %&gt;% filter(str_detect(term, &quot;agri&quot;))</code></pre>
<pre><code>## # A tibble: 3 × 1
##   term               
##   &lt;chr&gt;              
## 1 agricultur         
## 2 agriculturist      
## 3 agricultural-colleg</code></pre>
<pre class="r"><code># semisupervised LDA
dict &lt;- dictionary(list(military = c(&quot;armi&quot;, &quot;war&quot;, &quot;militari&quot;, &quot;conflict&quot;),
                        liberty = c(&quot;freedom&quot;, &quot;liberti&quot;, &quot;free&quot;),
                        nation = c(&quot;nation&quot;, &quot;countri&quot;, &quot;citizen&quot;),
                        law = c(&quot;law&quot;, &quot;court&quot;, &quot;prison&quot;),
                        treaty = c(&quot;claim&quot;, &quot;treati&quot;, &quot;negoti&quot;),
                        indian = c(&quot;indian&quot;, &quot;tribe&quot;, &quot;territori&quot;),
                        labor = c(&quot;labor&quot;, &quot;work&quot;, &quot;condit&quot;),
                        money = c(&quot;bank&quot;, &quot;silver&quot;, &quot;gold&quot;, &quot;currenc&quot;, &quot;money&quot;),
                        finance = c(&quot;debt&quot;, &quot;invest&quot;, &quot;financ&quot;),
                        wealth = c(&quot;prosper&quot;, &quot;peac&quot;, &quot;wealth&quot;),
                        industry = c(&quot;produc&quot;, &quot;industri&quot;, &quot;manufactur&quot;),
                        navy = c(&quot;navi&quot;, &quot;ship&quot;, &quot;vessel&quot;, &quot;naval&quot;),
                        consitution = c(&quot;constitut&quot;, &quot;power&quot;, &quot;state&quot;),
                        agriculture = c(&quot;agricultur&quot;, &quot;grow&quot;, &quot;land&quot;),
                        office = c(&quot;office&quot;, &quot;serv&quot;, &quot;duti&quot;)))
tmod_slda &lt;- seededlda::textmodel_seededlda(ctxts, 
                                            dict, 
                                            residual = TRUE, 
                                            min_termfreq = 2)
# inspect
seededlda::terms(tmod_slda)</code></pre>
<pre><code>##       military   liberty   nation     law       treaty    indian     
##  [1,] &quot;war&quot;      &quot;free&quot;    &quot;countri&quot;  &quot;law&quot;     &quot;treati&quot;  &quot;territori&quot;
##  [2,] &quot;militari&quot; &quot;peopl&quot;   &quot;nation&quot;   &quot;court&quot;   &quot;claim&quot;   &quot;indian&quot;   
##  [3,] &quot;armi&quot;     &quot;can&quot;     &quot;citizen&quot;  &quot;case&quot;    &quot;govern&quot;  &quot;tribe&quot;    
##  [4,] &quot;forc&quot;     &quot;govern&quot;  &quot;govern&quot;   &quot;person&quot;  &quot;negoti&quot;  &quot;mexico&quot;   
##  [5,] &quot;offic&quot;    &quot;must&quot;    &quot;foreign&quot;  &quot;may&quot;     &quot;unite&quot;   &quot;part&quot;     
##  [6,] &quot;servic&quot;   &quot;upon&quot;    &quot;american&quot; &quot;provis&quot;  &quot;minist&quot;  &quot;new&quot;      
##  [7,] &quot;command&quot;  &quot;liberti&quot; &quot;upon&quot;     &quot;upon&quot;    &quot;relat&quot;   &quot;texa&quot;     
##  [8,] &quot;men&quot;      &quot;everi&quot;   &quot;right&quot;    &quot;offic&quot;   &quot;two&quot;     &quot;will&quot;     
##  [9,] &quot;conflict&quot; &quot;public&quot;  &quot;time&quot;     &quot;execut&quot;  &quot;convent&quot; &quot;govern&quot;   
## [10,] &quot;order&quot;    &quot;will&quot;    &quot;properti&quot; &quot;subject&quot; &quot;britain&quot; &quot;made&quot;     
##       labor       money      finance      wealth     industry     navy       
##  [1,] &quot;condit&quot;    &quot;bank&quot;     &quot;year&quot;       &quot;peac&quot;     &quot;produc&quot;     &quot;vessel&quot;   
##  [2,] &quot;work&quot;      &quot;money&quot;    &quot;debt&quot;       &quot;prosper&quot;  &quot;industri&quot;   &quot;navi&quot;     
##  [3,] &quot;labor&quot;     &quot;gold&quot;     &quot;amount&quot;     &quot;will&quot;     &quot;manufactur&quot; &quot;ship&quot;     
##  [4,] &quot;report&quot;    &quot;currenc&quot;  &quot;expenditur&quot; &quot;us&quot;       &quot;product&quot;    &quot;naval&quot;    
##  [5,] &quot;depart&quot;    &quot;silver&quot;   &quot;treasuri&quot;   &quot;peopl&quot;    &quot;import&quot;     &quot;port&quot;     
##  [6,] &quot;congress&quot;  &quot;govern&quot;   &quot;increas&quot;    &quot;great&quot;    &quot;foreign&quot;    &quot;coast&quot;    
##  [7,] &quot;secretari&quot; &quot;treasuri&quot; &quot;fiscal&quot;     &quot;everi&quot;    &quot;trade&quot;      &quot;construct&quot;
##  [8,] &quot;recommend&quot; &quot;note&quot;     &quot;last&quot;       &quot;interest&quot; &quot;increas&quot;    &quot;sea&quot;      
##  [9,] &quot;servic&quot;    &quot;issu&quot;     &quot;revenu&quot;     &quot;happi&quot;    &quot;revenu&quot;     &quot;commerc&quot;  
## [10,] &quot;attent&quot;    &quot;public&quot;   &quot;estim&quot;      &quot;world&quot;    &quot;articl&quot;     &quot;great&quot;    
##       consitution agriculture  office     other     
##  [1,] &quot;state&quot;     &quot;land&quot;       &quot;duti&quot;     &quot;congress&quot;
##  [2,] &quot;power&quot;     &quot;agricultur&quot; &quot;will&quot;     &quot;act&quot;     
##  [3,] &quot;constitut&quot; &quot;public&quot;     &quot;may&quot;      &quot;last&quot;    
##  [4,] &quot;unite&quot;     &quot;grow&quot;       &quot;can&quot;      &quot;repres&quot;  
##  [5,] &quot;govern&quot;    &quot;improv&quot;     &quot;subject&quot;  &quot;session&quot; 
##  [6,] &quot;right&quot;     &quot;larg&quot;       &quot;congress&quot; &quot;presid&quot;  
##  [7,] &quot;union&quot;     &quot;year&quot;       &quot;consider&quot; &quot;senat&quot;   
##  [8,] &quot;shall&quot;     &quot;reserv&quot;     &quot;measur&quot;   &quot;hous&quot;    
##  [9,] &quot;one&quot;       &quot;now&quot;        &quot;shall&quot;    &quot;upon&quot;    
## [10,] &quot;act&quot;       &quot;acr&quot;        &quot;object&quot;   &quot;author&quot;</code></pre>
<p>Now, we extract files and create a data frame of topics and
documents. This shows what topic is dominant in which file in tabular
form.</p>
<pre class="r"><code># generate data frame
data.frame(tmod_slda$data$date, tmod_slda$data$president, seededlda::topics(tmod_slda)) %&gt;%
  dplyr::rename(Date = 1,
                President = 2,
                Topic = 3) %&gt;%
  dplyr::mutate(Date = stringr::str_remove_all(Date, &quot;-.*&quot;),
                Date = stringr::str_replace_all(Date, &quot;.$&quot;, &quot;0&quot;)) %&gt;%
  dplyr::mutate_if(is.character, factor) -&gt; topic_df
# inspect
head(topic_df)</code></pre>
<pre><code>##       Date         President  Topic
## text1 1790 George Washington  other
## text2 1790 George Washington wealth
## text3 1790 George Washington office
## text4 1790 George Washington wealth
## text5 1790 George Washington office
## text6 1790 George Washington office</code></pre>
<p>Using the table (or data frame) we have just created, we can
visualize the use of topics over time.</p>
<pre class="r"><code>topic_df %&gt;%
  dplyr::group_by(Date, Topic) %&gt;%
  dplyr::summarise(freq = n()) %&gt;%
  ggplot(aes(x = Date, y = freq, fill = Topic)) +
  geom_bar(stat=&quot;identity&quot;, position=&quot;fill&quot;, color = &quot;black&quot;) + 
  theme_bw() +
  labs(x = &quot;Decade&quot;) +
  scale_fill_manual(values = rev(colorRampPalette(brewer.pal(8, &quot;RdBu&quot;))(ntopics+1))) +
  scale_y_continuous(name =&quot;Percent of paragraphs&quot;, labels = seq(0, 100, 25))</code></pre>
<pre><code>## `summarise()` has grouped output by &#39;Date&#39;. You can override using the
## `.groups` argument.</code></pre>
<p><img src="topic_files/figure-html/vis-1.png" width="672" /></p>
<p>The figure illustrates the relative frequency of topics over time in
the State of the Union (SOTU) texts. Notably, paragraphs discussing the
topic of “office,” characterized by key terms such as <em>office</em>,
<em>serv</em>, and <em>duti</em>, have become less prominent over time.
This trend suggests a decreasing emphasis on this particular theme, as
evidenced by the diminishing number of paragraphs dedicated to it.</p>
</div>
</div>
<div id="data-driven-topic-modelling" class="section level2 unnumbered">
<h2 class="unnumbered">Data-driven Topic Modelling</h2>
<p>In this part of the tutorial, we show an alternative approaches for
performing data-driven topic modelling using LDA.</p>
<div id="loading-and-preparing-data-1"
class="section level3 unnumbered">
<h3 class="unnumbered">Loading and preparing data</h3>
<p>When readying the data for analysis, we follow consistent
pre-processing steps, employing the <code>tm</code> package <span
class="citation">(<a href="#ref-tm2024package">Feinerer and Hornik
2024</a>; <a href="#ref-tm2008pub">Feinerer, Hornik, and Meyer
2008</a>)</span> for efficient data preparation and cleaning. First, we
load the data and convert it into a corpus object. Next, we convert the
text to lowercase, eliminating superfluous white spaces, and removing
stop words. Subsequently, we proceed to strip the data of punctuation,
symbols, and numerical characters. Finally, we apply stemming to
standardize words to their base form, ensuring uniformity throughout the
data set.</p>
<pre class="r"><code># load data
textdata &lt;- base::readRDS(url(&quot;https://slcladal.github.io/data/sotu_paragraphs.rda&quot;, &quot;rb&quot;))
# create corpus object
tm::Corpus(DataframeSource(textdata)) %&gt;%
  # convert to lower case
  tm::tm_map(content_transformer(tolower))  %&gt;%
  # remove superfluous white spaces
  tm::tm_map(stripWhitespace)  %&gt;%
  # remove stop words
  tm::tm_map(removeWords, quanteda::stopwords()) %&gt;% 
  # remove punctuation
  tm::tm_map(removePunctuation, preserve_intra_word_dashes = TRUE) %&gt;%
  # remove numbers
  tm::tm_map(removeNumbers) %&gt;% 
  # stemming
  tm::tm_map(stemDocument, language = &quot;en&quot;) -&gt; textcorpus
# inspect data
str(textcorpus)</code></pre>
<pre><code>## Classes &#39;SimpleCorpus&#39;, &#39;Corpus&#39;  hidden list of 3
##  $ content: Named chr [1:8833] &quot;fellow-citizen senat hous repres&quot; &quot;embrac great satisfact opportun now present congratul present favor prospect public affair recent access import&quot;| __truncated__ &quot;resum consult general good can deriv encourag reflect measur last session satisfactori constitu novelti difficu&quot;| __truncated__ &quot;among mani interest object engag attent provid common defens merit particular regard prepar war one effectu mean preserv peac&quot; ...
##   ..- attr(*, &quot;names&quot;)= chr [1:8833] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ...
##  $ meta   :List of 1
##   ..$ language: chr &quot;en&quot;
##   ..- attr(*, &quot;class&quot;)= chr &quot;CorpusMeta&quot;
##  $ dmeta  :&#39;data.frame&#39;: 8833 obs. of  4 variables:
##   ..$ speech_doc_id: int [1:8833] 1 1 1 1 1 1 1 1 1 1 ...
##   ..$ speech_type  : Factor w/ 1 level &quot;State of the Union Address&quot;: 1 1 1 1 1 1 1 1 1 1 ...
##   ..$ president    : Factor w/ 23 levels &quot;Abraham Lincoln&quot;,..: 7 7 7 7 7 7 7 7 7 7 ...
##   ..$ date         : chr [1:8833] &quot;1790-01-08&quot; &quot;1790-01-08&quot; &quot;1790-01-08&quot; &quot;1790-01-08&quot; ...</code></pre>
</div>
<div id="model-calculation" class="section level3 unnumbered">
<h3 class="unnumbered">Model calculation</h3>
<p>Here’s the improved and expanded version of the paragraph:</p>
<p>After preprocessing, we have a clean corpus object called
<code>textcorpus</code>, which we use to calculate the unsupervised
Latent Dirichlet Allocation (LDA) topic model <span class="citation">(<a
href="#ref-blei2003lda">Blei, Ng, and Jordan 2003b</a>)</span>. To
perform this calculation, we first create a Document-Term Matrix (DTM)
from the <code>textcorpus</code>. In this step, we ensure that only
terms with a certain minimum frequency in the corpus are included (we
set the minimum frequency to 5). This selection process not only speeds
up the model calculation but also helps improve the model’s accuracy by
focusing on more relevant and frequently occurring terms. By filtering
out less common terms, we reduce noise and enhance the coherence of the
topics identified by the LDA model.</p>
<pre class="r"><code># compute document term matrix with terms &gt;= minimumFrequency
minimumFrequency &lt;- 5
DTM &lt;- tm::DocumentTermMatrix(textcorpus, 
                              control = list(bounds = list(global = c(minimumFrequency, Inf))))
# inspect the number of documents and terms in the DTM
dim(DTM)</code></pre>
<pre><code>## [1] 8833 4472</code></pre>
<p>Due to vocabulary pruning, some rows in our Document-Term Matrix
(DTM) may end up being empty. Latent Dirichlet Allocation (LDA) cannot
handle empty rows, so we must remove these documents from both the DTM
and the corresponding metadata. This step ensures that the topic
modeling process runs smoothly without encountering errors caused by
empty documents. Additionally, removing these empty rows helps maintain
the integrity of our analysis by focusing only on documents that contain
meaningful content.</p>
<pre class="r"><code>sel_idx &lt;- slam::row_sums(DTM) &gt; 0
DTM &lt;- DTM[sel_idx, ]
textdata &lt;- textdata[sel_idx, ]
# inspect the number of documents and terms in the DTM
dim(DTM)</code></pre>
<pre><code>## [1] 8811 4472</code></pre>
<p>The output shows that we have removed 22 documents (8833 - 8811) from
the DTM.</p>
<p>As an unsupervised machine learning method, topic models are
well-suited for exploring data. The primary goal of calculating topic
models is to determine the proportionate composition of a fixed number
of topics within the documents of a collection. Experimenting with
different parameters is essential to identify the most suitable settings
for your analysis needs.</p>
<p>For parameterized models such as Latent Dirichlet Allocation (LDA),
the number of topics <code>K</code> is the most critical parameter to
define in advance. Selecting the optimal <code>K</code> depends on
various factors. If <code>K</code> is too small, the collection is
divided into a few very general semantic contexts. Conversely, if
<code>K</code> is too large, the collection is divided into too many
topics, leading to overlaps and some topics being barely interpretable.
Finding the right balance is key to achieving meaningful and coherent
topics in your analysis.</p>
<p>An alternative to deciding on a set number of topics is to extract
parameters form a models using a rage of number of topics. This approach
can be useful when the number of topics is not theoretically motivated
or based on closer, qualitative inspection of the data. In the example
below, the determination of the optimal number of topics follows <span
class="citation">Murzintcev (<a
href="#ref-murzintcev2020idealtopics">n.d.</a>)</span>, but we only use
two metrics (<code>CaoJuan2009</code> and <code>Deveaud2014</code>) - it
is highly recommendable to inspect the results of the four metrics
available for the <code>FindTopicsNumber</code> function which are
<code>Griffiths2004</code> <span class="citation">(see <a
href="#ref-griffiths2004integrating">Griffiths et al. 2004</a>)</span>,
<code>CaoJuan2009</code> <span class="citation">(see <a
href="#ref-cao2009density">Cao et al. 2009</a>)</span>,
<code>Arun2010</code> <span class="citation">(see <a
href="#ref-arun2010finding">Arun et al. 2010</a>)</span>, and
<code>Deveaud2014</code> <span class="citation">(see <a
href="#ref-deveaud2014accurate">Deveaud, SanJuan, and Bellot
2014</a>)</span>.</p>
<pre class="r"><code># create models with different number of topics
result &lt;- ldatuning::FindTopicsNumber(
  DTM,
  topics = seq(from = 2, to = 20, by = 1),
  metrics = c(&quot;CaoJuan2009&quot;,  &quot;Deveaud2014&quot;),
  method = &quot;Gibbs&quot;,
  control = list(seed = 77),
  verbose = TRUE
)</code></pre>
<pre><code>## fit models... done.
## calculate metrics:
##   CaoJuan2009... done.
##   Deveaud2014... done.</code></pre>
<p>We can now plot the results. In this case, we have only use two
methods <code>CaoJuan2009</code> and <code>Griffith2004</code>. The best
number of topics shows low values for <code>CaoJuan2009</code> and high
values for <code>Griffith2004</code> (optimally, several methods should
converge and show peaks and dips respectively for a certain number of
topics).</p>
<pre class="r"><code>FindTopicsNumber_plot(result)</code></pre>
<p><img src="topic_files/figure-html/tm3c-1.png" width="672" /></p>
<p>For our first analysis, however, we choose a thematic “resolution” of
<code>K = 20</code> topics. In contrast to a resolution of 100 or more,
this number of topics can be evaluated qualitatively very easy.</p>
<pre class="r"><code># number of topics
K &lt;- 20
# set random number generator seed
set.seed(9161)
# compute the LDA model, inference via 1000 iterations of Gibbs sampling
topicModel &lt;- topicmodels::LDA(DTM, K, method=&quot;Gibbs&quot;, control=list(iter = 500, verbose = 25))</code></pre>
<pre><code>## K = 20; V = 4472; M = 8811
## Sampling 500 iterations!
## Iteration 25 ...
## Iteration 50 ...
## Iteration 75 ...
## Iteration 100 ...
## Iteration 125 ...
## Iteration 150 ...
## Iteration 175 ...
## Iteration 200 ...
## Iteration 225 ...
## Iteration 250 ...
## Iteration 275 ...
## Iteration 300 ...
## Iteration 325 ...
## Iteration 350 ...
## Iteration 375 ...
## Iteration 400 ...
## Iteration 425 ...
## Iteration 450 ...
## Iteration 475 ...
## Iteration 500 ...
## Gibbs sampling completed!</code></pre>
<pre class="r"><code># save results
tmResult &lt;- posterior(topicModel)
# save theta values
theta &lt;- tmResult$topics
# save beta values
beta &lt;- tmResult$terms
# reset topic names
topicNames &lt;- apply(terms(topicModel, 5), 2, paste, collapse = &quot; &quot;)</code></pre>
<p>Depending on the size of the vocabulary, the collection size and the
number K, the inference of topic models can take a very long time. This
calculation may take several minutes. If it takes too long, reduce the
vocabulary in the DTM by increasing the minimum frequency in the
previous step.</p>
<p>Let’s take a look at the 10 most likely terms within the term
probabilities <code>beta</code> of the inferred topics.</p>
<pre class="r"><code># create a data frame from the topic model data
tidytext::tidy(topicModel, matrix = &quot;beta&quot;) %&gt;% 
  # ensure topics are factors with specific levels
  dplyr::mutate(topic = paste0(&quot;topic&quot;, as.character(topic)),
                topic = factor(topic, levels = paste0(&quot;topic&quot;, 1:20))) %&gt;%
  # group the data by topic
  dplyr::group_by(topic) %&gt;%
  # arrange terms within each topic by beta value (ascending)
  dplyr::arrange(topic, -beta) %&gt;% 
  # select the top 10 terms with the highest beta values for each topic
  dplyr::top_n(10) %&gt;%
  # add beta to term
  dplyr::mutate(term = paste0(term, &quot; (&quot;, round(beta, 3), &quot;)&quot;)) %&gt;%
  # remove the beta column as it is now part of the term string
  dplyr::select(-beta) %&gt;%  
  # ungroup the data frame
  dplyr::ungroup() %&gt;%
  # create an id column for each term&#39;s position within the topic
  dplyr::mutate(id = rep(1:10, 20)) %&gt;%  
  # pivot the data to a wider format with topics as columns
  tidyr::pivot_wider(names_from = topic, values_from = term) -&gt; topterms  </code></pre>
<pre><code>## Selecting by beta</code></pre>
<pre class="r"><code># inspect
topterms</code></pre>
<pre><code>## # A tibble: 10 × 21
##       id topic1  topic2 topic3 topic4 topic5 topic6 topic7 topic8 topic9 topic10
##    &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;  
##  1     1 war (0… natio… gover… state… act (… may (… congr… new (… state… year (…
##  2     2 forc (… count… power… const… congr… upon … subje… great… terri… amount…
##  3     3 milita… peopl… natio… power… last … duti … recom… line … india… expend…
##  4     4 navi (… everi… indep… repre… made … shall… consi… const… mexic… increa…
##  5     5 armi (… prosp… princ… union… sessi… law (… atten… pacif… unit … treasu…
##  6     6 men (0… great… right… gover… autho… time … legis… estab… part … end (0…
##  7     7 offic … insti… polic… peopl… provi… requi… impor… compl… tribe… estim …
##  8     8 comman… happi… war (… hous … day (… prope… upon … coast… withi… revenu…
##  9     9 naval … honor… maint… exerc… first… neces… sugge… commu… exten… fiscal…
## 10    10 servic… gener… inter… gener… effec… execu… prese… impor… texa … sum (0…
## # ℹ 10 more variables: topic11 &lt;chr&gt;, topic12 &lt;chr&gt;, topic13 &lt;chr&gt;,
## #   topic14 &lt;chr&gt;, topic15 &lt;chr&gt;, topic16 &lt;chr&gt;, topic17 &lt;chr&gt;, topic18 &lt;chr&gt;,
## #   topic19 &lt;chr&gt;, topic20 &lt;chr&gt;</code></pre>
<p>For the next steps, we want to give the topics more descriptive names
than just numbers. Therefore, we simply concatenate the five most likely
terms of each topic to a string that represents a pseudo-name for each
topic.</p>
<pre class="r"><code>topicNames &lt;- apply(terms(topicModel, 5), 2, paste, collapse = &quot; &quot;)
# inspect first 3 topic names
topicNames[1:3]</code></pre>
<pre><code>##                                 Topic 1                                 Topic 2 
##           &quot;war forc militari navi armi&quot;    &quot;nation countri peopl everi prosper&quot; 
##                                 Topic 3 
## &quot;govern power nation independ principl&quot;</code></pre>
</div>
<div id="visualization-of-words-and-topics"
class="section level3 unnumbered">
<h3 class="unnumbered">Visualization of Words and Topics</h3>
<p>Although wordclouds may not be optimal for scientific purposes they
can provide a quick visual overview of a set of terms. Let’s look at
some topics as wordcloud.</p>
<p>In the following code, you can change the variable
<strong>topicToViz</strong> with values between 1 and 20 to display
other topics.</p>
<pre class="r"><code># visualize topics as word cloud
# choose topic of interest by a term contained in its name
topicToViz &lt;- grep(&#39;mexico&#39;, topicNames)[1] 
# select to 50 most probable terms from the topic by sorting the term-topic-probability vector in decreasing order
top50terms &lt;- sort(tmResult$terms[topicToViz,], decreasing=TRUE)[1:50]
words &lt;- names(top50terms)
# extract the probabilities of each of the 50 terms
probabilities &lt;- sort(tmResult$terms[topicToViz,], decreasing=TRUE)[1:50]
# visualize the terms as wordcloud
mycolors &lt;- brewer.pal(8, &quot;Dark2&quot;)
wordcloud(words, probabilities, random.order = FALSE, color = mycolors)</code></pre>
<p><img src="topic_files/figure-html/wordcloud-1.png" width="384" style="display: block; margin: auto;" /></p>
<p>Let us now look more closely at the distribution of topics within
individual documents. To this end, we visualize the distribution in 3
sample documents.</p>
<p>Let us first take a look at the contents of three sample
documents:</p>
<pre class="r"><code>exampleIds &lt;- c(2, 100, 200)
# first 400 characters of file 2
stringr::str_sub(txts$text[2], 1, 400)</code></pre>
<pre><code>## [1] &quot;I embrace with great satisfaction the opportunity which now presents itself\nof congratulating you on the present favorable prospects of our public\naffairs. The recent accession of the important state of North Carolina to\nthe Constitution of the United States (of which official information has\nbeen received), the rising credit and respectability of our country, the\ngeneral and increasing good will &quot;</code></pre>
<pre class="r"><code># first 400 characters of file 100
stringr::str_sub(txts$text[100], 1, 400)</code></pre>
<pre><code>## [1] &quot;Provision is likewise requisite for the reimbursement of the loan which has\nbeen made of the Bank of the United States, pursuant to the eleventh\nsection of the act by which it is incorporated. In fulfilling the public\nstipulations in this particular it is expected a valuable saving will be\nmade.&quot;</code></pre>
<pre class="r"><code># first 400 characters of file 200
stringr::str_sub(txts$text[200], 1, 400)</code></pre>
<pre><code>## [1] &quot;After many delays and disappointments arising out of the European war, the\nfinal arrangements for fulfilling the engagements made to the Dey and\nRegency of Algiers will in all present appearance be crowned with success,\nbut under great, though inevitable, disadvantages in the pecuniary\ntransactions occasioned by that war, which will render further provision\nnecessary. The actual liberation of all &quot;</code></pre>
<p>After looking into the documents, we visualize the topic
distributions within the documents.</p>
<pre class="r"><code>N &lt;- length(exampleIds)  # Number of example documents

# Get topic proportions from example documents
topicProportionExamples &lt;- theta[exampleIds,]
colnames(topicProportionExamples) &lt;- topicNames

# Reshape data for visualization
reshape2::melt(cbind(data.frame(topicProportionExamples), 
                                     document = factor(1:N)),
                               variable.name = &quot;topic&quot;, 
                               id.vars = &quot;document&quot;) %&gt;%  
  # create bar plot using ggplot2
  ggplot(aes(topic, value, fill = document), ylab = &quot;Proportion&quot;) +
  # plot bars
  geom_bar(stat=&quot;identity&quot;) +  
  # rotate x-axis labels
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) + 
  # flip coordinates to create horizontal bar plot
  coord_flip() +  
  # facet by document
  facet_wrap(~ document, ncol = N)  </code></pre>
<p><img src="topic_files/figure-html/vis2-1.png" width="960" style="display: block; margin: auto;" /></p>
</div>
<div id="topic-distributions" class="section level3 unnumbered">
<h3 class="unnumbered">Topic distributions</h3>
<p>The figure above illustrates how topics are distributed within a
document according to the model. In the current model, all three
documents exhibit at least a small percentage of each topic.</p>
<p>The topic distribution within a document can be controlled using the
<em>alpha</em> parameter of the model. Higher alpha priors result in an
even distribution of topics within a document, while lower alpha priors
ensure that the inference process concentrates the probability mass on a
few topics for each document.</p>
<p>In the previous model calculation, the alpha prior was automatically
estimated to fit the data, achieving the highest overall probability for
the model. However, this automatic estimate may not align with the
results that an analyst desires. Depending on our analysis goals, we
might prefer a more concentrated (peaky) or more evenly distributed set
of topics in the model.</p>
<p>Next, let us change the alpha prior to a lower value to observe how
this adjustment affects the topic distributions in the model. To do
this, we first extarct the alpha value of teh previous model.</p>
<pre class="r"><code># see alpha from previous model
attr(topicModel, &quot;alpha&quot;) </code></pre>
<pre><code>## [1] 2.5</code></pre>
<p>The alpha value of the previous model was
<code>attr(topicModel, "alpha")</code>. So now, we set a much lower
value (0.2) when we generate a new model.</p>
<pre class="r"><code># generate new LDA model with low alpha
topicModel2 &lt;- LDA(DTM, K, method=&quot;Gibbs&quot;, 
                   control=list(iter = 500, verbose = 25, alpha = 0.2))</code></pre>
<pre><code>## K = 20; V = 4472; M = 8811
## Sampling 500 iterations!
## Iteration 25 ...
## Iteration 50 ...
## Iteration 75 ...
## Iteration 100 ...
## Iteration 125 ...
## Iteration 150 ...
## Iteration 175 ...
## Iteration 200 ...
## Iteration 225 ...
## Iteration 250 ...
## Iteration 275 ...
## Iteration 300 ...
## Iteration 325 ...
## Iteration 350 ...
## Iteration 375 ...
## Iteration 400 ...
## Iteration 425 ...
## Iteration 450 ...
## Iteration 475 ...
## Iteration 500 ...
## Gibbs sampling completed!</code></pre>
<pre class="r"><code># save results
tmResult &lt;- posterior(topicModel2)
# save theta values
theta &lt;- tmResult$topics
# save beta values
beta &lt;- tmResult$terms
# reset topic names
topicNames &lt;- apply(terms(topicModel, 5), 2, paste, collapse = &quot; &quot;)</code></pre>
<pre class="r"><code>topicNames &lt;- apply(terms(topicModel, 5), 2, paste, collapse = &quot; &quot;)
topicNames</code></pre>
<pre><code>##                                      Topic 1 
##                &quot;war forc militari navi armi&quot; 
##                                      Topic 2 
##         &quot;nation countri peopl everi prosper&quot; 
##                                      Topic 3 
##      &quot;govern power nation independ principl&quot; 
##                                      Topic 4 
##         &quot;state constitut power repres union&quot; 
##                                      Topic 5 
##             &quot;act congress last made session&quot; 
##                                      Topic 6 
##                    &quot;may upon duti shall law&quot; 
##                                      Topic 7 
## &quot;congress subject recommend consider attent&quot; 
##                                      Topic 8 
##             &quot;new great line construct pacif&quot; 
##                                      Topic 9 
##         &quot;state territori indian mexico unit&quot; 
##                                     Topic 10 
##    &quot;year amount expenditur increas treasuri&quot; 
##                                     Topic 11 
##        &quot;import duti countri increas product&quot; 
##                                     Topic 12 
##                  &quot;land public work made use&quot; 
##                                     Topic 13 
##       &quot;depart report offic servic secretari&quot; 
##                                     Topic 14 
##        &quot;relat govern continu countri friend&quot; 
##                                     Topic 15 
##               &quot;object can great may without&quot; 
##                                     Topic 16 
##             &quot;citizen law case govern person&quot; 
##                                     Topic 17 
##                &quot;can must peopl everi condit&quot; 
##                                     Topic 18 
##              &quot;bank govern public money issu&quot; 
##                                     Topic 19 
##             &quot;govern treati unit state claim&quot; 
##                                     Topic 20 
##         &quot;state unit american commerc vessel&quot;</code></pre>
<p>Now visualize the topic distributions in the three documents again.
What are the differences in the distribution structure?</p>
<pre class="r"><code># get topic proportions form example documents
topicProportionExamples &lt;- theta[exampleIds,]
colnames(topicProportionExamples) &lt;- topicNames
vizDataFrame &lt;- reshape2::melt(cbind(data.frame(topicProportionExamples),
                                     document = factor(1:N)), 
                               variable.name = &quot;topic&quot;, 
                               id.vars = &quot;document&quot;) 
# plot alpha distribution 
ggplot(data = vizDataFrame, aes(topic, value, fill = document), ylab = &quot;proportion&quot;) + 
  geom_bar(stat=&quot;identity&quot;) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +  
  coord_flip() +
  facet_wrap(~ document, ncol = N)</code></pre>
<p><img src="topic_files/figure-html/vis3-1.png" width="960" style="display: block; margin: auto;" /></p>
<p>The figure above now shows that the documents are more clearly
assigned to specific topics. The difference in the probability of a
document belonging to a particular topic is much more distinct,
indicating a stronger association between documents and their respective
dominant topics.</p>
<p>By adjusting the alpha parameter to a lower value, we have
concentrated the probability mass on fewer topics for each document.
This change makes the topic distribution within documents less even and
more peaked, resulting in documents being more distinctly associated
with specific topics.</p>
<p>This adjustment can be particularly useful when analyzing data sets
where we expect documents to focus on a few key themes rather than
covering a broad range of topics. It allows for a clearer interpretation
of the primary topics discussed in each document, enhancing the overall
clarity and interpretability of the topic model.</p>
</div>
<div id="topic-ranking" class="section level3 unnumbered">
<h3 class="unnumbered">Topic ranking</h3>
<p>Determining the defining topics within a collection is a crucial step
in topic modeling, as it helps to organize and interpret the underlying
themes effectively. There are several approaches to uncover these topics
and arrange them in a meaningful order. Here, we present two different
methods: <strong>Ordering Topics by Probability</strong> and
<strong>Counting Primary Topic Appearances</strong>. These two
approaches complement each other and, when used together, can provide a
comprehensive understanding of the defining topics within a collection.
By combining the probabilistic ranking with the frequency count of
primary topics, we can achieve a more nuanced and accurate
interpretation of the underlying themes in the data.</p>
<div id="approach-1-ordering-topics-by-probability"
class="section level4 unnumbered">
<h4 class="unnumbered">Approach 1: Ordering Topics by Probability</h4>
<p>This approach involves ranking topics based on their overall
probability within the given collection. By examining the distribution
of words across topics and documents, we can identify which topics are
more dominant and relevant. This method helps to highlight the most
significant themes within the data.</p>
<pre class="r"><code># mean probabilities over all paragraphs
topicProportions &lt;- colSums(theta) / nDocs(DTM)  
# assign the topic names we created before
names(topicProportions) &lt;- topicNames     
# show summed proportions in decreased order
soP &lt;- sort(topicProportions, decreasing = TRUE)
# inspect ordering
paste(round(soP, 5), &quot;:&quot;, names(soP))</code></pre>
<pre><code>##  [1] &quot;0.06721 : congress subject recommend consider attent&quot;
##  [2] &quot;0.06448 : can must peopl everi condit&quot;               
##  [3] &quot;0.06313 : year amount expenditur increas treasuri&quot;   
##  [4] &quot;0.06098 : land public work made use&quot;                 
##  [5] &quot;0.06076 : may upon duti shall law&quot;                   
##  [6] &quot;0.06072 : bank govern public money issu&quot;             
##  [7] &quot;0.05322 : relat govern continu countri friend&quot;       
##  [8] &quot;0.05276 : import duti countri increas product&quot;       
##  [9] &quot;0.05151 : citizen law case govern person&quot;            
## [10] &quot;0.05042 : state constitut power repres union&quot;        
## [11] &quot;0.04691 : state unit american commerc vessel&quot;        
## [12] &quot;0.04573 : state territori indian mexico unit&quot;        
## [13] &quot;0.04453 : war forc militari navi armi&quot;               
## [14] &quot;0.04368 : new great line construct pacif&quot;            
## [15] &quot;0.04282 : act congress last made session&quot;            
## [16] &quot;0.04263 : object can great may without&quot;              
## [17] &quot;0.04099 : govern treati unit state claim&quot;            
## [18] &quot;0.03731 : nation countri peopl everi prosper&quot;        
## [19] &quot;0.03646 : depart report offic servic secretari&quot;      
## [20] &quot;0.03374 : govern power nation independ principl&quot;</code></pre>
<p>We recognize some topics that are way more likely to occur in the
corpus than others. These describe rather general thematic coherence.
Other topics correspond more to specific contents.</p>
</div>
<div id="approach-2-counting-primary-topic-appearances"
class="section level4 unnumbered">
<h4 class="unnumbered">Approach 2: Counting Primary Topic
Appearances</h4>
<p>Another method is to count how often a topic appears as the primary
topic within individual paragraphs or documents. This approach focuses
on the frequency with which each topic takes precedence in the text,
providing insight into which topics are most commonly addressed and
therefore, potentially more important.</p>
<pre class="r"><code>countsOfPrimaryTopics &lt;- rep(0, K)
names(countsOfPrimaryTopics) &lt;- topicNames
for (i in 1:nDocs(DTM)) {
  topicsPerDoc &lt;- theta[i, ] # select topic distribution for document i
  # get first element position from ordered list
  primaryTopic &lt;- order(topicsPerDoc, decreasing = TRUE)[1] 
  countsOfPrimaryTopics[primaryTopic] &lt;- countsOfPrimaryTopics[primaryTopic] + 1
}
# sort by primary topic
so &lt;- sort(countsOfPrimaryTopics, decreasing = TRUE)
# show ordering
paste(so, &quot;:&quot;, names(so))</code></pre>
<pre><code>##  [1] &quot;708 : year amount expenditur increas treasuri&quot;   
##  [2] &quot;684 : congress subject recommend consider attent&quot;
##  [3] &quot;630 : may upon duti shall law&quot;                   
##  [4] &quot;529 : bank govern public money issu&quot;             
##  [5] &quot;521 : can must peopl everi condit&quot;               
##  [6] &quot;491 : import duti countri increas product&quot;       
##  [7] &quot;482 : relat govern continu countri friend&quot;       
##  [8] &quot;472 : land public work made use&quot;                 
##  [9] &quot;425 : new great line construct pacif&quot;            
## [10] &quot;424 : citizen law case govern person&quot;            
## [11] &quot;389 : state unit american commerc vessel&quot;        
## [12] &quot;382 : state territori indian mexico unit&quot;        
## [13] &quot;378 : war forc militari navi armi&quot;               
## [14] &quot;377 : state constitut power repres union&quot;        
## [15] &quot;368 : act congress last made session&quot;            
## [16] &quot;365 : object can great may without&quot;              
## [17] &quot;342 : nation countri peopl everi prosper&quot;        
## [18] &quot;315 : govern treati unit state claim&quot;            
## [19] &quot;273 : depart report offic servic secretari&quot;      
## [20] &quot;256 : govern power nation independ principl&quot;</code></pre>
<p>Sorting topics by the Rank-1 method highlights topics with specific
thematic coherences, placing them at the upper ranks of the list. This
sorting approach is valuable for several subsequent analysis steps:</p>
<ul>
<li><p><em>Semantic Interpretation of Topics</em>: By examining topics
ranked higher in the list, researchers can gain insights into the most
salient and distinctive themes present in the collection. Understanding
these topics facilitates their semantic interpretation and allows for
deeper exploration of the underlying content.</p></li>
<li><p><em>Analysis of Time Series</em>: Examining the temporal
evolution of the most important topics over time can reveal trends,
patterns, and shifts in discourse. Researchers can track how the
prominence of certain topics fluctuates over different time periods,
providing valuable context for understanding changes in the subject
matter.</p></li>
<li><p><em>Filtering Based on Sub-Topics</em>: The sorted list of topics
can serve as a basis for filtering the original collection to focus on
specific sub-topics of interest. Researchers can selectively extract
documents or passages related to particular themes, enabling targeted
analysis and investigation of niche areas within the broader
context.</p></li>
</ul>
<p>By leveraging the Rank-1 method to sort topics, researchers can
enhance their understanding of the thematic landscape within the
collection and facilitate subsequent analytical tasks aimed at
extracting meaningful insights and knowledge.</p>
</div>
</div>
<div id="filtering-documents" class="section level3 unnumbered">
<h3 class="unnumbered">Filtering documents</h3>
<p>The inclusion of topic probabilities for each document or paragraph
in a topic model enables its application for thematic filtering of a
collection. This filtering process involves selecting only those
documents that surpass a predetermined threshold of probability for
specific topics. For instance, we may choose to retain documents
containing a particular topic, such as topic ‘X’, with a probability
exceeding 20 percent.</p>
<p>In the subsequent steps, we will implement this filtering approach to
select documents based on their topical content and visualize the
resulting document distribution over time. This analysis will provide
insights into the prevalence and distribution of specific themes within
the collection, allowing for a more targeted exploration of relevant
topics across different temporal intervals.</p>
<pre class="r"><code># selected by a term in the topic name (e.g. &#39;militari&#39;)
topicToFilter &lt;- grep(&#39;militari&#39;, topicNames)[1] 
topicThreshold &lt;- 0.2
selectedDocumentIndexes &lt;- which(theta[, topicToFilter] &gt;= topicThreshold)
filteredCorpus &lt;- txts$text[selectedDocumentIndexes]
# show length of filtered corpus
length(filteredCorpus)</code></pre>
<pre><code>## [1] 578</code></pre>
<pre class="r"><code># show first 5 paragraphs
head(filteredCorpus, 5)</code></pre>
<pre><code>## [1] &quot;The interests of the United States require that our intercourse with other\nnations should be facilitated by such provisions as will enable me to\nfulfill my duty in that respect in the manner which circumstances may\nrender most conducive to the public good, and to this end that the\ncompensation to be made to the persons who may be employed should,\naccording to the nature of their appointments, be defined by law, and a\ncompetent fund designated for defraying the expenses incident to the\nconduct of foreign affairs.&quot;                                                                
## [2] &quot;Your attention seems to be not less due to that particular branch of our\ntrade which belongs to the Mediterranean. So many circumstances unite in\nrendering the present state of it distressful to us that you will not think\nany deliberations misemployed which may lead to its relief and protection.&quot;                                                                                                                                                                                                                                                                                                 
## [3] &quot;The laws you have already passed for the establishment of a judiciary\nsystem have opened the doors of justice to all descriptions of persons. You\nwill consider in your wisdom whether improvements in that system may yet be\nmade, and particularly whether an uniform process of execution on sentences\nissuing from the Federal courts be not desirable through all the States.&quot;                                                                                                                                                                                                                      
## [4] &quot;The patronage of our commerce, of our merchants and sea men, has called for\nthe appointment of consuls in foreign countries. It seems expedient to\nregulate by law the exercise of that jurisdiction and those functions which\nare permitted them, either by express convention or by a friendly\nindulgence, in the places of their residence. The consular convention, too,\nwith His Most Christian Majesty has stipulated in certain cases the aid of\nthe national authority to his consuls established here. Some legislative\nprovision is requisite to carry these stipulations into full effect.&quot;
## [5] &quot;\&quot;In vain may we expect peace with the Indians on our frontiers so long as a\nlawless set of unprincipled wretches can violate the rights of hospitality,\nor infringe the most solemn treaties, without receiving the punishment they\nso justly merit.\&quot;&quot;</code></pre>
<p>Our filtered corpus contains 578 documents related to the topic 1 to
at least 20 %.</p>
</div>
<div id="topic-proportions-over-time" class="section level3 unnumbered">
<h3 class="unnumbered">Topic proportions over time</h3>
<p>In the final step, we offer a comprehensive overview of the topics
present in the data across different time periods. To achieve this, we
aggregate the mean topic proportions per decade for all State of the
Union (SOTU) speeches. These aggregated topic proportions provide a
distilled representation of the prevalent themes over time and can be
effectively visualized, such as through a bar plot. This visualization
offers valuable insights into the evolving discourse captured within the
SOTU speeches, highlighting overarching trends and shifts in thematic
emphasis across decades.</p>
<pre class="r"><code># append decade information for aggregation
textdata$decade &lt;- paste0(substr(textdata$date, 0, 3), &quot;0&quot;)
# get mean topic proportions per decade
topic_proportion_per_decade &lt;- aggregate(theta, by = list(decade = textdata$decade), mean)
# set topic names to aggregated columns
colnames(topic_proportion_per_decade)[2:(K+1)] &lt;- topicNames
# reshape data frame and generate plot
reshape2::melt(topic_proportion_per_decade, id.vars = &quot;decade&quot;) %&gt;%
  ggplot(aes(x=decade, y=value, fill=variable)) +
  geom_bar(stat = &quot;identity&quot;) + 
  labs(y = &quot;Proportion&quot;, x = &quot;Decade&quot;)  +
  scale_fill_manual(values = rev(colorRampPalette(brewer.pal(8, &quot;RdBu&quot;))(20))) + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))</code></pre>
<p><img src="topic_files/figure-html/vis4-1.png" width="864" style="display: block; margin: auto;" /></p>
<p>The visualization shows that topics around the relation between the
federal government and the states as well as inner conflicts clearly
dominate the first decades. Security issues and the economy are the most
important topics of recent SOTU addresses.</p>
</div>
</div>
</div>
<div id="citation-session-info" class="section level1 unnumbered">
<h1 class="unnumbered">Citation &amp; Session Info</h1>
<p>Schweinberger, Martin. 2024. <em>Topic Modeling with R</em>.
Brisbane: The University of Queensland. url: <a
href="https://slcladal.github.io/topic.html"
class="uri">https://slcladal.github.io/topic.html</a> (Version
2024.05.17).</p>
<pre><code>@manual{schweinberger2024topic,
  author = {Schweinberger, Martin},
  title = {Topic Modeling with R},
  note = {https://ladal.edu.au/topic.html},
  year = {2024},
  organization = &quot;The University of Queensland, Australia. School of Languages and Cultures},
  address = {Brisbane},
  edition = {2024.05.17}
}</code></pre>
<pre class="r"><code>sessionInfo()</code></pre>
<pre><code>## R version 4.3.2 (2023-10-31 ucrt)
## Platform: x86_64-w64-mingw32/x64 (64-bit)
## Running under: Windows 11 x64 (build 22621)
## 
## Matrix products: default
## 
## 
## locale:
## [1] LC_COLLATE=English_Australia.utf8  LC_CTYPE=English_Australia.utf8   
## [3] LC_MONETARY=English_Australia.utf8 LC_NUMERIC=C                      
## [5] LC_TIME=English_Australia.utf8    
## 
## time zone: Australia/Brisbane
## tzcode source: internal
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] wordcloud_2.6      topicmodels_0.2-16 tm_0.7-13          NLP_0.2-1         
##  [5] tidytext_0.4.2     tidyr_1.3.1        stringr_1.5.1      slam_0.1-50       
##  [9] reshape2_1.4.4     RColorBrewer_1.1-3 quanteda_4.0.1     ldatuning_1.0.2   
## [13] lda_1.4.2          ggplot2_3.5.0      flextable_0.9.5    dplyr_1.1.4       
## 
## loaded via a namespace (and not attached):
##  [1] tidyselect_1.2.1        farver_2.1.1            fastmap_1.1.1          
##  [4] seededlda_1.2.1         fontquiver_0.2.1        janeaustenr_1.0.0      
##  [7] promises_1.3.0          digest_0.6.35           mime_0.12              
## [10] lifecycle_1.0.4         gfonts_0.2.0            tokenizers_0.3.0       
## [13] magrittr_2.0.3          compiler_4.3.2          rlang_1.1.3            
## [16] sass_0.4.9              tools_4.3.2             utf8_1.2.4             
## [19] yaml_2.3.8              data.table_1.15.4       knitr_1.46             
## [22] labeling_0.4.3          askpass_1.2.0           stopwords_2.3          
## [25] curl_5.2.1              plyr_1.8.9              xml2_1.3.6             
## [28] httpcode_0.3.0          klippy_0.0.0.9500       withr_3.0.0            
## [31] purrr_1.0.2             grid_4.3.2              stats4_4.3.2           
## [34] fansi_1.0.6             gdtools_0.3.7           xtable_1.8-4           
## [37] colorspace_2.1-0        scales_1.3.0            crul_1.4.2             
## [40] cli_3.6.2               rmarkdown_2.26          crayon_1.5.2           
## [43] ragg_1.3.0              generics_0.1.3          rstudioapi_0.16.0      
## [46] cachem_1.0.8            modeltools_0.2-23       assertthat_0.2.1       
## [49] parallel_4.3.2          proxyC_0.4.1            vctrs_0.6.5            
## [52] Matrix_1.6-5            jsonlite_1.8.8          fontBitstreamVera_0.1.1
## [55] systemfonts_1.0.6       jquerylib_0.1.4         glue_1.7.0             
## [58] stringi_1.8.3           gtable_0.3.4            later_1.3.2            
## [61] munsell_0.5.1           tibble_3.2.1            pillar_1.9.0           
## [64] htmltools_0.5.8.1       openssl_2.1.2           R6_2.5.1               
## [67] textshaping_0.3.7       evaluate_0.23           shiny_1.8.1.1          
## [70] lattice_0.21-9          highr_0.10              SnowballC_0.7.1        
## [73] fontLiberation_0.1.0    httpuv_1.6.15           bslib_0.7.0            
## [76] Rcpp_1.0.12             zip_2.3.1               uuid_1.2-0             
## [79] fastmatch_1.1-4         officer_0.6.5           xfun_0.43              
## [82] pkgconfig_2.0.3</code></pre>
<hr />
<p><a href="#introduction">Back to top</a></p>
<p><a href="https://ladal.edu.au">Back to HOME</a></p>
<hr />
</div>
<div id="references" class="section level1 unnumbered">
<h1 class="unnumbered">References</h1>
<div id="refs" class="references csl-bib-body hanging-indent"
entry-spacing="0">
<div id="ref-arun2010finding" class="csl-entry">
Arun, Rajkumar, Venkatasubramaniyan Suresh, CE Veni Madhavan, and MN
Narasimha Murthy. 2010. <span>“On Finding the Natural Number of Topics
with Latent Dirichlet Allocation: Some Observations.”</span> In
<em>Advances in Knowledge Discovery and Data Mining: 14th Pacific-Asia
Conference, PAKDD 2010, Hyderabad, India, June 21-24, 2010. Proceedings.
Part i 14</em>, 391–402. Springer.
</div>
<div id="ref-benoit2018quanteda" class="csl-entry">
Benoit, Kenneth, Kohei Watanabe, Haiyan Wang, Paul Nulty, Adam Obeng,
Stefan Müller, and Akitaka Matsuo. 2018. <span>“Quanteda: An r Package
for the Quantitative Analysis of Textual Data.”</span> <em>Journal of
Open Source Software</em> 3 (30): 774.
</div>
<div id="ref-blei2003lda" class="csl-entry">
Blei, David M., Andrew Y. Ng, and Michael I. Jordan. 2003b.
<span>“Latent Dirichlet Allocation.”</span> <em>Journal of Machine
Learning Research</em> 3 (3): 993–1022.
</div>
<div id="ref-blei2003latent" class="csl-entry">
———. 2003a. <span>“Latent Dirichlet Allocation.”</span> <em>Journal of
Machine Learning Research</em> 3: 993–1022.
</div>
<div id="ref-brookes2019utility" class="csl-entry">
Brookes, Gavin, and Tony McEnery. 2019. <span>“The Utility of Topic
Modelling for Discourse Studies.”</span> <em>Discourse Studies</em> 21
(1): 3–21. <a
href="https://doi.org/10.1177/14614456188140">https://doi.org/10.1177/14614456188140</a>.
</div>
<div id="ref-busso2022operation" class="csl-entry">
Busso, Luciana, Monika Petyko, Steven Atkins, and Tim Grant. 2022.
<span>“Operation Heron: Latent Topic Changes in an Abusive Letter
Series.”</span> <em>Corpora</em> 17 (2): 225–58.
</div>
<div id="ref-cao2009density" class="csl-entry">
Cao, Juan, Tian Xia, Jintao Li, Yongdong Zhang, and Sheng Tang. 2009.
<span>“A Density-Based Method for Adaptive LDA Model Selection.”</span>
<em>Neurocomputing</em> 72 (7-9): 1775–81.
</div>
<div id="ref-deveaud2014accurate" class="csl-entry">
Deveaud, Romain, Eric SanJuan, and Patrice Bellot. 2014. <span>“Accurate
and Effective Latent Concept Modeling for Ad Hoc Information
Retrieval.”</span> <em>Document Num<span>é</span>rique</em> 17 (1):
61–84.
</div>
<div id="ref-tm2024package" class="csl-entry">
Feinerer, Ingo, and Kurt Hornik. 2024. <em>Tm: Text Mining Package</em>.
<a
href="https://CRAN.R-project.org/package=tm">https://CRAN.R-project.org/package=tm</a>.
</div>
<div id="ref-tm2008pub" class="csl-entry">
Feinerer, Ingo, Kurt Hornik, and David Meyer. 2008. <span>“Text Mining
Infrastructure in r.”</span> <em>Journal of Statistical Software</em> 25
(5): 1–54. <a
href="https://doi.org/10.18637/jss.v025.i05">https://doi.org/10.18637/jss.v025.i05</a>.
</div>
<div id="ref-gerlach2018network" class="csl-entry">
Gerlach, Martin, Tiago P. Peixoto, and Eduardo G. Altmann. 2018.
<span>“A Network Approach to Topic Models.”</span> <em>Science
Advances</em> 4: eaar1360.
</div>
<div id="ref-gillings2022interpretation" class="csl-entry">
Gillings, Mark, and Andrew Hardie. 2022. <span>“The Interpretation of
Topic Models for Scholarly Analysis: An Evaluation and Critique of
Current Practice.”</span> <em>Digital Scholarship in the
Humanities</em>. <a
href="https://doi.org/10.1093/llc/fqac075">https://doi.org/10.1093/llc/fqac075</a>.
</div>
<div id="ref-griffiths2004integrating" class="csl-entry">
Griffiths, Thomas, Mark Steyvers, David Blei, and Joshua Tenenbaum.
2004. <span>“Integrating Topics and Syntax.”</span> <em>Advances in
Neural Information Processing Systems</em> 17.
</div>
<div id="ref-topicmodels2011pub" class="csl-entry">
Grün, Bettina, and Kurt Hornik. 2011. <span>“<span
class="nocase">topicmodels</span>: An <span>R</span> Package for Fitting
Topic Models.”</span> <em>Journal of Statistical Software</em> 40 (13):
1–30. <a
href="https://doi.org/10.18637/jss.v040.i13">https://doi.org/10.18637/jss.v040.i13</a>.
</div>
<div id="ref-topicmodels2024package" class="csl-entry">
———. 2024. <em>Topicmodels: Topic Models</em>. <a
href="https://CRAN.R-project.org/package=topicmodels">https://CRAN.R-project.org/package=topicmodels</a>.
</div>
<div id="ref-hyland2021multilayer" class="csl-entry">
Hyland, Conor C., Yang Tao, Lida Azizi, Martin Gerlach, Tiago P.
Peixoto, and Eduardo G. Altmann. 2021. <span>“Multilayer Networks for
Text Analysis with Multiple Data Types.”</span> <em>EPJ Data
Science</em> 10: 33.
</div>
<div id="ref-murzintcev2020idealtopics" class="csl-entry">
Murzintcev, Nikita. n.d. <span>“Select Number of Topics for LDA
Model.”</span> <a
href="https://cran.r-project.org/web/packages/ldatuning/vignettes/topics.html">https://cran.r-project.org/web/packages/ldatuning/vignettes/topics.html</a>.
</div>
<div id="ref-roberts2016navigating" class="csl-entry">
Roberts, Margaret E., Brandon M. Stewart, and Dustin Tingley. 2016.
<span>“Navigating the Local Modes of Big Data: The Case of Topic
Models.”</span> In <em>Computational Social Science: Discovery and
Prediction</em>, edited by R. Michael Alvarez, 51–97. Cambridge
University Press.
</div>
<div id="ref-silge2017text" class="csl-entry">
Silge, Julia, and David Robinson. 2017. <em>Text Mining with r: A Tidy
Approach</em>. O’Reilly.
</div>
<div id="ref-seededlda2024" class="csl-entry">
Watanabe, Kohei, and Phan Xuan-Hieu. 2024. <em>Seededlda: Seeded
Sequential LDA for Topic Modeling</em>. <a
href="https://CRAN.R-project.org/package=seededlda">https://CRAN.R-project.org/package=seededlda</a>.
</div>
<div id="ref-WN17" class="csl-entry">
Wiedemann, Gregor, and Andreas Niekler. 2017. <span>“Hands-on:
<span>A</span> Five Day Text Mining Course for Humanists and Social
Scientists in <span>R</span>.”</span> In <em>Proceedings of the Workshop
on Teaching <span>NLP</span> for Digital Humanities
(<span>Teach4DH</span>), Berlin, Germany, September 12, 2017.</em>,
57–65. <a
href="http://ceur-ws.org/Vol-1918/wiedemann.pdf">http://ceur-ws.org/Vol-1918/wiedemann.pdf</a>.
</div>
</div>
</div>
<div class="footnotes footnotes-end-of-document">
<hr />
<ol>
<li id="fn1"><p>If you want to render the R Notebook on your machine,
i.e. knitting the document to html or a pdf, you need to make sure that
you have R and RStudio installed and you also need to download the <a
href="https://slcladal.github.io/content/bibliography.bib"><strong>bibliography
file</strong></a> and store it in the same folder where you store the
Rmd file.<a href="#fnref1" class="footnote-back">↩︎</a></p></li>
</ol>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // temporarily add toc-ignore selector to headers for the consistency with Pandoc
    $('.unlisted.unnumbered').addClass('toc-ignore')

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
