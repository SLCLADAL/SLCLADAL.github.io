[
  {
    "objectID": "Intror.html#goals-of-this-tutorial",
    "href": "Intror.html#goals-of-this-tutorial",
    "title": "Getting started with R",
    "section": "Goals of this tutorial",
    "text": "Goals of this tutorial\nThe goals of this tutorial are:\n\nHow to get started with R\nHow to orient yourself to R and RStudio\nHow to create and work in R projects\nHow to know where to look for help and to learn more about R\nUnderstand the basics of working with data: load data, save data, working with tables, create a simple plot\nLearn some best practices for using R scripts, using data, and projects\nUnderstand the basics of objects, functions, and indexing"
  },
  {
    "objectID": "Intror.html#audience",
    "href": "Intror.html#audience",
    "title": "Getting started with R",
    "section": "Audience",
    "text": "Audience\nThe intended audience for this tutorial is beginner-level, with no previous experience using R. Thus, no prior knowledge of R is required.\nIf you want to know more, would like to get some more practice, or would like to have another approach to R, please check out the workshops and resources on R provided by the UQ library. In addition, there are various online resources available to learn R (you can check out a very recommendable introduction here)."
  },
  {
    "objectID": "Intror.html#installing-r-and-rstudio",
    "href": "Intror.html#installing-r-and-rstudio",
    "title": "Getting started with R",
    "section": "Installing R and RStudio",
    "text": "Installing R and RStudio\n\nYou have NOT yet installed R on your computer?\n\nYou have a Windows computer? Then click here for downloading and installing R\nYou have a Mac? Then click here for downloading and installing R\n\nYou have NOT yet installed RStudio on your computer?\n\nClick here for downloading and installing RStudio.\n\n\nYou can find a more elaborate explanation of how to download and install R and RStudio here that was created by the UQ library."
  },
  {
    "objectID": "Intror.html#folder-structure-and-r-projects",
    "href": "Intror.html#folder-structure-and-r-projects",
    "title": "Getting started with R",
    "section": "Folder Structure and R projects",
    "text": "Folder Structure and R projects\nBefore actually starting with writing code, you should prepare the session by going through the following steps:\n\n1. Create a folder for your project\nIn that folder, create the following sub-folders (you can, of course, adapt this folder template to match your needs)\n\ndata (you do not create this folder for the present workshop as you can simply use the data folder that you downloaded for this workshop instead)\nimages\ntables\ndocs\n\nThe folder for your project could look like the the one shown below.\n\n\n\n\n\nOnce you have created your project folder, you can go ahead with RStudio.\n\n\n3. Open RStudio\nThis is what RStudio looks like when you first open it:\n\n\n\n\n\nIn RStudio, click on File\n\n\n\n\n\nYou can use the drop-down menu to create a R project\n\n\n4. R Projects\nIn RStudio, click on New Project\n\n\n\n\n\nNext, confirm by clicking OK and select Existing Directory.\nThen, navigate to where you have just created the project folder for this workshop.\n\n\n\n\n\nOnce you click on Open, you have created a new R project\n\n\n5. R Notebooks\nIn this project, click on File\n\n\n\n\n\nClick on New File and then on R Notebook as shown below.\n\n\n\n\n\nThis R Notebook will be the file in which you do all your work.\n\n\n6. Updating R\nIn case you encounter issues when opening the R Notebook (e.g., if you receive an error message saying that you need to update packages which then do not install properly), you may have to update your R version.\nTo update your current R version to the recent release please copy the code chunk shown below into the console pane (the bottom left pane) and click on Enter to run the code. The code will automatically update your version of R to the most recent release. During the update, you may be asked to specify some options - in that case, you can simply click on Accept and Next and accept the default settings.\n\n# install installr package\ninstall.packages(\"installr\")\n# load installr package\nlibrary(installr)\n# update r\nupdateR()\n\n\n\n7. Optimizing R project options\nWhen you work with projects, it is recommendable to control the so-called environment. This means that you make your R Project self-contained by storing all packages that are used in project in a library in the R Project (instead of in the general R library on your computer). Having a library in your R Project means that you can share your project folder wit other people and they will automatically have the same package versions that you have sued which makes your code more robust and reproducible.\nSo, how to create such an environment? You simply click on Tools (at the very top right of RStudio), then click on Project Options then click on Environments and then check Use renv with this project. Now, when you install packages, they will be installed in the package library (rather than the general R library on your computer).\n\n\n8. Getting started with R Notebooks\nYou can now start writing in this R Notebook. For instance, you could start by changing the title of the R Notebook and describe what you are doing (what this Notebook contains).\nBelow is a picture of what this document looked like when I started writing it.\n\n\n\n\n\nWhen you write in the R Notebook, you use what is called R Markdown which is explained below."
  },
  {
    "objectID": "Intror.html#r-markdown",
    "href": "Intror.html#r-markdown",
    "title": "Getting started with R",
    "section": "R Markdown",
    "text": "R Markdown\nThe Notebook is an R Markdown document: a Rmd (R Markdown) file is more than a flat text document: it’s a program that you can run in R and which allows you to combine prose and code, so readers can see the technical aspects of your work while reading about their interpretive significance.\nYou can get a nice and short overview of the formatting options in R Markdown (Rmd) files here.\nR Markdown allows you to make your research fully transparent and reproducible! If a couple of years down the line another researcher or a journal editor asked you how you have done your analysis, you can simply send them the Notebook or even the entire R-project folder.\nAs such, Rmd files are a type of document that allows to\n\ninclude snippets of code (and any outputs such as tables or graphs) in plain text while\nencoding the structure of your document by using simple typographical symbols to encode formatting (rather than HTML tags or format types such as Main header or Header level 1 in Word).\n\nMarkdown is really quite simple to learn and these resources may help:\n\nThe Markdown Wikipedia page includes a very handy chart of the syntax.\nJohn Gruber developed Markdown and his introduction to the syntax is worth browsing.\nThis interactive Markdown tutorial will teach you the syntax in a few minutes."
  },
  {
    "objectID": "Intror.html#rstudio-panes",
    "href": "Intror.html#rstudio-panes",
    "title": "Getting started with R",
    "section": "RStudio: Panes",
    "text": "RStudio: Panes\nThe GUI - Graphical User Interface - that RStudio provides divides the screen into four areas that are called panes:\n\nFile editor\nEnvironment variables\nR console\nManagement panes (File browser, plots, help display and R packages).\n\nThe two most important are the R console (bottom left) and the File editor (or Script in the top left). The Environment variables and Management panes are on the right of the screen and they contain:\n\nEnvironment (top): Lists all currently defined objects and data sets\nHistory (top): Lists all commands recently used or associated with a project\nPlots (bottom): Graphical output goes here\nHelp (bottom): Find help for R packages and functions. Don’t forget you can type ? before a function name in the console to get info in the Help section.\nFiles (bottom): Shows the files available to you in your working directory\n\nThese RStudio panes are shown below.\n\n\n\n\n\n\nR Console (bottom left pane)\nThe console pane allows you to quickly and immediately execute R code. You can experiment with functions here, or quickly print data for viewing.\nType next to the > and press Enter to execute.\n\n\n\n\nEXERCISE TIME!\n\n\n\n\n\n\n`\n\nYou can use R like a calculator. Try typing 2+8 into the R console.\n\n\n\nAnswer\n\n::: {.cell}\n  2+8\n::: {.cell-output .cell-output-stdout} [1] 10 ::: :::\n\n\n\nHere, the plus sign is the operator. Operators are symbols that represent some sort of action. However, R is, of course, much more than a simple calculator. To use R more fully, we need to understand objects, functions, and indexing - which we will learn about as we go.\nFor now, think of objects as nouns and functions as verbs."
  },
  {
    "objectID": "Intror.html#running-commands-from-a-script",
    "href": "Intror.html#running-commands-from-a-script",
    "title": "Getting started with R",
    "section": "Running commands from a script",
    "text": "Running commands from a script\nTo run code from a script, insert your cursor on a line with a command, and press CTRL/CMD+Enter.\nOr highlight some code to only run certain sections of the command, then press CTRL/CMD+Enter to run.\nAlternatively, use the Run button at the top of the pane to execute the current line or selection (see below).\n\n\n\n\n\n\nScript Editor (top left pane)\nIn contrast to the R console, which quickly runs code, the Script Editor (in the top left) does not automatically execute code. The Script Editor allows you to save the code essential to your analysis. You can re-use that code in the moment, refer back to it later, or publish it for replication.\nNow, that we have explored RStudio, we are ready to get started with R!"
  },
  {
    "objectID": "Intror.html#setting-up-an-r-session",
    "href": "Intror.html#setting-up-an-r-session",
    "title": "Getting started with R",
    "section": "Setting up an R session",
    "text": "Setting up an R session\nAt the beginning of a session, it is common practice to define some basic parameters. This is not required or even necessary, but it may just help further down the line. This session preparation may include specifying options. In the present case, we\n\nwant R to show numbers as numbers up to 100 decimal points (and not show them in mathematical notation (in mathematical notation, 0.007 would be represented as 0.7e-3))\nwant R to show maximally 100 results (otherwise, it can happen that R prints out pages-after-pages of some numbers).\n\nAgain, the session preparation is not required or necessary but it can help avoid errors.\n\n# set options\noptions(stringsAsFactors = F)                           \noptions(scipen = 100) \noptions(max.print=100) \n\nIn script editor pane of RStudio, this would look like this:"
  },
  {
    "objectID": "Intror.html#packages",
    "href": "Intror.html#packages",
    "title": "Getting started with R",
    "section": "Packages",
    "text": "Packages\nWhen using R, most of the functions are not loaded or even installing automatically. Instead, most functions are in contained in what are called packages.\nR comes with about 30 packages (“base R”). There are over 10,000 user-contributed packages; you can discover these packages online. A prevalent collection of packages is the Tidyverse, which includes ggplot2, a package for making graphics.\nBefore being able to use a package, we need to install the package (using the install.packages function) and load the package (using the library function). However, a package only needs to be installed once(!) and can then simply be loaded. When you install a package, this will likely install several other packages it depends on. You should have already installed tidyverse before the workshop.\nYou must load the package in any new R session where you want to use that package. Below I show what you need to type when you want to install the tidyverse, the tidytext, the quanteda, the readxl, and the tm packages (which are the packages that we will need in this workshop).\n\ninstall.packages(\"tidyverse\")\ninstall.packages(\"tidytext\")\ninstall.packages(\"quanteda\")\ninstall.packages(\"readxl\")\ninstall.packages(\"tm\")\ninstall.packages(\"tokenizers\")\ninstall.packages(\"here\")\ninstall.packages(\"flextable\")\n# install klippy for copy-to-clipboard button in code chunks\ninstall.packages(\"remotes\")\nremotes::install_github(\"rlesur/klippy\")\n\nTo load these packages, use the library function which takes the package name as its main argument.\n\nlibrary(tidyverse)\nlibrary(tidytext)\nlibrary(quanteda)\nlibrary(readxl)\nlibrary(tm)\nlibrary(tokenizers)\nlibrary(here)\nlibrary(flextable)\n# activate klippy for copy-to-clipboard button\nklippy::klippy()\n\n\n\n\nThe session preparation section of your Rmd file will thus also state which packages a script relies on.\nIn script editor pane of RStudio, the code blocks that install and activate packages would look like this:"
  },
  {
    "objectID": "Intror.html#getting-help",
    "href": "Intror.html#getting-help",
    "title": "Getting started with R",
    "section": "Getting help",
    "text": "Getting help\nWhen working with R, you will encounter issues and face challenges. A very good thing about R is that it provides various ways to get help or find information about the issues you face.\n\nFinding help within R\nTo get help regrading what functions a package contains, which arguments a function takes or to get information about how to use a function, you can use the help function or the apropos. function or you can simply type a ? before the package or two ?? if this does not give you any answers.\n\nhelp(tidyverse) \napropos(\"tidyverse\")\n?require\n\nThere are also other “official” help resources from R/RStudio.\n\nRead official package documentation, see vignettes, e.g., Tidyverse https://cran.r-project.org/package=tidyverse\nUse the RStudio Cheat Sheets at https://www.rstudio.com/resources/cheatsheets/\nUse the RStudio Help viewer by typing ? before a function or package\nCheck out the keyboard shortcuts Help under Tools in RStudio for some good tips\n\n\n\nFinding help online\nOne great thing about R is that you can very often find an answer to your question online.\n\nGoogle your error! See http://r4ds.had.co.nz/introduction.html#getting-help-and-learning-more for excellent suggestions on how to find help for a specific question online."
  },
  {
    "objectID": "Intror.html#loading-data-from-the-web",
    "href": "Intror.html#loading-data-from-the-web",
    "title": "Getting started with R",
    "section": "Loading data from the web",
    "text": "Loading data from the web\nTo show, how data can be downloaded from the web, we will download a tab-separated txt-file. Translated to prose, the code below means Create an object called icebio and in that object, store the result of the read.delim function.\nread.delim stands for read delimited file and it takes the URL from which to load the data (or the path to the data on your computer) as its first argument. The sep stand for separator and the \\t stands for tab-separated and represents the second argument that the read.delim function takes. The third argument, header, can take either T(RUE) or F(ALSE) and it tells R if the data has column names (headers) or not."
  },
  {
    "objectID": "Intror.html#functions-and-objects",
    "href": "Intror.html#functions-and-objects",
    "title": "Getting started with R",
    "section": "Functions and Objects",
    "text": "Functions and Objects\nIn R, functions always have the following form: function(argument1, argument2, ..., argumentN). Typically a function does something to an object (e.g. a table), so that the first argument typically specifies the data to which the function is applied. Other arguments then allow to add some information. Just as a side note, functions are also objects that do not contain data but instructions.\nTo assign content to an object, we use <- or = so that the we provide a name for an object, and then assign some content to it. For example, MyObject <- 1:3 means Create an object called MyObject. this object should contain the numbers 1 to 3.\n\n# load data\nicebio <- read.delim(\"https://slcladal.github.io/data/BiodataIceIreland.txt\", \n                      sep = \"\\t\", header = T)"
  },
  {
    "objectID": "Intror.html#inspecting-data",
    "href": "Intror.html#inspecting-data",
    "title": "Getting started with R",
    "section": "Inspecting data",
    "text": "Inspecting data\nThere are many ways to inspect data. We will briefly go over the most common ways to inspect data.\nThe head function takes the data-object as its first argument and automatically shows the first 6 elements of an object (or rows if the data-object has a table format).\n\nhead(icebio)\n\n  id file.speaker.id text.id spk.ref             zone      date    sex   age\n1  1     <S1A-001$A> S1A-001       A northern ireland 1990-1994   male 34-41\n2  2     <S1A-001$B> S1A-001       B northern ireland 1990-1994 female 34-41\n3  3     <S1A-002$?> S1A-002       ?             <NA>      <NA>   <NA>  <NA>\n4  4     <S1A-002$A> S1A-002       A northern ireland 2002-2005 female 26-33\n5  5     <S1A-002$B> S1A-002       B northern ireland 2002-2005 female 19-25\n6  6     <S1A-002$C> S1A-002       C northern ireland 2002-2005   male   50+\n  word.count\n1        765\n2       1298\n3         23\n4        391\n5         47\n6        200\n\n\nWe can also use the head function to inspect more or less elements and we can specify the number of elements (or rows) that we want to inspect as a second argument. In the example below, the 4 tells R that we only want to see the first 4 rows of the data.\n\nhead(icebio, 4)\n\n  id file.speaker.id text.id spk.ref             zone      date    sex   age\n1  1     <S1A-001$A> S1A-001       A northern ireland 1990-1994   male 34-41\n2  2     <S1A-001$B> S1A-001       B northern ireland 1990-1994 female 34-41\n3  3     <S1A-002$?> S1A-002       ?             <NA>      <NA>   <NA>  <NA>\n4  4     <S1A-002$A> S1A-002       A northern ireland 2002-2005 female 26-33\n  word.count\n1        765\n2       1298\n3         23\n4        391\n\n\n\n\n\n\nEXERCISE TIME!\n\n\n\n\n\n\n\nDownload and inspect the first 7 rows of the data set that you can find under this URL: https://slcladal.github.io/data/lmmdata.txt. Can you guess what the data is about?\n\n\n\nAnswer\n\n::: {.cell}\n  ex1data <- read.delim(\"https://slcladal.github.io/data/lmmdata.txt\", sep = \"\\t\")\n  head(ex1data, 7)\n::: {.cell-output .cell-output-stdout} Date         Genre    Text Prepositions Region   1 1736       Science   albin       166.01  North   2 1711     Education    anon       139.86  North   3 1808 PrivateLetter  austen       130.78  North   4 1878     Education    bain       151.29  North   5 1743     Education barclay       145.72  North   6 1908     Education  benson       120.77  North   7 1906         Diary  benson       119.17  North ::: :::\nThe data is about texts and the different columns provide information about the texts such as when the texts were written (Date), the genre the texts represent (Genre), the name of the texts (Text), the relative frequencies of prepositions the texts contain (Prepositions), and the region where the author was from (Region)."
  },
  {
    "objectID": "Intror.html#accessing-individual-cells-in-a-table",
    "href": "Intror.html#accessing-individual-cells-in-a-table",
    "title": "Getting started with R",
    "section": "Accessing individual cells in a table",
    "text": "Accessing individual cells in a table\nIf you want to access specific cells in a table, you can do so by typing the name of the object and then specify the rows and columns in square brackets (i.e. data[row, column]). For example, icebio[2, 4] would show the value of the cell in the second row and fourth column of the object icebio. We can also use the colon to define a range (as shown below, where 1:5 means from 1 to 5 and 1:3 means from 1 to 3) The command icebio[1:5, 1:3] thus means:\nShow me the first 5 rows and the first 3 columns of the data-object that is called icebio.\n\nicebio[1:5, 1:3]\n\n  id file.speaker.id text.id\n1  1     <S1A-001$A> S1A-001\n2  2     <S1A-001$B> S1A-001\n3  3     <S1A-002$?> S1A-002\n4  4     <S1A-002$A> S1A-002\n5  5     <S1A-002$B> S1A-002\n\n\n\n\n\n\nEXERCISE TIME!\n\n\n\n\n\n\n`\n\nHow would you inspect the content of the cells in 4th column, rows 3 to 5 of the icebio data set?\n\n\n\nAnswer\n\n::: {.cell}\n  icebio[3:5, 4]\n::: {.cell-output .cell-output-stdout} [1] \"?\" \"A\" \"B\" ::: :::\n\n\n\nInspecting the structure of data\nYou can use the str function to inspect the structure of a data set. This means that this function will show the number of observations (rows) and variables (columns) and tell you what type of variables the data consists of\n\nint = integer\nchr = character string\nnum = numeric\nfct = factor\n\n\nstr(icebio)\n\n'data.frame':   1332 obs. of  9 variables:\n $ id             : int  1 2 3 4 5 6 7 8 9 10 ...\n $ file.speaker.id: chr  \"<S1A-001$A>\" \"<S1A-001$B>\" \"<S1A-002$?>\" \"<S1A-002$A>\" ...\n $ text.id        : chr  \"S1A-001\" \"S1A-001\" \"S1A-002\" \"S1A-002\" ...\n $ spk.ref        : chr  \"A\" \"B\" \"?\" \"A\" ...\n $ zone           : chr  \"northern ireland\" \"northern ireland\" NA \"northern ireland\" ...\n $ date           : chr  \"1990-1994\" \"1990-1994\" NA \"2002-2005\" ...\n $ sex            : chr  \"male\" \"female\" NA \"female\" ...\n $ age            : chr  \"34-41\" \"34-41\" NA \"26-33\" ...\n $ word.count     : int  765 1298 23 391 47 200 464 639 308 78 ...\n\n\nThe summary function summarizes the data.\n\nsummary(icebio)\n\n       id         file.speaker.id      text.id            spk.ref         \n Min.   :   1.0   Length:1332        Length:1332        Length:1332       \n 1st Qu.: 333.8   Class :character   Class :character   Class :character  \n Median : 666.5   Mode  :character   Mode  :character   Mode  :character  \n Mean   : 666.5                                                           \n 3rd Qu.: 999.2                                                           \n Max.   :1332.0                                                           \n     zone               date               sex                age           \n Length:1332        Length:1332        Length:1332        Length:1332       \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n   word.count    \n Min.   :   0.0  \n 1st Qu.:  66.0  \n Median : 240.5  \n Mean   : 449.9  \n 3rd Qu.: 638.2  \n Max.   :2565.0"
  },
  {
    "objectID": "Intror.html#tabulating-data",
    "href": "Intror.html#tabulating-data",
    "title": "Getting started with R",
    "section": "Tabulating data",
    "text": "Tabulating data\nWe can use the table function to create basic tables that extract raw frequency information. The following command tells us how many instances there are of each level of the variable date in the icebio.\n\n\n\n\nTIP\n\n\n\n\n\n\nIn order to access specific columns of a data frame, you can first type the name of the data set followed by a $ symbol and then the name of the column (or variable).\n\n\n\ntable(icebio$date) \n\n\n1990-1994 1995-2001 2002-2005 \n      905        67       270 \n\n\nAlternatively, you could, of course, index the column by using its position in the data set like this: icebio[, 6] - the result of table(icebio[, 6]) and table(icebio$date) are the same! Also note that here we leave out indexes for rows to tell R that we want all rows.\nWhen you want to cross-tabulate columns, it is often better to use the ftable function (ftable stands for frequency table).\n\nftable(icebio$age, icebio$sex)\n\n       female male\n                  \n0-18        5    7\n19-25     163   65\n26-33      83   36\n34-41      35   58\n42-49      35   97\n50+        63  138\n\n\n\n\n\n\nEXERCISE TIME!\n\n\n\n\n\n\n\nUsing the table function, how many women are in the data collected between 2002 and 2005?\n\n\n\nAnswer\n\n::: {.cell}\n  table(icebio$date, icebio$sex)\n::: {.cell-output .cell-output-stdout} ```\n        female male\n1990-1994    338  562\n1995-2001      4   58\n2002-2005    186   84\n  :::\n  :::\n\n</details>\n\n2. Using the `ftable` function, how many men are are from northern Ireland in the data collected between 1990 and 1994?\n\n<details>\n  <summary>Answer</summary>\n\n  ::: {.cell}\n  \n  ```{.r .cell-code}\n  ftable(icebio$date, icebio$zone, icebio$sex)\n::: {.cell-output .cell-output-stdout} ``` female male\n1990-1994 mixed between ni and roi 18 13 non-corpus speaker 7 22 northern ireland 104 289 republic of ireland 209 238 1995-2001 mixed between ni and roi 0 0 non-corpus speaker 1 1 northern ireland 2 36 republic of ireland 1 21 2002-2005 mixed between ni and roi 19 7 non-corpus speaker 7 9 northern ireland 122 41 republic of ireland 38 27 ``` ::: :::"
  },
  {
    "objectID": "Intror.html#saving-data-to-your-computer",
    "href": "Intror.html#saving-data-to-your-computer",
    "title": "Getting started with R",
    "section": "Saving data to your computer",
    "text": "Saving data to your computer\nTo save tabular data on your computer, you can use the write.table function. This function requires the data that you want to save as its first argument, the location where you want to save the data as the second argument and the type of delimiter as the third argument.\n\nwrite.table(icebio, here::here(\"data\", \"icebio.txt\"), sep = \"\\t\") \n\nA word about paths\nIn the code chunk above, the sequence here::here(\"data\", \"icebio.txt\") is a handy way to define a path. A path is simply the location where a file is stored on your computer or on the internet (which typically is a server - which is just a fancy term for a computer - somewhere on the globe). The here function from thehere package allows to simply state in which folder a certain file is and what file you are talking about.\nIn this case, we want to access the file icebio (which is a txt file and thus has the appendix .txt) in the data folder. R will always start looking in the folder in which your project is stored. If you want to access a file that is stored somewhere else on your computer, you can also define the full path to the folder in which the file is. In my case, this would be D:/Uni/UQ/SLC/LADAL/SLCLADAL.github.io/data. However, as the data folder in in the folder where my Rproj file is, I only need to specify that the file is in the data folder within the folder in which my Rproj file is located.\nA word about package naming\nAnother thing that is notable in the sequence here::here(\"data\", \"icebio.txt\") is that I specified that the here function is part of the here package. This is what I meant by writing here::here which simply means use the here function from here package (package::function). This may appear to be somewhat redundant but it happens quite frequently, that different packages have functions that have the same names. In such cases, R will simply choose the function from the package that was loaded last. To prevent R from using the wrong function, it makes sense to specify the package AND the function (as I did in the sequence here::here). I only use functions without specify the package if the function is part of base R."
  },
  {
    "objectID": "Intror.html#loading-data-from-your-computer",
    "href": "Intror.html#loading-data-from-your-computer",
    "title": "Getting started with R",
    "section": "Loading data from your computer",
    "text": "Loading data from your computer\nTo load tabular data from within your project folder (if it is in a tab-separated txt-file) you can also use the read.delim function. The only difference to loading from the web is that you use a path instead of a URL. If the txt-file is in the folder called data in your project folder, you would load the data as shown below.\n\nicebio <- read.delim(here::here(\"data\", \"icebio.txt\"), sep = \"\\t\", header = T)\n\nHowever, you can always just use the full path (and you must do this is the data is not in your project folder).\n\n\n\n\nNOTEYou may have to change the path to the data!\n\n\n\n\n\n\n\nicebio <- read.delim(here::here(\"data\", \"icebio.txt\"), \n                      sep = \"\\t\", header = T)\n\nTo if this has worked, we will use the head function to see first 6 rows of the data\n\nhead(icebio)\n\n  id file.speaker.id text.id spk.ref             zone      date    sex   age\n1  1     <S1A-001$A> S1A-001       A northern ireland 1990-1994   male 34-41\n2  2     <S1A-001$B> S1A-001       B northern ireland 1990-1994 female 34-41\n3  3     <S1A-002$?> S1A-002       ?             <NA>      <NA>   <NA>  <NA>\n4  4     <S1A-002$A> S1A-002       A northern ireland 2002-2005 female 26-33\n5  5     <S1A-002$B> S1A-002       B northern ireland 2002-2005 female 19-25\n6  6     <S1A-002$C> S1A-002       C northern ireland 2002-2005   male   50+\n  word.count\n1        765\n2       1298\n3         23\n4        391\n5         47\n6        200"
  },
  {
    "objectID": "Intror.html#loading-excel-data",
    "href": "Intror.html#loading-excel-data",
    "title": "Getting started with R",
    "section": "Loading Excel data",
    "text": "Loading Excel data\nTo load Excel spreadsheets, you can use the read_excel function from the readxl package as shown below. However, it may be necessary to install and activate the readxl package first.\n\nicebio <- readxl::read_excel(here::here(\"data\", \"ICEdata.xlsx\"))\n\nWe now briefly check column names to see if the loading of the data has worked.\n\ncolnames(icebio)\n\n[1] \"id\"              \"file.speaker.id\" \"text.id\"         \"spk.ref\"        \n[5] \"zone\"            \"date\"            \"sex\"             \"age\"            \n[9] \"word.count\""
  },
  {
    "objectID": "Intror.html#loading-text-data",
    "href": "Intror.html#loading-text-data",
    "title": "Getting started with R",
    "section": "Loading text data",
    "text": "Loading text data\nThere are many functions that we can use to load text data into R. For example, we can use the readLines function as shown below.\n\ntext <- readLines(here::here(\"data\", \"text2.txt\"))\n# inspect first text element\ntext[1]\n\n[1] \"The book is presented as a manuscript written by its protagonist, a middle-aged man named Harry Haller, who leaves it to a chance acquaintance, the nephew of his landlady. The acquaintance adds a short preface of his own and then has the manuscript published. The title of this \\\"real\\\" book-in-the-book is Harry Haller's Records (For Madmen Only).\"\n\n\nTo load many texts, we can use a loop to read all texts in a folder as shown below. In a first step, we define the paths of the texts and then, we use the sapply function to loop over the paths and read them into R.\n\n# define paths\npaths <- list.files(here::here(\"data/testcorpus\"), full.names = T)\n# load texts\ntexts <- sapply(paths, function(x){ readLines(x) })\n# inspect first text element\ntexts[1]\n\n$`/home/sam/programming/SLCLADAL.github.io/data/testcorpus/linguistics01.txt`\n[1] \"Linguistics is the scientific study of language. It involves analysing language form language meaning and language in context. The earliest activities in the documentation and description of language have been attributed to the th-century-BC Indian grammarian Pa?ini who wrote a formal description of the Sanskrit language in his A??adhyayi.\"                                                                                                                                                                                                                                                                      \n[2] \"\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n[3] \"Linguists traditionally analyse human language by observing an interplay between sound and meaning. Phonetics is the study of speech and non-speech sounds and delves into their acoustic and articulatory properties. The study of language meaning on the other hand deals with how languages encode relations between entities properties and other aspects of the world to convey process and assign meaning as well as manage and resolve ambiguity. While the study of semantics typically concerns itself with truth conditions pragmatics deals with how situational context influences the production of meaning. \"\n\n\nA method achieving the same result which uses piping (more on what that is below) and tidyverse R code is shown below.\n\n# define paths\ntexts <- list.files(here::here(\"data/testcorpus\"), full.names = T, pattern = \".*txt\") %>%\n  purrr::map_chr(~ readr::read_file(.))\n# inspect first text element\ntexts[1]\n\n[1] \"Linguistics is the scientific study of language. It involves analysing language form language meaning and language in context. The earliest activities in the documentation and description of language have been attributed to the th-century-BC Indian grammarian Pa?ini who wrote a formal description of the Sanskrit language in his A??adhyayi.\\n\\nLinguists traditionally analyse human language by observing an interplay between sound and meaning. Phonetics is the study of speech and non-speech sounds and delves into their acoustic and articulatory properties. The study of language meaning on the other hand deals with how languages encode relations between entities properties and other aspects of the world to convey process and assign meaning as well as manage and resolve ambiguity. While the study of semantics typically concerns itself with truth conditions pragmatics deals with how situational context influences the production of meaning. \""
  },
  {
    "objectID": "Intror.html#renaming-piping-and-filtering",
    "href": "Intror.html#renaming-piping-and-filtering",
    "title": "Getting started with R",
    "section": "Renaming, Piping, and Filtering",
    "text": "Renaming, Piping, and Filtering\nTo rename existing columns in a table, you can use the rename command which takes the table as the first argument, the new name as the second argument, the an equal sign (=), and finally, the old name es the third argument. For example, renaming a column OldName as NewName in a table called MyTable would look like this: rename(MyTable, NewName = OldName).\nPiping is done using the %>% sequence and it can be translated as and then. In the example below, we create a new object (icebio_edit) from the existing object (icebio) and then we rename the columns in the new object. When we use piping, we do not need to name the data we are using as this is provided by the previous step.\n\nicebio_edit <- icebio %>%\n  dplyr::rename(Id = id,\n         FileSpeakerId = file.speaker.id,\n         File = colnames(icebio)[3],\n         Speaker = colnames(icebio)[4])\n# inspect data\nicebio_edit[1:5, 1:6]\n\n# A tibble: 5 × 6\n     Id FileSpeakerId File    Speaker zone             date     \n  <dbl> <chr>         <chr>   <chr>   <chr>            <chr>    \n1     1 <S1A-001$A>   S1A-001 A       northern ireland 1990-1994\n2     2 <S1A-001$B>   S1A-001 B       northern ireland 1990-1994\n3     3 <S1A-002$?>   S1A-002 ?       NA               NA       \n4     4 <S1A-002$A>   S1A-002 A       northern ireland 2002-2005\n5     5 <S1A-002$B>   S1A-002 B       northern ireland 2002-2005\n\n\nA very handy way to rename many columns simultaneously, you can use the str_to_title function which capitalizes first letter of a word. In the example below, we capitalize all first letters of the column names of our current data.\n\ncolnames(icebio_edit) <- stringr::str_to_title(colnames(icebio_edit))\n# inspect data\nicebio_edit[1:5, 1:6]\n\n# A tibble: 5 × 6\n     Id Filespeakerid File    Speaker Zone             Date     \n  <dbl> <chr>         <chr>   <chr>   <chr>            <chr>    \n1     1 <S1A-001$A>   S1A-001 A       northern ireland 1990-1994\n2     2 <S1A-001$B>   S1A-001 B       northern ireland 1990-1994\n3     3 <S1A-002$?>   S1A-002 ?       NA               NA       \n4     4 <S1A-002$A>   S1A-002 A       northern ireland 2002-2005\n5     5 <S1A-002$B>   S1A-002 B       northern ireland 2002-2005\n\n\nTo remove rows based on values in columns you can use the filter function.\n\nicebio_edit2 <- icebio_edit %>%\n  dplyr::filter(Speaker != \"?\",\n         Zone != is.na(Zone),\n         Date == \"2002-2005\",\n         Word.count > 5)\n# inspect data\nhead(icebio_edit2)\n\n# A tibble: 6 × 9\n     Id Filespeakerid File    Speaker Zone          Date  Sex   Age   Word.count\n  <dbl> <chr>         <chr>   <chr>   <chr>         <chr> <chr> <chr>      <dbl>\n1     4 <S1A-002$A>   S1A-002 A       northern ire… 2002… fema… 26-33        391\n2     5 <S1A-002$B>   S1A-002 B       northern ire… 2002… fema… 19-25         47\n3     6 <S1A-002$C>   S1A-002 C       northern ire… 2002… male  50+          200\n4     7 <S1A-002$D>   S1A-002 D       northern ire… 2002… fema… 50+          464\n5     8 <S1A-002$E>   S1A-002 E       mixed betwee… 2002… male  34-41        639\n6     9 <S1A-002$F>   S1A-002 F       northern ire… 2002… fema… 26-33        308\n\n\nTo select specific columns you can use the select function.\n\nicebio_selection <- icebio_edit2 %>%\n  dplyr::select(File, Speaker, Word.count)\n# inspect data\nhead(icebio_selection)\n\n# A tibble: 6 × 3\n  File    Speaker Word.count\n  <chr>   <chr>        <dbl>\n1 S1A-002 A              391\n2 S1A-002 B               47\n3 S1A-002 C              200\n4 S1A-002 D              464\n5 S1A-002 E              639\n6 S1A-002 F              308\n\n\nYou can also use the select function to remove specific columns.\n\nicebio_selection2 <- icebio_edit2 %>%\n  dplyr::select(-Id, -File, -Speaker, -Date, -Zone, -Age)\n# inspect data\nhead(icebio_selection2)\n\n# A tibble: 6 × 3\n  Filespeakerid Sex    Word.count\n  <chr>         <chr>       <dbl>\n1 <S1A-002$A>   female        391\n2 <S1A-002$B>   female         47\n3 <S1A-002$C>   male          200\n4 <S1A-002$D>   female        464\n5 <S1A-002$E>   male          639\n6 <S1A-002$F>   female        308"
  },
  {
    "objectID": "Intror.html#ordering-data",
    "href": "Intror.html#ordering-data",
    "title": "Getting started with R",
    "section": "Ordering data",
    "text": "Ordering data\nTo order data, for instance, in ascending order according to a specific column you can use the arrange function.\n\nicebio_ordered_asc <- icebio_selection2 %>%\n  dplyr::arrange(Word.count)\n# inspect data\nhead(icebio_ordered_asc)\n\n# A tibble: 6 × 3\n  Filespeakerid Sex    Word.count\n  <chr>         <chr>       <dbl>\n1 <S1B-009$D>   female          6\n2 <S1B-005$C>   female          7\n3 <S1B-009$C>   male            7\n4 <S1B-020$F>   male            7\n5 <S1B-006$G>   female          9\n6 <S2A-050$B>   male            9\n\n\nTo order data in descending order you can also use the arrange function and simply add a - before the column according to which you want to order the data.\n\nicebio_ordered_desc <- icebio_selection2 %>%\n  dplyr::arrange(-Word.count)\n# inspect data\nhead(icebio_ordered_desc)\n\n# A tibble: 6 × 3\n  Filespeakerid Sex    Word.count\n  <chr>         <chr>       <dbl>\n1 <S2A-055$A>   female       2355\n2 <S2A-047$A>   male         2340\n3 <S2A-035$A>   female       2244\n4 <S2A-048$A>   male         2200\n5 <S2A-015$A>   male         2172\n6 <S2A-054$A>   female       2113\n\n\nThe output shows that the female speaker in file S2A-005 with the speaker identity A has the highest word count with 2,355 words.\n\n\n\n\nEXERCISE TIME!\n\n\n\n\n\n\n`\n\nUsing the data called icebio, create a new data set called ICE_Ire_ordered and arrange the data in descending order by the number of words that each speaker has uttered. Who is the speaker with the highest word count?\n\n\n\nAnswer\n\n::: {.cell}\n  ICE_Ire_ordered <- icebio %>%\n  dplyr::arrange(-word.count)\n  # inspect data\n  head(ICE_Ire_ordered)\n::: {.cell-output .cell-output-stdout} # A tibble: 6 × 9        id file.speaker.id text.id spk.ref zone        date  sex   age   word.count     <dbl> <chr>           <chr>   <chr>   <chr>       <chr> <chr> <chr>      <dbl>   1   956 <S2A-037$A>     S2A-037 A       republic o… 1990… male  NA          2565   2   919 <S2A-016$A>     S2A-016 A       republic o… 1995… fema… 34-41       2482   3   933 <S2A-023$A>     S2A-023 A       northern i… 1990… male  50+         2367   4   992 <S2A-055$A>     S2A-055 A       northern i… 2002… fema… 42-49       2355   5   979 <S2A-047$A>     S2A-047 A       republic o… 2002… male  50+         2340   6   997 <S2A-059$A>     S2A-059 A       republic o… 1990… fema… NA          2305 ::: :::\n\n\n`"
  },
  {
    "objectID": "Intror.html#creating-and-changing-variables",
    "href": "Intror.html#creating-and-changing-variables",
    "title": "Getting started with R",
    "section": "Creating and changing variables",
    "text": "Creating and changing variables\nNew columns are created, and existing columns can be changed, by using the mutate function. The mutate function takes two arguments (if the data does not have to be specified): the first argument is the (new) name of column that you want to create and the second is what you want to store in that column. The = tells R that the new column will contain the result of the second argument.\nIn the example below, we create a new column called Texttype.\nThis new column should contain\n\nthe value PrivateDialoge if Filespeakerid contains the sequence S1A,\nthe value PublicDialogue if Filespeakerid contains the sequence S1B,\nthe value UnscriptedMonologue if Filespeakerid contains the sequence S2A,\nthe value ScriptedMonologue if Filespeakerid contains the sequence S2B,\nthe value of Filespeakerid if Filespeakerid neither contains S1A, S1B, S2A, nor S2B.\n\n\nicebio_texttype <- icebio_selection2 %>%\n  dplyr::mutate(Texttype = \n                  dplyr::case_when(stringr::str_detect(Filespeakerid ,\"S1A\") ~ \"PrivateDialoge\",\n                                   stringr::str_detect(Filespeakerid ,\"S1B\") ~ \"PublicDialogue\",\n                                   stringr::str_detect(Filespeakerid ,\"S2A\") ~ \"UnscriptedMonologue\",\n                                   stringr::str_detect(Filespeakerid ,\"S2B\") ~ \"ScriptedMonologue\",\n                                   TRUE ~ Filespeakerid))\n# inspect data\nhead(icebio_texttype)\n\n# A tibble: 6 × 4\n  Filespeakerid Sex    Word.count Texttype      \n  <chr>         <chr>       <dbl> <chr>         \n1 <S1A-002$A>   female        391 PrivateDialoge\n2 <S1A-002$B>   female         47 PrivateDialoge\n3 <S1A-002$C>   male          200 PrivateDialoge\n4 <S1A-002$D>   female        464 PrivateDialoge\n5 <S1A-002$E>   male          639 PrivateDialoge\n6 <S1A-002$F>   female        308 PrivateDialoge"
  },
  {
    "objectID": "Intror.html#if-statements",
    "href": "Intror.html#if-statements",
    "title": "Getting started with R",
    "section": "If-statements",
    "text": "If-statements\nWe should briefly talk about if-statements (or case_when in the present case). The case_when function is both very powerful and extremely helpful as it allows you to assign values based on a test. As such, case_when-statements can be read as:\nWhen/If X is the case, then do A and if X is not the case do B! (When/If -> Then -> Else)\nThe nice thing about ifelse or case_when-statements is that they can be used in succession as we have done above. This can then be read as:\nIf X is the case, then do A, if Y is the case, then do B, else do Z\n\n\n\n\nEXERCISE TIME!\n\n\n\n\n\n\n\n1.Using the data called icebio, create a new data set called ICE_Ire_AgeGroup in which you create a column called AgeGroup where all speakers who are younger than 42 have the value young and all speakers aged 42 and over old.\nTip: use if-statements to assign the old and young values.\n\n\nAnswer\n\n\nICE_Ire_AgeGroup <- icebio %>%\ndplyr::mutate(AgeGroup = dplyr::case_when(age == \"42-49\" ~ \"old\",\n                                          age == \"50+\" ~ \"old\", \n                                          age == \"0-18\" ~ \"young\",\n                                          age == \"19-25\" ~ \"young\",\n                                          age == \"26-33\" ~ \"young\",\n                                          age == \"34-41\" ~ \"young\", \n                                          TRUE ~age))\n# inspect data\nhead(ICE_Ire_AgeGroup); table(ICE_Ire_AgeGroup$AgeGroup)\n::: {.cell-output .cell-output-stdout} # A tibble: 6 × 10        id file.speaker.id text.id spk.ref zone        date  sex   age   word.count     <dbl> <chr>           <chr>   <chr>   <chr>       <chr> <chr> <chr>      <dbl>   1     1 <S1A-001$A>     S1A-001 A       northern i… 1990… male  34-41        765   2     2 <S1A-001$B>     S1A-001 B       northern i… 1990… fema… 34-41       1298   3     3 <S1A-002$?>     S1A-002 ?       NA          NA    NA    NA            23   4     4 <S1A-002$A>     S1A-002 A       northern i… 2002… fema… 26-33        391   5     5 <S1A-002$B>     S1A-002 B       northern i… 2002… fema… 19-25         47   6     6 <S1A-002$C>     S1A-002 C       northern i… 2002… male  50+          200   # … with 1 more variable: AgeGroup <chr> :::\n::: {.cell-output .cell-output-stdout} ```\n NA   old young \n547   333   452 \n:::\n:::\n\n</details>\n</div>\n\n***\n\n## Summarizing data{-}\n\nSummarizing is really helpful and can be done using the `summarise` function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nicebio_summary1 <- icebio_texttype %>%\ndplyr::summarise(Words = sum(Word.count))\n# inspect data\nhead(icebio_summary1)\n\n# A tibble: 1 × 1\n   Words\n   <dbl>\n1 141876\n\n\nTo get summaries of sub-groups or by variable level, we can use the group_by function and then use the summarise function.\n\nicebio_summary2 <- icebio_texttype %>%\n  dplyr::group_by(Texttype, Sex) %>%\n  dplyr::summarise(Speakers = n(),\n            Words = sum(Word.count))\n# inspect data\nhead(icebio_summary2)\n\n# A tibble: 6 × 4\n# Groups:   Texttype [3]\n  Texttype            Sex    Speakers Words\n  <chr>               <chr>     <int> <dbl>\n1 PrivateDialoge      female      105 60024\n2 PrivateDialoge      male         18  9628\n3 PublicDialogue      female       63 24647\n4 PublicDialogue      male         41 16783\n5 UnscriptedMonologue female        3  6712\n6 UnscriptedMonologue male         16 24082\n\n\n\n\n\n\nEXERCISE TIME!\n\n\n\n\n\n\n\nUse the icebio and determine the number of words uttered by female speakers from Northern Ireland above an age of 50.\n\n\n\nAnswer\n\n::: {.cell}\n  words_fni50 <- icebio %>%\n  dplyr::select(zone, sex, age, word.count) %>%\n  dplyr::group_by(zone, sex, age) %>%\n  dplyr::summarize(Words = sum(word.count)) %>%\n  dplyr::filter(sex == \"female\",\n         age == \"50+\",\n         zone == \"northern ireland\")\n::: {.cell-output .cell-output-stderr} `summarise()` has grouped output by 'zone', 'sex'. You can override using the   `.groups` argument. :::\n  # inspect data\n  words_fni50\n::: {.cell-output .cell-output-stdout} # A tibble: 1 × 4   # Groups:   zone, sex [1]     zone             sex    age   Words     <chr>            <chr>  <chr> <dbl>   1 northern ireland female 50+   23210 ::: :::\n\n\nLoad the file exercisedata.txt and determine the mean scores of groups A and B.\n\nTip: to extract the mean, combine the summary function with the mean function.\n\n\nAnswer\n\n::: {.cell}\n  exercisedata <- read.delim(here::here(\"data\", \"exercisedata.txt\"), sep = \"\\t\", header = T) %>%\n  dplyr::group_by(Group) %>%\n  dplyr::summarize(Mean = mean(Score))\n  # inspect data\n  exercisedata\n::: {.cell-output .cell-output-stdout} # A tibble: 2 × 2     Group  Mean     <chr> <dbl>   1 A      14.9   2 B      11.8 ::: :::"
  },
  {
    "objectID": "Intror.html#gathering-and-spreading-data",
    "href": "Intror.html#gathering-and-spreading-data",
    "title": "Getting started with R",
    "section": "Gathering and spreading data",
    "text": "Gathering and spreading data\nThe tidyr package has two very useful functions for gathering and spreading data that can be sued to transform data to long and wide formats (you will see what this means below). The functions are called gather and spread.\nWe will use the data set called icebio_summary2, which we created above, to demonstrate how this works.\nWe will first check out the spread-function to create different columns for women and men that show how many of them are represented in the different text types.\n\nicebio_summary_wide <- icebio_summary2 %>%\n  dplyr::select(-Words) %>%\n  tidyr::spread(Sex, Speakers)\n# inspect\nicebio_summary_wide\n\n# A tibble: 3 × 3\n# Groups:   Texttype [3]\n  Texttype            female  male\n  <chr>                <int> <int>\n1 PrivateDialoge         105    18\n2 PublicDialogue          63    41\n3 UnscriptedMonologue      3    16\n\n\nThe data is now in what is called a wide-format as values are distributed across columns.\nTo reformat this back to a long-format where each column represents exactly one variable, we use the gather-function:\n\nicebio_summary_long <- icebio_summary_wide %>%\n  tidyr::gather(Sex, Speakers, female:male)\n# inspect\nicebio_summary_long\n\n# A tibble: 6 × 3\n# Groups:   Texttype [3]\n  Texttype            Sex    Speakers\n  <chr>               <chr>     <int>\n1 PrivateDialoge      female      105\n2 PublicDialogue      female       63\n3 UnscriptedMonologue female        3\n4 PrivateDialoge      male         18\n5 PublicDialogue      male         41\n6 UnscriptedMonologue male         16"
  },
  {
    "objectID": "Intror.html#loading-text-data-1",
    "href": "Intror.html#loading-text-data-1",
    "title": "Getting started with R",
    "section": "Loading text data",
    "text": "Loading text data\nTo load text data from the web, we can use the read_file function which takes the URL of the text as its first argument. In this case will will load the 2016 rally speeches Donald Trump.\n\nTrump <-base::readRDS(url(\"https://slcladal.github.io/data/Trump.rda\", \"rb\"))\n# inspect data\nstr(Trump)\n\n'data.frame':   2694 obs. of  1 variable:\n $ SPEECH: chr  \"...Thank you so much.  That's so nice.  Isn't he a great guy.  He doesn't get a fair press; he doesn't get it. \"| __truncated__ \"With that said, our country is really headed in the wrong direction with a president who is doing an absolutely\"| __truncated__ \"And I'm a conservative, actually very conservative, and I'm a Republican.  And I'm very disappointed by our Rep\"| __truncated__ \"You look at Obamacare.  A total catastrophe and by the way it really kicks in in '16 and it is going to be a di\"| __truncated__ ...\n\n\nIt is very easy to extract frequency information and to create frequency lists. We can do this by first using the unnest_tokens function which splits texts into individual words, an then use the count function to get the raw frequencies of all word types in a text.\n\nTrump %>%\n  tibble(text = SPEECH) %>%\n  unnest_tokens(word, text) %>%\n  dplyr::count(word, sort=T)\n\n# A tibble: 6,102 × 2\n   word      n\n   <chr> <int>\n 1 the    5924\n 2 to     5460\n 3 and    5438\n 4 i      4873\n 5 a      3592\n 6 you    3055\n 7 of     2953\n 8 we     2565\n 9 it     2421\n10 that   2317\n# … with 6,092 more rows\n\n\nExtracting N-grams is also very easy as the unnest_tokens function can an argument called token in which we can specify that we want to extract n-grams, If we do this, then we need to specify the n as a separate argument. Below we specify that we want the frequencies of all 4-grams.\n\nTrump %>%\n  tibble(text = SPEECH) %>%\n  unnest_tokens(word, text, token=\"ngrams\", n=4) %>%\n  dplyr::count(word, sort=T) %>%\n  head(10)\n\n# A tibble: 10 × 2\n   word                    n\n   <chr>               <int>\n 1 and we’re going to     93\n 2 we are going to        75\n 3 <NA>                   68\n 4 it’s going to be       65\n 5 we’re going to do      61\n 6 we’re going to have    61\n 7 not going to happen    60\n 8 and by the way         53\n 9 thank you very much    52\n10 we’re going to win     50"
  },
  {
    "objectID": "Intror.html#splitting-up-texts",
    "href": "Intror.html#splitting-up-texts",
    "title": "Getting started with R",
    "section": "Splitting-up texts",
    "text": "Splitting-up texts\nWe can use the str_split function to split texts. However, there are two issues when using this (very useful) function:\n\nthe pattern that we want to split on disappears\nthe output is a list (a special type of data format)\n\nTo remedy these issues, we\n\ncombine the str_split function with the unlist function\nadd something right at the beginning of the pattern that we use to split the text. To add something to the beginning of the pattern that we want to split the text by, we use the str_replace_all function. The str_replace_all function takes three arguments, 1. the text, 2. the pattern that should be replaced, 3. the replacement. In the example below, we add ~~~ to the sequence SPEECH and then split on the ~~~ rather than on the sequence “SPEECH” (in other words, we replace SPEECH with ~~~SPEECH and then split on ~~~).\n\n\nTrump_split <- unlist(str_split(\n  stringr::str_replace_all(Trump, \"SPEECH\", \"~~~SPEECH\"),\n  pattern = \"~~~\"))\n# inspect data\nnchar(Trump_split)#; str(Trump_split)\n\n [1]  21311  26963   2701   6004   1342  32880  28497 425099   1404 311770\n[11]  43776"
  },
  {
    "objectID": "Intror.html#cleaning-texts",
    "href": "Intror.html#cleaning-texts",
    "title": "Getting started with R",
    "section": "Cleaning texts",
    "text": "Cleaning texts\nWhen working with texts, we usually need to clean the data. Below, we do some very basic cleaning using a pipeline.\n\nTrump_split_clean <- Trump_split %>%\n  # replace elements\n  stringr::str_replace_all(fixed(\"\\n\"), \" \") %>%\n  # remove strange symbols\n  stringr::str_replace_all(\"[^[:alnum:][:punct:]]+\", \" \") %>%\n  # combine contractions\n  stringr::str_replace_all(\" re \", \"'re \") %>%\n  stringr::str_replace_all(\" ll \", \"'ll \") %>%\n  stringr::str_replace_all(\" d \", \"'d \") %>%\n  stringr::str_replace_all(\" m \", \"'m \") %>%\n  stringr::str_replace_all(\" s \", \"'s \") %>%\n  stringr::str_replace_all(\"n t \", \"n't \") %>%\n  # remove \\\"\n  stringr::str_remove_all(\"\\\"\") %>%\n  # remove superfluous white spaces\n  stringr::str_squish()\n# remove very short elements\nTrump_split_clean <- Trump_split_clean[nchar(Trump_split_clean) > 5]\n# inspect data\nnchar(Trump_split_clean)\n\n [1]  20878  26754   2687   5960   1321  32539  28303 422165   1404 309275\n[11]  43477\n\n\nInspect text\n\nTrump_split_clean[5]"
  },
  {
    "objectID": "Intror.html#concordancing-and-kwics",
    "href": "Intror.html#concordancing-and-kwics",
    "title": "Getting started with R",
    "section": "Concordancing and KWICs",
    "text": "Concordancing and KWICs\nCreating concordances or key-word-in-context displays is one of the most common practices when dealing with text data. Fortunately, there exist ready-made functions that make this a very easy task in R. We will use the kwic function from the quanteda package to create kwics here.\n\nkwic_multiple <- quanteda::kwic(Trump_split_clean, \n       pattern = phrase(\"great again\"),\n       window = 3, \n       valuetype = \"regex\") %>%\n  as.data.frame()\n# inspect data\nhead(kwic_multiple)\n\n  docname from   to               pre     keyword            post     pattern\n1   text1 3063 3064  make our country great again       . We have great again\n2   text1 4502 4503   to make America great again        . We can great again\n3   text1 4510 4511 make this country great again . The potential great again\n4   text2 5329 5330 will make America great again        . And if great again\n5   text4  628  629   to make America great again       , folks , great again\n6   text4  643  644   to make America great again         . , And great again\n\n\nWe can now also select concordances based on specific features. For example, we only want those instances of “great again” if the preceding word was “america”.\n\nkwic_multiple_select <- kwic_multiple %>%\n  # last element before search term is \"america\"\n  dplyr::filter(str_detect(pre, \"america$\"))\n# inspect data\nhead(kwic_multiple_select)\n\n[1] docname from    to      pre     keyword post    pattern\n<0 rows> (or 0-length row.names)\n\n\nAgain, we can use the write.table function to save our kwics to disc.\n\nwrite.table(kwic_multiple_select, here::here(\"data\", \"kwic_multiple_select.txt\"), sep = \"\\t\")\n\nAs most of the data that we use is on out computers (rather than being somewhere on the web), we now load files with text from your computer. It is important to note that you need to use \\\\ when you want to load data from a Windows PC (rather than single \\).\nTo load many files, we first create a list of all files in a the directory that we want to load data from and then use the sapply function (which works just like a loop). The sapply function takes a a vector of elements and then performs a sequence of steps on each of these elements. In the example below, we feed the file locations to the sapply function and then we scan each text (i.e. we read it into R), then we paste all the content of one file together.\n\n\n\n\nNOTEYou may have to change the path to the data!\n\n\n\n\n\n\n\nfiles <- list.files(here::here(\"data\", \"ICEIrelandSample\"),\n                    pattern = \".txt\", full.names = T)\nICE_Ire_sample <- sapply(files, function(x) {\n  x <- scan(x, what = \"char\")\n  x <- paste(x, sep = \" \", collapse = \" \")\n  })\n# inspect data\nstr(ICE_Ire_sample)\n\n Named chr [1:20] \"<S1A-001 Riding> <I> <S1A-001$A> <#> Well how did the riding go tonight <S1A-001$B> <#> It was good so it was <\"| __truncated__ ...\n - attr(*, \"names\")= chr [1:20] \"/home/sam/programming/SLCLADAL.github.io/data/ICEIrelandSample/S1A-001.txt\" \"/home/sam/programming/SLCLADAL.github.io/data/ICEIrelandSample/S1A-002.txt\" \"/home/sam/programming/SLCLADAL.github.io/data/ICEIrelandSample/S1A-003.txt\" \"/home/sam/programming/SLCLADAL.github.io/data/ICEIrelandSample/S1A-004.txt\" ...\n\n\nAs the texts do not have column names (but simply names), we can clean these by removing everything before a / and by removing the .txt.\n\nnames(ICE_Ire_sample) <- names(ICE_Ire_sample) %>%\n  stringr::str_remove_all(\".*/\") %>%\n  stringr::str_remove_all(\".txt\")\n# inspect\nnames(ICE_Ire_sample)\n\n [1] \"S1A-001\" \"S1A-002\" \"S1A-003\" \"S1A-004\" \"S1A-005\" \"S1A-006\" \"S1A-007\"\n [8] \"S1A-008\" \"S1A-009\" \"S1A-010\" \"S1A-011\" \"S1A-012\" \"S1A-013\" \"S1A-014\"\n[15] \"S1A-015\" \"S1A-016\" \"S1A-017\" \"S1A-018\" \"S1A-019\" \"S1A-020\""
  },
  {
    "objectID": "Intror.html#further-splitting-of-texts",
    "href": "Intror.html#further-splitting-of-texts",
    "title": "Getting started with R",
    "section": "Further splitting of texts",
    "text": "Further splitting of texts\nTo split the texts into speech units where each speech unit begins with the speaker that has uttered it, we again use the sapply function.\n\nICE_Ire_split <- as.vector(unlist(sapply(ICE_Ire_sample, function(x){\n x <- as.vector(str_split(str_replace_all(x, \"(<S1A-)\", \"~~~\\\\1\"), \"~~~\"))  \n})))\n# inspect\nhead(ICE_Ire_split)\n\n[1] \"\"                                                                                                                                                                                    \n[2] \"<S1A-001 Riding> <I> \"                                                                                                                                                               \n[3] \"<S1A-001$A> <#> Well how did the riding go tonight \"                                                                                                                                 \n[4] \"<S1A-001$B> <#> It was good so it was <#> Just I I couldn't believe that she was going to let me jump <,> that was only the fourth time you know <#> It was great <&> laughter </&> \"\n[5] \"<S1A-001$A> <#> What did you call your horse \"                                                                                                                                       \n[6] \"<S1A-001$B> <#> I can't remember <#> Oh Mary s Town <,> oh\\n\""
  },
  {
    "objectID": "Intror.html#basics-of-regular-expressions",
    "href": "Intror.html#basics-of-regular-expressions",
    "title": "Getting started with R",
    "section": "Basics of regular expressions",
    "text": "Basics of regular expressions\nNext, we extract the File and the Speaker and combine Text, File, and Speaker in a table.\nWe use this to show the power of regular expressions (to learn more about regular expression, have a look at this very recommendable tutorial). Regular expressions are symbols or sequences of symbols that stand for\n\nsymbols or patterns (e.g. [a-z] stands for any lowercase character)\nthe frequency of symbols or patterns (e.g. {1,3} stands for between 1 and 3)\nclasses of symbols (e.g. [:punct:] stands for any punctuation symbol)\nstructural properties (e.g. [^[:blank:]] stands for any non-space character, \\t stands for tab-stop and \\n stands for a line break)\n\nWe can not go into any detail here and only touch upon the power of regular expressions.\nThe symbol . is one of the most powerful and most universal regular expressions as it represents (literally) any symbol or character and it thus stands for a pattern. The * is a regular expression that refers to the frequency of a pattern and it stands for 0 to an infinite number of instances. Thus, .* stands for 0 to an infinite number of any character. You can find an overview of the regular expressions that you can use in R here.\nAlso, if you put patterns in round brackets, R will remember the sequence within brackets and you can paste it back into a string from memory when you replace something.\nWhen referring to symbols that are used a regular expressions such as \\ or \\$, you need to inform R that you actually mean the real symbol and not the regular expression and you do that by typing two \\\\ before the sequence in question. Have a look at the example below and try to see what the regular expressions (.*(S1A-[0-9]{3,3}).*, \\n, and .*\\\\$([A-Z]{1,2}\\\\?{0,1})>.*) stand for.\n\nICE_Ire_split_tb <- ICE_Ire_split %>%\n  as.data.frame()\n# add column names\ncolnames(ICE_Ire_split_tb)[1] <- \"Text\"\n# add file and speaker\nICE_Ire_split_tb <- ICE_Ire_split_tb %>%\n  dplyr::filter(!str_detect(Text, \"<I>\"),\n         Text != \"\") %>%\n  dplyr::mutate(File = str_replace_all(Text, \".*(S1A-[0-9]{3,3}).*\", \"\\\\1\"),\n         File = str_remove_all(File, \"\\\\\\n\"),\n         Speaker = str_replace_all(Text, \".*\\\\$([A-Z]{1,2}\\\\?{0,1})>.*\", \"\\\\1\"),\n         Speaker = str_remove_all(Speaker, \"\\\\\\n\"))\n\n\n\n\n\n\n\n\nTextFileSpeaker<S1A-001$A> <#> Well how did the riding go tonight S1A-001A<S1A-001$B> <#> It was good so it was <#> Just I I couldn't believe that she was going to let me jump <,> that was only the fourth time you know <#> It was great <&> laughter </&> S1A-001B<S1A-001$A> <#> What did you call your horse S1A-001A<S1A-001$B> <#> I can't remember <#> Oh Mary s Town <,> ohS1A-001B<S1A-001$A> <#> And how did Mabel doS1A-001A<S1A-001$B> <#> Did you not see her whenever she was going over the jumps <#> There was one time her horse refused and it refused three times <#> And then <,> she got it round and she just lined it up straight and she just kicked it and she hit it with the whip <,> and over it went the last time you know <#> And Stephanie told her she was very determined and very well-ridden <&> laughter </&> because it had refused the other times you know <#> But Stephanie wouldn t let her give up on it <#> She made her keep coming back and keep coming back <,> until <,> it jumped it you know <#> It was good S1A-001B<S1A-001$A> <#> Yeah I m not so sure her jumping  s improving that much <#> She uh <,> seemed to be holding the reins very tight S1A-001A<S1A-001$B> <#> Yeah she was <#> That s what Stephanie said <#> <{> <[> She </[> needed to <,> give the horse its headS1A-001B<S1A-001$A> <#> <[> Mm </[> </{>S1A-001A<S1A-001$A> <#> She wasn t really getting into the jumping position the way she used to S1A-001A"
  },
  {
    "objectID": "Intror.html#combining-tables",
    "href": "Intror.html#combining-tables",
    "title": "Getting started with R",
    "section": "Combining tables",
    "text": "Combining tables\nWe often want to combine different tables. This is very easy in R and we will show how it can be done by combining our bio data about speakers that are represented in the ICE Ireland corpus with the texts themselves so that we get a table which holds both the text as well as the speaker information.\nThus, we now join the text data with the bio data by using the left_join function. We join the text with the bio data based on the contents of the File and the Speaker columns. In contract to right_join, and full_join, left_join will drop all rows from the right table that are not present in left table (and vice verse for right_join. In contrast, full_join will retain all rows from both the left and the right table.\n\nICE_Ire <- dplyr::left_join(ICE_Ire_split_tb, icebio_edit, by = c(\"File\", \"Speaker\"))\n\n\n\n\n\n\n\n\nTextFileSpeakerIdFilespeakeridZoneDateSexAgeWord.count<S1A-001$A> <#> Well how did the riding go tonight S1A-001A1<S1A-001$A>northern ireland1990-1994male34-41765<S1A-001$B> <#> It was good so it was <#> Just I I couldn't believe that she was going to let me jump <,> that was only the fourth time you know <#> It was great <&> laughter </&> S1A-001B2<S1A-001$B>northern ireland1990-1994female34-411,298<S1A-001$A> <#> What did you call your horse S1A-001A1<S1A-001$A>northern ireland1990-1994male34-41765<S1A-001$B> <#> I can't remember <#> Oh Mary s Town <,> ohS1A-001B2<S1A-001$B>northern ireland1990-1994female34-411,298<S1A-001$A> <#> And how did Mabel doS1A-001A1<S1A-001$A>northern ireland1990-1994male34-41765<S1A-001$B> <#> Did you not see her whenever she was going over the jumps <#> There was one time her horse refused and it refused three times <#> And then <,> she got it round and she just lined it up straight and she just kicked it and she hit it with the whip <,> and over it went the last time you know <#> And Stephanie told her she was very determined and very well-ridden <&> laughter </&> because it had refused the other times you know <#> But Stephanie wouldn t let her give up on it <#> She made her keep coming back and keep coming back <,> until <,> it jumped it you know <#> It was good S1A-001B2<S1A-001$B>northern ireland1990-1994female34-411,298<S1A-001$A> <#> Yeah I m not so sure her jumping  s improving that much <#> She uh <,> seemed to be holding the reins very tight S1A-001A1<S1A-001$A>northern ireland1990-1994male34-41765<S1A-001$B> <#> Yeah she was <#> That s what Stephanie said <#> <{> <[> She </[> needed to <,> give the horse its headS1A-001B2<S1A-001$B>northern ireland1990-1994female34-411,298<S1A-001$A> <#> <[> Mm </[> </{>S1A-001A1<S1A-001$A>northern ireland1990-1994male34-41765<S1A-001$A> <#> She wasn t really getting into the jumping position the way she used to S1A-001A1<S1A-001$A>northern ireland1990-1994male34-41765\n\n\nYou can then perform concordancing on the Text column in the table.\n\nkwic_iceire <- quanteda::kwic(ICE_Ire$Text,\n                 pattern = phrase(\"Irish\"),\n                 window = 5, \n                 valuetype = \"regex\") %>%\n  as.data.frame()\n\n\n\n\n\n\n\n\ndocnamefromtoprekeywordpostpatterntext14303737Ireland you know it wasIrishbacon and it was lovelyIrishtext17606262/ & > being goodIrishCatholics we always had toIrishtext17841313> We should do theIrish< . > ver <Irishtext17842323< / . > theIrishversion of the Matrix <Irish"
  },
  {
    "objectID": "Intror.html#tokenization-and-counting-words",
    "href": "Intror.html#tokenization-and-counting-words",
    "title": "Getting started with R",
    "section": "Tokenization and counting words",
    "text": "Tokenization and counting words\nWe will now use the tokenize_words function from the tokenizer package to find out how many words are in each file. Before we count the words, however, we will clean the data by removing everything between pointy brackets (e.g. <#>) as well as all punctuation.\n\nwords <- as.vector(sapply(Trump_split_clean, function(x){\n  x <- tm::removeNumbers(x)\n  x <- tm::removePunctuation(x)\n  x <- unlist(tokenize_words(x))\n  x <- length(x)}))\nwords\n\n [1]  3846  4641   521  1119   241  5880  5301 76772   266 56940  7848\n\n\nThe nice thing about the tokenizer package is that it also allows to split texts into sentences. To show this, we return to the rally speeches by Donald Trump and split the first of his rally speeches into sentences.\n\nSentences <- unlist(tokenize_sentences(Trump_split_clean[6]))\n# inspect\nhead(Sentences)\n\n[1] \"SPEECH 6, Thank you.\"                                                                           \n[2] \"It’s true, and these are the best and the finest.\"                                              \n[3] \"When Mexico sends its people, they’re not sending their best.\"                                  \n[4] \"They’re not sending you.\"                                                                       \n[5] \"They’re not sending you.\"                                                                       \n[6] \"They’re sending people that have lots of problems, and they’re bringing those problems with us.\"\n\n\nWe now want to find associations between words. To do this, we convert all characters to lower case, remove (some) non lexical words (also called stop words), remove punctuation, and superfluous white spaces and then create a document-term-matrix (DTM) which shows how often any word occurs in any of the sentences (in this case, the sentences are treated as documents).\nOnce we have a DTM, we can then use the findAssocs function to see which words associate most strongly with target words that we want to investigate. We can use the argument “corlimit” to show the terms that are most strongly associated with our target words.\n\n# clean sentences\nSentences <- Sentences %>%\n  # convert to lowercase\n  tolower() %>%\n  # remove stop words\n  tm::removeWords(stopwords(\"english\")) %>%\n  # remove punctuation\n  tm::removePunctuation() %>%\n  # remove numbers\n  tm::removeNumbers() %>%\n  # remove superfluous white spaces\n  stringr::str_squish()\n# create DTM\nDTM <- DocumentTermMatrix(VCorpus(VectorSource(Sentences)))\nfindAssocs(DTM, c(\"problems\", \"america\"), corlimit = c(.5, .5))\n\n$problems\nbrilliantly     devalue  obligation      russia         buy  everything \n       0.67        0.67        0.67        0.67        0.50        0.50 \n\n$america\namericas   avenue     bank \n    0.71     0.57     0.57 \n\n\nWe now turn to data visualization basics."
  },
  {
    "objectID": "Intror.html#basics-of-ggplot2-syntax",
    "href": "Intror.html#basics-of-ggplot2-syntax",
    "title": "Getting started with R",
    "section": "Basics of ggplot2 syntax",
    "text": "Basics of ggplot2 syntax\nSpecify data, aesthetics and geometric shapes\nggplot(data, aes(x=, y=, color=, shape=, size=)) +\ngeom_point(), or geom_histogram(), or geom_boxplot(), etc.\n\nThis combination is very effective for exploratory graphs.\nThe data must be a data frame.\nThe aes() function maps columns of the data frame to aesthetic properties of geometric shapes to be plotted.\nggplot() defines the plot; the geoms show the data; each component is added with +\nSome examples should make this clear"
  },
  {
    "objectID": "Intror.html#practical-examples",
    "href": "Intror.html#practical-examples",
    "title": "Getting started with R",
    "section": "Practical examples",
    "text": "Practical examples\nWe will now create some basic visualizations or plots.\nBefore we start plotting, we will create data that we want to plot. In this case, we will extract the mean word counts by gender and age.\n\nplotdata <- ICE_Ire %>%\n  # only private dialogue\n  dplyr::filter(stringr::str_detect(File, \"S1A\"),\n         # without speaker younger than 19\n         Age != \"0-18\",\n         Age != \"NA\") %>%\n  dplyr::group_by(Sex, Age) %>%\n  dplyr::summarise(Words = mean(Word.count))\n# inspect\nhead(plotdata)\n\n# A tibble: 6 × 3\n# Groups:   Sex [2]\n  Sex    Age   Words\n  <chr>  <chr> <dbl>\n1 female 19-25  608.\n2 female 26-33  461.\n3 female 34-41  691.\n4 female 50+    648.\n5 male   19-25  628.\n6 male   26-33 1075 \n\n\nIn the example below, we specify that we want to visualize the plotdata and that the x-axis should represent Age and the y-axis Words(the mean frequency of words). We also tell R that we want to group the data by Sex (i.e. that we want to distinguish between men and women). Then, we add geom_line which tells R that we want a line graph. The result of this is shown below.\n\nggplot(plotdata, aes(x = Age, y = Words, color = Sex, group = Sex)) +\n  geom_line()\n\n\n\n\nOnce you have a basic plot like the one above, you can prettify the plot. For example, you can\n\nchange the width of the lines (size = 1.25)\nchange the y-axis limits (coord_cartesian(ylim = c(0, 1000)))\nuse a different theme (theme_bw() means black and white theme)\nmove the legend to the top\nchange the default colors to colors you like (*scale_color_manual …`)\nchange the linetype (scale_linetype_manual ...)\n\n\nggplot(plotdata, aes(x = Age, y = Words,\n                     color = Sex, \n                     group = Sex, \n                     linetype = Sex)) +\n  geom_line(size = 1.25) +\n  coord_cartesian(ylim = c(0, 1500)) +\n  theme_bw() + \n  theme(legend.position = \"top\") + \n  scale_color_manual(breaks = c(\"female\", \"male\"),\n                     values = c(\"gray20\", \"gray50\")) +\n  scale_linetype_manual(breaks = c(\"female\", \"male\"),\n                        values = c(\"solid\", \"dotted\"))\n\n\n\n\nAn additional and very handy feature of this way of producing graphs is that you\n\ncan integrate them into pipes\ncan easily combine plots.\n\n\nICE_Ire %>%\n  dplyr::filter(Sex != \"NA\",\n         Age != \"NA\",\n         is.na(Sex) == F,\n         is.na(Age) == F) %>%\n  dplyr::mutate(Age = factor(Age),\n         Sex = factor(Sex)) %>%\n  ggplot(aes(x = Age, \n             y = Word.count,\n             color = Sex,\n             linetype = Sex)) +\n  geom_point() +\n  stat_summary(fun=mean, geom=\"line\", aes(group=Sex)) +\n  coord_cartesian(ylim = c(0, 2000)) +\n  theme_bw() + \n  theme(legend.position = \"top\") + \n  scale_color_manual(breaks = c(\"female\", \"male\"),\n                     values = c(\"indianred\", \"darkblue\")) +\n  scale_linetype_manual(breaks = c(\"female\", \"male\"),\n                        values = c(\"solid\", \"dotted\"))\n\n\n\n\nYou can also create different types of graphs very easily and split them into different facets.\n\nICE_Ire %>%\n  drop_na() %>%\n  dplyr::filter(Age != \"NA\") %>%\n  dplyr::mutate(Date = factor(Date)) %>%\n  ggplot(aes(x = Age, \n             y = Word.count, \n             fill = Sex)) +\n  facet_grid(vars(Date)) +\n  geom_boxplot() +\n  coord_cartesian(ylim = c(0, 2000)) +\n  theme_bw() + \n  theme(legend.position = \"top\") + \n  scale_fill_manual(breaks = c(\"female\", \"male\"),\n                     values = c(\"#E69F00\", \"#56B4E9\"))\n\n\n\n\n\n\n\n\nEXERCISE TIME!\n\n\n\n\n\n\n\nCreate a box plot showing the Date on the x-axis and the words uttered by speakers on the y-axis and group by Sex.\n\n\n\nAnswer\n\n::: {.cell}\n  ICE_Ire %>%\n  drop_na() %>%\n  dplyr::filter(Sex != \"NA\") %>%\n  dplyr::mutate(Date = factor(Date)) %>%\n  ggplot(aes(x = Date, \n             y = Word.count, \n             fill = Sex)) +\n  geom_boxplot() +\n  coord_cartesian(ylim = c(0, 2000)) +\n  theme_bw() + \n  theme(legend.position = \"top\") + \n  scale_fill_manual(breaks = c(\"female\", \"male\"),\n                     values = c(\"#E69F00\", \"#56B4E9\"))\n::: {.cell-output-display}  ::: :::\n\n\nCreate a scatter plot showing the Date on the x-axis and the words uttered by speakers on the y-axis and create different facets for Sex.\n\n\n\nAnswer\n\n::: {.cell}\n  ICE_Ire %>%\n  drop_na() %>%\n  dplyr::filter(Sex != \"NA\",\n         Date != \"NA\") %>%\n  dplyr::mutate(Date = factor(Date),\n         Sex = factor(Sex)) %>%\n  ggplot(aes(Date, Word.count,\n         color = Date)) +\n  facet_wrap(vars(Sex), ncol = 2) +\n  geom_point() +\n  coord_cartesian(ylim = c(0, 2000)) +\n  theme_bw() + \n  scale_color_manual(breaks = c(\"1990-1994\", \"2002-2005\"),\n                     values = c(\"#E69F00\", \"#56B4E9\"))\n::: {.cell-output-display}  ::: :::\n\n\n\nAdvanced\nCreate a bar plot showing the number of men and women by Date.\n\n\n Solution"
  },
  {
    "objectID": "Intror.html#extracting-session-information",
    "href": "Intror.html#extracting-session-information",
    "title": "Getting started with R",
    "section": "Extracting session information",
    "text": "Extracting session information\nYou can extract the session information by running the sessionInfo function (without any arguments)\n\nsessionInfo()\n\nR version 4.2.1 (2022-06-23)\nPlatform: x86_64-pc-linux-gnu (64-bit)\nRunning under: Ubuntu 22.04.1 LTS\n\nMatrix products: default\nBLAS:   /usr/lib/x86_64-linux-gnu/blas/libblas.so.3.10.0\nLAPACK: /usr/lib/x86_64-linux-gnu/lapack/liblapack.so.3.10.0\n\nlocale:\n [1] LC_CTYPE=en_AU.UTF-8       LC_NUMERIC=C              \n [3] LC_TIME=en_AU.UTF-8        LC_COLLATE=en_AU.UTF-8    \n [5] LC_MONETARY=en_AU.UTF-8    LC_MESSAGES=en_AU.UTF-8   \n [7] LC_PAPER=en_AU.UTF-8       LC_NAME=C                 \n [9] LC_ADDRESS=C               LC_TELEPHONE=C            \n[11] LC_MEASUREMENT=en_AU.UTF-8 LC_IDENTIFICATION=C       \n\nattached base packages:\n[1] stats     graphics  grDevices datasets  utils     methods   base     \n\nother attached packages:\n [1] flextable_0.7.3  here_1.0.1       tokenizers_0.2.1 tm_0.7-8        \n [5] NLP_0.2-1        readxl_1.4.0     quanteda_3.2.1   tidytext_0.3.3  \n [9] forcats_0.5.1    stringr_1.4.0    dplyr_1.0.9      purrr_0.3.4     \n[13] readr_2.1.2      tidyr_1.2.0      tibble_3.1.7     ggplot2_3.3.6   \n[17] tidyverse_1.3.2 \n\nloaded via a namespace (and not attached):\n [1] fs_1.5.2            lubridate_1.8.0     httr_1.4.3         \n [4] rprojroot_2.0.3     SnowballC_0.7.0     tools_4.2.1        \n [7] backports_1.4.1     utf8_1.2.2          R6_2.5.1           \n[10] DBI_1.1.3           colorspace_2.0-3    withr_2.5.0        \n[13] tidyselect_1.1.2    compiler_4.2.1      cli_3.3.0          \n[16] rvest_1.0.2         xml2_1.3.3          officer_0.4.3      \n[19] labeling_0.4.2      slam_0.1-50         scales_1.2.0       \n[22] systemfonts_1.0.4   digest_0.6.29       rmarkdown_2.14     \n[25] base64enc_0.1-3     pkgconfig_2.0.3     htmltools_0.5.2    \n[28] dbplyr_2.2.1        fastmap_1.1.0       htmlwidgets_1.5.4  \n[31] rlang_1.0.4         farver_2.1.1        generics_0.1.3     \n[34] jsonlite_1.8.0      zip_2.2.0           googlesheets4_1.0.0\n[37] magrittr_2.0.3      Matrix_1.4-1        Rcpp_1.0.8.3       \n[40] munsell_0.5.0       fansi_1.0.3         gdtools_0.2.4      \n[43] lifecycle_1.0.1     stringi_1.7.8       yaml_2.3.5         \n[46] grid_4.2.1          parallel_4.2.1      crayon_1.5.1       \n[49] lattice_0.20-45     haven_2.5.0         hms_1.1.1          \n[52] knitr_1.39          klippy_0.0.0.9500   pillar_1.7.0       \n[55] uuid_1.1-0          stopwords_2.3       fastmatch_1.1-3    \n[58] reprex_2.0.1        glue_1.6.2          evaluate_0.15      \n[61] data.table_1.14.2   renv_0.15.4         RcppParallel_5.1.5 \n[64] modelr_0.1.8        vctrs_0.4.1         tzdb_0.3.0         \n[67] cellranger_1.1.0    gtable_0.3.0        assertthat_0.2.1   \n[70] xfun_0.31           broom_1.0.0         janeaustenr_0.1.5  \n[73] googledrive_2.0.0   gargle_1.2.0        ellipsis_0.3.2"
  },
  {
    "objectID": "atapevents.html",
    "href": "atapevents.html",
    "title": "ATAP Events",
    "section": "",
    "text": "This page advertised events organized or hosted by the Australian Text Analytics Platform (ATAP) of which LADAL is a member. Please see the ATAP events page for more details about upcoming and past ATAP events.\n\n\nATAP Webinar Series\nThe LADAL is also involved in the ATAP Webinar Series 2022: please also check out the talks and presentations of ATAP Webinar Series 2022 if you are interested in data science or the data management space! All ATAP webinars take place at 8:00 pm Brisbane time which is UTC+10. Zoom links will be available one week prior to the event. The next talk in the ATAP webinar series 2022 is:\n\nAugust 1 2022 - Václav Cvrček: The Czech national Corpus\nOctober 3 2022 - Paweł Kamocki (topic tba)\n\nPast ATAP Webinars are:\n\n4 April 2022 - Keoni Mahelona: A practical approach to Indigenous data sovereignty\nJune 6 2022 - Barbara McGillivray: The Journal of Open Humanities Data\n\n\nBack to HOME"
  },
  {
    "objectID": "base.html",
    "href": "base.html",
    "title": "Style guide for LADAL tutorials",
    "section": "",
    "text": "Introduction\nThis document represents an empty R Markdown file (or Rmd file) for a LADAL tutorial. The R Markdown document for this tutorial can be downloaded here.\nYou will also have to download the bibliography file from https://slcladal.github.io/content/bibliography.bib for the tutorial to be knitted correctly. Although the knitted (or rendered) html file will look different from the LADAL design (because we have modified the theme for the LADAL page), it will be just like a proper LADAL tutorial once we have knitted the Rmd file on our machines and integrated your tutorial into the LADAL website.\nPreparation and session set up\nThis tutorial is based on R. If you have not installed R or are new to it, you will find an introduction to and more information how to use R here. For this tutorials, we need to install certain packages from an R library so that the scripts shown below are executed without errors. Before turning to the code below, please install the packages by running the code below this paragraph. If you have already installed the packages mentioned below, then you can skip ahead and ignore this section. To install the necessary packages, simply run the following code - it may take some time (between 1 and 5 minutes to install all of the libraries so you do not need to worry if it takes some time).\n\n# set options\noptions(stringsAsFactors = F)         # no automatic data transformation\noptions(\"scipen\" = 100, \"digits\" = 4) # suppress math annotation\n# install packages\ninstall.packages(\"tidyverse\")\ninstall.packages(\"flextable\")\n# install klippy for copy-to-clipboard button in code chunks\ninstall.packages(\"remotes\")\nremotes::install_github(\"rlesur/klippy\")\n\nNext, we activate the packages. Please include klippy in the installation and loading chunks to allow easy copy&pasting of code.\n\n# activate packages\nlibrary(tidyverse)\nlibrary(flextable)\n# activate klippy for copy-to-clipboard button\nklippy::klippy()\n\n\n\n\nOnce you have installed R, RStudio, and have also initiated the session by executing the code shown above, you are good to go.\n\n\n1 Tutorial content\nLoad some data and show what you want to show.\n\n# load data\ndata <- base::readRDS(url(\"https://slcladal.github.io/data/sld.rda\", \"rb\"))\n\nUse flextable for displaying tabular data as shown below.\n\ndata %>%\n  as.data.frame() %>%\n  head(10) %>%\n  flextable() %>%\n  flextable::set_table_properties(width = .5, layout = \"autofit\") %>%\n  flextable::theme_zebra() %>%\n  flextable::fontsize(size = 12) %>%\n  flextable::fontsize(size = 12, part = \"header\") %>%\n  flextable::align_text_col(align = \"center\") %>%\n  flextable::set_caption(caption = \"\")  %>%\n  flextable::border_outer()\n\n\n\n\n\n\nDateGenreTextPrepositionsRegion1,736Sciencealbin166.01North1,711Educationanon139.86North1,808PrivateLetterausten130.78North1,878Educationbain151.29North1,743Educationbarclay145.72North1,908Educationbenson120.77North1,906Diarybenson119.17North1,897Philosophyboethja132.96North1,785Philosophyboethri130.49North1,776Diaryboswell135.94North\n\n\nBelow is the code chunk for exercises.\n\n\n\n\nEXERCISE TIME!\n\n\n\n\n\n\n`\n\nThis is an example question.\n\n\n\nAnswer\n\n::: {.cell}\n  # this is some code\n  1+1\n::: {.cell-output .cell-output-stdout} [1] 2 ::: :::\n\n\n`\n\n\n\n2 Remarks on type setting\nLevel 1 headers with numbers,, lower level headers without numbering (add {-} at the end of the header to suppress numbering).\nFunction and package names in package style (different from normal text).\nUse italics for emphasis rather than bold type.\n\n\nCitation & Session Info\nYour last name, your first name. 2022. The title of your tutorial. Your location: your affiliation (in case you have one). url: https://slcladal.github.io/shorttitleofyourtutorial.html (Version 2022.08.31).\n@manual{yourlastname2022net,\n  author = {YourLastName, YourFirstName},\n  title = {The title of your tutorials},\n  note = {https://slcladal.github.io/shorttitleofyourtutorial.html},\n  year = {2022},\n  organization = {Your affiliation},\n  address = {Your location},\n  edition = {2022.08.31}\n}\n\nsessionInfo()\n\nR version 4.2.1 (2022-06-23)\nPlatform: x86_64-pc-linux-gnu (64-bit)\nRunning under: Ubuntu 22.04.1 LTS\n\nMatrix products: default\nBLAS:   /usr/lib/x86_64-linux-gnu/blas/libblas.so.3.10.0\nLAPACK: /usr/lib/x86_64-linux-gnu/lapack/liblapack.so.3.10.0\n\nlocale:\n [1] LC_CTYPE=en_AU.UTF-8       LC_NUMERIC=C              \n [3] LC_TIME=en_AU.UTF-8        LC_COLLATE=en_AU.UTF-8    \n [5] LC_MONETARY=en_AU.UTF-8    LC_MESSAGES=en_AU.UTF-8   \n [7] LC_PAPER=en_AU.UTF-8       LC_NAME=C                 \n [9] LC_ADDRESS=C               LC_TELEPHONE=C            \n[11] LC_MEASUREMENT=en_AU.UTF-8 LC_IDENTIFICATION=C       \n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] flextable_0.7.3 forcats_0.5.1   stringr_1.4.0   dplyr_1.0.9    \n [5] purrr_0.3.4     readr_2.1.2     tidyr_1.2.0     tibble_3.1.7   \n [9] ggplot2_3.3.6   tidyverse_1.3.2\n\nloaded via a namespace (and not attached):\n [1] Rcpp_1.0.8.3        lubridate_1.8.0     assertthat_0.2.1   \n [4] digest_0.6.29       utf8_1.2.2          R6_2.5.1           \n [7] cellranger_1.1.0    backports_1.4.1     reprex_2.0.1       \n[10] evaluate_0.15       httr_1.4.3          pillar_1.7.0       \n[13] gdtools_0.2.4       rlang_1.0.4         uuid_1.1-0         \n[16] googlesheets4_1.0.0 readxl_1.4.0        rstudioapi_0.13    \n[19] data.table_1.14.2   klippy_0.0.0.9500   rmarkdown_2.14     \n[22] googledrive_2.0.0   htmlwidgets_1.5.4   munsell_0.5.0      \n[25] broom_1.0.0         compiler_4.2.1      modelr_0.1.8       \n[28] xfun_0.31           systemfonts_1.0.4   pkgconfig_2.0.3    \n[31] base64enc_0.1-3     htmltools_0.5.2     tidyselect_1.1.2   \n[34] fansi_1.0.3         crayon_1.5.1        tzdb_0.3.0         \n[37] dbplyr_2.2.1        withr_2.5.0         grid_4.2.1         \n[40] jsonlite_1.8.0      gtable_0.3.0        lifecycle_1.0.1    \n[43] DBI_1.1.3           magrittr_2.0.3      scales_1.2.0       \n[46] zip_2.2.0           cli_3.3.0           stringi_1.7.8      \n[49] fs_1.5.2            xml2_1.3.3          ellipsis_0.3.2     \n[52] generics_0.1.3      vctrs_0.4.1         tools_4.2.1        \n[55] glue_1.6.2          officer_0.4.3       hms_1.1.1          \n[58] fastmap_1.1.0       yaml_2.3.5          colorspace_2.0-3   \n[61] gargle_1.2.0        rvest_1.0.2         knitr_1.39         \n[64] haven_2.5.0        \n\n\n\nBack to top\nBack to HOME\n\n\n\nReferences"
  },
  {
    "objectID": "basicquant.html#types-of-hypotheses",
    "href": "basicquant.html#types-of-hypotheses",
    "title": "Basic Concepts in Quantitative Research",
    "section": "Types of Hypotheses",
    "text": "Types of Hypotheses\nOn a very fundamental level, we can differentiate between null-hypotheses (H\\(_{0}\\)), that claim non-existence of either a state of being or a difference, and alternative or test-hypothesis (H\\(_{1}\\)) that claim or postulate the existence of of either a state of being or a difference. Among test-hypotheses, we can furthermore distinguish between non-directed hypotheses which claim that one sample is different from another sample, and directed hypotheses which claim that a feature of one sample is bigger, smaller, more frequent, or less frequent, etc. Thus, a hypothesis that group A will perform better in an exam is a directed test-hypothesis while an non-directed hypothesis would merely claim that they differ in their test results. In contrast, the null-hypothesis would claim that there is no difference between the groups in terms of their performance in that exam.\nAn additional distinction among hypotheses is the difference between deterministic and probabilistic hypotheses. While we are dealing with a deterministic hypotheses in (10) because it is a categorical claim, we are dealing with a probabilistic hypothesis in (11) because, here, the hypothesis simply claims that the likelihood of Y is higher if X is the case (but not necessarily categorically).\n\nIf the length of two words in an English phrase is different, then the shorter word will always proceed the longer word.\nIf the length of two words in an English phrase is different, then it is more likely for the shorter word to proceed the longer word than vice versa."
  },
  {
    "objectID": "basicquant.html#why-test-the-h0",
    "href": "basicquant.html#why-test-the-h0",
    "title": "Basic Concepts in Quantitative Research",
    "section": "Why test the H0?!",
    "text": "Why test the H0?!\nAlthough it is counter-intuitive, we do not actually test the test-hypothesis but we test the null-hypothesis. We will now have a closer look at how to formulate hypotheses and that formulating hypotheses is formulating expected outcomes/explanations in a formal description.\n\nNull hypothesis (H0) Groups A and B do not differ systematically! (\\(\\mu\\)A = \\(\\mu\\)B)\nTest hypothesis (H\\(_{1a}\\)) Groups A and B differ systematically! (\\(\\mu\\)A \\(\\neq\\) \\(\\mu\\)B; non-directed)\nTest hypothesis (H\\(_{1b}\\)) Group A has significantly better results/higher levels of x compared with group B. (\\(\\mu\\)A \\(>\\) \\(\\mu\\)B; directed)\n\nWhat does that mean and what are we testing? In non-technical terms, we test how likely it is that the results came about by accident. If the probability is high (p > .05) that the results happen to be random, then we do not discard the H0. If the likelihood is low (p < .05) that the results came about randomly, then we discard the H0 and assume the H1 instead! To better understand this logic, we will discuss probabilities and their role in quantitative research.\n\n\n\n\nEXERCISE TIME!\n\n\n\n\n\n\n`\n\nWhat four characteristics do hypotheses have?\n\n\n\nAnswer\n\nThey are are falsifiable (either true or false), they are testable, they are unambiguous, and they are inherently consistent (logically coherent).\n\n\nCome up with (a) three directed hypotheses and (b) three non-directed hypotheses.\n\n\n\nAnswer (examples)\n\nDirectedIn a 100 meter dash, Martin runs faster than Jack.In informal conversations, Martin uses fewer swearwords than Michael. Per minute, Martin can produce more words than Erich.Non-directedMartin and Michael differ in their use of swear words.Martin and Erich differ in their speech speed.Stefan and Martin differ in their need for sleep.\n\n\nOftentimes, it is not that easy to differentiate between hypotheses and other types of statements. Find a partner and come with statements that are not hypotheses and discuss why these statements are not hypotheses.\nFind a partner and come up with statements that can be classified as both hypotheses and non-hypotheses and be prepared to explain your reasoning to the group.\n\n\n`"
  },
  {
    "objectID": "basicquant.html#research-designs",
    "href": "basicquant.html#research-designs",
    "title": "Basic Concepts in Quantitative Research",
    "section": "Research Designs",
    "text": "Research Designs\nNow that the terms sample and population are clear, we can continue to think about how we could test hypotheses. In the empirical sciences, we try to answer questions we have by analyzing data. However, before we can attempt to answer a question and test if we are right with the answer we have in mind (our hypothesis), we have to decide on how to tackle the question. Thus, we have to come up with a plan about how to test our hypothesis. This plan is called a research design. The research design informs how to collect, process, visualize, and analyze the data. As such, the research design focuses on all aspects of research that are related to the data.\nThus, after comping up with a research question and a hypothesis, the next step typically consists in collecting the right type of data. There are different ways to collecting data and these different ways to collect data represent different research designs. We cannot go over all the different designs (and they can be combined, for instance, introspection is often a part of questionnaire designs), but we will focus and briefly discuss the most common ones.\nIn general, we can distinguish between experimental and observational designs. In contrast to observational designs (which can only confirm correlations, i.e. that X and Y occur together), experimental designs are more powerful in that they can determine causality (i.e., if X causes Y). We the most common research designs in the language sciences are\n\nexperimental\n\nexperimental designs (e.g., reaction times, eye-tracking)\n\nobservational\n\nbehavioral designs (e.g., corpora)\narchival/reviews (e.g., grammars, systematic literature reviews)\nintrospection (e.g., grammaticality judgements)\nquestionnaire designs (e.g., elicitation via surveys and questionnaires)\n\n\nThere are more designs(e.g. meta-studies) and also other classifications, but in the language sciences, I think that the ones listed above are the most frequent ones. Each of these designs has advantages and disadvantages which are briefly summarized in the table below.\n\ninstall.packages(\"e1071\")\ninstall.packages(\"flextable\")\ninstall.packages(\"ggplot2\")\ninstall.packages(\"ggpubr\")\ninstall.packages(\"graphics\")\ninstall.packages(\"grDevices\")\ninstall.packages(\"knitr\")\ninstall.packages(\"tidyverse\")\n\n\n\n\n\n\nOverview of advantages and disadvantages of different research designs.\n\n\nTypeResearch designExampleDirectness of access to phenomenonCost/Labor intensityExternal validity (generalizability)Internal validityEase of data collection (e.g. access to participants)Experimental researchExperimentalReaction time measurementshigh/variablehigh/variablelow/variablehighlowObservational researchBehavioralCorpus studyhighlow/variablehighhighmediumArchival/ReviewSystematic literature reviewlowlowlowmediumhighIntrospectionGrammaticality judgementshighlowlow/variablehighlowQuestionnaireLanguage use surveymediumlowmediummediumhigh\n\n\nWe will now briefly focus on each design and discuss it.\n\nExperimental research\nExperimental designs can determine if there is a causal link between predictors and response which sets experimental designs apart from observational designs. Experiments typically take place in very well-controlled, but unnatural environments, .e.g, in laboratories.\nIn experimental designs, subjects are typically randomly assigned into test groups and control groups and the experimenter. If the subjects are not assigned randomly to test and control groups, the design is referred to as quasi-experimental (while experimental designs with randomized allocation of subjects are referred to as true experiments). The aim of experiments, in the most basic sense, is to determine if a result differs based on a change in some condition. The experimenter would change the condition only in the test group while not changing it in the control group while everything else remains constant. That way, it is possible to test if the change in the condition is responsible for the change in the outcome.\nDuring an experiment, subjects are shown stimuli (e.g., sentences, words, pictures) and the subjects (should) react to those stimuli showing a response (pressing buttons, providing judgments). The response does not have to be voluntary, meaning that responses can also simply be neuronal activity that the subject has no control over. In repeated measure designs, the same subjects are tested repeatedly while different subjects are tested in between-subjects designs.\nIn addition to the ability to establish causality, experimental designs allow a very direct access to a phenomenon and have high internal validity which means that experiments can test the phenomena in a very well-circumscribed and precise manner. Disadvantages of experimental designs are that they are typically rather costly and labor intensive and that the results are not necessarily generalizable due to the un-natural setting of most experiments (low external validity).\n\n\nQuasi-experimental research\nIn contract to experimental designs, subjects (or participants) in quasi-experimental designs are not randomly assigned to groups. Because of this, quasi-experimental designs can be affected by confounding (i.e., that changes in the dependent variable are caused by a variable that is not controlled for). As such, quasi-experimental designs are not experimental designs but can be considered observational. Examples for quasi-experimental designs are corpus-based variationist studies of the occurrence of linguistics variants in a variable context.\nQuasi-experimental designs encompass studies where subjects cannot be randomly assigned into test groups because either, this is not practically possible because the observation has already taken place or because the characteristic is inherent in the participants themselves. For instance, analyses of historical data can at best be quasi-experimental because the data have already been collected and the experimenter cannot assign participants to test groups. Also, some studies that analyze sex differences via the behavior of men and women cannot be experimental because participants cannot be randomly assigned to the male or female group.\n\n\nObservational designs\nIn the language sciences, observational designs encompass all forms of data acquisition and collection where the subjects are observed in their “natural habitat”. Hence, observational or behavioral designs are the most natural and thus have high external validity which means that we can assume that the results are transferable to the real world. The most common type of this design are corpus studies in which texts (which encompasses both written and transcribed spoken language) are searched for the occurrence of certain linguistic patterns and then correlated with some other feature, the use of a certain linguistic construction such as a specific suffix or a fixed expression like in order that in two different time periods or dialects.\nThe cost and labor intensity of corpus studies depends heavily upon if the corpus that you want to use has already been compiled - if the corpus is already available, the the costs and labor intensity are minimal but they can be high if the corpus has to be compiled first. However, even compiling a corpus can be comparatively easy and cheap, for example, if you compile a corpus consisting of a collection of texts that are available online but the costs can be substantive if you, for example, first need to record and transcribe spoken language in the field).\nThe problem with observational designs are that the contexts in which the data were collected were not controlled with means that the results are more likely to be affected by contextual or confounding factors that the researchers have no control over or don’t even know about.\n\n\nArchival research and reviews\nArchival research is more common in historical linguistics where researchers look for primary texts or records in archives. However, archival research also encompasses digital archives such as libraries which means that systematic literature reviews also fall under this research design.\nThe advantages of archival research designs are that the data collections is typically comparatively easy and associated with minimal costs. Also, depending on the phenomenon, the access to the phenomenon can be quite indirect, for example, entirely filtered through the eyes of eye witnesses or experts.\n\n\nIntrospection\nIntrospection has been the dominant form of “data collection” in the 20^th century and it is strongly connected to what is called armchair linguistics (linguists coming to insights about language by musing about what they can or would say and what they could or would not say). Introspection is, of course, by far the easiest and cheapest way to “collect data”. Unfortunately, introspection is subjective and can be heavily biased by the expectations and theoretical framework of subjects. Nonetheless, introspection can be and is extremely useful in particular regarding hypothesis generation and finding potential predictors for a certain linguistic behavior that can then be tested empirically.\nSince the 1990s and the rise of empirical linguistics, which is characterized by the employment of both experimental designs as well as corpus linguistics and increased use of questionnaire designs, introspection has been relegated to playing a part of other designs while only rarely being used as the sole source of data collection.\n\n\nQuestionnaire designs\nQuestionnaire designs represent any method of data collection where respondents are asked to provide (conscious) information to answers or ratings/evaluations to prompts (such as statements). The process to ask for information from respondents is called elicitation.\nIn addition to surveys which ask for socio-demographic information about speakers or about language use and language background information, questionnaires frequently ask respondents to rate their agreement with items, or the likelihood, frequency, or acceptability of items on Likert scales. A Likert scale, pronounced /lick.ert/ and named after the psychologist Rensis Likert, and often ranges from strongly disagree to strongly agree (or analogously from never to very frequently or extremely unlikely to extremely likely). Typically, respondents are given 5 or 7 options with the endpoints noted above representing the endpoints and respondents are asked to select the options that best matches their evaluation.\nA type of elicitation that is special in the language science is called discourse completion task or DCT. In DCTs, respondents are asked to imagine a certain discourse context or situation and are then asked how they would answer, respond, or express something. While DCTs have been shown to provide data that mirrors real-world behaviors, it is less reliable compared to corpus data as respondents cannot necessarily provide accurate information about their linguistic behavior.\nTo counter fatigue effects (respondents get tired or annoyed and do no longer answer to the best of their knowledge/ability), questionnaire items are often randomized so that their order differs between respondents. To counter what is called agreeability effects, researchers commonly have respondents rate test items (items that contain the phenomenon researchers are interested in) and fillers (items that do not contain what researchers are interested in). Fillers are included to disguise what the questionnaire aim to test so that respondents cannot (unconsciously) try to provide agreeable answers (the answers that researchers hope to get)."
  },
  {
    "objectID": "basicquant.html#variable-types-scaling",
    "href": "basicquant.html#variable-types-scaling",
    "title": "Basic Concepts in Quantitative Research",
    "section": "Variable Types (Scaling)",
    "text": "Variable Types (Scaling)\nIn the following, variable types (also referred to as scaling level or scaling) are discussed. It is very important to know which type of variables one is dealing with because the type of variable has affects many of the methods discussed, both in descriptive and in inferential statistics.\n\nNominal and Categorical Variables\nNominal and categorical variables only list the membership of a particular class. For nominal variables, there are exactly two occurrences (yes/no or on/off), while for categorical variables there can be several groups, e.g. the state in which someone was born.\n\n\nOrdinal Variables\nWith ordinal variables it is possible to rank the values, but the distances between the ranks can not be exactly quantified. An example of an ordinal variable is the ranking in a 100-meter dash. The 2nd in a 100-meter dash did not go twice as fast as the 4th. It is often the case that ordinal variables consist of integer, positive numbers (1, 2, 3, 4, etc.).\n\n\n(True) Numeric Variables\nThere are two basic types of numeric variables: interval-scaled variables and ratio-scaled variables. For interval scaled variables, the differences between levels are significant, but not the relationship between levels. For instance, 20 degree Celsius is not twice as hot as 10 degree Celsius.\nWith respect to ratio-scaled variables, both the differences and the relationship between the levels are significant. An example of this is the times in a 100-meter dash. For ratio-scaled variables, 10 is exactly twice as high as 5 and half as much as 20.\nIt is very important to keep in mind that both interval-scaled variables and ratio-scaled variables are numeric variables. This will play a role later because many tests can either only or never be applied to numeric variables.\n\n\n\n\n\nVariable types/Variable scales with examples.\n\n\nVariable TypeVariable LevelNominal Scaled / Categorical VariablesGender, Nationality, Part of SpeechOrdinal Scaled VariablesGraduation, School Grades, Military RankInterval Scaled VariablesTemperature, Acceptability JudgmentsRatio-scaled VariablesAge, Duration, Number of Syllables\n\n\nIt is enormously important to know variable types and levels, as the type of variable requires which tests are possible and which are not. For example, a \\(\\chi\\)2-test can only be applied to nominal or categorical variables, and a t-test to numeric variables only.\nIt is often necessary to translate variables into another type of variable. It should be noted that variables can only be transferred to variable types with less information content. The least informative variables are nominal, while the most informative variables are ratio scaled. The variable types thus form an implicit hierarchy:\nnominal/categorical < ordinal < interval/ratio\nHere is an example to illustrate this: let’s say you are investigating gender differences in the use of swear words in spoken Irish English and you find that you cannot use a linear regression or an ANOVA because too many speakers use no swear words (which violates the requirement that the errors must be normally distributed). In such a case, what one can do is to rank the speakers by their frequency of swear or curse words. Rank 1 would represent the speaker with the highest frequency of swear words, rank 2 would represent the speaker with the second highest frequency of swear words and so on. After you do this, you can, for example, use a Mann-Whitney U test to determine the relationship between the gender of speakers and their swear word ranking. You could also split the speakers into two groups (swear word users and non-swear-word users) and then perform a \\(\\chi\\)2-test of the frequencies of men and women in these groups. The important thing is that you cannot transform a categorical variable into a numeric variable as this would mean that you transform a less information-rich variable into a more information-rich variable.\n\n\n\n\nEXERCISE TIME!\n\n\n\n\n\n\n`\n\nFor each of the variables listed below, consider how you could operationalize them and what kind of variables it would be.a. weather (cloudy, sunny, rainy, etc.)b. citizenshipc. Tense of matrix verbsd. Structural complexitye. word lengthf. Number of syllables in a wordg. The length of pauses in a sample of conversationsh. The appearance or non-appearance of finite verbs in a particular texti. Estimation of words on an acceptability scale from 1 to 5\n\n\n\nAnswer (examples)\n\nThere are other solutions so the following are just possible options!a. weather could, e.g., be operationalized as the number of sunny days or the amount of rain per year. Optimally, we would want weather to be operationalized as a numeric variable.b).citizenship would typically be operationalized as a categorical variable (e.g. German or Norwegian or Australian or other).c. This answer depends on the grammatical structure of the language in question. In English, we would probably operationalize the tense of matrix verbs as a nominal variable with the levels present and non-present.d. Structural complexity can be operationalized in many different ways, e.g., as the number of syntactic nodes, sentence length, number of phrases, average phrase length, etc. Optimally, we would want structural complexity to be operationalized as a numeric variable.e. Word length could,e.g. be operationalized as number of letters, number of phonemes, or the time it takes to produce that word. Thus, word length would typically be operationalized as a numerically scaled variable or an integer (which, in R, would be a type of numeric variable).f. The number of syllables per word would be operationalized as an integer (which, in R, would be a type of numeric variable).g. The length of pauses in a sample of conversations could be operationalized as the time of the pause (numeric) or even just as an ordered factor (short, middle, long). Optimally, however, we would want the length of pauses to be operationalized as a numeric variable.h. The appearance or non-appearance of finite verbs in a particular text would be a nominal variable (present versus absent).i. (The estimation of words on an acceptability scale from 1 to 5 represents judgements on a Likert scale which means that the resulting variable would represent an order factor (ordinal variable).\n\n\nAs you have seen from the above exercise, concepts can be operationalized differently. Find a partner and imagine that you are tasked with performing a study in which age of subjects is an important variable. Discuss with a partner how you would age. What advantages and disadvantages do the different operationalizations have?\nExample: When it rains, more people get wet than when it’s not raining. (If X, then Y)What is the dependent variable here and what is the independent variable?\n\n\n\nAnswer\n\nDependent variable: wet people Independent variable: rain\n\n\nWhich variable scales are time, rank, and name in the table below?\n\n\n\n\n\n\nResults of a 100 meter dash.\n\n\nNameRankTimeCarl Lewis19.86Ben Johnson29.97Steve Davis310.06\n\n\n\n\nAnswer\n\nName is categorical, Rank is ordinal, and Time is numeric.\n\n\nDiscuss with a partner: what obstacles might exist, so that a well operationalized variable has low extrinsic validity?\nConsider the following scenario: in a large representative study, shoe size is found to be an excellent predictor of intelligence. Given this result, find a partner and discuss if intrinsic validity is necessary?\n\n\n`"
  },
  {
    "objectID": "basicquant.html#significance-levels",
    "href": "basicquant.html#significance-levels",
    "title": "Basic Concepts in Quantitative Research",
    "section": "Significance Levels",
    "text": "Significance Levels\nBefore conducting a study, it is advisable to determine the so-called significance level or \\(\\alpha\\) level. This \\(\\alpha\\) level or level of significance indicates how high the p-value can be without having to assume that there is a significant relationship between the variables. It is customary to differentiate between three levels of significance (also called \\(\\alpha\\) levels):\n\np < .001: highly significant - indicated by three stars (***)\np < .01: very significant - indicated by two stars (**)\np < .05: significant - indicated by one star (*)\n\nVariables with a p value that is smaller than .1 but larger than .05 are sometimes referred to as being marginally significant (+).\nVariables that are not significant are commonly referred to or labeled as n.s. (not significant). As we stated above, before we perform a test, we determine a value above which we reject the null hypothesis, the so-called significance level. It’s usually 5%. If the error probability is less than 5% (p <. 05), we reject the null hypothesis. Conclusion: The relationship between the variables is statistically significant. It is important to note here that the H1 (or Test Hypothesis) is correct only because the null hypothesis can be rejected! Statistics can NEVER prove hypotheses but only reject Null Hypotheses which leads us to accept the H1 as preliminary accepted or not-yet-rejected. So all knowledge is preliminary in the empirical sciences."
  },
  {
    "objectID": "basicquant.html#probability",
    "href": "basicquant.html#probability",
    "title": "Basic Concepts in Quantitative Research",
    "section": "Probability",
    "text": "Probability\nIn the following, we will turn to probability and try to understand why probability is relevant for testing hypotheses. This is important at this point because statistics, and thus hypothesis testing, fundamentally builds upon probabilities and probability distributions. In order to understand how probability works, we will investigate what happens when we flip a coin. The first question that we will be addressing is “What is the probability of getting three Heads when flipping a coin three times?”.\nThe probability of getting three heads when flipping a coin three times is .5 to the power of 3: .53 = .5 times .5 times .5 = .125. The probability of getting Heads twice when flipping the coin three times is .375. How do we know?\nThe probability of getting 3 heads in tree tosses is 12.5 percent:\n.53 = .5 * .5 * .5 = .125\nThe probability of getting 2 heads in tree tosses is 37.5 percent:\n.125 + .125 + .125 = 0.375\nBut how do we know this? Well, have look at the table below.\n\n\n\n\n\n\nAll possible results of 3 coin tosses.\n\n\nToss 1Toss 2Toss 3HeadsTailsProbabiltyHeadHeadHead300.125HeadHeadTails210.125HeadTailsHead210.125TailsHeadHead210.125HeadTailsTails120.125TailsHeadTails120.125TailsTailsHead120.125TailsTailsTails030.125\n\n\n\nGiven this table, we are in fact, in a position to calculate the probability of getting 100 heads in 100 coin tosses because we can simply fill in the numbers in the formulas used above: .5100 = 7.888609 * 10-31\nOkay, let us make a bet..\n\nIf head shows, I win a dollar.\nIf tails shows, you win a dollar.\n\nBut given that you know I am cheeky bastard, you do not trust me and claim that I will cheat. But how will you know that I cheat? At which point can you claim that the result is so unlikely that you are (scientifically backed) allowed to claim that I cheat and have manipulated the coin?\n\n\n\n\n\nSo before we actually start with the coin tossing, you operationalize your hypothesis:\n\nH0: The author (Martin) is not cheating (heads shows just as often as tails).\nH1: The author (Martin) is cheating (heads shows so often that the probability of the author not cheating is lower than 5 percent)\n\nWe now toss the coin and head shows twice. The question now is whether head showing twice is lower than 5 percent.\nWe toss the coin 3 times. Head shows twice. How likely is it that I do not cheat and head falls more than twice anyway? (In other words, what is the probability p that I win twice or more and not cheat?) If you set the significance level at .05, could you then accuse me of being a cheater?\nAs you can see in the fourth column, there are three options that lead to heads showing twice (rows 2, 3, and 4). If we add these up (0.125 + 0.125 + 0.125 = 0.375). Also, we need to add the case where head shows 3 times which is another .125 (0.375 + 0.125 = .5), then we find out that the probability of heads showing at least twice in three coin tosses is 50 percent and thus 10 times more than the 5-percent threshold that we set initially. Therefore, you cannot claim that I cheated.\n\n\n\n\n\nProbability of successes in coin tosses.\n\n\n0 Heads1 Head2 Heads3 Heads0.1250.3750.3750.125\n\n\n\n\n\n\n\nWe can do the same and check the probabilities of having heads in 10 coin tosses - if we plot the resulting probabilities, we get the bar plot shown below.\n\n\n\n\n\nThe distribution looks more bell-shaped with very low probabilities of getting 0 or 1 as well as 9 or 10 times heads in the 10 coin tosses and a maximum at 5 times heads in 10 coin tosses. In fact, the probability of having 0 and 10 times head is .001 (or 1 in a 1000 attempts - one attempt is tossing a coin 10 times) - the probability of having heads once and 9 times is.01 or 1 in 100 attempts. In contrast, the probability of having 5 times head is .246 or about 25% (1 in 4 attempts will have 5 heads). Also, we can sum up probabilities: the probability of getting 8 or more heads in 10 coin tosses is the probability of getting 8, 9, and 10 times heads (.044 + .010 + 0.001 = .055 = 5.5 percent).\nCalculating the probabilities for three or even 10 coin tosses is still manageable manually but is there an easier way to calculate probabilities? A handier way is have a computer calculate probabilities and the code below shows how to do that in R - a very powerful and flexible programming environment that has been designed for quantitative analysis (but R can, in fact, do much more - this website, for instance, is programmed in R).\nThe code chunk below calculates the probabilities of having 0, 1, 2, and 3 times head in 3 tosses.\n\n# probabilities of  0, 1, 2 and 3 times head in 3 coin tosses\ndbinom(0:3, 3, 0.5)\n\n[1] 0.125 0.375 0.375 0.125\n\n\nThe code chunk below calculates the probabilities of having 2 or 3 times head in 3 tosses.\n\n# probabilities of  2 or 3 times head in 3 coin tosses\nsum(dbinom(2:3, 3, 0.5))\n\n[1] 0.5\n\n\nThe code chunk below calculates the probabilities of having 100 head in 100 tosses.\n\n# probability of  100 times head in 100 coin tosses\ndbinom(100, 100, 0.5)\n\n[1] 7.888609e-31\n\n\nThe code chunk below calculates the probabilities of having 58 or more times head in 100 tosses.\n\n# probability of  58 to a 100 times head in 100 coin tosses\nsum(dbinom(58:100, 100, 0.5))\n\n[1] 0.06660531\n\n\nThe code chunk below calculates the probabilities of having 59 or more times head in 100 tosses.\n\n# probability of  59 to a 100 times head in 100 coin tosses\nsum(dbinom(59:100, 100, 0.5))\n\n[1] 0.04431304\n\n\nThe code chunk below calculates the number of heads in 100 tosses where the probability of getting that number of heads or more sums up to 5 percent (0.05).\n\n# at which point does the probability of getting head \n# dip below 5 percent in 100 coin tosses?\nqbinom(0.05, 100, 0.5, lower.tail=FALSE)\n\n[1] 58\n\n\n\n\n\nLet’s go back to our example scenario. In our example scenario, we are dealing with a directed hypothesis and not with an un-directed/non-directed hypothesis because we claimed in our H1 that I was cheating and would get more heads than would be expected by chance (\\(\\mu_{Martin}\\) \\(>\\) \\(\\mu_{NormalCoin}\\)). For this reason, the test we use is one-tailed. When dealing with un-directed hypotheses, you simply claim that the outcome is either higher or lower - in other words the test is two-tailed as you do not know in which direction the effect will manifest itself.\nTo understand this a more thoroughly, we will consider tossing a coin not merely 3 but 100 times. The Figure below shows the probabilities for the number of heads showing when we toss a coin 100 from 0 occurrences to 100 occurrences.\n\n\n\n\n\nThe next figure shows at which number of heads the cumulative probabilities dip below 5 percent for two-tailed hypotheses. According to the graph, if head shows up to 40 or more often than 60 times, the cumulative probability dips below 5 percent. Applied to our initial bet, you could thus claim that I cheated if head shows less than 41 times or more than 60 times (if out hypothesis were two-tailed - which it is not).\n\n\n\n\n\nThe Figure below shows at which point the probability of heads showing dips below 5 percent for one-tailed hypotheses. Thus, according to the figure below, if we toss a coin 100 times and head shows 59 or more often, then you are justified in claiming that I cheated.\n\n\n\n\n\nWhen comparing the two figures above, it is notable that the number at which you can claim I cheated differs according to whether the H1 as one- or two-tailed. When formulating a one-tailed hypothesis, then the number is lower compared with the the number at which you can reject the H0 if your H1 is two-tailed. This is actually the reason for why it is preferable to formulate more precise, one-tailed hypotheses (because then, it is easier for the data to be sufficient to reject the H0)."
  },
  {
    "objectID": "basicquant.html#the-normal-distribution",
    "href": "basicquant.html#the-normal-distribution",
    "title": "Basic Concepts in Quantitative Research",
    "section": "The Normal Distribution",
    "text": "The Normal Distribution\nIt is important to note here that the above described calculation of probabilities does not work for numeric variables that are interval-scaled. The reason for this is that it is not possible to calculate the probabilities for all possible outcomes of a reaction time experiment (where time is a continuous variable). In such cases, we rely on distributions in order to determine how likely or probable a certain outcome is.\nWhen relying on distributions, we determine whether a certain value falls within or outside of the area of a distribution that accounts for 5 percent of the entire area of the distribution - if it falls within the area that accounts for less than 5 percent of the total area, then the result is called statistically significant (see the normal distribution below) because the outcome is very unlikely.\nThe normal distribution (or Gaussian curve or Gaussian distribution) shown in the figure above has certain characteristics that can be derived mathematically. Some of these characteristics relate to the area of certain sections of that distribution. More generally, the normal distribution is a symmetric, bell-shaped probability distribution, used as the theoretical model for the distribution of physical and psychological variables. In fact, many variables in the real world are normally distributed: shoe sizes, IQs, sleep lengths, and many , many more.\nThe normal distribution is very important because it underlies many statistical procedures in one form or another. The normal distribution is a symmetrical, continuous distribution where\n\nthe arithmetic mean, the median, and the mode are identical and have a value of 0 (and are thus identical);\n\n50% of the area under the bell-shaped curve are smaller than the mean;\n\n50% of the area under the bell-shaped curve are bigger than the mean;\n\n50% of the area under the bell-shaped curve are within -0.675 and +0.675 standard deviations from the mean (0);\n\n95% of the area under the bell-shaped curve are within -1.96 and +1.96 standard deviations from the mean (0);\n\n99% of the area under the bell-shaped curve are within -2.576 and +2.576 standard deviations from the mean (0).\n\nThere is also a very interesting aspect to the normal distribution that relates to the means (averages) of samples drawn from any type of distribution: no matter what type of distribution we draw samples from, the distribution of the means will approximate a normal distribution. In other words, if we draw samples form a very weird looking distribution, the means of these samples will approximate a normal distribution. This fact is called the Central Limit Theorem. What makes the central limit theorem so remarkable is that this result holds no matter what shape the original population distribution may have been.\n\n\n\n\n\nAs shown above, 50 percent of the total area under the curve are to left and 50 percent of the right of the mean value. Furthermore, 68 percent of the area are within -1 and +1 standard deviations from the mean; 95 percent of the area lie between -2 and +2 standard deviations from the mean; 99.7 percent of the area lie between -3 and +3 standard deviations from the mean. Also, 5 percent of the area lie outside -1.96 and +1.96 standard deviations from the mean (if these areas are combined) (see the Figure below). This is important. because we can reject null hypotheses if the probability of them being true is lower than 5 percent. This means that we can use the normal distribution to reject null hypotheses if our dependent variable is normally distributed.\n\n\n\n\n\nFinally, 5 percent of the area lies beyond +1.68 standard deviations from the mean (see the Figure below).\n\n\n\n\n\nThese properties are extremely useful when determining the likelihood of values or outcomes that reflect certain interval-scaled variables.\n\n\n\n\nEXERCISE TIME!\n\n\n\n\n\n\n`\n\nCreate a table with the possible outcomes and probabilities of 4 coin tosses (you can consider the table showing the outcomes of three coin tosses above as a guideline).\n\n\n\nAnswer\n\n::: {.cell}\n  sum(dbinom(4, 7, 0.5))\n::: {.cell-output .cell-output-stdout} [1] 0.2734375 ::: :::\n\n\nHow likely is it for heads to show exactly 3 times when tossing a coin 7 times?\n\n\n\nAnswer\n\n::: {.cell}\n  sum(dbinom(3, 7, 0.5))\n::: {.cell-output .cell-output-stdout} [1] 0.2734375 ::: :::\n\n\nHow likely is it for heads to show exactly 2 or 5 times when tossing a coin 7 times?\n\n\n\nAnswer\n\n::: {.cell}\n  sum(dbinom(c(2, 5), 7, 0.5))\n::: {.cell-output .cell-output-stdout} [1] 0.328125 ::: :::\n\n\nHow likely is it for heads to show 5 or more times when tossing a coin 7 times?\n\n\n\nAnswer\n\n::: {.cell}\n  sum(dbinom(5:7, 7, 0.5))\n::: {.cell-output .cell-output-stdout} [1] 0.2265625 ::: :::\n\n\nHow likely is it for heads to show between 3 and 6 times when tossing a coin 7 times?\n\n\n\nAnswer\n\n::: {.cell}\n  sum(dbinom(3:6, 7, 0.5))\n::: {.cell-output .cell-output-stdout} [1] 0.765625 ::: :::\n\n\n`"
  },
  {
    "objectID": "basicquant.html#non-normality-skewness-and-kurtosis",
    "href": "basicquant.html#non-normality-skewness-and-kurtosis",
    "title": "Basic Concepts in Quantitative Research",
    "section": "Non-normality: skewness and kurtosis",
    "text": "Non-normality: skewness and kurtosis\nDepending on the phenomenon you are investigating, the distribution of that phenomenon can be distributed non-normally. Two factors causing distributions to differ from the normal distribution (that is two factors causing distributions to be non-normal) are skewness and kurtosis.\n\nSkewness\nSkewed distributions are asymmetrical and they can be positively or negatively skewed. The tail of a negatively skewed distribution points towards negative values (to the left) which means that the distribution leans towards the right (towards positive values) while the tail of a positively skewed distribution points towards positive values (to the right) which means that the distribution leans towards the left (towards negative values).\nAnother characteristic of skewed distributions is that the mean and median of a distribution differ. If the median is greater than the mean, the distribution is negatively skewed (the long tail points towards negative values). If the median is smaller than the mean, the distribution is positively skewed (the long tail points towards positive values).\nNegatively skewed\n\nTail points towards negative values (to the left)\nMedian is greater than the mean\n\nPositively skewed\n\nTail points towards positive values (to the right)\nMedian is lower than the mean\n\n\n\n\n\n\nTo show how we can calculate skewness (or if a distribution is skewed), we generate a sample of values.\n\nSampleValues <- sample(1:100, 50)\n# inspect data\nsummary(SampleValues)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   1.00   19.25   41.50   45.20   74.75   99.00 \n\n\nWe apply the skewness function from the e1071 package to the sample scores to calculated skewness. The skewness function allows to calculate skewness in three different ways:\nType 1 (This is the typical definition used in many older textbooks): \\[\\begin{equation}\ng_1 = m_3 / m_2^{(3/2)}.\n\\end{equation}\\]\nType 2 (Used in SAS and SPSS): \\[\\begin{equation}\nG_1 = g_1 * sqrt(n(n-1)) / (n-2)\n\\end{equation}\\]\nType 3 (Used in MINITAB and BMDP): \\[\\begin{equation}\nb_1 = m_3 / s^3 = g_1 ((n-1)/n)^{(3/2)}\n\\end{equation}\\]\nAll three formulas have in common that the more negative values are, the more strongly positively skewed are the data (distribution leans to the left) and the more positive the values are, the more strongly negatively skewed are the data (distribution leans to the right). Here we use the second formula (by setting type = 2) that is also used in SPSS.\n\nlibrary(e1071)\nskewness(SampleValues, type = 2)\n\n[1] 0.282253\n\n\nIf the reported skewness value is negative, then the distribution is positively skewed. If the value is positive, then the distribution is negatively skewed. If the value is lower than -1 or greater than +1, then the distribution can be assumed to be substantively skewed [3].\n\n\nKurtosis\nAnother way in which distributions can differ from the normal distribution relates to the thickness of the tails and the spikiness of the distribution. If distributions are bell-shaped like the normal distribution, they are called mesokurtic. If distributions are symmetrical but they are more spiky than the normal distribution, they are called leptokurtic. If symmetrical distributions are flatter and have bigger tails than the normal distribution, the distributions are called platykurtic.\n\n\n\n\n\nTo show how we can calculate kurtosis (or if a distribution suffers from kurtosis), we apply the kurtosis function from the e1071 package to the sample scores we generated above /when we calculated skewness).\n\nkurtosis(SampleValues)\n\n[1] -1.307106\n\n\nAs the kurtosis value is positive, the distribution is leptokurtic (if it were negative, the distribution would be platykurtic). As a rule of thumb, values greater than +1 indicate that the distribution is too peaked while values lower than –1 indicate that the distribution is substantively platykurtic [3].\nThe kurtosis score can thus be interpreted as follows:\n\nA values of 0 means that the distribution is perfectly mesokurtic with Values between -0.5 and 0.5 suggesting that the distribution is approximately mesokurtic\nValues between -0.5 and -1 mean that the distribution is moderately platykurtic with values smaller than -1 indicating that the distribution is platykurtic.\nValues between 0.5 and 1 mean that the distribution is moderately leptokurtic with values greater than 1 indicating that the distribution is leptokurtic"
  },
  {
    "objectID": "basicquant.html#the-binomial-distribution",
    "href": "basicquant.html#the-binomial-distribution",
    "title": "Basic Concepts in Quantitative Research",
    "section": "The Binomial Distribution",
    "text": "The Binomial Distribution\nA distribution which is very similar to the normal distribution is the “binomial distribution”. The binomial distribution displays the probability of binary outcomes. So, in fact, the distribution of the coin flips above represents a binomial distribution. However, the binomial distribution cane be (as above in case of the coin flips) but does not have to be symmetric (like the normal distribution).\nTo illustrate this, let us consider the following example: we are interested in how many people use discourse like in Australian English. We have analyzed a corpus, for instance the Australian component of the International Corpus of English and found that 10 percent of all speakers have used discourse like. In this example, the corpus encompasses 100 speakers (actually the the spoken section of the Australian component of the ICE represents more than 100 speakers but it will make things easier for us in this example).\nNow, given the result of our corpus analysis, we would like to know what percentage of speakers of Australian English use discourse like with a confidence of 95 percent. Given the corpus study results, we can plot the expected binomial distribution of discourse like users in Australian English.\n\n\n\n\n\nThe bar graph above shows the probability distribution of discourse like users in Australian English. The first thing we notice is that the binomial distribution is not symmetric as it is slightly left-skewed. If we would increase the number of draws or speakers in our example, the distribution would, however, approximate the normal distribution very quickly because the binomial distribution approximates the normal distribution for large N or if the probability is close to .5. Thus, it is common practice to use the normal distribution instead of the binomial distribution.\nThe bar graph below also shows the probability distribution of discourse like users in Australian English but, in addition, it is color-coded: bars within the 95 percent confidence interval are lightgray, the bars in red are outside the 95 percent confidence interval. In other words, if we repeated the corpus analysis, for instance 1000 times, on corpora which represent speakers from the same population, then 95 percent of samples would have between 1 and 8 discourse like users.\n\n\n\n\n\nIn addition to the normal distribution and the binomial distribution, there are many other distributions which underlay common procedures in quantitative analyses. For instance, the t- and the F-distribution or the Chi-distribution. However, we will not deal with these distributions here."
  },
  {
    "objectID": "basicstatz.html#preparation-and-session-set-up",
    "href": "basicstatz.html#preparation-and-session-set-up",
    "title": "Basic Inferential Statistics using R",
    "section": "Preparation and session set up",
    "text": "Preparation and session set up\nThis tutorial is based on R. If you have not installed R or are new to it, you will find an introduction to and more information how to use R here. For this tutorials, we need to install certain packages from an R library so that the scripts shown below are executed without errors. Before turning to the code below, please install the packages by running the code below this paragraph. If you have already installed the packages mentioned below, then you can skip ahead and ignore this section. To install the necessary packages, simply run the following code - it may take some time (between 1 and 5 minutes to install all of the libraries so you do not need to worry if it takes some time).\n\n# install packages\ninstall.packages(\"dplyr\") \ninstall.packages(\"ggplot2\") \ninstall.packages(\"tidyr\") \ninstall.packages(\"flextable\") \ninstall.packages(\"e1071\") \ninstall.packages(\"lawstat\") \ninstall.packages(\"fGarch\") \ninstall.packages(\"gridExtra\") \ninstall.packages(\"cfa\") \ninstall.packages(\"effectsize\")\ninstall.packages(\"report\")\n# install klippy for copy-to-clipboard button in code chunks\ninstall.packages(\"remotes\")\nremotes::install_github(\"rlesur/klippy\")\n\nNow, we load the packages that we will need in this tutorial.\n\n# load packages\nlibrary(dplyr)        # for data processing\nlibrary(ggplot2)      # for data vis\nlibrary(tidyr)        # for data transformation\nlibrary(flextable)    # for creating tables\nlibrary(e1071)        # for checking assumptions\nlibrary(lawstat)      # for statistical tests\nlibrary(fGarch)       # for statistical tests\nlibrary(gridExtra)    # for plotting\nlibrary(cfa)          # for stats\n# activate klippy for copy-to-clipboard button\nklippy::klippy()\n\n\n\n\nAlso we load some sample data sets that we will use in this tutorial.\n\n# data for indep. t-test\nitdata  <- base::readRDS(url(\"https://slcladal.github.io/data/itdata.rda\", \"rb\"))\n# data for paired t-test\nptdata  <- base::readRDS(url(\"https://slcladal.github.io/data/ptdata.rda\", \"rb\"))\n# data for fishers exact test\nfedata  <- base::readRDS(url(\"https://slcladal.github.io/data/fedata.rda\", \"rb\"))\n# data for mann-whitney u-test\nmwudata  <- base::readRDS(url(\"https://slcladal.github.io/data/mwudata.rda\", \"rb\"))\n# data for wilcox test\nuhmdata  <- base::readRDS(url(\"https://slcladal.github.io/data/uhmdata.rda\", \"rb\"))\n# data for friedmann-test\nfrdata  <- base::readRDS(url(\"https://slcladal.github.io/data/frdata.rda\", \"rb\"))\n# data for x2-test\nx2data  <- base::readRDS(url(\"https://slcladal.github.io/data/x2data.rda\", \"rb\"))\n# data for extensions of x2-test\nx2edata  <- base::readRDS(url(\"https://slcladal.github.io/data/x2edata.rda\", \"rb\"))\n# multi purpose data\nmdata <- base::readRDS(url(\"https://slcladal.github.io/data/mdata.rda\", \"rb\")) \n\nOnce you have installed R, RStudio, and once you have also initiated the session by executing the code shown above, you are good to go."
  },
  {
    "objectID": "basicstatz.html#visual-inspection-of-normality",
    "href": "basicstatz.html#visual-inspection-of-normality",
    "title": "Basic Inferential Statistics using R",
    "section": "Visual inspection of normality",
    "text": "Visual inspection of normality\nTo check if data are distributed normally, we extract a sample of words uttered my 100 men and 100 women from a sample corpus (the details of the sample corpus are represented by the mdata data set).\n\nndata <- mdata %>%\n  dplyr::rename(Gender = sex,\n                Words = word.count) %>%\n  dplyr::select(Gender, Words) %>%\n  dplyr::filter(!is.na(Words),\n                !is.na(Gender)) %>%\n  dplyr::group_by(Gender) %>%\n  dplyr::sample_n(100)\n\nThe table below shows the first ten lines of the women sample.\n\n\n\n\nGenderWordsfemale128female323female906female1,280female12female508female2,305female978female29female240\n\n\nWe can now go ahead and visualize the data to check for normality.\n\nHistograms\nThe first type of visualization we are going to use are histograms (with densities) as histograms will give us an impression about the distribution of the values. If the histograms and density functions follow a symmetric bell-shaped course, then the data are approximately normally distributed.\n\nggplot(ndata, aes(x = Words)) +\n  facet_grid(~Gender) +\n  geom_histogram(aes(y=..density..)) +\n  geom_density(aes(y=..density..), color = \"red\") +\n  theme_bw() +\n  labs(title = \"Histograms with denisty of words uttered by men and women in a sample corpus\")\n\n\n\n\nThe histograms shows that the words uttered by men and women in the sample corpus are non-normal (this means that the errors will also be distributed non-normally).\n\n\nQuantile-Quantile Plots\nQuantile-Quantile Plots (or QQ-Plots) compare the quantiles between two distributions - typically the observed distribution and an assumed distribution like the normal distribution. If the points fall on the diagonal line, then the distributions match. If the points differ from the diagonal line, then the distributions differ.\nA very clear and concise explanation of how to manually create QQ-plots is shown is this StatQuest video by Josh Starmer (I can highly recommend all of his really informative, overall fantastic, and entertaining videos!) and explained in this TowardsDataScience post.\n\nggplot(ndata, aes(sample = Words)) +\n  facet_grid(~Gender) +\n  geom_qq() +\n  geom_qq_line(color = \"red\") +\n  theme_bw() +\n  labs(title = \"QQ-plot of words uttered by men and women in a sample corpus\", x = \"\", y = \"\")\n\n\n\n\nThe deviation of the points from the line show that the data differs substantively from a normal distribution."
  },
  {
    "objectID": "basicstatz.html#statistical-measures",
    "href": "basicstatz.html#statistical-measures",
    "title": "Basic Inferential Statistics using R",
    "section": "Statistical measures",
    "text": "Statistical measures\nAnother way to test if data are distributed normally is to calculate certain parameter which tell us about the skewness (whether the distribution is asymmetrical) or the kurtosis (is the data is too spiky or too flat) of the data.\n\nSkewness\nSkewed distributions are asymmetrical and they can be positively or negatively skewed. The tail of a negatively skewed distribution points towards negative values (to the left) which means that the distribution leans towards the right (towards positive values) while the tail of a positively skewed distribution points towards positive values (to the right) which means that the distribution leans towards the left (towards negative values).\nAnother characteristic of skewed distributions is that the mean and median of a distribution differ. If the median is greater than the mean, the distribution is negatively skewed (the long tail points towards negative values). If the median is smaller than the mean, the distribution is positively skewed (the long tail points towards positive values).\nNegatively skewed\n\nTail points towards negative values (to the left)\nMedian is greater than the mean\n\nPositively skewed\n\nTail points towards positive values (to the right)\nMedian is lower than the mean\n\n\n\n\n\n\nAs we need to test skewness within groups, we start by extracting the word counts of only women and then test if the distribution of the women’s word counts are normal.\n\n# extract word counts for one group\nwords_women <- ndata %>%\n  dplyr::filter(Gender == \"female\") %>%\n  dplyr::pull(Words)\n# inspect\nhead(words_women)\n\n[1]  128  323  906 1280   12  508\n\n\nTo see if a distribution is skewed, we can use the summary function to check if the mean and the median differ.\n\nsummary(words_women)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n    0.0   127.5   411.0   491.0   683.8  2305.0 \n\n\nIn our example, the mean is larger than the median which suggests that the data are positively skewed.\nWe apply the skewness function from the e1071 package to the sample scores to calculated skewness. The skewness function allows to calculate skewness in three different ways:\nType 1 (This is the typical definition used in many older textbooks): \\[\\begin{equation}\ng_1 = m_3 / m_2^{(3/2)}.\n\\end{equation}\\]\nType 2 (Used in SAS and SPSS): \\[\\begin{equation}\nG_1 = g_1 * sqrt(n(n-1)) / (n-2)\n\\end{equation}\\]\nType 3 (Used in MINITAB and BMDP): \\[\\begin{equation}\nb_1 = m_3 / s^3 = g_1 ((n-1)/n)^{(3/2)}\n\\end{equation}\\]\nAll three formulas have in common that the more negative values are, the more strongly positively skewed are the data (distribution leans to the left) and the more positive the values are, the more strongly negatively skewed are the data (distribution leans to the right). Here we use the second formula (by setting type = 2) that is also used in SPSS.\n\nskewness(words_women, type = 2)            \n\n[1] 1.461404\nattr(,\"method\")\n[1] \"moment\"\n\n\nIf the reported skewness value is negative, then the distribution is positively skewed. If the value is positive, then the distribution is negatively skewed. If the value is lower than -1 or greater than +1, then the distribution can be assumed to be substantively skewed [1].\n\n\nKurtosis\nAnother way in which distributions can differ from the normal distribution relates to the thickness of the tails and the spikiness of the distribution. If distributions are bell-shaped like the normal distribution, they are called mesokurtic. If distributions are symmetrical but they are more spiky than the normal distribution, they are called leptokurtic. If symmetrical distributions are flatter and have bigger tails than the normal distribution, the distributions are called platykurtic.\n\n\n\n\n\nTo show how we can calculate kurtosis (or if a distribution suffers from kurtosis), we apply the kurtosis function from the e1071 package to the sample scores we generated above /when we calculated skewness).\n\nkurtosis(words_women)            \n\n[1] 2.466992\nattr(,\"method\")\n[1] \"excess\"\n\n\nAs the kurtosis value is positive, the distribution is leptokurtic (if it were negative, the distribution would be platykurtic). As a rule of thumb, values greater than +1 indicate that the distribution is too peaked while values lower than –1 indicate that the distribution is substantively platykurtic [1].\nThe kurtosis score can thus be interpreted as follows:\n\nA values of 0 means that the distribution is perfectly mesokurtic with Values between -0.5 and 0.5 suggesting that the distribution is approximately mesokurtic\nValues between -0.5 and -1 mean that the distribution is moderately platykurtic with values smaller than -1 indicating that the distribution is platykurtic.\nValues between 0.5 and 1 mean that the distribution is moderately leptokurtic with values greater than 1 indicating that the distribution is leptokurtic"
  },
  {
    "objectID": "basicstatz.html#statistical-test-of-assumptions",
    "href": "basicstatz.html#statistical-test-of-assumptions",
    "title": "Basic Inferential Statistics using R",
    "section": "Statistical test of assumptions",
    "text": "Statistical test of assumptions\nThe two most common tests to check assumptions are the Shapiro-Wilk test, which tests if the data differ significantly from a normal distribution, and the Levene’s test which tests if the variances are of two groups are approximately equal.\n\nShapiro-Wilk test\nThe Shapiro-Wilk test [cf. 2] can be used to check if the data differs significantly from a normal distribution (within groups). However, it has been shown to be too lenient when dealing with small sample sizes (below 50 to 100 cases per variable level), but too strict when dealing with larger sample sizes (500 or more cases per variable level). A such, the Shapiro-Wilk test should only be used in combination with visual inspection on the data.\nIn this example, we will only test if the words uttered by women differ significantly from a normal distribution (we use the words_women vector created above). Once we have a a vector of word counts of one group only (in this case the women’s word counts), we can continue by performing the Shapiro-Wilk test.\n\nshapiro.test(words_women)\n\n\n    Shapiro-Wilk normality test\n\ndata:  words_women\nW = 0.86145, p-value = 3.217e-08\n\n\nIf the p-value of the Shapiro-Wilk test is greater than .05, the data do not support the hypothesis that they differ from normality. In other words, if the p-value is greater than .05, we can assume that the data are approximately normally distributed.\nThe output of the Shapiro-Wilk test shown above thus indicates that our data differs significantly from normal (W = 0.7925, p < .001***). For more information about the implementation of the Shapiro-Wilk test in R, type ?shapiro.test into the console.\n\n\nLevene’s test\nThe Levene’s test [cf. 3] evaluates if the variances of two groups is approximately equal - this is referred to as homoskedasticity. This is important, because unequal variances - or heteroskedasticity - strongly suggest that there is another factor, a confound, that is not included in the model but that significantly affects the dependent variable which renders results of an analysis unreliable.\nTo implement a Levene’s test in R, we need to install and load thelawstat package.\n\nlevene.test(mdata$word.count, mdata$sex)\n\n\n    Modified robust Brown-Forsythe Levene-type test based on the absolute\n    deviations from the median\n\ndata:  mdata$word.count\nTest Statistic = 0.0050084, p-value = 0.9436\n\n\nIf the p-values of the Levene’s test is greater than .05, we can assume that the variances are approximately equal. Thus, the output of the Levene’s test shown above thus indicates that the variances of men and women in our data are approximately equal (W = 0.005, p = .9436). For more information about the implementation of the Levene’s test in R, type ?levene.test into the console."
  },
  {
    "objectID": "basicstatz.html#students-t-test",
    "href": "basicstatz.html#students-t-test",
    "title": "Basic Inferential Statistics using R",
    "section": "Student’s t-test",
    "text": "Student’s t-test\nThere are two basic types of t-tests: the dependent or paired t-test and the independent t-test. Paired t-test are used when the data points are not independent, for example, because they come form the same subjects in a pre-post test design. In contrast, Independent t-tests are used when the data points are independent and come from two different groups (e.g., from learners and native speakers or from men and women).\nThe assumptions of the Student’s t-test are that\n\nthe dependent variable is a continuous, numeric variable;\nthe independent variable is a nominal variable (two levels / groups)\nthe variances within each group are approximately normal;\nthe errors within each group are approximately normal (this implies that the distributions of the scores of each group are approximately normal).\n\nIf the variances are not normal, then this indicates that another important variable is confounding the results. In such cases, you should go back to the data and check what other variable could cause the unequal variances. If you decide to proceed with the analysis, you can switch to a Welch t-test which does not assume equal variances within each group.\n\nPaired t-test\nPaired t-tests take into account that the scores (or values) come from the same individuals in two conditions (e.g. before and after a treatment).There are two equations for the paired t-test that are used.\n\\[\\begin{equation}\nt = \\frac{\\sum D}{\\sqrt{\\frac{N \\sum D^2 - (\\sum D)^2}{N-1}}}\n\\end{equation}\\]\nor\n\\[\\begin{equation}\nt = \\frac{\\bar D}{\\frac{s_D}{\\sqrt{N}}}\n\\end{equation}\\]\nTo show how a paired t-tests works, we will test if a treatment (a teaching method) reduces the number of spelling errors in a long essay of 6 students. In a first step, we generate some data representing the errors in two essays of the same length written before and after the teaching method was used for 8 weeks.\n\nPretest <- c(78, 65, 71, 68, 76, 59)\nPosttest <- c(71, 62, 70, 60, 66, 48)\nptd <- data.frame(Pretest, Posttest)\n\nThe data look like as shown in the table below.\n\n\n\n\nPretestPosttest787165627170686076665948\n\n\nTo perform a paired t-test in R, we use the t.test function and specify the argument paired as TRUE.\n\nt.test(ptd$Pretest,\n       ptd$Posttest,\n       paired=TRUE,\n       conf.level=0.95)\n\n\n    Paired t-test\n\ndata:  ptd$Pretest and ptd$Posttest\nt = 4.1523, df = 5, p-value = 0.00889\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n  2.539479 10.793854\nsample estimates:\nmean difference \n       6.666667 \n\n\nIn addition to testing if the groups differ significantly, we also want to calculate the effect size of the difference. We can use the effectsize package to extract Cohen’s \\(d\\) which is the standard effect size measure for t-tests.\n\neffectsize::cohens_d(x = ptd$Pretest, \n                     y = ptd$Posttest,\n                     paired = TRUE)\n\nCohen's d |       95% CI\n------------------------\n1.70      | [0.41, 3.25]\n\n\nTo check if the effect is small or big - that is if a Cohen’s \\(d\\) value can be interpreted as being small or big, we can use the following overview.\n\n\n\n\nEffectSizedReferenceVery small0.01Sawilowsky (2009)Small0.20Cohen (1988)Medium0.50Cohen (1988)Large0.80Cohen (1988)Very large1.20Sawilowsky (2009)Huge2.00Sawilowsky (2009)\n\n\nThe classification combines [4] and [5]. The analysis can be summarized using the reports package [6] as follows.\n\nreport::report(t.test(ptd$Pretest,ptd$Posttest, paired=TRUE, conf.level=0.95))\n\nEffect sizes were labelled following Cohen's (1988) recommendations.\n\nThe Paired t-test testing the difference between ptd$Pretest and ptd$Posttest (mean difference = 6.67) suggests that the effect is positive, statistically significant, and large (difference = 6.67, 95% CI [2.54, 10.79], t(5) = 4.15, p = 0.009; Cohen's d = 1.70, 95% CI [0.41, 3.25])\n\n\nWe can use this output to write up a final report:\nA paired t-test test was applied to the data and it confirmed that the number of spelling errors after the 8 weeks of using the new teaching method significantly decreased (t5: 4.1523, p = .009**). The treatment had a very large, statistically significant, positive effect (Cohen’s \\(d\\) = 1.70 [CIs: 0.41, 3.25]) [cf. 4].\n\n\nIndependent t-tests\nIndependent t-tests are used very widely and they determine if the means of two groups are significantly different. As such, t-tests are used when we have a normally distributed (or parametric), numeric dependent variable and a nominal predictor variable.\n\\[\\begin{equation}\nt = \\frac{\\bar X_1 - \\bar X_2}{\\sqrt{\\frac{s^2_p}{N_1} + \\frac{s^2_p}{N_2}}}\n\\end{equation}\\]\nwhere\n\\[\\begin{equation}\ns^2_p = \\frac{(N_1 - 1)s^2_1 + (N_2 - 1)s^2_2}{N_1 + N_2 - 2}\n\\end{equation}\\]\nWe now load some data that we can apply a t-test to. The data represents scores on a proficiency test of native speakers and learners of English. We want to use a t-test to determine if the native speakers and learners differ in their proficiency.\n\n# load data\ntdata <- base::readRDS(url(\"https://slcladal.github.io/data/d03.rda\", \"rb\")) %>%\n  dplyr::rename(NativeSpeakers = 1,\n                Learners = 2) %>%\n  tidyr::gather(Group, Score, NativeSpeakers:Learners) %>%\n  dplyr::mutate(Group = factor(Group))\n\n\n\n\n\nGroupScoreNativeSpeakers6NativeSpeakers65NativeSpeakers12NativeSpeakers56NativeSpeakers45NativeSpeakers84NativeSpeakers38NativeSpeakers46NativeSpeakers64NativeSpeakers24\n\n\nWe now apply an independent t-test to the data.\n\nt.test(Score ~ Group, \n       var.equal = T,\n       data = tdata)\n\n\n    Two Sample t-test\n\ndata:  Score by Group\nt = -0.054589, df = 18, p-value = 0.9571\nalternative hypothesis: true difference in means between group Learners and group NativeSpeakers is not equal to 0\n95 percent confidence interval:\n -19.74317  18.74317\nsample estimates:\n      mean in group Learners mean in group NativeSpeakers \n                        43.5                         44.0 \n\n\nAs the p-value is higher than .05, we cannot reject the H-0- and we thus have to conclude that our evidence does not suffice to say that learners and Native Speakers differ in their proficiency. However, we still extract the effect size, again using Cohen’s \\(d\\). In contract to the extraction of the effect size for paired t-tests, however, we will set the argument paired to FALSE (in fact, we could simply leave it out as the paired = FALSE is the default).\n\neffectsize::cohens_d(tdata$Score ~ tdata$Group,\n                     paired = FALSE)\n\nCohen's d |        95% CI\n-------------------------\n-0.02     | [-0.90, 0.85]\n\n- Estimated using pooled SD.\n\n\nThe analysis can be summarized using the reports package [6] as follows.\n\nreport::report(t.test(Score ~ Group, var.equal = T, data = tdata))\n\nEffect sizes were labelled following Cohen's (1988) recommendations.\n\nThe Two Sample t-test testing the difference of Score by Group (mean in group Learners = 43.50, mean in group NativeSpeakers = 44.00) suggests that the effect is negative, statistically not significant, and very small (difference = -0.50, 95% CI [-19.74, 18.74], t(18) = -0.05, p = 0.957; Cohen's d = -0.03, 95% CI [-0.95, 0.90])\n\n\nWe can use this output to write up a final report:\nAn independent t-test test was applied to the data and it reported that the scores between the two groups did not differ significantly (t18: -0.0546, p = .9571). In addition to not differing significantly, the effect size of the difference between the groups was also very small (Cohen’s \\(d\\) = -0.03 [CIs: -0.95, 0.90]) [cf. 4]."
  },
  {
    "objectID": "basicstatz.html#fishers-exact-test",
    "href": "basicstatz.html#fishers-exact-test",
    "title": "Basic Inferential Statistics using R",
    "section": "Fisher’s Exact Test",
    "text": "Fisher’s Exact Test\nFisher’s Exact test is very useful because it does not rely on distributional assumptions relying on normality. Instead, Fisher’s Exact Test calculates the probabilities of all possible outcomes and uses these to determine significance. To understand how a Fisher’s Exact test, we will use a very simple example.\nImagine you are interested in adjective modification and you want to find out if very and truly differ in their collocational preferences. So you extract all instances of cool, all instances of very, and all instances of truly from a corpus. Now that you have gathered this data, you want to test if truly and very differ with respect to their preference to co-occur with cool. Accordingly, you tabulate the results and get the following table.\n\n\n\n\nAdverbwith coolwith other adjectivestruly540very1741\n\n\nTo perform a Fisher’s Exact test, we first create a table with these results and then use the fisher.test function to perform the Fisher’s Exact Test to see if very and truly differ in their preference to co-occur with cool (as shown below). The null hypothesis is that there is no difference between the adverbs.\n\n# create table\ncoolmx <- matrix(\n  c(5, 17, 40, 41),\n  nrow = 2, # number of rows of the table\n  # def. dimension names\n  dimnames = list(\n    Adverbs = c(\"truly\", \"very\"),\n    Adjectives = c(\"cool\", \"other adjective\"))\n)\n# perform test\nfisher.test(coolmx)\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  coolmx\np-value = 0.03024\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n 0.08015294 0.96759831\nsample estimates:\nodds ratio \n 0.3048159 \n\n\nThe results of the Fisher’s Exact test show that the p-value is lower than .05, which means we reject the null hypothesis, and we are therefore justified in assuming that very and truly differ in their collocational preferences to co-occur with cool.\nThe analysis can be summarized as follows:\nA Fisher’s Exact test was applied to the data to determine if there was a significant difference in the modification of adjective cool. Specifically, we tested if the preference of cool to be modified by very and by truly differed significantly. The results of the Fisher’s Exact test confirmed that there was a statistically significant difference in the modification preference of cool(p: .030*). However, the effect size of the preference is small (Odds Ratio: 0.305)."
  },
  {
    "objectID": "basicstatz.html#mann-whitney-u-test",
    "href": "basicstatz.html#mann-whitney-u-test",
    "title": "Basic Inferential Statistics using R",
    "section": "Mann-Whitney U-Test",
    "text": "Mann-Whitney U-Test\nIt is actually quite common that numeric depend variables need to be transformed or converted into ranks, i.e. ordinal variables, because the distribution of residuals does not allow the application of parametric tests such as t-tests or linear regression. In such cases, as we are dealing with rank (ordinal) data, the application of a chi-square test is unwarranted and we need to use another test. There are different alternatives depending on whether the data are paired (coming from the same individuals) or if all observations are independent.\nThe non-parametric alternative for independent t-tests, i.e. for data where we are dealing with two separate groups and a numeric dependent variable that violates parametric assumptions (or an ordinal dependent variable), is the Mann-Whitney U-test. In contrast, if the groups under investigation represent identical participants that are tested under two conditions, the appropriate alternative is a Wilcoxon Signed Rank test (which is thus the alternative for paired t-test).\nImagine we wanted to determine if two language families differed with respect to the size of their phoneme inventories. You have already ranked the inventory sizes and would now like to now if language family correlates with inventory size. As such, we are dealing with two independent groups and we want to implement a non-parametric alternative of a t-test. To answer this question, you create the table shown below.\n\n# create table\nRank <- c(1,3,5,6,8,9,10,11,17,19, 2,4,7,12,13,14,15,16,18,20)\nLanguageFamily <- c(rep(\"Kovati\", 10), rep(\"Urudi\", 10))\nlftb <- data.frame(LanguageFamily, Rank)\n\n\n\n\n\nLanguageFamilyRankKovati1Kovati3Kovati5Kovati6Kovati8Kovati9Kovati10Kovati11Kovati17Kovati19Urudi2Urudi4Urudi7Urudi12Urudi13Urudi14Urudi15Urudi16Urudi18Urudi20\n\n\nWe will also briefly inspect the data visually using a box plot.\n\nggplot(lftb, aes(x = LanguageFamily, y = Rank, fill = LanguageFamily)) +\n  geom_boxplot() +\n  scale_fill_manual(values = c(\"orange\", \"darkgrey\")) +\n  theme_bw() +\n  theme(legend.position = \"none\")\n\n\n\n\nTo use the Mann-Whitney U test, the dependent variable (Rank) must be ordinal and independent variable (Group) must be a binary factor. We briefly check this by inspecting the structure of the data.\n\n# inspect structure\nstr(lftb)\n\n'data.frame':   20 obs. of  2 variables:\n $ LanguageFamily: chr  \"Kovati\" \"Kovati\" \"Kovati\" \"Kovati\" ...\n $ Rank          : num  1 3 5 6 8 9 10 11 17 19 ...\n\n\nAs the variables are what we need them to be, we can now perform the Mann-Whitney U test on the table. The null hypothesis is that there is no difference between the 2 groups.\n\n# perform test\nwilcox.test(lftb$Rank ~ lftb$LanguageFamily) \n\n\n    Wilcoxon rank sum exact test\n\ndata:  lftb$Rank by lftb$LanguageFamily\nW = 34, p-value = 0.2475\nalternative hypothesis: true location shift is not equal to 0\n\n\nSince the p-value is greater than 0.05, we fail to reject the null hypothesis. The results of the Mann-Whitney U test tell us that the two language families do not differ significantly with respect to their phoneme inventory size.\nThe analysis can be summarized using the reports package [6] as follows.\n\nreport::report(wilcox.test(lftb$Rank ~ lftb$LanguageFamily))\n\nEffect sizes were labelled following Funder's (2019) recommendations.\n\nThe Wilcoxon rank sum exact test testing the difference in ranks between lftb$Rank and lftb$LanguageFamily suggests that the effect is negative, statistically not significant, and large (W = 34.00, p = 0.247; r (rank biserial) = -0.32, 95% CI [-0.69, 0.18])\n\n\nWe can use this output to write up a final report:\nThe Wilcoxon rank sum exact test suggests that there is no statistically significant relationship between the size of phoneme inventories and being member of selected language families. Despite being statistically insignificant, the effect may be large (W = 34.00, p = 0.247; r (rank biserial) = -0.32, 95% CI [-0.69, 0.18]).\nMann-Whitney U tests with continuity correction\nThe Mann-Whitney U test can also be used with continuity correction. A continuity correction is necessary when both variables represent numeric values that are non-normal. In the following example, we want to test if the reaction time for identifying a word as real is correlated with its token frequency.\nFor this example, we generate data is deliberately non-normal.\n\n\n\n\n\n\n\nFrequencyNormalizedReactionReaction20,0001,946.51210,0001,986.81566,6671,954.32105,0003,081.42164,0001,717.8853,3331,520.7362,8572,165.26722,5001,734.05122,2222,438.03422,0001,880.1180\n\n\nWhen we plot the data, we see that both the frequency of words (Frequency) and the reaction times that it took subjects to recognize the token as a word (Reaction) are non-normal (in this case, the distributions are negative skewed).\n\n\n\n\n\nBoth variables are negatively skewed (non-normally distributed) but we can use the wilcox.test function to perform the Mann-Whitney U test with continuity correction which takes the skewness into account. The null hypothesis is that there is no difference between the 2 groups. Although the output states that the test that was performed is a Wilcoxon rank sum test with continuity correction, we have actually performed a Mann-Whitney U test - this is because the nomenclature for the tests is not unanimous.\n\n# perform test\nwilcox.test(wxdata$Reaction, wxdata$Frequency) \n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  wxdata$Reaction and wxdata$Frequency\nW = 7645.5, p-value = 1.028e-10\nalternative hypothesis: true location shift is not equal to 0\n\n\nAgain, we use the reports package [6] to summarize the analysis.\n\nreport::report(wilcox.test(wxdata$Reaction, wxdata$Frequency))\n\nEffect sizes were labelled following Funder's (2019) recommendations.\n\nThe Wilcoxon rank sum test with continuity correction testing the difference in ranks between wxdata$Reaction and wxdata$Frequency suggests that the effect is positive, statistically significant, and very large (W = 7645.50, p < .001; r (rank biserial) = 0.53, 95% CI [0.40, 0.63])\n\n\nWe can use this output to write up a final report:\nThe Wilcoxon rank sum exact test suggests that there is a strong, positive, statistically significant relationship between the reaction time for identifying a word as real and its token frequency (W = 7613.50, p < .001; r (rank biserial) = 0.52, 95% CI [0.40, 0.63])."
  },
  {
    "objectID": "basicstatz.html#wilcoxon-rank-sum-test",
    "href": "basicstatz.html#wilcoxon-rank-sum-test",
    "title": "Basic Inferential Statistics using R",
    "section": "Wilcoxon rank sum test",
    "text": "Wilcoxon rank sum test\nThe Wilcoxon rank sum test is the non-parametric alternative to a paired or dependent t-test. Thus, a Wilcoxon rank sum test is used when the data represent the same individuals that were tested under two condition. To tell R that we are dealing with paired data, we have to set the argument paired to TRUE while we can still use the wilcox.test function (as we did before when using the Mann-Whitney U test which is applied to independent groups).\nIn this example, the same individuals had to read tongue twisters when they were sober and when they were intoxicated. A Wilcoxon signed rank test with continuity correction is used to test if the number of errors that occur when reading tongue twisters correlates with being sober/intoxicated. Again, we create fictitious data.\n\n# create data\nsober <- sample(0:9, 15, replace = T)\nintoxicated <-  sample(3:12, 15, replace = T) \n# tabulate data\nintoxtb <- data.frame(sober, intoxicated) \n\n\n\n\n\nsoberintoxicated912676928737124661115836120804710912\n\n\nNow, we briefly plot the data.\n\nintoxtb2 <- data.frame(c(rep(\"sober\", nrow(intoxtb)),\n                         rep(\"intoxicated\", nrow(intoxtb))),\n                       c(intoxtb$sober, intoxtb$intoxicated)) %>%\n  dplyr::rename(State = 1,\n                Errors = 2)\nggplot(intoxtb2, aes(State, Errors)) +\n  geom_boxplot(fill = c(\"orange\", \"darkgrey\"), width=0.5) +\n  labs(y = \"Number of errors\", x = \"State\") +\n  theme_bw()\n\n\n\n\nThe boxes indicate a significant difference. Finally, we perform the Wilcoxon signed rank test with continuity correction. The null hypothesis is that the two groups are the same.\n\n# perform test\nwilcox.test(intoxtb$intoxicated, intoxtb$sober, paired=T) \n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  intoxtb$intoxicated and intoxtb$sober\nV = 101, p-value = 0.02094\nalternative hypothesis: true location shift is not equal to 0\n\n\nThe p-value is lower than 0.05 (rejecting the null hypothesis) which means that the number of errors when reading tongue twisters is affected by one’s state (sober/intoxicated) - at least in this fictitious example.\nAgain, we use the reports package [6] to summarize the analysis.\n\nreport::report(wilcox.test(intoxtb$intoxicated, intoxtb$sober, paired=T) )\n\nEffect sizes were labelled following Funder's (2019) recommendations.\n\nThe Wilcoxon signed rank test with continuity correction testing the difference in ranks between intoxtb$intoxicated and intoxtb$sober suggests that the effect is positive, statistically significant, and very large (W = 101.00, p = 0.021; r (rank biserial) = 0.68, 95% CI [0.25, 0.89])\n\n\nWe can use this output to write up a final report:\nThe Wilcoxon rank sum exact test suggests that there is a very large, positive, statistically significant relationship between the number of errors produced in tongue twisters and being intoxicated (W = 6.50, p = 0.003; r (rank biserial) = -0.89, 95% CI [-0.97, -0.64])."
  },
  {
    "objectID": "basicstatz.html#kruskal-wallis-rank-sum-test",
    "href": "basicstatz.html#kruskal-wallis-rank-sum-test",
    "title": "Basic Inferential Statistics using R",
    "section": "Kruskal-Wallis Rank Sum Test",
    "text": "Kruskal-Wallis Rank Sum Test\nThe Kruskal-Wallis rank sum test is a type of ANOVA (Analysis of Variance). For this reason, the Kruskal Wallis Test is also referred to as a one-way Anova by ranks which can handle numeric and ordinal data.\nIn the example below, uhm represents the number of filled pauses in a short 5 minute interview while speaker represents whether the speaker was a native speaker or a learner of English. As before, the data is generated and thus artificial.\n\n# create data\nuhms <- c(15, 13, 10, 8, 37, 23, 31, 52, 11, 17)\nSpeaker <- c(rep(\"Learner\", 5), rep(\"NativeSpeaker\", 5))\n# create table\nuhmtb <- data.frame(Speaker, uhms)\n\n\n\n\n\nSpeakeruhmsLearner15Learner13Learner10Learner8Learner37NativeSpeaker23NativeSpeaker31NativeSpeaker52NativeSpeaker11NativeSpeaker17\n\n\nNow, we briefly plot the data.\n\nggplot(uhmtb, aes(Speaker, uhms)) +\n  geom_boxplot(fill = c(\"orange\", \"darkgrey\")) +\n  theme_bw() +\n  labs(x = \"Speaker type\", y = \"Errors\")\n\n\n\n\nNow, we test for statistical significance. The null hypothesis is that there is no difference between the groups.\n\nkruskal.test(uhmtb$Speaker~uhmtb$uhms) \n\n\n    Kruskal-Wallis rank sum test\n\ndata:  uhmtb$Speaker by uhmtb$uhms\nKruskal-Wallis chi-squared = 9, df = 9, p-value = 0.4373\n\n\nThe p-value is greater than 0.05, therefore we fail to reject the null hypothesis. The Kruskal-Wallis test does not report a significant difference for the number of uhms produced by native speakers and learners of English in the fictitious data."
  },
  {
    "objectID": "basicstatz.html#the-friedman-rank-sum-test",
    "href": "basicstatz.html#the-friedman-rank-sum-test",
    "title": "Basic Inferential Statistics using R",
    "section": "The Friedman Rank Sum Test",
    "text": "The Friedman Rank Sum Test\nThe Friedman rank sum test is also called a randomized block design and it is used when the correlation between a numeric dependent variable, a grouping factor and a blocking factor is tested. The Friedman rank sum test assumes that each combination of the grouping factor (Gender) and the blocking factor (Age) occur only once. Thus, imagine that the values of uhms represent the means of the respective groups.\n\n# create data\nuhms <- c(7.2, 9.1, 14.6, 13.8)\nGender <- c(\"Female\", \"Male\", \"Female\", \"Male\")\nAge <- c(\"Young\", \"Young\", \"Old\", \"Old\")\n# create table\nuhmtb2 <- data.frame(Gender, Age, uhms)\n\n\n\n\n\nGenderAgeuhmsFemaleYoung7.2MaleYoung9.1FemaleOld14.6MaleOld13.8\n\n\nWe now perform the Friedman rank sum test.\n\nfriedman.test(uhms ~ Age | Gender, data = uhmtb2)\n\n\n    Friedman rank sum test\n\ndata:  uhms and Age and Gender\nFriedman chi-squared = 2, df = 1, p-value = 0.1573\n\n\nIn our example, age does not affect the use of filled pauses even if we control for gender as the p-value is higher than .05."
  },
  {
    "objectID": "basicstatz.html#pearsonss-chi-square-test",
    "href": "basicstatz.html#pearsonss-chi-square-test",
    "title": "Basic Inferential Statistics using R",
    "section": "(Pearsons’s) Chi-Square Test",
    "text": "(Pearsons’s) Chi-Square Test\nOne of the most frequently used statistical test in linguistics is the \\(\\chi\\)2 test (or Pearsons’s chi-square test, chi-squared test, or chi-square test). We will use a simple, practical example to explore how this test works. In this example, we will test whether speakers of American English (AmE) and speakers of British English (BrE) differ in their use of the near-synonyms sort of and kind of as in “He’s sort of stupid” and “He’s kind of stupid”. As a first step, we formulate the hypothesis that we want to test (H1) and its null hypothesis (H0). The alternative- or test hypothesis reads:\nH1: Speakers of AmE and BrE differ with respect to their preference for sort of and kind of.\nwhile the Null Hypothesis (H0) states\nH0: Speakers of AmE and BrE do not differ with respect to their preference for sort of and kind of.\nThe H0 claims the non-existence of something (which is the more conservative position) and in our example the non-existence of a correlation between variety of English and the use of sort of and kind of. The question now arises what has to be the case in order to reject the H0 in favor of the H1.\nTo answer this question, we require information about the probability of error, i.e. the probability that the H0 does indeed hold for the entire population. Before performing the chi-square test, we follow the convention that the required significance level is 5 percent. In other words, we will reject the H0 if the likelihood for the H\\(_{0}\\) being true is less than 5 percent given the distribution of the data. In that case, i.e. in case that the likelihood for the H0 being true is less than 5 percent, we consider the result of the chi-square test as statistically significant. This means that the observed distribution makes it very unlikely that there is no correlation between the variety of English and the use of sort of and kind of.\nLet us now assume that we have performed a search for sort of and kind of in two corpora representing American and British English and that we have obtained the following frequencies:\n\n\n\n\nHedgeBrEAmEkindof181655sortof17767\n\n\nIn a first step, we now have to calculate the row and column sums of our table.\n\n\n\nHedgeBrEAmETotalkindof181655836sortof17767244Total3587221,080\n\nNext, we calculate, the values that would have expected if there was no correlation between variety of English and the use of sort of and kind of. In order to get these expected frequencies, we apply the equation below to all cells in our table.\n\\[\\begin{equation}\n\\frac{Column total*Row total}{Overall total}\n\\end{equation}\\]\nIn our example this means that for the cell with [+]BrE [+]kindof we get:\n\\[\\begin{equation}\n\\frac{836*358}{1080} = \\frac{299288}{1080} = 277.1185\n\\end{equation}\\]\nFor the entire table this means we get the following expected values:\n\n\n\nHedgeBrEAmETotalkindof277.11850558.8815836sortof80.88148163.1185244Total358.00000722.00001,080\n\nIn a next step, we calculate the contribution of each cell to the overall \\(\\chi\\)2 value (\\(\\chi\\)2 contribution). To get \\(\\chi\\)2 contribution for each cell, we apply the equation below to each cell.\n\\[\\begin{equation}\n\\frac{(observed – expected)^{2}}{expected}\n\\end{equation}\\]\nIn our example this means that for the cell with [+]BrE [+]kindof we get:\n\\[\\begin{equation}\n\\frac{(181 – 277.1185)^{2}}{277.1185} = \\frac{-96.1185^{2}}{277.1185} = \\frac{9238.766}{277.1185} = 33.33868\n\\end{equation}\\]\nFor the entire table this means we get the following \\(\\chi^{2}\\) values:\n\n\n\nHedgeBrEAmETotalkindof33.3386916.5308249.86951sortof114.2260256.63839170.86440Total147.5647073.16921220.73390\n\nThe sum of \\(\\chi\\)2 contributions in our example is 220.7339. To see if this value is statistically significant, we need to calculate the degrees of freedom because the \\(\\chi\\) distribution differs across degrees of freedom. Degrees of freedom are calculated according to the equation below.\n\\[\\begin{equation}\nDF = (rows -1) * (columns – 1) = (2-1) * (2-1) = 1 * 1 = 1\n\\end{equation}\\]\nIn a last step, we check whether the \\(\\chi\\)2 value that we have calculated is higher than a critical value (in which case the correlation in our table is significant). Degrees of freedom are relevant here because the critical values are dependent upon the degrees of freedom: the more degrees of freedom, the higher the critical value, i.e. the harder it is to breach the level of significance.\nSince there is only 1 degree of freedom in our case, we need to consider only the first column in the table of critical values below.\n\n\n\nDFp<.05p<.01p<.00113.846.6410.8325.999.2113.8237.8211.3516.2749.4913.2818.47511.0715.0920.52\n\nSince the \\(\\chi\\)2 value that we have calculated is much higher than the critical value provided for p<.05, we can reject the H0 and may now claim that speakers of AmE and BrE differ with respect to their preference for sort of and kind of.\nBefore we summarize the results, we will calculate the effect size which is a measure for how strong the correlations are.\n\nEffect Sizes in Chi-Square\nEffect sizes are important because they correlations may be highly significant but the effect between variables can be extremely weak. The effect size is therefore a measure how strong the correlation or the explanatory and predictive power between variables is.\nThe effect size measure for \\(\\chi\\)2 tests can be either the \\(\\phi\\)-coefficient (phi-coefficient) or Cramer’s \\(\\phi\\) (Cramer’s phi). The \\(\\phi\\)-coefficient is used when dealing with 2x2 tables while Cramer’s \\(\\phi\\) is used when dealing with tables with more than 4 cells. The \\(\\phi\\) coefficient can be calculated by using the equation below (N = overall sample size).\n\\[\\begin{equation}\n\\phi = \\sqrt{\\frac{\\chi^{2}}{N}}\n\\end{equation}\\]\nIn our case, this means:\n\\[\\begin{equation}\n\\phi = \\sqrt{\\frac{220.7339}{1080}} = \\sqrt{0.2043832} = 0.4520876\n\\end{equation}\\]\nThe \\(\\phi\\) coefficient varies between 0 (no effect) and 1 (perfect correlation). For the division into weak, moderate and strong effects one can follow the division for \\(\\omega\\) (small omega), so that with values beginning with .1 represent weak, values between 0.3 and .5 represent moderate and values above .5 represent strong effects [7]. So, in this example we are dealing with a medium-sized effect/correlation.\n\n\nChi-Square in R\nBefore we summarize the results, we will see how to perform a chi-square test in R. In addition to what we have done above, we will also visualize the data. To begin with, we will have a look at the data set (which is the same data we have used above).\n\n# inspect data\nchidata %>%\n  as.data.frame() %>%\n  tibble::rownames_to_column(\"Hedge\") %>%\n  flextable() %>%\n  flextable::set_table_properties(width = .75, layout = \"autofit\") %>%\n  flextable::theme_zebra() %>%\n  flextable::fontsize(size = 12) %>%\n  flextable::fontsize(size = 12, part = \"header\") %>%\n  flextable::align_text_col(align = \"center\") %>%\n  flextable::border_outer()\n\n\n\nHedgeBrEAmEkindof181655sortof17767\n\n\nWe will now visualize the data with an association. Bars above the dashed line indicate that a feature combination occurs more frequently than expected by chance. The width of the bars indicates the frequency of the feature combination.\n\n\n\n\n\nThe fact that the bars are distributed complimentary (top left red and below bar; top right black above bar; bottom left black above bar; bottom right red below bar) indicates that the use of sort of and kind of differs across AmE and BrE. We will check whether the mosaic plot confirms this impression.\n\nmosaicplot(chidata, shade = TRUE, type = \"pearson\", main = \"\")  # mosaic plot\n\n\n\n\nThe color contrasts in the mosaic plot substantiate the impression that the two varieties of English differ significantly. To ascertain whether the differences are statistically significant, we can now apply the chi-square test.\n\n# perform chi square test without Yate's correction\nchisq.test(chidata, corr = F)  \n\n\n    Pearson's Chi-squared test\n\ndata:  chidata\nX-squared = 220.73, df = 1, p-value < 2.2e-16\n\n\nThe results reported by R are identical to the results we derived by hand and confirm that BrE and AmE differ significantly in their use of sort of and kind of.\n\n\n\n\nNOTE\n\nWhen conducting a \\(\\chi\\)2 test of independence and in order to interpret the Pearson’s \\(\\chi\\)2 statistic, by default, most of the statistical packages, including R, assume that the observed frequencies in a contingency table can be approximated by the continuous \\(\\chi\\)2 distribution.  To avoid-reduce the error that the approximation introduces, the chisq.test function in base R includes the correct argument that, by default, is set to TRUE. This argument integrates the Frank Yate’s adjustment that aims at compensating for deviations from the (smooth) theoretical chi-squared distribution and it is considered especially useful if the frequency in each cell is less than a small number. Some statisticians set this number to 5 and others to 10. Although this case scenario is relevant to linguistic data, this is not always the case. Moreover, there is a strong tendency in other fields to avoid the Yates’ continuity correction altogether due to the overestimated amount of adjustment that it introduces. That is why, it is helpful to know how to avoid the Yates’ continuity correction. To do that, you simply set the correct argument to FALSE, as follows (see also above):\n\n# X2-test without Yate's correction\nchisq.test(chidata, correct=FALSE)\n\nAlso, in case you would like to deactivate the scientific notation used for displaying the p-value, as it is displayed in the output of the chisq.test function, you can do the following:\n\nformat(chisq.test(chidata)$p.value, scientific=FALSE)\n\n\n\n\n\n\n\nIn a next step, we calculate the effect size.\n\n# calculate effect size\nsqrt(chisq.test(chidata, corr = F)$statistic / sum(chidata) * (min(dim(chidata))-1))\n\nX-squared \n0.4520877 \n\n\nThe \\(\\phi\\) coefficient of .45 shows that variety of English correlates moderately with the use of sort of and kind of. We will now summarize the results.\n\n\nSummarizing Chi-Square Results\nThe results of our analysis can be summarized as follows: A \\(\\chi\\)2-test confirms a highly significant correlation of moderate size between the variety of English and the use of the near-synonymous hedges sort of and kind of (\\(\\chi\\)2 = 220.73, df = 1, p < .001***, \\(\\phi\\) = .452).\n\n\nRequirements of Chi-Square\nChi-square tests depend on certain requirements that, if violated, negatively affect the reliability of the results of the test. To provide reliable results, 80 percent of cells in a table to which the chi-square test is applied have to have expected values of 5 or higher and at most 20 percent of expected values can be smaller than 5 [see 8]. In addition, none of the expected values can be smaller than 1 [see 8] because then, the estimation, which relies on the \\(\\chi\\)2-distribution, becomes too imprecise to allow meaningful inferences [9].\nIf these requirements are violated, then the Fisher’s Exact Test is more reliable and offers the additional advantage that these tests can also be applied to data that represent very small sample sizes. When applying the Fisher’s Exact Test, the probabilities for all possible outcomes are calculated and the summed probability for the observed or more extreme results are determined. If this sum of probabilities exceeds five percent, then the result is deemed statistically significant.\n\n\n\n\nEXERCISE TIME!\n\n\n\n\n\n\n`\n\nImagine you are interested in whether older or younger speakers tend to refer to themselves linguistically. The underlying hypothesis is that - contrary to common belief - older people are more narcissistic compared with younger people. Given this research question, perform a chi-square test and summarize the results on the data below.\n\n\n\n\n\nAge1SGPNPN without 1SGTotalYoung6143104Old423678Total10379182\n\n\n\n\nAnswer\n\n::: {.cell}\n  # generate data\n  ex1dat <- matrix(c(61, 43,  42, 36), ncol = 2, byrow = T)\n  # add column names\n  colnames(ex1dat) <- c(\"1SGPN\", \"PN without 1SG\")\n  # perform x2-test\n  x2_ex1 <- chisq.test(ex1dat)\n  # inspect results\n  x2_ex1\n::: {.cell-output .cell-output-stdout} ```\nPearson’s Chi-squared test with Yates’ continuity correction\ndata: ex1dat X-squared = 0.2465, df = 1, p-value = 0.6195 ``` ::: :::\nGiven the data, we cannot reject the H0 according to which old people are more narcissistic compared to young people measured by their use of 1st person pronouns in conversation (\\(\\chi\\)~{1}: 0.2465026, p: 0.6195485).\n\n\nImagine you are interested in whether young men or young women exhibit a preference for the word whatever because you have made the unsystematic, anecdotal observation that young men use this word more frequently than young women. Given this research question, perform a chi-square test and summarize the results on the data below.\n\n\n\n\n\nItemYoungMalesYoungFemalesTotalwhatever175571other words3451289165521261680Total3451459166071261752\n\n\n\n\nAnswer\n\n::: {.cell}\n  # generate data\n  ex2dat <- matrix(c(17, 55,  345128, 916552), ncol = 2, byrow = T)\n  # add column names\n  colnames(ex2dat) <- c(\"YoungMales\", \"YoungFemales\")\n  # perform x2-test\n  x2_ex2 <- chisq.test(ex2dat)\n  # inspect results\n  x2_ex2\n::: {.cell-output .cell-output-stdout} ```\nPearson’s Chi-squared test with Yates’ continuity correction\ndata: ex2dat X-squared = 0.33682, df = 1, p-value = 0.5617 ``` ::: :::\nGiven the data, we cannot reject the H0 according to which there is no difference between young men and young women in their use of the word whatever (\\(\\chi\\)~{1}: 0.3368202, p: 0.5616705).\n\n\nFind a partner and discuss the relationship between significance and effect size. Then, go and find another partner and discuss problems that may arise when testing the frequency of certain words compared with the overall frequency of words in a corpus.\n\n\n`"
  },
  {
    "objectID": "basicstatz.html#extensions-of-chi-square",
    "href": "basicstatz.html#extensions-of-chi-square",
    "title": "Basic Inferential Statistics using R",
    "section": "Extensions of Chi-Square",
    "text": "Extensions of Chi-Square\nIn the following, we will have a look at tests and methods that can be used if the requirements for ordinary (Pearson’s) chi-square tests are violated and their use would be inappropriate\n\nThe Yates-Correction\nIf all requirements for ordinary chi-square tests are acceptable and only the sample size is the issue, then applying a so-called Yates-correction may be appropriate. This type of correction is applied in cases where the overall sample size lies in-between 60 and 15 cases [8]. The difference between the ordinary chi-square and a Yates-corrected chi-square lies in the fact that the Yates-corrected chi-square is calculated according to the equation below.\n\\[\\begin{equation}\n\\frac{(|observed – expected|-0.5)^{2}}{expected}\n\\end{equation}\\]\nAccording to this formula, we would get the values shown below rather than the values tabulated above. It is important to note here that this is only a demonstration because a Yates-Correction would actually be inappropriate as our sample size exceeds 60 cases.\n\n\n\n\nVariantBrEAmETotalkind of32.9927113.0407146.0335sort of16.359356.050772.41Total49.352169.0914218.4434\n\n\nIf the Yates-correction were applied, then this results in a slightly lower \\(\\chi\\)2-value and thus in more conservative results compared with the traditional test according to Pearson.\n\n\nChi-Square within 2-by-k tables\nAlthough the \\(\\chi\\)2-test is widely used, it is often used inappropriately. This is especially the case when chi-square tests are applied to data representing tables with more than two rows and more than two columns. It is important to note that applying the common Pearson’s’ chi-square test to sub-tables of a larger table is inappropriate because, in such cases, a modified variant of Pearson’s’ chi-square test is warranted. We will go through two examples that represent different scenarios where we are dealing with sub-samples of larger tables and a modified version of the \\(\\chi\\)2-test should be used rather than Pearson’s’ chi-square.\nIn this first example, we are dealing with a table consisting of two columns and multiple rows, a so-called 2*k table (two-by-k table). In order to test if a feature combination, that is represented by a row in the 2*k table, is significantly more common compared with other feature combinations, we need to implement the \\(\\chi\\)2-equation from [8].\nIn this example, we want to find out whether soft and hard X-rays differ in their effect on grasshopper larva. The question is whether the larva reach or do not reach a certain life cycle depending on whether they are exposed to soft X-rays, hard X-rays, light, or beta rays. The data for this example is provided below.\n\n\n\nTreatmentMitosis not reachedMitosis reachedTotalX-ray soft211435X-ray hard181331Beta-rays241236Light133043Total7669145\n\nIf we would apply an ordinary chi-square test, we would ignore that all data were collected together and using only a sub-sample would ignore the data set of which the sub-sample is part of. In other words, the sub-sample is not independent from the other data (as it represents a subsection of the whole data set). However, for exemplary reasons, we will apply an ordinary chi-square test first and then compare its results to results provided by the correct version of the chi-square test. In a first step, we create a table with all the data.\n\n# create data\nwholetable <- matrix(c(21, 14, 18, 13, 24, 12, 13, 30), byrow = T, nrow = 4)\ncolnames(wholetable) <- c(\"reached\", \"notreached\")           # add column names\nrownames(wholetable) <- c(\"rsoft\", \"rhard\", \"beta\", \"light\") # add row names\n\n\n\n\n\nTreatmentreachednotreachedrsoft2114rhard1813beta2412light1330\n\n\nNow, we extract the sub-sample from the data.\n\nsubtable <- wholetable[1:2,] # extract subtable\n\n\n\n\n\nTreatmentreachednotreachedrsoft2114rhard1813\n\n\nNext, we apply the ordinary chi-square test to the sub-sample.\n\n# simple x2-test\nchisq.test(subtable, corr = F)\n\n\n    Pearson's Chi-squared test\n\ndata:  subtable\nX-squared = 0.025476, df = 1, p-value = 0.8732\n\n\nFinally, we perform the correct chi-square test.\n\n# load function for correct chi-square\nsource(\"https://slcladal.github.io/rscripts/x2.2k.r\") \nx2.2k(wholetable, 1, 2)\n\n$Description\n[1] \"rsoft  against  rhard  by  reached  vs  notreached\"\n\n$`Chi-Squared`\n[1] 0.025\n\n$df\n[1] 1\n\n$`p-value`\n[1] 0.8744\n\n$Phi\n[1] 0.013\n\n$Report\n[1] \"Conclusion: the null hypothesis cannot be rejected! Results are not significant!\"\n\n\nBelow is a table comparing the results of the two chi-square tests.\n\n\n\n\nStatisticchi-squarechi-square in 2*k-tableschi-squared0.02550.025p-value0.87320.8744\n\n\nThe comparison shows that, in this example, the results of the two tests are very similar but this may not always be the case.\n\n\nChi-Square within z-by-k tables\nAnother application in which the \\(\\chi\\)2 test is often applied incorrectly is when ordinary Parsons’s \\(\\chi\\)2 tests are used to test portions of tables with more than two rows and more than two columns, that is z*k tables (z: row, k: column). An example is discussed by [10] who also wrote the R Script for the correct version of the \\(\\chi\\)2 test.\nLet’s first load the data discussed in the example of [10] 9. The example deals with metaphors across registers. Based on a larger table, a \\(\\chi\\)2 confirmed that registers differ with respect to the frequency of EMOTION metaphors. The more refined question is whether the use of the metaphors EMOTION IS LIGHT and EMOTION IS A FORCE OF NATURE differs between spoken conversation and fiction.\n\n# create table\nwholetable <- matrix(c(8, 31, 44, 36, 5, 14, 25, 38, 4, 22, 17, 12, 8, 11, 16, 24), ncol=4)\nattr(wholetable, \"dimnames\")<-list(Register=c(\"acad\", \"spoken\", \"fiction\", \"new\"),\nMetaphor = c(\"Heated fluid\", \"Light\", \"NatForce\", \"Other\"))\n\nBased on the table above, we can extract the following sub-table.\n\n\n\nRegisterHeated fluidLightNatForceOtheracad8548spoken31142211fiction44251716new36381224\n\nIf we used an ordinary Pearson’s \\(\\chi\\)2 test (the use of which would be inappropriate here), it would reveal that spoken conversations do not differ significantly from fiction in their use of EMOTION IS LIGHT and EMOTION IS A FORCE OF NATURE (\\(\\chi\\)2=3.3016, df=1, p=.069, \\(\\phi\\)=.2057).\n\n# create table\nsubtable <- matrix(c(14, 25, 22, 17), ncol=2)\nchisq.results <- chisq.test(subtable, correct=FALSE) # WRONG!\nphi.coefficient = sqrt(chisq.results$statistic / sum(subtable) * (min(dim(subtable))-1))\nchisq.results\n\n\n    Pearson's Chi-squared test\n\ndata:  subtable\nX-squared = 3.3016, df = 1, p-value = 0.06921\n\nphi.coefficient\n\nX-squared \n0.2057378 \n\n\nThe correct analysis takes into account that it is a sub-table that is not independent of the overall table. This means that the correct analysis should take into account the total number of cases, as well as the row and column totals [cf. 8].\nIn order to perform the correct analysis, we must either implement the equation proposed in [8] or read in the function written by [10] and apply it to the sub-table.\n\n# load function for chi square test for subtables\nsource(\"https://slcladal.github.io/rscripts/sub.table.r\") \n# apply test\nresults <- sub.table(wholetable, 2:3, 2:3, out=\"short\")\n# inspect results\nresults\n\n$`Whole table`\n         Metaphor\nRegister  Heated fluid Light NatForce Other Sum\n  acad               8     5        4     8  25\n  spoken            31    14       22    11  78\n  fiction           44    25       17    16 102\n  new               36    38       12    24 110\n  Sum              119    82       55    59 315\n\n$`Sub-table`\n         Metaphor\nRegister  Light NatForce Sum\n  spoken     14       22  36\n  fiction    25       17  42\n  Sum        39       39  78\n\n$`Chi-square tests`\n                                  Chi-square Df    p-value\nCells of sub-table to whole table  7.2682190  3 0.06382273\nRows (within sub-table)            0.2526975  1 0.61518204\nColumns (within sub-table)         3.1519956  1 0.07583417\nContingency (within sub-table)     3.8635259  1 0.04934652\n\n\nThe results show that the difference is, in fact, statistically significant at an \\(\\alpha\\)-level of .05 (\\(\\chi^{2}\\)=3.864, df=1, p<.05*)."
  },
  {
    "objectID": "basicstatz.html#configural-frequency-analysis-cfa",
    "href": "basicstatz.html#configural-frequency-analysis-cfa",
    "title": "Basic Inferential Statistics using R",
    "section": "Configural Frequency Analysis (CFA)",
    "text": "Configural Frequency Analysis (CFA)\nConfigural Frequency Analysis (CFA) is a multivariate extension of the \\(\\chi^{2}\\)-test. If we perform a \\(\\chi^{2}\\)-test on a table with more than 2 rows and 2 columns, the test tells us that somewhere in that table there is at least one cell tat differs significantly from the expected value. However, we do not know which cell or cells this is. To determine which of the cells differ significantly from the expected value, we use the CFA.\nTo perform a CFA in R, we need to load the cfa package and load some data (as shown below).\n\n# load package\nlibrary(cfa)\n# load data\ncfadata  <- base::readRDS(url(\"https://slcladal.github.io/data/cfd.rda\", \"rb\"))\n\n\n\n\n\nVarietyAgeGenderClassFrequencyAmericanOldManMiddle7BritishOldWomanMiddle9BritishOldManMiddle6AmericanOldWomanWorking2AmericanOldManMiddle5BritishOldManMiddle8AmericanOldManMiddle8BritishOldManMiddle6BritishOldManMiddle3AmericanOldWomanMiddle8\n\n\nIn a next step, we define the configurations and separate them from the counts. The configurations are the independent variables in this design and the counts represent the dependent variable.\n\n# define configurations\nconfigs <- cfadata %>%\n  dplyr::select(Variety, Age, Gender, Class)\n# define counts\ncounts <- cfadata$Frequency\n\nNow that configurations and counts are separated, we can perform the configural frequency analysis.\n\n# perform cfa\ncfa(configs,counts)\n\n\n*** Analysis of configuration frequencies (CFA) ***\n\n                         label   n   expected            Q       chisq\n1     American Old Man Working   9  17.269530 0.0074991397 3.959871781\n2    American Young Man Middle  20  13.322419 0.0060338993 3.346996519\n3    British Old Woman Working  33  24.277715 0.0079603059 3.133665860\n4   British Young Woman Middle  12  18.728819 0.0061100471 2.417504403\n5  American Young Woman Middle  10   6.362422 0.0032663933 2.079707490\n6      British Old Man Working  59  50.835658 0.0076361897 1.311214959\n7     British Young Man Middle  44  39.216698 0.0044257736 0.583424432\n8    American Old Woman Middle  81  76.497023 0.0043152503 0.265066491\n9     British Old Woman Middle 218 225.181379 0.0080255135 0.229025170\n10     American Old Man Middle 156 160.178850 0.0043537801 0.109020569\n11  American Old Woman Working   8   8.247454 0.0002225797 0.007424506\n12      British Old Man Middle 470 471.512390 0.0023321805 0.004851037\n      p.chisq sig.chisq          z        p.z sig.z\n1  0.04659725     FALSE -2.1267203 0.98327834 FALSE\n2  0.06732776     FALSE  1.7026500 0.04431680 FALSE\n3  0.07669111     FALSE  1.6871254 0.04578962 FALSE\n4  0.11998594     FALSE -1.6845116 0.95395858 FALSE\n5  0.14926878     FALSE  1.2474422 0.10611771 FALSE\n6  0.25217480     FALSE  1.1002146 0.13561931 FALSE\n7  0.44497317     FALSE  0.6962784 0.24312726 FALSE\n8  0.60666058     FALSE  0.4741578 0.31769368 FALSE\n9  0.63224759     FALSE -0.5726832 0.71657040 FALSE\n10 0.74126197     FALSE -0.3993470 0.65518123 FALSE\n11 0.93133480     FALSE -0.2612337 0.60304386 FALSE\n12 0.94447273     FALSE -0.1217934 0.54846869 FALSE\n\n\nSummary statistics:\n\nTotal Chi squared         =  17.44777 \nTotal degrees of freedom  =  11 \np                         =  2.9531e-05 \nSum of counts             =  1120 \n\nLevels:\n\nVariety     Age  Gender   Class \n      2       2       2       2 \n\n\nThe output table contains the configurations (here called lables), then the observed frequencies (the counts) and the expected frequencies, the Q and the \\(\\chi^{2}\\) statistic as well as the p-value associated with the \\(\\chi^{2}\\) value. In addition, the table provides the z-transformed \\(\\chi^{2}\\)-values and their p-value.\nIf a p-value is below the level of significance - typically below .05 - the configuration occurs with a frequency that differs significantly from the expected frequency. If the observed value is higher than the expected value, then the configuration occurs significantly more frequently than would be expected by chance. If the observed value is lower than the expected value, then the configuration occurs significantly less frequently than would be expected by chance."
  },
  {
    "objectID": "basicstatz.html#hierarchical-configural-frequency-analysis-hcfa",
    "href": "basicstatz.html#hierarchical-configural-frequency-analysis-hcfa",
    "title": "Basic Inferential Statistics using R",
    "section": "Hierarchical Configural Frequency Analysis (HCFA)",
    "text": "Hierarchical Configural Frequency Analysis (HCFA)\nA hierarchical alternative to CFA is Hierarchical Configural Frequency Analysis (HCFA). In contrast to CFA, in HCFA, the data is assumed to be nested! We begin by defining the configurations and separate them from the counts.\n\n# define configurations\nconfigs <- cfadata %>%\n  dplyr::select(Variety, Age, Gender, Class)\n# define counts\ncounts <- cfadata$Frequency\n\nNow that configurations and counts are separated, we can perform the hierarchical configural frequency analysis.\n\n# perform cfa\nhcfa(configs,counts)\n\n\n*** Hierarchical CFA ***\n\n                     Overall chi squared df          p order\nVariety Age Class              12.218696  4 0.01579696     3\nVariety Gender Class            8.773578  4 0.06701496     3\nVariety Age Gender              7.974102  4 0.09253149     3\nVariety Class                   6.078225  1 0.01368582     2\nVariety Class                   6.078225  1 0.01368582     2\nAge Gender Class                5.164357  4 0.27084537     3\nVariety Age                     4.466643  1 0.03456284     2\nVariety Age                     4.466643  1 0.03456284     2\nAge Gender                      1.934543  1 0.16426233     2\nAge Gender                      1.934543  1 0.16426233     2\nAge Class                       1.673538  1 0.19578534     2\nAge Class                       1.673538  1 0.19578534     2\nGender Class                    1.546666  1 0.21362833     2\nGender Class                    1.546666  1 0.21362833     2\nVariety Gender                  1.120155  1 0.28988518     2\nVariety Gender                  1.120155  1 0.28988518     2\n\n\nAccording to the HCFA, only a single configuration (Variety : Age : Class) is significant (X2 = 12.21, p = .016)."
  },
  {
    "objectID": "clust.html#underlying-concepts",
    "href": "clust.html#underlying-concepts",
    "title": "Cluster and Correspondence Analysis in R",
    "section": "Underlying Concepts",
    "text": "Underlying Concepts\nThe next section focuses on the basic idea that underlies all cluster analyses. WE will have a look at some very basic examples to highlight and discuss the principles that cluster analyses rely on.\nThe underlying idea of cluster analysis is very simple and rather intuitive as we ourselves perform cluster analyses every day in our lives. This is so because we group things together under certain labels and into concepts. The first example used to show this, deals with types of trees and how we group these types of trees based on their outward appearance.\nImagine you see six trees representing different types of trees: a pine tree, a fir tree, an oak tree, a beech tree, a phoenix palm tree, and a nikau palm tree. Now, you were asked to group these trees according to similarity. Have a look at the plot below and see whether you would have come up with a similar type of grouping.\n\n\n\nAn alternative way to group the trees would be the following.\n\n\n\nIn this display, conifers and broad-leaf trees are grouped together because there are more similar to each other compared to palm trees. This poses the question of what is meant by similarity. Consider the display below.\n\n\n\nAre the red and the blue line more similar because they have the same shape or are the red and the black line more similar because they are closer together? There is no single correct answer here. Rather the plot intends to raise awareness about the fact that how cluster analyses group data depends on how similarity is defined in the respective algorithm.\nLet’s consider another example to better understand how cluster analyses determine which data points should be merged when. Imagine you have five students and want to group them together based on their overall performance in school. The data that you rely on are their grades in math, music, and biology (with 1 being the best grade and 6 being the worst).\n\n\n\n\nSample of five students and their grades in math, music, and biology.\n\n\nStudentMathMusicBiologyStudentA232StudentB132StudentC121StudentD244StudentE343\n\nThe first step in determining the similarity among students is to create a distance matrix.\ndiststudents <- dist(students, method = \"manhattan\") # create a distance matrix\nThe distance matrix below shows that Student A and Student B only differ by one grade. Student B and Student C differ by 2 grades. Student A and Student C differ by 3 grades and so on.\n\n\n\n\nDistance matrix based of students based on grades in math, music, and biology.\n\n\nStudentStudentAStudentBStudentCStudentDStudentB1StudentC32StudentD346StudentE3462\n\nBased on this distance matrix, we can now implement a cluster analysis in R."
  },
  {
    "objectID": "clust.html#cluster-analysis-on-numeric-data",
    "href": "clust.html#cluster-analysis-on-numeric-data",
    "title": "Cluster and Correspondence Analysis in R",
    "section": "Cluster Analysis on Numeric Data",
    "text": "Cluster Analysis on Numeric Data\nTo create a simple cluster object in R, we use the hclust function from the cluster package. The resulting object is then plotted to create a dendrogram which shows how students have been amalgamated (combined) by the clustering algorithm (which, in the present case, is called ward.D).\n# create hierarchical cluster object with ward.D as linkage method\nclusterstudents <- hclust(diststudents, method=\"ward.D\")\n# plot result as dendrogram\nplot(clusterstudents, hang = 0)\n\n\n\nLet us have a look at how the clustering algorithm has amalgamated the students. The amalgamation process takes the distance matrix from above as a starting point and, in a first step, has merged Student A and Student B (because they were the most similar students in the data based on the distance matrix). After collapsing Student A and Student B, the resulting distance matrix looks like the distance matrix below (notice that Student A and Student B now form a cluster that is represented by the means of the grades of the two students).\n\nstudents2 <- matrix(c(1.5, 3, 2, 1,  2,  1, 2,  4,  4, 3,  4,  3),\n  nrow = 4, byrow = T)\nstudents2 <- as.data.frame(students2)\nrownames(students2) <- c(\"Cluster1\", \"StudentC\", \"StudentD\", \"StudentE\")\ndiststudents2 <- dist(students2, method = \"manhattan\")\n\n\n\n\n\nDistance matrix of students based on grades in math, music, and biology.\n\n\nStudentCluster 1Student CStudent DStudent C2.5Student D3.56.0Student E3.56.02.0\n\nThe next lowest distance now is 2.0 between Student D and Student E which means that these two students are merged next. The resulting distance matrix is shown below.\n\nstudents3 <- matrix(c(1.5,3,2,1,2,1,2.5,4,3.5),\n                    nrow = 3, byrow = T)\nstudents3 <- as.data.frame(students3)\nrownames(students3) <- c(\"Cluster1\", \"StudentC\", \"Cluster2\")\ndiststudents3 <- dist(students3, \n                      method = \"manhattan\")\n\n\n\n\n\n\nDistance matrix based of students based on grades in math, music, and biology.\n\n\nStudentCluster 1Student CStudent C2.5Cluster 23.56.0\n\n\nNow, the lowest distance value occurs between Cluster 1 and Student C. Thus, Student C and Cluster 1 are merged. In the final step, the Cluster 2 is merged with the new cluster encompassing Student C and Cluster 1. This amalgamation process can then be displayed visually as a dendrogram (see above).\nHow and which elements are merged depends on the what is understood as distance. Since “distance” is such an important concept in cluster analyses, we will briefly discuss this notion to understand why there are so many different types of clustering algorithms and this cluster analyses."
  },
  {
    "objectID": "clust.html#distance-and-similarity-measures",
    "href": "clust.html#distance-and-similarity-measures",
    "title": "Cluster and Correspondence Analysis in R",
    "section": "Distance and Similarity Measures",
    "text": "Distance and Similarity Measures\nTo understand how a cluster analysis determines to which cluster a given data point belongs, we need to understand what different distance measures represent. Have a look at the Figure below which visually represents three different ways to conceptualize distance.\n\n\n\n\n\nThe Figure above depicts three ways to measure distance: the euclidean distance represents the distance between points as the hypotenuse of the x- and y-axis distances while the “maximum distance” represents distance as the longer distance of either the distance on the x- or the y-axis. The manhatten distance (or block distance) is the sum of the distances on the x- and the y-axis.\nWe will now turn to another example in order to delve a little deeper into how clustering algorithms work. In this example, we will find cluster of varieties of English based on the relative frequency of selected non-standard features (such as the relative frequencies of cleft constructions and tag questions). As a first step, we generate some fictional data set for this analysis.\n\n# generate data\nIrishEnglish <- round(sqrt((rnorm(10, 9.5, .5))^2), 3)\nScottishEnglish <- round(sqrt((rnorm(10, 9.3, .4))^2), 3)\nBritishEnglish <- round(sqrt((rnorm(10, 6.4, .7))^2), 3)\nAustralianEnglish <- round(sqrt((rnorm(10, 6.6, .5))^2), 3)\nNewZealandEnglish <- round(sqrt((rnorm(10, 6.5, .4))^2), 3)\nAmericanEnglish <- round(sqrt((rnorm(10, 4.6, .8))^2), 3)\nCanadianEnglish <- round(sqrt((rnorm(10, 4.5, .7))^2), 3)\nJamaicanEnglish <- round(sqrt((rnorm(10, 1.4, .2))^2), 3)\nPhillipineEnglish <- round(sqrt((rnorm(10, 1.5, .4))^2), 3)\nIndianEnglish <- round(sqrt((rnorm(10, 1.3, .5))^2), 3)\nclus <- data.frame(IrishEnglish, ScottishEnglish, BritishEnglish, \n                   AustralianEnglish, NewZealandEnglish, AmericanEnglish, \n                   CanadianEnglish, JamaicanEnglish, PhillipineEnglish, IndianEnglish)\n# add row names\nrownames(clus) <- c(\"nae_neg\", \"like\", \"clefts\", \"tags\", \"youse\", \"soitwas\", \n                    \"dt\", \"nsr\", \"invartag\", \"wh_cleft\")\n\n\n\n\n\n\nFrequencies of on-standard features across selected varieties of English.\n\n\nFeatureIrishEnglishScottishEnglishBritishEnglishAustralianEnglishNewZealandEnglishAmericanEnglishCanadianEnglishJamaicanEnglishPhillipineEnglishIndianEnglishnae_neg9.1188.3925.5757.1705.7944.6154.7131.1841.2920.554like9.1619.7416.4616.1016.3724.2365.0631.5211.9831.348clefts9.8479.7167.3377.1226.1195.2034.6971.3701.1972.112tags8.2879.5337.1556.3496.8873.4103.6851.3671.6840.866youse9.7719.9486.4956.1876.4275.0783.6391.3931.2211.906soitwas9.8208.9165.8447.6186.5745.1205.8481.2710.4861.162dt10.0819.2448.3706.1036.5625.1503.4521.7381.8321.376nsr9.9449.5556.2736.7396.4005.1435.0621.0081.5801.002invartag9.3728.7996.1096.8787.3625.2583.9181.1692.3101.406wh_cleft9.9029.3206.9245.6936.4456.2094.8971.3681.8142.065\n\n\nAs a next step, we create a cluster object based on the data we have just generated.\n\n# clean data\nclusm <- as.matrix(clus)\nclust <- t(clusm)            # transpose data\nclust <- na.omit(clust)     # remove missing values\nclusts <- scale(clust)      # standardize variables\nclusts <- as.matrix(clusts) # convert into matrix\n\n\n\n\n\n\nScaled frequencies of on-standard features across selected varieties of English.\n\n\nVarietynae_neglikecleftstagsyousesoitwasdtnsrinvartagwh_cleftIrishEnglish9.1189.1619.8478.2879.7719.82010.0819.9449.3729.902ScottishEnglish8.3929.7419.7169.5339.9488.9169.2449.5558.7999.320BritishEnglish5.5756.4617.3377.1556.4955.8448.3706.2736.1096.924AustralianEnglish7.1706.1017.1226.3496.1877.6186.1036.7396.8785.693NewZealandEnglish5.7946.3726.1196.8876.4276.5746.5626.4007.3626.445AmericanEnglish4.6154.2365.2033.4105.0785.1205.1505.1435.2586.209CanadianEnglish4.7135.0634.6973.6853.6395.8483.4525.0623.9184.897JamaicanEnglish1.1841.5211.3701.3671.3931.2711.7381.0081.1691.368PhillipineEnglish1.2921.9831.1971.6841.2210.4861.8321.5802.3101.814IndianEnglish0.5541.3482.1120.8661.9061.1621.3761.0021.4062.065\n\n\nWe assess if data is “clusterable” by testing if the data contains non-randomness. To this end, we calculate the Hopkins statistic which indicates how similar the data is to a random distribution.\n\nA Hopkins value of 0.5 indicates that the data is random and that there are no inherent clusters.\nIf the Hopkins statistic is close to 1, then the data is highly clusterable.\nValues of 0 indicate that the data is uniform [6].\n\nThe n in the get_clust_tendency functions represents the maximum number of clusters to be tested which should be number of predictors in the data.\n\n# apply get_clust_tendency to cluster object\nclusttendency <- get_clust_tendency(clusts,    \n                                    # define number of points from sample space\n                                    n = 9,      \n                   gradient = list(\n                     # define color for low values\n                     low = \"steelblue\",\n                     # define color for high values\n                     high = \"white\"))    \nclusttendency[1]\n\n$hopkins_stat\n[1] 0.7430173\n\n\nAs the Hopkins value is substantively higher than .5 (randomness) and closer to 1 (highly clusterable) than to .5, thus indicating that there is sufficient structure in the data to warrant a cluster analysis. As such, we can assume that there are actual clusters in the data and continue by generating a distance matrix using euclidean distances.\n\nclustd <- dist(clusts,                 # create distance matrix\n               method = \"euclidean\")   # use euclidean (!) distance\n\n\n\n\n\n\nDistance matrix of scaled frequencies of non-standard features across selected varieties of English.\n\n\nVarietyIrishEnglishScottishEnglishBritishEnglishAustralianEnglishNewZealandEnglishAmericanEnglishCanadianEnglishJamaicanEnglishPhillipineEnglishIndianEnglishIrishEnglish0.000.703.043.083.154.665.178.338.128.28ScottishEnglish0.700.002.802.922.924.534.988.117.908.07BritishEnglish3.042.800.001.190.852.022.555.435.225.40AustralianEnglish3.082.921.190.000.761.932.245.375.195.36NewZealandEnglish3.152.920.850.760.001.802.265.265.045.24AmericanEnglish4.664.532.021.931.800.001.023.743.543.66CanadianEnglish5.174.982.552.242.261.020.003.313.163.29JamaicanEnglish8.338.115.435.375.263.743.310.000.550.48PhillipineEnglish8.127.905.225.195.043.543.160.550.000.71IndianEnglish8.288.075.405.365.243.663.290.480.710.00\n\n\nBelow are other methods to create distance matrices with some comments on when using which metric is appropriate.\n# create distance matrix (euclidean method: not good when dealing with many dimensions)\nclustd <- dist(clusts, method = \"euclidean\")\n# create distance matrix (maximum method: here the difference between points dominates)\nclustd_maximum <- round(dist(clusts, method = \"maximum\"), 2)\n# create distance matrix (manhattan method: most popular choice)\nclustd_manhatten <- round(dist(clusts, method = \"manhattan\"), 2) \n# create distance matrix (canberra method: for count data only - focuses on small differences and neglects larger differences)\nclustd_canberra <- round(dist(clusts, method = \"canberra\"), 2)\n# create distance matrix (binary method: for binary data only!)\nclustd_binary <- round(dist(clusts, method = \"binary\"), 2) \n# create distance matrix (minkowski method: is not a true distance measure)\nclustd_minkowski <- round(dist(clusts, method = \"minkowski\"), 2) \n# distance method for words: daisy (other possible distances are \"manhattan\" and \"gower\")\nclustd_daisy <- round(daisy(clusts, metric = \"euclidean\"), 2) \nIf you call the individual distance matrices, you will see that depending on which distance measure is used, the distance matrices differ dramatically! Have a look at the distance matrix created using the manhatten metric and compare it to the distance matrix created using the euclidian metric (see above).\n\nclustd_maximum\n\n\n\n\n\n\nDistance matrix of selected varieties of English.\n\n\nVarietyIrishEnglishScottishEnglishBritishEnglishAustralianEnglishNewZealandEnglishAmericanEnglishCanadianEnglishJamaicanEnglishPhillipineEnglishIndianEnglishIrishEnglish0.000.401.211.411.171.652.062.862.832.83ScottishEnglish0.400.001.101.221.131.971.982.762.742.82BritishEnglish1.211.100.000.700.561.211.532.062.032.17AustralianEnglish1.411.220.700.000.460.951.001.982.172.19NewZealandEnglish1.171.130.560.460.001.121.162.091.852.01AmericanEnglish1.651.971.210.951.120.000.531.621.471.39CanadianEnglish2.061.981.531.001.160.530.001.391.631.42JamaicanEnglish2.862.762.061.982.091.621.390.000.380.23PhillipineEnglish2.832.742.032.171.851.471.630.380.000.30IndianEnglish2.832.822.172.192.011.391.420.230.300.00\n\n\nNext, we create a distance plot using the distplot function. If the distance plot shows different regions (non-random, non-uniform gray areas) then clustering the data is permittable as the data contains actual structures.\n\n# create distance plot\ndissplot(clustd) \n\n\n\n\nThe most common method for clustering is called ward.D or ward.D2. Both of these linkage functions seek to minimize variance. This means that they cluster in a way that the amount of variance is at a minimum (comparable to the regression line in an ordinary least squares (OLS) design).\n# create cluster object\ncd <- hclust(clustd, method=\"ward.D2\") \n# display dendrogram              \nplot(cd, hang = -1)              \n\n\n\nWe will briefly go over some other, alternative linkage methods. Which linkage method is and should be used depends on various factors, for example, the type of variables (nominal versus numeric) or whether the focus should be placed on commonalities or differences.\n# single linkage: cluster with nearest data point\ncd_single <- hclust(clustd, method=\"single\") \n# create cluster object (ward.D linkage)\ncd_wardd <- hclust(clustd, method=\"ward.D\")\n# create cluster object (ward.D2 linkage): \n# cluster in a way to achieve minimum variance\ncd_wardd2 <- hclust(clustd, method=\"ward.D2\")\n# average linkage: cluster with closest mean\ncd_average <- hclust(clustd, method=\"average\") \n# mcquitty linkage\ncd_mcquitty <- hclust(clustd, method=\"mcquitty\") \n# median linkage: cluster with closest median\ncd_median <- hclust(clustd, method=\"median\")\n# centroid linkage: cluster with closest prototypical point of target cluster\ncd_centroid <- hclust(clustd, method=\"centroid\") \n# complete linkage: cluster with nearest/furthest data point of target cluster\ncd_complete <- hclust(clustd, method=\"complete\")  \nNow, we determine the optimal number of clusters based on silhouette widths which shows the ratio of internal similarity of clusters against the similarity between clusters. If the silhouette widths have values lower than .2 then this indicates that clustering is not appropriate [7]. The function below displays the silhouette width values of 2 to 8 clusters.\n\noptclus <- sapply(2:8, function(x) summary(silhouette(cutree(cd, k = x), clustd))$avg.width)\noptclus # inspect results\n\n[1] 0.6087576 0.6614379 0.6749439 0.5494091 0.4463603 0.3984969 0.2467143\n\noptnclust <- which(optclus == max(optclus)) # determine optimal number of clusters\ngroups <- cutree(cd, k=optnclust) # cut tree into optimal number of clusters\n\nThe optimal number of clusters is the cluster solution with the highest silhouette width. We cut the tree into the optimal number of clusters and plot the result.\ngroups <- cutree(cd, k=optnclust)          # cut tree into optimal clusters\nplot(cd, hang = -1, cex = .75)             # plot result as dendrogram\nrect.hclust(cd, k=optnclust, border=\"red\") # draw red borders around clusters\n\n\n\nIn a next step, we aim to determine which factors are particularly important for the clustering - this step is comparable to measuring the effect size in inferential designs.\n\n# which factors are particularly important\nceltic <- clusts[c(1,2),]\nothers <- clusts[-c(1,2),]\n# calculate column means\nceltic.cm <- colMeans(celtic)\nothers.cm <- colMeans(others)\n# calculate difference between celtic and other englishes\ndiff <- celtic.cm - others.cm\nsort(diff, decreasing = F)\n\n soitwas     tags invartag  nae_neg       dt   clefts      nsr wh_cleft \n1.556691 1.604529 1.611924 1.619124 1.656250 1.695621 1.725908 1.736202 \n    like    youse \n1.785558 1.826903 \n\n\nplot(sort(diff),           # y-values\n  1:length(diff),       # x-values \n  type= \"n\",            # plot type (empty)\n  cex.axis = .75,       # axis font size\n  cex.lab = .75,        # label font size\n  xlab =\"Prototypical for Non-Celtic Varieties (Cluster 2) <-----> Prototypical for Celtic Varieties (Cluster 1)\", # x-axis label\n  yaxt = \"n\",           # no y-axis tick marks\n  ylab = \"\")            # no y-axis label\ntext(sort(diff), 1:length(diff), names(sort(diff)), cex = .75) # plot text into plot\n\n\n\n\nOuter <- clusts[c(6:8),]     # data of outer circle varieties\nInner <- clusts[-c(6:8),]    # data of inner circle varieties\nOuter.cm <- colMeans(Outer)  # column means for outer circle\nInner.cm <- colMeans(Inner)  # column means for inner circle\ndiff <- Outer.cm - Inner.cm  # difference between inner and outer circle\nsort(diff, decreasing = F)   # order difference between inner and outer circle\n\n      tags   invartag         dt      youse     clefts       like        nsr \n-0.9664380 -0.8710742 -0.8614757 -0.8240724 -0.7713340 -0.7640016 -0.6750898 \n   nae_neg   wh_cleft    soitwas \n-0.6319051 -0.6246979 -0.5144678 \n\n\nplot(                   # start plot\n  sort(diff),           # y-values\n  1:length(diff),       # x-values \n  type= \"n\",            # plot type (empty)\n  cex.axis = .75,       # axis font size\n  cex.lab = .75,        # label font size\n  xlab =\"Prototypical for Inner Circle Varieties (Cluster 2) <-----> Prototypical for Outer Circle Varieties (Cluster 1)\", # x-axis label\n  yaxt = \"n\",           # no y-axis tick marks\n  ylab = \"\")            # no y-axis label\ntext(sort(diff), 1:length(diff), names(sort(diff)), cex = .75) # plot text into plot\n\n\n\nWe see that discourse like is typical for other varieties and that the use of youse as 2nd person plural pronoun and invariant tags are typical for Celtic Englishes.\nWe will now test whether the cluster is justified by validating the cluster solution using bootstrapping.\n\nres.pv <- pvclust(clus,                     # apply pvclust method to clus data\n                  method.dist=\"euclidean\",  # use eucledian distance\n                  method.hclust=\"ward.D2\",  # use ward.d2 linkage\n                  nboot = 100)              # use 100 bootstrap runs\n\nBootstrap (r = 0.5)... Done.\nBootstrap (r = 0.6)... Done.\nBootstrap (r = 0.7)... Done.\nBootstrap (r = 0.8)... Done.\nBootstrap (r = 0.9)... Done.\nBootstrap (r = 1.0)... Done.\nBootstrap (r = 1.1)... Done.\nBootstrap (r = 1.2)... Done.\nBootstrap (r = 1.3)... Done.\nBootstrap (r = 1.4)... Done.\n\n\nThe clustering provides approximately unbiased p-values and bootstrap probability value [see 7].\n\nplot(res.pv, cex = .75)\npvrect(res.pv)\n\n\n\n\nWe can also use other packages to customize the dendrograms.\nplot(as.phylo(cd),      # plot cluster object\n     cex = 0.75,        # .75 font size\n     label.offset = .5) # .5 label offset\n\n\n\nOne useful customization is to display an unrooted rather than a rooted tree diagram.\n# plot as unrooted tree\nplot(as.phylo(cd),      # plot cluster object\n     type = \"unrooted\", # plot as unrooted tree\n     cex = .75,         # .75 font size\n     label.offset = 1)  # .5 label offset"
  },
  {
    "objectID": "clust.html#cluster-analysis-on-nominal-data",
    "href": "clust.html#cluster-analysis-on-nominal-data",
    "title": "Cluster and Correspondence Analysis in R",
    "section": "Cluster Analysis on Nominal Data",
    "text": "Cluster Analysis on Nominal Data\nSo far, all analyses were based on numeric data. However, especially when working with language data, the data is nominal or categorical rather than numeric. The following will thus show to implement a clustering method for nominal data.\nIn a first step, we will create a simple data set representing the presence and absence of features across varieties of English.\n\n# generate data\nIrishEnglish <- c(1,1,1,1,1,1,1,1,1,1)\nScottishEnglish <- c(1,1,1,1,1,1,1,1,1,1)\nBritishEnglish <- c(0,1,1,1,0,0,1,0,1,1)\nAustralianEnglish <- c(0,1,1,1,0,0,1,0,1,1)\nNewZealandEnglish <- c(0,1,1,1,0,0,1,0,1,1)\nAmericanEnglish <- c(0,1,1,1,0,0,0,0,1,0)\nCanadianEnglish <- c(0,1,1,1,0,0,0,0,1,0)\nJamaicanEnglish <- c(0,0,1,0,0,0,0,0,1,0)\nPhillipineEnglish <- c(0,0,1,0,0,0,0,0,1,0)\nIndianEnglish <- c(0,0,1,0,0,0,0,0,1,0)\nclus <- data.frame(IrishEnglish, ScottishEnglish, BritishEnglish, \n                   AustralianEnglish, NewZealandEnglish, AmericanEnglish, \n                   CanadianEnglish, JamaicanEnglish, PhillipineEnglish, IndianEnglish)\n# add row names\nrownames(clus) <- c(\"nae_neg\", \"like\", \"clefts\", \"tags\", \"youse\", \"soitwas\", \n                    \"dt\", \"nsr\", \"invartag\", \"wh_cleft\")\n# convert into factors\nclus <- apply(clus, 1, function(x){\n  x <- as.factor(x) })\n\n\n\n\n\n\nOccurrence of non-standard features across selected varieties of English.\n\n\nVarietynae_neglikecleftstagsyousesoitwasdtnsrinvartagwh_cleftIrishEnglish1111111111ScottishEnglish1111111111BritishEnglish0111001011AustralianEnglish0111001011NewZealandEnglish0111001011AmericanEnglish0111000010CanadianEnglish0111000010JamaicanEnglish0010000010PhillipineEnglish0010000010IndianEnglish0010000010\n\n\nNow that we have our data, we will create a distance matrix but in contrast to previous methods, we will use a different distance measure that takes into account that we are dealing with nominal (or binary) data.\n\n# clean data\nclusts <- as.matrix(clus)\n# create distance matrix\nclustd <- dist(clusts, method = \"binary\")   # create a distance object with binary (!) distance\n\n\n\n\n\n\nDistance matrix of selected varieties of English.\n\n\nVarietyIrishEnglishScottishEnglishBritishEnglishAustralianEnglishNewZealandEnglishAmericanEnglishCanadianEnglishJamaicanEnglishPhillipineEnglishIndianEnglishIrishEnglish0.00.00.400.400.400.600.600.800.800.80ScottishEnglish0.00.00.400.400.400.600.600.800.800.80BritishEnglish0.40.40.000.000.000.330.330.670.670.67AustralianEnglish0.40.40.000.000.000.330.330.670.670.67NewZealandEnglish0.40.40.000.000.000.330.330.670.670.67AmericanEnglish0.60.60.330.330.330.000.000.500.500.50CanadianEnglish0.60.60.330.330.330.000.000.500.500.50JamaicanEnglish0.80.80.670.670.670.500.500.000.000.00PhillipineEnglish0.80.80.670.670.670.500.500.000.000.00IndianEnglish0.80.80.670.670.670.500.500.000.000.00\n\n\nAs before, we can now use hierarchical clustering to display the results as a dendrogram\n\n# create cluster object (ward.D2 linkage)   : cluster in a way to achieve minimum variance\ncd <- hclust(clustd, method=\"ward.D2\")\n# plot result as dendrogram\nplot(cd, hang = -1)              # display dendogram\n\n\n\n\nIn a next step, we want to determine which features are particularly distinctive for one cluster (the “Celtic” cluster containing Irish and Scottish English).\n\n# create factor with celtic varieties on one hand and other varieties on other\ncluster <- as.factor(ifelse(as.character(rownames(clusts)) == \"IrishEnglish\", \"1\",\n  ifelse(as.character(rownames(clusts)) == \"ScottishEnglish\", \"1\", \"0\")))\n# convert into data frame\nclsts.df <- as.data.frame(clusts)\n# determine significance\nlibrary(exact2x2)\npfish <- fisher.exact(table(cluster, clsts.df$youse))\npfish[[1]]\n\n[1] 0.02222222\n\n# determine effect size\nassocstats(table(cluster, clsts.df$youse))\n\n                    X^2 df  P(> X^2)\nLikelihood Ratio 10.008  1 0.0015586\nPearson          10.000  1 0.0015654\n\nPhi-Coefficient   : 1 \nContingency Coeff.: 0.707 \nCramer's V        : 1 \n\nassocstats(table(cluster, clsts.df$like))\n\n                    X^2 df P(> X^2)\nLikelihood Ratio 1.6323  1  0.20139\nPearson          1.0714  1  0.30062\n\nPhi-Coefficient   : 0.327 \nContingency Coeff.: 0.311 \nCramer's V        : 0.327 \n\n\nClustering is a highly complex topic and there many more complexities to it. However, this should have helped to get you started."
  },
  {
    "objectID": "coll.html#extracting-n-grams-with-quanteda",
    "href": "coll.html#extracting-n-grams-with-quanteda",
    "title": "Analyzing Co-Occurrences and Collocations in R",
    "section": "Extracting N-Grams with quanteda",
    "text": "Extracting N-Grams with quanteda\nThe quanteda package [see 2] offers excellent and very fast functions for extracting bigrams.\n\n#clean corpus\ndarwin_clean <- darwin %>%\n  stringr::str_to_title()\n# tokenize corpus\ndarwin_tokzd <- quanteda::tokens(darwin_clean)\n# extract bigrams\nBiGrams <- darwin_tokzd %>% \n       quanteda::tokens_remove(stopwords(\"en\")) %>% \n       quanteda::tokens_select(pattern = \"^[A-Z]\", \n                               valuetype = \"regex\",\n                               case_insensitive = FALSE, \n                               padding = TRUE) %>% \n       quanteda.textstats::textstat_collocations(min_count = 5, tolower = FALSE)\n\n\n\n\n\n\n\n\ncollocationcountcount_nestedlengthlambdazNatural Selection405028.04295559.11301Conditions Life119025.94180243.44276Organic Beings107028.49580038.54199Closely Allied64026.79698935.14891South America44027.63298430.34581Widely Different51025.47249029.73579Modified Descendants41026.10745028.83663Distinct Species105023.44632528.82699State Nature45025.39034028.29911Theory Natural52024.82082827.64616Individual Differences35026.09038127.36223North America31027.16471526.78067Reason Believe34026.40731226.77187Forms Life57023.98797726.18518Throughout World30026.41813626.02976\n\n\nWe can also extract bigrams very easily using the tokens_compound function which understands that we are looking for two-word expressions.\n\nngram_extract <- quanteda::tokens_compound(darwin_tokzd, pattern = BiGrams)\n\nWe can now generate concordances (and clean the resulting kwic table - the keyword-in-context table).\n\n\n\n\n\n\n\n\n\n\ndocnameprekeywordposttext1Distribution Of The Organic_Beings InhabitingSouth_America, And In The Geologicaltext1We Shall Then See HowNatural_SelectionAlmost_Inevitably Causes Much_Extinction Of Thetext1, I Am Convinced ThatNatural_SelectionHas Been The Most Importanttext1By A Process Of \"Natural_Selection, \" As Will Hereaftertext1They Thus Aft'ord Materials ForNatural_SelectionTo Act On And Accumulatetext1On And Rendered Definite ByNatural_Selection, As Hereafter To Betext1To The Cumulative Action OfNatural_Selection, Hereafter To Be Explainedtext1For Existence Its Bearing OnNatural_Selection- The Term Used Intext1Struggle For Existence Bears OnNatural_Selection. It Has Been Seentext1Preserved , By The TermNatural_Selection, In Order To Marktext1Hand Of Nature . ButNatural_Selection, As We Shall_Hereafter_See ,text1Slow-Breeding Cattle And Horses InSouth_America, And Latterly In Australiatext1To The Feral Animals OfSouth_America. Here I Will Maketext1Have Observeu In Parts OfSouth_America) The Vegetation : Thistext1And Multiply . Chapter IvNatural_Selection; Or The Survival Of\n\n\nThe disadvantage here is that we are strictly speaking only extracting N-Grams but not collocates as collocates do not necessarily have to occur in direct adjacency. The following section shoes how to expand the extraction of n-grams to the extraction of collocates."
  },
  {
    "objectID": "coll.html#association-strength",
    "href": "coll.html#association-strength",
    "title": "Analyzing Co-Occurrences and Collocations in R",
    "section": "Association Strength",
    "text": "Association Strength\nWe start with the most basic and visualize the collocation strength using a simple dot chart. We use the vector of association strengths generated above and transform it into a table. Also, we exclude elements with an association strength lower than 30.\n\ncoocdf <- coocs %>%\n  as.data.frame() %>%\n  dplyr::mutate(CollStrength = coocs,\n                Term = names(coocs)) %>%\n  dplyr::filter(CollStrength > 30)\n\n\n\n\n\n\n\n\nTermCollStrengthnatural1,766.44751theory127.86330variations124.94947effects69.94048modifications63.52192acts53.15165power53.14602slight48.94158advantage48.21207disuse47.84167accumulated46.99429sexual44.31103variation43.64372principle42.17271useful39.36931\n\n\nWe can now visualize the association strengths as shown in the code chunk below.\n\nggplot(coocdf, aes(x = reorder(Term, CollStrength, mean), y = CollStrength)) +\n  geom_point() +\n  coord_flip() +\n  theme_bw() +\n  labs(y = \"\")\n\n\n\n\nThe dot chart shows that natural is collocating more strongly with selection compared to any other term. This confirms that natural and selection form a collocation in Darwin’s Origin."
  },
  {
    "objectID": "coll.html#dendrograms",
    "href": "coll.html#dendrograms",
    "title": "Analyzing Co-Occurrences and Collocations in R",
    "section": "Dendrograms",
    "text": "Dendrograms\nAnother method for visualizing collocations are dendrograms. Dendrograms (also called tree-diagrams) show how similar elements are based on one or many features. As such, dendrograms are used to indicate groupings as they show elements (words) that are notably similar or different with respect to their association strength. To use this method, we first need to generate a distance matrix from our co-occurrence matrix.\n\ncoolocs <- c(coocdf$Term, \"selection\")\n# remove non-collocating terms\ncollocates_redux <- collocates[rownames(collocates) %in% coolocs, ]\ncollocates_redux <- collocates_redux[, colnames(collocates_redux) %in% coolocs]\n# create distance matrix\ndistmtx <- dist(collocates_redux)\n\nclustertexts <- hclust(    # hierarchical cluster object\n  distmtx,                 # use distance matrix as data\n  method=\"ward.D2\")        # ward.D as linkage method\n\nggdendrogram(clustertexts) +\n  ggtitle(\"Terms strongly collocating with *selection*\")"
  },
  {
    "objectID": "coll.html#network-graphs",
    "href": "coll.html#network-graphs",
    "title": "Analyzing Co-Occurrences and Collocations in R",
    "section": "Network Graphs",
    "text": "Network Graphs\nNetwork graphs are a very useful tool to show relationships (or the absence of relationships) between elements. Network graphs are highly useful when it comes to displaying the relationships that words have among each other and which properties these networks of words have.\n\nBasic Network Graphs\nIn order to display a network, we need to create a network graph by using the network function from the network package.\n\nnet = network::network(collocates_redux, \n                       directed = FALSE,\n                       ignore.eval = FALSE,\n                       names.eval = \"weights\")\n# vertex names\nnetwork.vertex.names(net) = rownames(collocates_redux)\n# inspect object\nnet\n\n Network attributes:\n  vertices = 26 \n  directed = FALSE \n  hyper = FALSE \n  loops = FALSE \n  multiple = FALSE \n  bipartite = FALSE \n  total edges= 265 \n    missing edges= 0 \n    non-missing edges= 265 \n\n Vertex attribute names: \n    vertex.names \n\n Edge attribute names: \n    weights \n\n\nNow that we have generated a network object, we visualize the network.\n\nggnet2(net, \n       label = TRUE, \n       label.size = 4,\n       alpha = 0.2,\n       size.cut = 3,\n       edge.alpha = 0.3) +\n  guides(color = FALSE, size = FALSE)\n\n\n\n\nThe network is already informative but we will customize the network object so that the visualization becomes more appealing and informative. To add information, we create vector of words that contain different groups, e.g. terms that rarely, sometimes, and frequently collocate with selection (I used the dendrogram which displayed the cluster analysis as the basis for the categorization).\nBased on these vectors, we can then change or adapt the default values of certain attributes or parameters of the network object (e.g. weights. linetypes, and colors).\n\n# create vectors with collocation occurrences as categories\nmid <- c(\"theory\", \"variations\", \"slight\", \"variation\")\nhigh <- c(\"natural\", \"selection\")\ninfreq <- colnames(collocates_redux)[!colnames(collocates_redux) %in% mid & !colnames(collocates_redux) %in% high]\n# add color by group\nnet %v% \"Collocation\" = ifelse(network.vertex.names(net) %in% infreq, \"weak\", \n                   ifelse(network.vertex.names(net) %in% mid, \"medium\", \n                   ifelse(network.vertex.names(net) %in% high, \"strong\", \"other\")))\n# modify color\nnet %v% \"color\" = ifelse(net %v% \"Collocation\" == \"weak\", \"gray60\", \n                  ifelse(net %v% \"Collocation\" == \"medium\", \"orange\", \n                  ifelse(net %v% \"Collocation\" == \"strong\", \"indianred4\", \"gray60\")))\n# rescale edge size\nnetwork::set.edge.attribute(net, \"weights\", ifelse(net %e% \"weights\" < 1, 0.1, \n                                   ifelse(net %e% \"weights\" <= 2, .5, 1)))\n# define line type\nnetwork::set.edge.attribute(net, \"lty\", ifelse(net %e% \"weights\" <=.1, 3, \n                               ifelse(net %e% \"weights\" <= .5, 2, 1)))\n\nWe can now display the network object and make use of the added information.\n\nggnet2(net, \n       color = \"color\", \n       label = TRUE, \n       label.size = 4,\n       alpha = 0.2,\n       size = \"degree\",\n       edge.size = \"weights\",\n       edge.lty = \"lty\",\n       edge.alpha = 0.2) +\n  guides(color = FALSE, size = FALSE)\n\nWarning: `guides(<scale> = FALSE)` is deprecated. Please use `guides(<scale> =\n\"none\")` instead."
  },
  {
    "objectID": "coll.html#biplots",
    "href": "coll.html#biplots",
    "title": "Analyzing Co-Occurrences and Collocations in R",
    "section": "Biplots",
    "text": "Biplots\nAn alternative way to display co-occurrence patterns are bi-plots which are used to display the results of Correspondence Analyses. They are useful, in particular, when one is not interested in one particular key term and its collocations but in the overall similarity of many terms. Semantic similarity in this case refers to a shared semantic and this distributional profile. As such, words can be deemed semantically similar if they have a similar co-occurrence profile - i.e. they co-occur with the same elements. Biplots can be used to visualize collocations because collocates co-occur and thus share semantic properties which renders then more similar to each other compared with other terms.\n\n# perform correspondence analysis\nres.ca <- CA(collocates_redux, graph = FALSE)\n# plot results\nfviz_ca_row(res.ca, repel = TRUE, col.row = \"gray20\")\n\n\n\n\nThe bi-plot shows that natural and selection collocate as they are plotted in close proximity. The advantage of the biplot becomes apparent when we focus on other terms because the biplot also shows other collocates such as vary and independently or might injurious."
  },
  {
    "objectID": "coll.html#simple-collexeme-analysis",
    "href": "coll.html#simple-collexeme-analysis",
    "title": "Analyzing Co-Occurrences and Collocations in R",
    "section": "Simple Collexeme Analysis",
    "text": "Simple Collexeme Analysis\nSimple Collexeme Analysis determines if a word is significantly attracted to a specific construction within a corpus. The idea is that the frequency of the word that is attracted to a construction is significantly higher within the construction than would be expected by chance.\nThe example here analyzes the Go + Verb construction (e.g. Go suck a nut!). The question is which verbs are attracted to this constructions (in this case, if suck is attracted to this construction).\nTo perform these analyses, we use the collostructions package. Information about how to download and install this package can be found here.\n\n\n\n\nNOTEDownloading and installing the collostructions package is a bit tricky and nor really user friendly. To install this package, go the website of Susanne Flach, who has written the collustructions package. On that website, you find different versions of that package for different operating systems (OS) like Windows and Mac. Next, download the version that is the right one for your OS, unzip the package file and copy it into your R library. Once you have copied the collustructions package in your R library, you can run the code chunks below to install and activate all the required packages and steps.\n\n\n\n\n\n\nInstall and activate the devtools package and installing the collostructions package.\n\n# install devtools package\ninstall.packages(\"devtools\")\n# load devtools package\nlibrary(devtools)\n# install collostructions package\ninstall_local(here::here(\"renv/library/R-4.2/x86_64-w64-mingw32/collostructions_0.2.0.zip\"), repos=NULL, type=\"source\")\n\nWe can now, finally, load the collostructions package.\n\n# load collostructions package\nlibrary(collostructions)\n\nNext, we inspect the data. In this case, we will only use a sample of 100 rows from the data set as the output would become hard to read.\n\n# draw a sample of the data\ngoVerb <- goVerb[sample(nrow(goVerb), 100),]\n\n\n\n\n\n\n\n\nWORDCXN.FREQCORP.FREQpeddle1388borrow25,791update235,183liberate11,243drool1483put59232,063bugger1738get1,197686,545consider384,446grabbed16,009stare12,958convince110,164discuss239,076apply454,374sift2931\n\n\nThe collex function which calculates the results of a simple collexeme analysis requires a data frame consisting out of three columns that contain in column 1 the word to be tested, in column 2 the frequency of the word in the construction (CXN.FREQ), and in column 3 the frequency of the word in the corpus (CORP.FREQ).\nTo perform the simple collexeme analysis, we need the overall size of the corpus, the frequency with which a word occurs in the construction under investigation and the frequency of that construction.\n\n# define corpus size\ncrpsiz <- sum(goVerb$CORP.FREQ)\n# perform simple collexeme analysis\nscollex_results <- collex(goVerb, corpsize = crpsiz, am = \"logl\", \n                          reverse = FALSE, decimals = 5,\n                          threshold = 1, cxn.freq = NULL, \n                          str.dir = FALSE)\n\n\n\n\n\n\n\n\nCOLLEXCORP.FREQOBSEXPASSOCCOLL.STR.LOGLSIGNIFsee571,6601,226530.2attr841.31817*****fuck7,2361406.7attr591.63913*****get686,5451,197636.7attr508.05162*****find315,596485292.7attr117.09092*****hang13,4305412.5attr75.95130*****indoctrinate18120.2attr6.26624*frag22120.2attr5.53774*drown1,56041.4attr3.03513nsreread97530.9attr3.00981nscluck9710.1attr3.00547nslisten33,9654131.5attr2.64220nsslink12510.1attr2.54789nssaunter12910.1attr2.49206nslop16110.1attr2.10670nsspank17010.2attr2.01428ns\n\n\nThe results show which words are significantly attracted to the construction. If the ASSOC column did not show attr, then the word would be repelled by the construction."
  },
  {
    "objectID": "coll.html#covarying-collexeme-analysis",
    "href": "coll.html#covarying-collexeme-analysis",
    "title": "Analyzing Co-Occurrences and Collocations in R",
    "section": "Covarying Collexeme Analysis",
    "text": "Covarying Collexeme Analysis\nCovarying collexeme analysis determines if the occurrence of a word in the first slot of a constructions correlates with the occurrence of a word in the second slot of the construction. As such, covarying collexeme analysis analyzes constructions with two slots and how the lexical elements within the two slots affect each other.\nThe data we will use is called vsmdata and consist of 5,000 observations of adjectives and if the adjective is amplified. As such, vsmdata contains two columns: one column with the adjectives (Adjectives) and another column telling if the adjective has been amplified (0 means that the adjective occurred without an amplifier). The first six rows of the data are shown below.\n\n# load data\nvsmdata <- base::readRDS(url(\"https://slcladal.github.io/data/vsd.rda\", \"rb\")) %>%\n  dplyr::mutate(Amplifier = ifelse(Amplifier == 0, 0, 1))\n\n\n\n\n\n\n\n\nAmplifierAdjective0serious0sure1many0many0good0much\n\n\nWe now perform the collexeme analysis and inspect the results.\n\ncovar_results <- collex.covar(vsmdata)\n\n\n\n\n\n\n\n\nSLOT1SLOT2fS1fS2OBSEXPASSOCCOLL.STR.LOGLSIGNIF0last4,397308307270.9attr72.26078*****1difficult60354266.5attr43.13631*****0little4,397309301271.7attr38.65379*****0great4,397179177157.4attr32.74427*****0same4,397171169150.4attr30.79976*****0new4,397182179160.1attr28.81377*****1nice603952911.5attr23.34775*****0old4,397132130116.1attr21.51969*****1good6033857646.4attr20.23791*****0second4,39710310290.6attr19.43779****\n\n\nThe results show if a words in the first and second slot attract or repel each other (ASSOC) and provide uncorrected significance levels."
  },
  {
    "objectID": "coll.html#distinctive-collexeme-analysis",
    "href": "coll.html#distinctive-collexeme-analysis",
    "title": "Analyzing Co-Occurrences and Collocations in R",
    "section": "Distinctive Collexeme Analysis",
    "text": "Distinctive Collexeme Analysis\nDistinctive Collexeme Analysis determines if the frequencies of items in two alternating constructions or under two conditions differ significantly. This analysis can be extended to analyze if the use of a word differs between two corpora.\nAgain, we use the vsmdata data.\n\ncollexdist_results <- collex.dist(vsmdata, raw = TRUE)\n\n\n\n\n\n\n\n\nCOLLEXO.CXN1E.CXN1O.CXN2E.CXN2ASSOCCOLL.STR.LOGLSIGNIFSHAREDlast307270.9137.1072.26078*****Ylittle301271.7837.3038.65379*****Ygreat177157.4221.6032.74427*****Ysame169150.4220.6030.79976*****Ynew179160.1321.9028.81377*****Yold130116.1215.9021.51969*****Ysecond10290.6112.4019.43779****Ysure128116.1415.9014.24614***Yopen4539.605.4011.62229***Nreal9283.5311.509.83958**Y\n\n\nThe results show if words are significantly attracted or repelled by a modifier variant."
  },
  {
    "objectID": "comp.html#restart-your-computer-regularly",
    "href": "comp.html#restart-your-computer-regularly",
    "title": "Working with Computers",
    "section": "Restart your computer regularly",
    "text": "Restart your computer regularly\n\nWhile it can be very handy to use sleep mode repeatedly, you should shut down and restart your computer regularly. This is necessary to install already downloaded updates and it also allows your computer to dump a lot of data that accumulates when it keeps running. So, rather than using sleep mode, shut down and restart your computer on a regular basis (at least once per week) so automatic software updates can be installed. If there’s a high security risk, you may be forced to restart your computer immediately because updates often close gaps in the security of either specific programs or your operating system."
  },
  {
    "objectID": "comp.html#keep-your-computer-up-to-date",
    "href": "comp.html#keep-your-computer-up-to-date",
    "title": "Working with Computers",
    "section": "Keep your computer up-to-date",
    "text": "Keep your computer up-to-date\n\nOne thing you can do to keep your computer running smoothly is to keep your computer up-to-date by checking for updates. In fact, outdated software is not updated with the latest security features and puts your computer at risk.\nUpdates can be annoying but they also help to close security gaps and improve the functionality of the software that you are using. If you do not know how to check if updates are available, you can find a step-by-step tutorial here."
  },
  {
    "objectID": "comp.html#look-out-for-leeches-and-malware",
    "href": "comp.html#look-out-for-leeches-and-malware",
    "title": "Working with Computers",
    "section": "Look out for leeches and malware",
    "text": "Look out for leeches and malware\n\nWhen you download software, it is quite common that, in addition to the software you are looking for, other additional software will be downloaded and installed as a default. To avoid this, make sure to uncheck such options when installing the software that you want. This simply requires that you pay attention and read the options that you can check or uncheck during the installation when installing software."
  },
  {
    "objectID": "comp.html#use-anti-virus-software",
    "href": "comp.html#use-anti-virus-software",
    "title": "Working with Computers",
    "section": "Use anti-virus software",
    "text": "Use anti-virus software\n\nAntivirus software checks if any software on your computer has been reported as malware or if your software differs from what it should look like if it were not infected.\nSymantec Endpoint Protection (SEP) is an anti-virus software that is installed on all UQ computers. This software protects your computer from malware but also checks if your computer is already “infected”. Such checks are performed regularly but to run such a check manually, you can simply click on the antivirus software icon in the lower right corner of your PC and follow the instructions. While Symantec Endpoint Protection is not free and you have to pay a fee if you want to install it on a private PC, UQ has a deal with the manufacturer that gives UQ members a discount.\nThere are also free alternatives available such as the free version of Avira in case you do not want to pay for anti-virus software. Both the free and the commercial versions of Avira have the advantage that they also allow you to check if the performance of your PC can be improved (in addition to merely protecting your computer) and - depending on the version - they can also implement these improvements.\nAnother option that helps to detect software on your computer is Malwarebytes which also has a free version and which has the most up-to-date data base of malware which means that it is able to detect even very “fresh” malware."
  },
  {
    "objectID": "comp.html#no-data-on-desktop-or-c-drive",
    "href": "comp.html#no-data-on-desktop-or-c-drive",
    "title": "Working with Computers",
    "section": "No data on Desktop or C-drive",
    "text": "No data on Desktop or C-drive\n\nWhen you start your computer, different parts of the computer are started at different times with different priorities. The Desktop is always started with the highest priority which means that if you have a lot of stuff on your desktop, then the computer will load all that stuff once it kicks into action (which will cause it to work quite heavily at each start and also slow down quite dramatically).\nThis means that you should avoid storing data on any part of your system that is activated routinely. Rather, try to separate things that need to be loaded from things that only need to be loaded if they are actually used. For this reason, you should also avoid storing data on your C-drive. In fact, the C-drive should only contain programs as it is activated automatically at each start.\nYou can, for example, store all your projects on your D-Drive or, even better, on OneDrive, Google’s MyDrive, or in Dropbox where it is only started once you actively click and open a folder. If you use cloud-based storage options (OneDrive, Google’s MyDrive, or Dropbox) the files are also backed up automatically. However, you should not use either of these for sensitive data (sensitive data should be stored on your PC, an external hard drive and UQ’s RDM.)\nIf you want to have data accessible via your desktop, you can still do so by using links (also called short-cuts): place a link to your data (stored on another drive) on your desktop and you can load your data easily without it being activated at every start."
  },
  {
    "objectID": "comp.html#tidy-your-room",
    "href": "comp.html#tidy-your-room",
    "title": "Working with Computers",
    "section": "Tidy your room!",
    "text": "Tidy your room!\n\nJust like in real life, you should clean your computer. Full bins, for instance, will slow down your computer so you should empty it regularly. In addition, your computer will store and collect files when it is running. These files (temp files, cookies, etc.) also slow your computer down. As such files are redundant, they should be deleted regularly. You can remove such files manually using the msconfig prompt (you find a video tutorial on how to do this here). If you want to optimize your computer manually via the msconfig prompt, simply enter msconfig in the Window’s search box in the lower left corner of your PC (or search for it in the search box that opens when you click on the Window’s symbol). However, an easier way is to use software to help you with cleaning your computer."
  },
  {
    "objectID": "comp.html#software-to-clean-your-computer",
    "href": "comp.html#software-to-clean-your-computer",
    "title": "Working with Computers",
    "section": "Software to clean your computer",
    "text": "Software to clean your computer\n\nWhile UQ provides various software applications that keep your computer secure, it does not have any specific recommendations for software to keep your computer digitally clean.\nLuckily, there are numerous software applications that can help you with keeping your computer clean and up-to-date (you will find a list of software options for PCs here). We will only look at two options here (The two applications we will discuss are CCleaner and Avira) but a quick Google search will provide you with many different alternatives.\nThe most widely used program to clean your computer (if you have a PC rather than a Mac) is CCleaner. There are different versions of CCleaner but the free version suffices to delete any superfluous files and junk from your computer. When using this program, you should, however, be careful not to remove information that is useful. For instance, I like to keep all tabs of my current session in my browser and I therefore have to change the default options in CCleaner to avoid having to reopen all my tabs when I next open my browser. Here is a short video tutorial on how to use the CCleaner.\nIn addition, the free version of Avira also has a function that you can use to clean your computer. In fact, Avira will also inform you about any software that is out-of-date and other issues. Here is a short video tutorial on how to use the Avira for cleaning your computer and performing an anti-virus scan."
  },
  {
    "objectID": "comp.html#encryption-and-computer-security",
    "href": "comp.html#encryption-and-computer-security",
    "title": "Working with Computers",
    "section": "Encryption and Computer Security",
    "text": "Encryption and Computer Security\nEnsuring that your computer and network are secured means that you have far less a chance of a data breach or hack.\nAs some information is sensitive (especially when it comes to exams and attendance in courses), I encrypt folders and files that contain such information. To encrypt a file or folder I right-click on the file or folder and go to properties > advanced, the I check encrypt contents to secure data and confirm the changes by checking OK. Then I back-up the encryption key where I check enable certificate privacy and create password and store the encrypted file in the original folder. You can find a step-by-step guide on how to encrypt files in this video.\nYou can also encrypt your entire computer. Information about how to do this can be found here and tips specific for\n\nMacs can be found here\nWindows 10 can be found here\nWindows 7 can be found here\n\nIt is also recommendable to use or create strong passwords. Here are some tips for creating secure passwords:\n\nDon’t just use one password - use a different password for every account\nUse a pass phrase - instead of a singular word, try a sequence of words for instance, DogsandCatsareawesome (Do not use this as your password)\nInclude numbers, capital letters and symbols\nThe longer the password, the better\nDon’t write passwords down\nTurn on two-factor authentication\n\nAn alternative is to use a password manager. Again, the Digital Essentials module has a lot of information about password management (password managers explained in detail in section 4).\nPassword managers provide a similar level of convenience to “Login with Facebook” but are much safer. Password managers create an encrypted database of all your usernames and passwords, that only you can access with a master password. This means you only need to remember one password to have access to all of your accounts. Most password managers will include the ability to generate secure passwords that you can use for new or existing account logins. Because you only need to remember one master password, you can generate and store complex passwords for your needs. This way, you are not relying on your memory and easy passwords to remember many different account login details.\nAlso, to find out if your email has been compromised, you can check this here\nRecently, UQ has adopted Multi-Factor Authentication which is more secure than simple authentication. You should use it when the option is available (Signing in with a password and an email to your account with a pin).\nAs a general tip, avoid unsecured wifi and, if its available, Eduroam is usually a better option than free wifi/cafe wifi.\nFor Beginners\n\nHave good strong passwords and encrypt your computer’s hard drive\n\nFor Intermediates\n\nGet set up on a password manager\n\nFor Advanced passwordists\n\nTry to ensure that your team/cluster is encrypted and practicing safe habits."
  },
  {
    "objectID": "compthink.html",
    "href": "compthink.html",
    "title": "Computational Thinking in the Humanities",
    "section": "",
    "text": "The workshop Computational Thinking in the Humanities is a 3-hour online workshop featuring two plenary talks, lightning presentations, as well as a panel discussion. The workshop is co-organized by the Australian Text Analytics Platform (ATAP), FIN-CLARIAH and its UEF representatives, and the Australian Digital Observatory.\nThe workshop has received financial supported from the Digital Cultures and Societies Hub at the University of Queensland.\nThe workshop takes place\nThursday, Sep. 1, 2022, 5-8pm Queensland, 10am-1pm Finland, 8-11am UK.\nAll sessions start sharp (no academic quarter!)\nZoom link: https://uqz.zoom.us/j/86311263161"
  },
  {
    "objectID": "compthink.html#krista-lagus",
    "href": "compthink.html#krista-lagus",
    "title": "Computational Thinking in the Humanities",
    "section": "Krista Lagus",
    "text": "Krista Lagus\nKrista is full professor at the University of Helsinki in the Centre for Social Data Science, CSDS. There, she applies quantitative and qualitative data analysis methods in order to understand various individual and social practices that affect the well-being of individuals. Recent topics of interest include loneliness, peer support, mindfulness practices and life-philosophical lecturing."
  },
  {
    "objectID": "compthink.html#barbara-mcgillivray",
    "href": "compthink.html#barbara-mcgillivray",
    "title": "Computational Thinking in the Humanities",
    "section": "Barbara McGillivray",
    "text": "Barbara McGillivray\n\n\n\n\n\nBarbara is lecturer in Digital Humanities and Cultural Computation at the Department of Digital Humanities of King’s College London and a Turing Research Fellow at The Alan Turing Institute. She is also Editor in Chief of the Journal of Open Humanities Data. Before going back to academia, Barbara was language technologist in the Dictionary division of Oxford University Press and data scientist in the Open Research Group of Springer Nature.\nBarbara has always been passionate about how the sciences and the humanities can meet. Her most recent book is Applying Language Technology in Humanities Research. Design, Application, and the Underlying Logic, co-authored with Gábor Mihály Tóth and published by Palgrave Macmillan in 2020.\n\nBack to top\nBack to HOME"
  },
  {
    "objectID": "compthinking.html",
    "href": "compthinking.html",
    "title": "Computational Thinking in the Humanities",
    "section": "",
    "text": "The workshop Computational Thinking in the Humanities is a 3-hour online workshop featuring two plenary talks, lightning presentations, as well as a panel discussion. The workshop is co-organized by the Australian Text Analytics Platform (ATAP), FIN-CLARIAH and its UEF representatives, and the Australian Digital Observatory.\nThe workshop has received financial supported from the Digital Cultures and Societies Hub at the University of Queensland.\nThe workshop takes place\nThursday, Sep. 1, 2022, 5-8pm Queensland, 10am-1pm Finland, 8-11am UK.\nAll sessions start sharp (no academic quarter!)\nZoom link: https://uqz.zoom.us/j/86311263161"
  },
  {
    "objectID": "compthinking.html#krista-lagus",
    "href": "compthinking.html#krista-lagus",
    "title": "Computational Thinking in the Humanities",
    "section": "Krista Lagus",
    "text": "Krista Lagus\nKrista is full professor at the University of Helsinki in the Centre for Social Data Science, CSDS. There, she applies quantitative and qualitative data analysis methods in order to understand various individual and social practices that affect the well-being of individuals. Recent topics of interest include loneliness, peer support, mindfulness practices and life-philosophical lecturing."
  },
  {
    "objectID": "compthinking.html#barbara-mcgillivray",
    "href": "compthinking.html#barbara-mcgillivray",
    "title": "Computational Thinking in the Humanities",
    "section": "Barbara McGillivray",
    "text": "Barbara McGillivray\n\n\n\n\n\nBarbara is lecturer in Digital Humanities and Cultural Computation at the Department of Digital Humanities of King’s College London and a Turing Research Fellow at The Alan Turing Institute. She is also Editor in Chief of the Journal of Open Humanities Data. Before going back to academia, Barbara was language technologist in the Dictionary division of Oxford University Press and data scientist in the Open Research Group of Springer Nature.\nBarbara has always been passionate about how the sciences and the humanities can meet. Her most recent book is Applying Language Technology in Humanities Research. Design, Application, and the Underlying Logic, co-authored with Gábor Mihály Tóth and published by Palgrave Macmillan in 2020.\n\nBack to top\nBack to HOME"
  },
  {
    "objectID": "contact.html#reporting-errors-via-email",
    "href": "contact.html#reporting-errors-via-email",
    "title": "CONTACT",
    "section": "Reporting errors via Email",
    "text": "Reporting errors via Email\nIf you simply want to report an error, you can contact the LADAL team via ladal@uq.edu.au or you can contact Martin via m.schweinberger@uq.edu.au.\nIt may take some time until we manage to check and fix the bug, but we try to be as swift as possible and we will get to it eventually."
  },
  {
    "objectID": "contact.html#fixing-bugs-directly-on-github",
    "href": "contact.html#fixing-bugs-directly-on-github",
    "title": "CONTACT",
    "section": "Fixing bugs directly on GitHub",
    "text": "Fixing bugs directly on GitHub\nAn even better option than reporting an error is to fix the bug via GitHub. That way, we receive an email that someone found an error and has submitted a fix which we can then review and accept (or decline). Below is a detailed description of how exactly you can do that.\nTo be able to correct errors via GitHub, you need a GitHub account (you can register and sign in here). We will create a tutorial in which we show you how to do this - in the mean time, we have to rely on your skills to try and find out how to do this yourself.\nServices {-} LADAL offers services to members of the School of Languages and Cultures of UQ as well as to external partners or customers including institutions, businesses, teams, or individual researchers and the public. These services include:\n\n\n\n\n\n\nAccess to resources\nWorkshops and training\n\nIf you are interested or would like to learn more, please contact the LADAL team via ladal@uq.edu.au."
  },
  {
    "objectID": "contr.html#reporting-errors-via-email",
    "href": "contr.html#reporting-errors-via-email",
    "title": "Fixing errors and becoming a LADAL contributer",
    "section": "Reporting errors via Email",
    "text": "Reporting errors via Email\nIf you simply want to report an error, you can contact the LADAL team via ladal@uq.edu.au or you can contact Martin via m.schweinberger@uq.edu.au.\nIt may take some time until we manage to check and fix the bug, but we try to be as swift as possible and we will get to it eventually."
  },
  {
    "objectID": "contr.html#fixing-bugs-on-github",
    "href": "contr.html#fixing-bugs-on-github",
    "title": "Fixing errors and becoming a LADAL contributer",
    "section": "Fixing bugs on GitHub",
    "text": "Fixing bugs on GitHub\nAn even better option than reporting an error is to fix the bug via GitHub. That way, we receive an email that someone found an error and has submitted a fix which we can then review and accept (or decline). Below is a detailed description of how exactly you can do that.\nTo be able to correct errors via GitHub, you need a GitHub account (you can register and sign in here).\nYou then"
  },
  {
    "objectID": "corplingr.html#notes-on-loading-corpus-data-into-r",
    "href": "corplingr.html#notes-on-loading-corpus-data-into-r",
    "title": "Corpus Linguistics with R",
    "section": "Notes on loading corpus data into R",
    "text": "Notes on loading corpus data into R\nBefore we continue with the case studies, it is important to see how what we will be doing in the case studies differs from what you will most likely do if you conduct corpus-based research.\nIf a corpus is not accessed via a web application, corpora (collections of electronic language samples) typically - almost always - come in the form of text or audio files in a folder. That is, when using corpora, researchers typically download that corpus from a repository (for instance a website) or from some other storage media (for instance a CD, USB-stick, etc.). This means that non-web-based corpora are typically somewhere on a researcher’s computer where they can then be loaded into some software, e.g. AntConc.\nFor the present tutorial, however, we will simply load data that is available via the LADAL GitHub repository. Nonetheless, it is important to know how to load corpus data into R - which is why I will show this below.\nLoading corpus data into R consists of two steps:\n\ncreate a list of paths of the corpus files\nloop over these paths and load the data in the files identified by the paths.\n\nTo create a list of corpus files, you could use the code chunk below (the code chunk assumes that the corpus data is in a folder called Corpus in the data sub-folder of your Rproject folder).\n\ncorpusfiles <- list.files(here::here(\"data/Corpus\"), # path to the corpus data\n                          # file types you want to analyze, e.g. txt-files\n                          pattern = \".*.txt\",\n                          # full paths - not just the names of the files\n                          full.names = T)            \n\nYou can then use the sapply function to loop over the paths and load the data int R using e.g. the scan function as shown below. In addition to loading the file content, we also paste all the content together using the paste0 function and remove superfluous white spaces using the str_squish function from the stringr package.\n\ncorpus <- sapply(corpusfiles, function(x){\n  x <- scan(x, \n            what = \"char\", \n            sep = \"\", \n            quote = \"\", \n            quiet = T, \n            skipNul = T)\n  x <- paste0(x, sep = \" \", collapse = \" \")\n  x <- stringr::str_squish(x)\n})\n\nOnce you have loaded your data into R, you can then continue with processing and transforming the data according to your needs.\n\n\n\n\nNOTEThere are many different ways in which you can load text data into R. What I have shown above is just one way of doing this. However, I found this procedure to load text data very useful. In the case study which exemplifies how you can analyze sociolinguistic variation, we show how you can load text data in a very similar yet slightly different way (the tidyverse style of loading text data)."
  },
  {
    "objectID": "corplingr.html#preparation-and-session-set-up",
    "href": "corplingr.html#preparation-and-session-set-up",
    "title": "Corpus Linguistics with R",
    "section": "Preparation and session set up",
    "text": "Preparation and session set up\nThe case studies shown below are based on R. Thus, you should already be familiar with R and RStudio. If you have not installed R or are new to it, you will find an introduction to and more information how to use R here. In addition, it is recommended to be familiar with regular expressions (this tutorials contains an overview of regular expressions that are used in this tutorial).\nYou should also have downloaded and installed R and RStudio. This tutorial contains links to detailed how-tos on how to download and install R and RStudio.\nFor this case study, we need to install certain packages from an R library so that the scripts shown below are executed without errors. Before turning to the code below, please install the packages by running the code below this paragraph. If you have already installed the packages mentioned below, then you can skip ahead ignore this section. To install the necessary packages, simply run the following code - it may take some time (between 1 and 5 minutes to install all of the libraries so you do not need to worry if it takes some time).\n\n# installing packages\ninstall.packages(\"dplyr\")\ninstall.packages(\"stringr\")\ninstall.packages(\"ggplot2\")\ninstall.packages(\"flextable\")\ninstall.packages(\"quanteda\")\ninstall.packages(\"cfa\")\n# install klippy for copy-to-clipboard button in code chunks\ninstall.packages(\"remotes\")\nremotes::install_github(\"rlesur/klippy\")\n\n\n# set options\noptions(stringsAsFactors = F)         # no automatic data transformation\noptions(\"scipen\" = 100, \"digits\" = 4) # suppress math annotation\n# load packages\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(ggplot2)\nlibrary(flextable)\nlibrary(quanteda)\nlibrary(cfa)\n# activate klippy for copy-to-clipboard button\nklippy::klippy()\n\n\n\n\nOnce you have installed RStudio and initiated the session by executing the code shown above, you are good to go."
  },
  {
    "objectID": "corplingr.html#using-childes-data",
    "href": "corplingr.html#using-childes-data",
    "title": "Corpus Linguistics with R",
    "section": "Using CHILDES data",
    "text": "Using CHILDES data\nThe Child Language Data Exchange System (CHILDES) [5] is a browsable data base which provides corpora consisting of transcripts of conversations with children. CHILDES was established in 1984 by Brian MacWhinney and Catherine Snow and it represents the central repository for data of first language acquisition. Its earliest transcripts date from the 1960s, and it now has contents (transcripts, audio, and video) in 26 languages from 130 different corpora, all of which are publicly available worldwide.\n\n\n\n\n\nCHILDES is the child language part of the TalkBank system which is a system for sharing and studying conversational interactions.\n\nTo download corpora from CHILDES:\n\nGo to the CHILDES website - the landing page looks like the website shown in the image to the right. For the present tutorial, the only relevant part of that website is section labeled Database which contains the links to the different CHILDS corpora that you can download fro free.\nIn the section called Database click on Index to Corpora which will take you to a table which contains links to different kinds of corpora - all containing transcripts of children’s speech. The types of corpora available cover many different language, including monolingual and bilingual children, children with speech disorders, transcripts of frog stories, etc.\n\n\n\n\n\n\n\nTo download a corpus, click on one of the section, e.g. on Eng-NA which stands for English recorded in North America (but you can, of course, also download other CHILDES corpora), and then scroll down to the corpus you are interested in and click on it, e.g. scroll down to and click on HSLLD.\n\n\n\n\n\nClick on Download transcripts and then download and store the zip-folder somewhere on your computer.\nNext, unzip the zip-file and store the resulting unzipped corpus in the data sub-folder in your Rproject folder.\n\nOnce you have downloaded, stored the data on your computer, and unzipped it, you are good to go and you can now access and analyze data from CHILDES."
  },
  {
    "objectID": "corplingr.html#hslld-corpus",
    "href": "corplingr.html#hslld-corpus",
    "title": "Corpus Linguistics with R",
    "section": "HSLLD corpus",
    "text": "HSLLD corpus\nFor this case study, we will use data from the Home-School Study of Language and Literacy Development corpus (HSLLD) which part of the CHILDES data base. The Home-School Study of Language and Literacy Development began in 1987 under the leadership of Patton Tabors and with Catherine E. Snow and David K. Dickinson as primary investigators. The original purpose of the HSLLD was to investigate the social prerequisites to literacy success.\nThe initial number of participants was 83 American English speaking, racially diverse, preschool age children from low-income families growing up in or around Boston, Massachusetts. Seventy-four of these children were still participating at age 5. The sample consists of 38 girls and 36 boys. Forty-seven children were Caucasian, 16 were African American, six were of Hispanic origin, and five were biracial.\nChildren were visited once a year in their home from age 3 – 5 and then again when they were in 2nd and 4th grade. Each visit lasted between one and three hours. Home visits consisted of a number of different tasks depending on the year. An outline of the different tasks for each visit is presented below.\nActivities during Home Visit 1 (HV1): Book reading (BR), Elicited report (ER), Mealtime (MT), Toy Play (TP)\nActivities during Home Visit 2 (HV2): Book reading (BR), Elicited report (ER), Mealtime (MT), Toy Play (TP)\nActivities during Home Visit 3 (HV3): Book reading (BR), Elicited report (ER), Experimental task (ET), Mealtime (MT), Reading (RE), Toy play (TP)\nActivities during Home Visit 5 (HV5): Book reading (BR), Letter writing (LW), Mealtime (MT)\nActivities during Home Visit 7 (HV7): Experimental task (ET), Letter writing (LW), Mother definitions (MD), Mealtime (MT)"
  },
  {
    "objectID": "corplingr.html#data-processing",
    "href": "corplingr.html#data-processing",
    "title": "Corpus Linguistics with R",
    "section": "Data processing",
    "text": "Data processing\nWe now load the data and inspect its structure using the str function - as the HSLLD has many files, we will only check the first 3.\n\nhslld <- readRDS(url(\"https://slcladal.github.io/data/hslld.rda\"))\n\n# If you have already downloaded the data file to data/hslld.rda you can also load it locally\n# hslld <- readRDS(\"data/hslld.rda\")\n\n# inspect\nstr(hslld[1:3])\n\nList of 3\n $ D:\\Uni\\UiT\\Workshops\\RCHILDES\\data\\HSLLD/HV1/BR/acebr1.cha   : chr [1:1931] \"@UTF8\" \"@PID:\" \"11312/c-00034768-1\" \"@Begin\" ...\n $ D:\\Uni\\UiT\\Workshops\\RCHILDES\\data\\HSLLD/HV1/BR/acebr1pt2.cha: chr [1:21] \"@UTF8\" \"@PID:\" \"11312/a-00012630-1\" \"@Begin\" ...\n $ D:\\Uni\\UiT\\Workshops\\RCHILDES\\data\\HSLLD/HV1/BR/admbr1.cha   : chr [1:1369] \"@UTF8\" \"@PID:\" \"11312/c-00034769-1\" \"@Begin\" ...\n\n\nWe continue and split the data up into files. The sapply function loops each element in the hslld object and performs specified actions on the (here, loading the content via the scan function, getting rid of white spaces and splitting the files when it finds the following sequences *ABC1: or %ABC:)\n\n# create version of corpus fit for concordancing\ncorpus <- sapply(hslld, function(x) {\n  # clean data\n  x <- stringr::str_trim(x, side = \"both\") # remove superfluous white spaces at the edges of strings\n  x <- stringr::str_squish(x)              # remove superfluous white spaces within strings\n  x <- paste0(x, collapse = \" \")           # paste all utterances in a file together\n  # split files into individual utterances\n  x <- strsplit(gsub(\"([%|*][a-z|A-Z]{2,4}[0-9]{0,1}:)\", \"~~~\\\\1\", x), \"~~~\")\n})\n# inspect results\nstr(corpus[1:3])\n\nList of 3\n $ D:\\Uni\\UiT\\Workshops\\RCHILDES\\data\\HSLLD/HV1/BR/acebr1.cha   : chr [1:793] \"@UTF8 @PID: 11312/c-00034768-1 @Begin @Languages: eng @Participants: CHI Target_Child , MOT Mother , EX1 Invest\"| __truncated__ \"*MOT: okay her favorite books (.) I don't read the whole stories they're  too long . \" \"%mor: co|okay det:poss|her adj|favorite n|book-PL pro:sub|I  mod|do~neg|not v|read&ZERO det:art|the adj|whole n\"| __truncated__ \"%gra: 1|4|COM 2|4|DET 3|4|MOD 4|0|INCROOT 5|8|SUBJ 6|8|AUX 7|6|NEG 8|4|CMOD  9|11|DET 10|11|MOD 11|8|OBJ 12|13|\"| __truncated__ ...\n $ D:\\Uni\\UiT\\Workshops\\RCHILDES\\data\\HSLLD/HV1/BR/acebr1pt2.cha: chr \"@UTF8 @PID: 11312/a-00012630-1 @Begin @Languages: eng @Participants: CHI Target_Child, INV Investigator @ID: en\"| __truncated__\n $ D:\\Uni\\UiT\\Workshops\\RCHILDES\\data\\HSLLD/HV1/BR/admbr1.cha   : chr [1:517] \"@UTF8 @PID: 11312/c-00034769-1 @Begin @Languages: eng @Participants: CHI Target_Child , MOT Mother , UNC Uncle \"| __truncated__ \"*MOT: gonna read this one too which one do you want me to read first ? \" \"%mor: part|go-PRESP~inf|to v|read&ZERO det:dem|this pro:indef|one adv|too  pro:rel|which det:num|one v|do pro:p\"| __truncated__ \"%gra: 1|0|INCROOT 2|3|INF 3|1|COMP 4|5|DET 5|3|OBJ 6|3|JCT 7|9|LINK 8|9|QUANT  9|1|CJCT 10|11|SUBJ 11|9|COMP 12\"| __truncated__ ...\n\n\nWe have now loaded the files into R, but the format is not yet structured in a way that we can use it - remember: we want the data to be in a tabular format.\nExtract file information\nNow, we extract information about the recording, e.g., the participants, the age of the child, the date of the recording etc. For this, we extract the first element of each file (because this first element contains all the relevant information bout the recording). To do this, we again use the sapply function (which is our looping function) and then tell R that it shall only retain the first element of each element (x <- x[1]).\n\n# extract file info for each file\nfileinfo <- sapply(corpus, function(x){ \n  # extract first element of each corpus file because this contains the file info\n  x <- x[1]\n  })\n#inspect\nfileinfo[1:3]\n\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            D:\\\\Uni\\\\UiT\\\\Workshops\\\\RCHILDES\\\\data\\\\HSLLD/HV1/BR/acebr1.cha \n\"@UTF8 @PID: 11312/c-00034768-1 @Begin @Languages: eng @Participants: CHI Target_Child , MOT Mother , EX1 Investigator @ID: eng|HSLLD|CHI|3;07.08|female|||Target_Child||| @ID: eng|HSLLD|MOT||female|||Mother|14|| @ID: eng|HSLLD|EX1|||||Investigator||| @Birth of CHI: 11-DEC-1984 @Media: acebr1, audio, unlinked @Comment: SES of Mot is lower @Date: 19-JUL-1988 @Location: Boston , MA USA @Situation: Home Visit 1; took place in kitchen; Chi and Mot sat next  to each other in separate chairs , television is running;  7pm. @Activities: Book Reading 1 (The Very Hungry Caterpillar) @Bg: bookreading @Types: long, book, TD \" \n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         D:\\\\Uni\\\\UiT\\\\Workshops\\\\RCHILDES\\\\data\\\\HSLLD/HV1/BR/acebr1pt2.cha \n                                                                                                                                                                                                                                                                  \"@UTF8 @PID: 11312/a-00012630-1 @Begin @Languages: eng @Participants: CHI Target_Child, INV Investigator @ID: eng|HSLLD|CHI|||||Target_Child||| @ID: eng|HSLLD|INV|||||Investigator||| @Media: acebr1pt2, audio, notrans @Comment: This is a dummy file to permit playback from the TalkBank  browser. Please use the slider at the left to control media  playback. @End\" \n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            D:\\\\Uni\\\\UiT\\\\Workshops\\\\RCHILDES\\\\data\\\\HSLLD/HV1/BR/admbr1.cha \n                                                         \"@UTF8 @PID: 11312/c-00034769-1 @Begin @Languages: eng @Participants: CHI Target_Child , MOT Mother , UNC Uncle Relative, EX1  Investigator @ID: eng|HSLLD|CHI|4;01.09|male|||Target_Child||| @ID: eng|HSLLD|MOT||female|||Mother|12+|| @ID: eng|HSLLD|UNC|||||Relative||| @ID: eng|HSLLD|EX1|||||Investigator||| @Birth of CHI: 05-MAY-1985 @Media: admbr1, audio, unlinked @Comment: SES of Mot is lower @Date: 14-JUN-1989 @Location: Boston , MA USA @Situation: Home Visit 1 @Activities: Book Reading 1 The Very Hungry Caterpillar @Bg: bookreading @Types: long, book, TD \" \n\n\nNow, we have one element for each file that contains all the relevant information about the file, like when the recording took place, how old the target child was, how was present during the recording etc.\nExtract file content\nNow, we extract the raw content from which we will extract the speaker, the utterance, the pos-tagged utterance, and any comments.Here, we loop over the corpus object with the sapply function and we remove the first element in each list (and we retain the second to last element of each element (x <- x[2:length(x)])), then we paste everything else together using the paste0 function and then, we split the whole conversation into utterances that start with a speaker id (e.g. *MOT:). The latter is done by the sequence stringr::str_split(stringr::str_replace_all(x, \"(\\\\*[A-Z])\", \"~~~\\\\1\"), \"~~~\").\n\ncontent <- sapply(corpus, function(x){\n  x <- x[2:length(x)]\n  x <- paste0(x, collapse = \" \")\n  x <- stringr::str_split(stringr::str_replace_all(x, \"(\\\\*[A-Z])\", \"~~~\\\\1\"), \"~~~\")\n})\n# inspect data\ncontent[[1]][1:6]\n\n[1] \"\"                                                                                                                                                                                                                                                                                                                                                                                                                                           \n[2] \"*MOT: okay her favorite books (.) I don't read the whole stories they're  too long .  %mor: co|okay det:poss|her adj|favorite n|book-PL pro:sub|I  mod|do~neg|not v|read&ZERO det:art|the adj|whole n|story-PL  pro:sub|they~cop|be&PRES adv|too adj|long .  %gra: 1|4|COM 2|4|DET 3|4|MOD 4|0|INCROOT 5|8|SUBJ 6|8|AUX 7|6|NEG 8|4|CMOD  9|11|DET 10|11|MOD 11|8|OBJ 12|13|SUBJ 13|11|CMOD 14|15|JCT 15|13|PRED  16|4|PUNCT  %tim: 37:16  \"\n[3] \"*MOT: I give my own version .  %mor: pro:sub|I v|give det:poss|my adj|own n|version .  %gra: 1|2|SUBJ 2|0|ROOT 3|5|DET 4|5|MOD 5|2|OBJ 6|2|PUNCT  \"                                                                                                                                                                                                                                                                                         \n[4] \"*EX1: okay .  %mor: co|okay .  %gra: 1|0|INCROOT 2|1|PUNCT  \"                                                                                                                                                                                                                                                                                                                                                                               \n[5] \"*EX1: that's fine .  %mor: pro:dem|that~cop|be&3S adj|fine .  %gra: 1|2|SUBJ 2|0|ROOT 3|2|PRED 4|2|PUNCT  \"                                                                                                                                                                                                                                                                                                                                 \n[6] \"*EX1: whatever you usually do .  %mor: pro:int|whatever pro:per|you adv|usual&dadj-LY v|do .  %gra: 1|4|LINK 2|4|SUBJ 3|4|JCT 4|0|ROOT 5|4|PUNCT  \"                                                                                                                                                                                                                                                                                         \n\n\nThe data now consists of utterances but also the pos-tagged utterances and any comments. However, we use this form of the data to extract the clean utterances, the pos-tagged utterances and the comments and store them in different columns.\nExtract information\nNow, we extract how many elements (or utterances) there are in each file by looping over the content object and extracting the number of elements within each element of the content object by using the lenght function.\n\nelements <- sapply(content, function(x){\n  x <- length(x)\n})\n# inspect\nhead(elements)\n\n   D:\\\\Uni\\\\UiT\\\\Workshops\\\\RCHILDES\\\\data\\\\HSLLD/HV1/BR/acebr1.cha \n                                                                261 \nD:\\\\Uni\\\\UiT\\\\Workshops\\\\RCHILDES\\\\data\\\\HSLLD/HV1/BR/acebr1pt2.cha \n                                                                  1 \n   D:\\\\Uni\\\\UiT\\\\Workshops\\\\RCHILDES\\\\data\\\\HSLLD/HV1/BR/admbr1.cha \n                                                                178 \n   D:\\\\Uni\\\\UiT\\\\Workshops\\\\RCHILDES\\\\data\\\\HSLLD/HV1/BR/aimbr1.cha \n                                                                346 \n   D:\\\\Uni\\\\UiT\\\\Workshops\\\\RCHILDES\\\\data\\\\HSLLD/HV1/BR/alibr1.cha \n                                                                435 \n   D:\\\\Uni\\\\UiT\\\\Workshops\\\\RCHILDES\\\\data\\\\HSLLD/HV1/BR/allbr1.cha \n                                                                135 \n\n\nGenerate table\nWe now have the file names, the metadata for each file, and the content of each file (that is split into utterances). We use this information to generate a first table which holds the file name in one column, the file information in one column, and the raw file content in another column. To combine these three pieces of information though, we need to repeat the file names and the file information as often as there are utterances in each file. We perform this repetition using the rep function. Once we have as many file names and file information as there are utterances in each file, we can combine these three vectors into a table using the data.frame function.\n\nfiles <- rep(names(elements), elements)\nfileinfo <- rep(fileinfo, elements)\nrawcontent <- as.vector(unlist(content))\nchitb <- data.frame(1:length(rawcontent),\n                    files,\n                    fileinfo,\n                    rawcontent)\n\nThe table in its current form is shown below. We can see that the table has three columns: the first column holds the path to each file, the second contains the file information, and the third the utterances.\n\n\n\n\nX1.length.rawcontent.filesfileinforawcontent1D:\\Uni\\UiT\\Workshops\\RCHILDES\\data\\HSLLD/HV1/BR/acebr1.cha@UTF8 @PID: 11312/c-00034768-1 @Begin @Languages: eng @Participants: CHI Target_Child , MOT Mother , EX1 Investigator @ID: eng|HSLLD|CHI|3;07.08|female|||Target_Child||| @ID: eng|HSLLD|MOT||female|||Mother|14|| @ID: eng|HSLLD|EX1|||||Investigator||| @Birth of CHI: 11-DEC-1984 @Media: acebr1, audio, unlinked @Comment: SES of Mot is lower @Date: 19-JUL-1988 @Location: Boston , MA USA @Situation: Home Visit 1; took place in kitchen; Chi and Mot sat next  to each other in separate chairs , television is running;  7pm. @Activities: Book Reading 1 (The Very Hungry Caterpillar) @Bg: bookreading @Types: long, book, TD 2D:\\Uni\\UiT\\Workshops\\RCHILDES\\data\\HSLLD/HV1/BR/acebr1.cha@UTF8 @PID: 11312/c-00034768-1 @Begin @Languages: eng @Participants: CHI Target_Child , MOT Mother , EX1 Investigator @ID: eng|HSLLD|CHI|3;07.08|female|||Target_Child||| @ID: eng|HSLLD|MOT||female|||Mother|14|| @ID: eng|HSLLD|EX1|||||Investigator||| @Birth of CHI: 11-DEC-1984 @Media: acebr1, audio, unlinked @Comment: SES of Mot is lower @Date: 19-JUL-1988 @Location: Boston , MA USA @Situation: Home Visit 1; took place in kitchen; Chi and Mot sat next  to each other in separate chairs , television is running;  7pm. @Activities: Book Reading 1 (The Very Hungry Caterpillar) @Bg: bookreading @Types: long, book, TD *MOT: okay her favorite books (.) I don't read the whole stories they're  too long .  %mor: co|okay det:poss|her adj|favorite n|book-PL pro:sub|I  mod|do~neg|not v|read&ZERO det:art|the adj|whole n|story-PL  pro:sub|they~cop|be&PRES adv|too adj|long .  %gra: 1|4|COM 2|4|DET 3|4|MOD 4|0|INCROOT 5|8|SUBJ 6|8|AUX 7|6|NEG 8|4|CMOD  9|11|DET 10|11|MOD 11|8|OBJ 12|13|SUBJ 13|11|CMOD 14|15|JCT 15|13|PRED  16|4|PUNCT  %tim: 37:16  3D:\\Uni\\UiT\\Workshops\\RCHILDES\\data\\HSLLD/HV1/BR/acebr1.cha@UTF8 @PID: 11312/c-00034768-1 @Begin @Languages: eng @Participants: CHI Target_Child , MOT Mother , EX1 Investigator @ID: eng|HSLLD|CHI|3;07.08|female|||Target_Child||| @ID: eng|HSLLD|MOT||female|||Mother|14|| @ID: eng|HSLLD|EX1|||||Investigator||| @Birth of CHI: 11-DEC-1984 @Media: acebr1, audio, unlinked @Comment: SES of Mot is lower @Date: 19-JUL-1988 @Location: Boston , MA USA @Situation: Home Visit 1; took place in kitchen; Chi and Mot sat next  to each other in separate chairs , television is running;  7pm. @Activities: Book Reading 1 (The Very Hungry Caterpillar) @Bg: bookreading @Types: long, book, TD *MOT: I give my own version .  %mor: pro:sub|I v|give det:poss|my adj|own n|version .  %gra: 1|2|SUBJ 2|0|ROOT 3|5|DET 4|5|MOD 5|2|OBJ 6|2|PUNCT  \n\n\nProcess table\nWe can now use the information in the two last columns to extract specific pieces of information from the data (which we will store in additional columns that we add to the table). But first, we rename the id column (which is simply an index of each utterance) using the rename function from the dplyr package. Then, we clean the file name column (called files) so that it only contains the name of the file, so we remove the rest of the path information that we do not need anymore. We do this by using the mutate function from the dplyr package (which changes columns or creates new columns). Within the mutate function, we use the gsub function which substitutes something with something else: here the full path is replaced with on that part of the path that contains the file name. The gsub function has the following form\n gsub(*look for pattern*, *replacement of the pattern*, object) \nThis means that the gsub function needs an object and in that object it looks for a pattern and then replaces instances f that pattern with something.\nIn our case, that what we look for is the file name which is located between the symbol / and the file ending (.cha). So, we extract everything that comes between a / and a .cha in the path and keep that what is between the / and a .cha in R’s memory (this is done by placing something in round brackets in a regular expression). Then, we paste that what we have extracted back (and which is stored in memory) by using the \\\\1 which grabs the first element that is in memory and puts it into the replace with part of the gsub function.\n\nhslld <- chitb %>%\n  # rename id column\n  dplyr::rename(id = colnames(chitb)[1]) %>%\n  # clean file names\n  dplyr::mutate(files = gsub(\".*/(.*?).cha\", \"\\\\1\", files))\n\nLet’s have a look at the data.\n\n\n\n\nidfilesfileinforawcontent1acebr1@UTF8 @PID: 11312/c-00034768-1 @Begin @Languages: eng @Participants: CHI Target_Child , MOT Mother , EX1 Investigator @ID: eng|HSLLD|CHI|3;07.08|female|||Target_Child||| @ID: eng|HSLLD|MOT||female|||Mother|14|| @ID: eng|HSLLD|EX1|||||Investigator||| @Birth of CHI: 11-DEC-1984 @Media: acebr1, audio, unlinked @Comment: SES of Mot is lower @Date: 19-JUL-1988 @Location: Boston , MA USA @Situation: Home Visit 1; took place in kitchen; Chi and Mot sat next  to each other in separate chairs , television is running;  7pm. @Activities: Book Reading 1 (The Very Hungry Caterpillar) @Bg: bookreading @Types: long, book, TD 2acebr1@UTF8 @PID: 11312/c-00034768-1 @Begin @Languages: eng @Participants: CHI Target_Child , MOT Mother , EX1 Investigator @ID: eng|HSLLD|CHI|3;07.08|female|||Target_Child||| @ID: eng|HSLLD|MOT||female|||Mother|14|| @ID: eng|HSLLD|EX1|||||Investigator||| @Birth of CHI: 11-DEC-1984 @Media: acebr1, audio, unlinked @Comment: SES of Mot is lower @Date: 19-JUL-1988 @Location: Boston , MA USA @Situation: Home Visit 1; took place in kitchen; Chi and Mot sat next  to each other in separate chairs , television is running;  7pm. @Activities: Book Reading 1 (The Very Hungry Caterpillar) @Bg: bookreading @Types: long, book, TD *MOT: okay her favorite books (.) I don't read the whole stories they're  too long .  %mor: co|okay det:poss|her adj|favorite n|book-PL pro:sub|I  mod|do~neg|not v|read&ZERO det:art|the adj|whole n|story-PL  pro:sub|they~cop|be&PRES adv|too adj|long .  %gra: 1|4|COM 2|4|DET 3|4|MOD 4|0|INCROOT 5|8|SUBJ 6|8|AUX 7|6|NEG 8|4|CMOD  9|11|DET 10|11|MOD 11|8|OBJ 12|13|SUBJ 13|11|CMOD 14|15|JCT 15|13|PRED  16|4|PUNCT  %tim: 37:16  3acebr1@UTF8 @PID: 11312/c-00034768-1 @Begin @Languages: eng @Participants: CHI Target_Child , MOT Mother , EX1 Investigator @ID: eng|HSLLD|CHI|3;07.08|female|||Target_Child||| @ID: eng|HSLLD|MOT||female|||Mother|14|| @ID: eng|HSLLD|EX1|||||Investigator||| @Birth of CHI: 11-DEC-1984 @Media: acebr1, audio, unlinked @Comment: SES of Mot is lower @Date: 19-JUL-1988 @Location: Boston , MA USA @Situation: Home Visit 1; took place in kitchen; Chi and Mot sat next  to each other in separate chairs , television is running;  7pm. @Activities: Book Reading 1 (The Very Hungry Caterpillar) @Bg: bookreading @Types: long, book, TD *MOT: I give my own version .  %mor: pro:sub|I v|give det:poss|my adj|own n|version .  %gra: 1|2|SUBJ 2|0|ROOT 3|5|DET 4|5|MOD 5|2|OBJ 6|2|PUNCT  4acebr1@UTF8 @PID: 11312/c-00034768-1 @Begin @Languages: eng @Participants: CHI Target_Child , MOT Mother , EX1 Investigator @ID: eng|HSLLD|CHI|3;07.08|female|||Target_Child||| @ID: eng|HSLLD|MOT||female|||Mother|14|| @ID: eng|HSLLD|EX1|||||Investigator||| @Birth of CHI: 11-DEC-1984 @Media: acebr1, audio, unlinked @Comment: SES of Mot is lower @Date: 19-JUL-1988 @Location: Boston , MA USA @Situation: Home Visit 1; took place in kitchen; Chi and Mot sat next  to each other in separate chairs , television is running;  7pm. @Activities: Book Reading 1 (The Very Hungry Caterpillar) @Bg: bookreading @Types: long, book, TD *EX1: okay .  %mor: co|okay .  %gra: 1|0|INCROOT 2|1|PUNCT  5acebr1@UTF8 @PID: 11312/c-00034768-1 @Begin @Languages: eng @Participants: CHI Target_Child , MOT Mother , EX1 Investigator @ID: eng|HSLLD|CHI|3;07.08|female|||Target_Child||| @ID: eng|HSLLD|MOT||female|||Mother|14|| @ID: eng|HSLLD|EX1|||||Investigator||| @Birth of CHI: 11-DEC-1984 @Media: acebr1, audio, unlinked @Comment: SES of Mot is lower @Date: 19-JUL-1988 @Location: Boston , MA USA @Situation: Home Visit 1; took place in kitchen; Chi and Mot sat next  to each other in separate chairs , television is running;  7pm. @Activities: Book Reading 1 (The Very Hungry Caterpillar) @Bg: bookreading @Types: long, book, TD *EX1: that's fine .  %mor: pro:dem|that~cop|be&3S adj|fine .  %gra: 1|2|SUBJ 2|0|ROOT 3|2|PRED 4|2|PUNCT  6acebr1@UTF8 @PID: 11312/c-00034768-1 @Begin @Languages: eng @Participants: CHI Target_Child , MOT Mother , EX1 Investigator @ID: eng|HSLLD|CHI|3;07.08|female|||Target_Child||| @ID: eng|HSLLD|MOT||female|||Mother|14|| @ID: eng|HSLLD|EX1|||||Investigator||| @Birth of CHI: 11-DEC-1984 @Media: acebr1, audio, unlinked @Comment: SES of Mot is lower @Date: 19-JUL-1988 @Location: Boston , MA USA @Situation: Home Visit 1; took place in kitchen; Chi and Mot sat next  to each other in separate chairs , television is running;  7pm. @Activities: Book Reading 1 (The Very Hungry Caterpillar) @Bg: bookreading @Types: long, book, TD *EX1: whatever you usually do .  %mor: pro:int|whatever pro:per|you adv|usual&dadj-LY v|do .  %gra: 1|4|LINK 2|4|SUBJ 3|4|JCT 4|0|ROOT 5|4|PUNCT  \n\n\nWe now continue in the same manner (by remove what is before what interests us and what comes after) and thereby extract pieces of information that we store in new columns.\nCreating a speaker column. We create a new column called speaker using the mutate function from the dplyr package. Then, we use the str_replace_all function from the stringr package to remove everything that comes after a :. Everything that comes after can be defined by a regular expression - in this case the sequence .*. The . is a regular expression that stands for any symbol - be it a letter, or a number, or any punctuation symbol, or a white space. The * is a numerating regular expression that tells R how many times the regular expression (the .) is repeated - in our case, the * stands for zero to an infinite number. So the sequence .* stands for any symbol, repeated zero to an infinite number of times. In combination, the sequence :.* stands for *look for a colon and anything that comes after. And because we have put this into the str_replace_all function, the colon and everything that comes after is removed.\n\nhslld <- hslld %>%  \n  dplyr::mutate(speaker = stringr::str_remove_all(rawcontent, \":.*\"),\n                speaker = stringr::str_remove_all(speaker, \"\\\\W\"))\n\nIn the following, we will create many different columns, but we will always follow the same scheme: generate a new column using the mutate function from the dplyr package and then remove stuff that we do not need by using the str_remove_all function from the stringr package or just the gsub function - which is a simple replacement function. We can also use str_squish to get rid of superfluous white spaces. We will always remove sequences that are defined by a string (a sequence of characters and a regular expression consisting of the regular expression that determines what type of symbol R is supposed to look for and a numerator which tells R how many times that symbol can occur). For example, %mor:.* tells R to look for the sequence %mor: and any symbol, repeated between zero and an infinite number of times, that comes after the %mor: sequence. As this is put into the str_replace_all function and applied to the rawcontent file, it will replace everything that comes after %mor: and the sequence %mor: itself.\nCreating an utterance column.\n\nhslld <- hslld %>%  \n  dplyr::mutate(utterance = stringr::str_remove_all(rawcontent, \"%mor:.*\"),\n                utterance = stringr::str_remove_all(utterance, \"%.*\"),\n                utterance = stringr::str_remove_all(utterance, \"\\\\*\\\\w{2,6}:\"),\n                utterance = stringr::str_squish(utterance))\n\nCreating a column with the pos-tagged utterances.\n\nhslld <- hslld %>%  \n  dplyr::mutate(postag = stringr::str_remove_all(rawcontent, \".*%mor:\"),\n                postag = stringr::str_remove_all(postag, \"%.*\"),\n                postag = stringr::str_remove_all(postag, \"\\\\*\\\\w{2,6}:\"),\n                postag = stringr::str_squish(postag))\n\nCreating a column with comments. In the following chunk, we use the ? in combination with .*. In this case, the ? does not mean the literal symbol ? but it tells R to be what is called non-greedy which means that R will look for something until the first occurrence of something. So the sequence .*?% tells R to look for any symbol repeated between zero and an infinite number of times until the first occurrence(!) of the symbol %. If we did not include the ?, R would look until the last (not the first) occurrence of %.\n\nhslld <- hslld %>%  \n  dplyr::mutate(comment = stringr::str_remove_all(rawcontent, \".*%mor:\"),\n                comment = stringr::str_remove(comment, \".*?%\"),\n                comment = stringr::str_remove_all(comment, \".*|.*\"),\n                comment = stringr::str_squish(comment))\n\nCreating a column with the participants that were present during the recording.\n\nhslld <- hslld %>%  \n  dplyr::mutate(participants = gsub(\".*@Participants:(.*?)@.*\", \"\\\\1\", fileinfo))\n\nCreating a column with the age of the target child. In the following, the sequence [0-9]{1,3} means look for any sequence containing between 1 and 3 (this is defined by the {1,3}) numbers (the numbers are defined by the [0-9] part). Also, when we put \\\\ before something, then we tell R that this refers to the actual symbol and not its meaning as a regular expression. For example, the symbol | is a regular expression that means or as in You can paint my walls blue OR orange, but if we put \\\\ before |, we tell R that we really mean the symbol |.\n\nhslld <- hslld %>%\n  dplyr::mutate(age_targetchild = gsub(\".*\\\\|([0-9]{1,3};[0-9]{1,3}\\\\.[0-9]{1,3})\\\\|.*\", \"\\\\1\", fileinfo)) \n\nCreating a column with the age of the target child in years.\n\nhslld <- hslld %>%\n  dplyr::mutate(age_years_targetchild = stringr::str_remove_all(age_targetchild, \";.*\")) \n\nCreating a column with the gender of the target child.\n\nhslld <- hslld %>%\n  dplyr::mutate(gender_targetchild = gsub(\".*\\\\|([female]{4,6})\\\\|.*\", \"\\\\1\", fileinfo))\n\nCreating columns with the date-of-birth of the target child, more comments, and the date of the recording.\n\nhslld <- hslld %>%  \n  # create dob_targetchild column\n  dplyr::mutate(dob_targetchild = gsub(\".*@Birth of CHI:(.*?)@.*\",\"\\\\1\", fileinfo)) %>%\n  # create comment_file column\n  dplyr::mutate(comment_file = gsub(\".*@Comment: (.*?)@.*\", \"\\\\1\", fileinfo)) %>%\n  # create date column\n  dplyr::mutate(date = gsub(\".*@Date: (.*?)@.*\", \"\\\\1\", fileinfo))\n\nCreating columns with the location where the recording took place and the situation type of the recording.\n\nhslld <- hslld %>%  \n  # create location column,\n  dplyr::mutate(location = gsub(\".*@Location: (.*?)@.*\", \"\\\\1\", fileinfo)) %>%\n  # create situation column\n  dplyr::mutate(situation = gsub(\".*@Situation: (.*?)@.*\", \"\\\\1\", fileinfo))\n\nCreating columns with the activity during the recording and the home-visit number.\n\nhslld <- hslld %>%  \n  # create homevisit_activity column\n  dplyr::mutate(homevisit_activity = stringr::str_remove_all(situation, \";.*\")) %>%\n  # create activity column\n  dplyr::mutate(activity = gsub(\".*@Activities: (.*?)@.*\", \"\\\\1\", fileinfo)) %>%\n  # create homevisit column\n  dplyr::mutate(homevisit = stringr::str_sub(files, 4, 6))\n\nCreating a column with the number of words in each utterance.\n\nhslld <- hslld %>%  \n  # create words column\n  dplyr::mutate(words = stringr::str_replace_all(utterance, \"\\\\W\", \" \"),\n                words = stringr::str_squish(words),\n                words = stringr::str_count(words, \"\\\\w+\"))\n\nCleaning the data: removing rows without speakers, rows where the age of the target child was incorrect, and removing superfluous columns.\n\nhslld <- hslld %>%  \n  # remove rows without speakers (contain only metadata)\n  dplyr::filter(speaker != \"\") %>%\n  # remove rows with incorrect age of child\n  dplyr::filter(nchar(age_years_targetchild) < 5) %>%\n  # remove superfluous columns\n  dplyr::select(-fileinfo, -rawcontent, -situation)  %>%\n  # create words column\n  dplyr::mutate(collection = \"EngNA\",\n                corpus = \"HSLLD\") %>%\n  dplyr::rename(transcript_id = files) %>%\n    # code activity\n  dplyr::mutate(visit = substr(transcript_id, 6, 6)) %>%\n  dplyr::mutate(situation = substr(transcript_id, 4, 5),\n                situation = str_replace_all(situation, \"br\", \"Book reading\"),\n                situation = str_replace_all(situation, \"er\", \"Elicited report\"),\n                situation = str_replace_all(situation, \"et\", \"Experimental task\"),\n                situation = str_replace_all(situation, \"lw\", \"Letter writing\"),\n                situation = str_replace_all(situation, \"md\", \"Mother defined situation\"),\n                situation = str_replace_all(situation, \"mt\", \"Meal time\"),\n                situation = str_replace_all(situation, \"re\", \"Reading\"),\n                situation = str_replace_all(situation, \"tp\", \"Toy play\"))\n\n\n\n\n\nidtranscript_idspeakerutterancepostagcommentparticipantsage_targetchildage_years_targetchildgender_targetchilddob_targetchildcomment_filedatelocationhomevisit_activityactivityhomevisitwordscollectioncorpusvisitsituation2acebr1MOTokay her favorite books (.) I don't read the whole stories they're too long .co|okay det:poss|her adj|favorite n|book-PL pro:sub|I mod|do~neg|not v|read&ZERO det:art|the adj|whole n|story-PL pro:sub|they~cop|be&PRES adv|too adj|long . CHI Target_Child , MOT Mother , EX1 Investigator 3;07.083female 11-DEC-1984 SES of Mot is lower 19-JUL-1988 Boston , MA USA Home Visit 1Book Reading 1 (The Very Hungry Caterpillar) br115EngNAHSLLD1Book Readingading3acebr1MOTI give my own version .pro:sub|I v|give det:poss|my adj|own n|version . CHI Target_Child , MOT Mother , EX1 Investigator 3;07.083female 11-DEC-1984 SES of Mot is lower 19-JUL-1988 Boston , MA USA Home Visit 1Book Reading 1 (The Very Hungry Caterpillar) br15EngNAHSLLD1Book Readingading4acebr1EX1okay .co|okay . CHI Target_Child , MOT Mother , EX1 Investigator 3;07.083female 11-DEC-1984 SES of Mot is lower 19-JUL-1988 Boston , MA USA Home Visit 1Book Reading 1 (The Very Hungry Caterpillar) br11EngNAHSLLD1Book Readingading5acebr1EX1that's fine .pro:dem|that~cop|be&3S adj|fine . CHI Target_Child , MOT Mother , EX1 Investigator 3;07.083female 11-DEC-1984 SES of Mot is lower 19-JUL-1988 Boston , MA USA Home Visit 1Book Reading 1 (The Very Hungry Caterpillar) br13EngNAHSLLD1Book Readingading6acebr1EX1whatever you usually do .pro:int|whatever pro:per|you adv|usual&dadj-LY v|do . CHI Target_Child , MOT Mother , EX1 Investigator 3;07.083female 11-DEC-1984 SES of Mot is lower 19-JUL-1988 Boston , MA USA Home Visit 1Book Reading 1 (The Very Hungry Caterpillar) br14EngNAHSLLD1Book Readingading7acebr1EX1just read to her as if we weren't even here .adv|just v|read&ZERO prep|to pro:obj|her prep|as conj|if pro:sub|we cop|be&PAST~neg|not adv|even adv|here . CHI Target_Child , MOT Mother , EX1 Investigator 3;07.083female 11-DEC-1984 SES of Mot is lower 19-JUL-1988 Boston , MA USA Home Visit 1Book Reading 1 (The Very Hungry Caterpillar) br111EngNAHSLLD1Book Readingading\n\n\nNow that we have the data in a format that we can use, we can use this table to continue with our case studies."
  },
  {
    "objectID": "corplingr.html#case-study-1-use-of-no",
    "href": "corplingr.html#case-study-1-use-of-no",
    "title": "Corpus Linguistics with R",
    "section": "Case study 1: Use of NO",
    "text": "Case study 1: Use of NO\nTo extract all instances of a single word, in this example the word no, that are uttered by a specific interlocutor we filter by speaker and define that we only want rows where the speaker is equal to CHI (target child).\n\nno <- hslld %>%\n  dplyr::filter(speaker == \"CHI\") %>%\n  dplyr::filter(stringr::str_detect(utterance, \"\\\\b[Nn][Oo]\\\\b\")) \n\n\n\n\n\nidtranscript_idspeakerutterancepostagcommentparticipantsage_targetchildage_years_targetchildgender_targetchilddob_targetchildcomment_filedatelocationhomevisit_activityactivityhomevisitwordscollectioncorpusvisitsituation267admbr1CHIno you can't read it .co|no pro:per|you mod|can~neg|not v|read&ZERO pro:per|it . CHI Target_Child , MOT Mother , UNC Uncle Relative, EX1  Investigator 4;01.094female 05-MAY-1985 SES of Mot is lower 14-JUN-1989 Boston , MA USA Home Visit 1 Book Reading 1 The Very Hungry Caterpillar br16EngNAHSLLD1Book Readingading409admbr1CHIno .co|no . CHI Target_Child , MOT Mother , UNC Uncle Relative, EX1  Investigator 4;01.094female 05-MAY-1985 SES of Mot is lower 14-JUN-1989 Boston , MA USA Home Visit 1 Book Reading 1 The Very Hungry Caterpillar br11EngNAHSLLD1Book Readingading411admbr1CHIno no I'm loco@s:spa &=laughingly .co|no co|no pro:sub|I~cop|be&1S L2|loco . CHI Target_Child , MOT Mother , UNC Uncle Relative, EX1  Investigator 4;01.094female 05-MAY-1985 SES of Mot is lower 14-JUN-1989 Boston , MA USA Home Visit 1 Book Reading 1 The Very Hungry Caterpillar br18EngNAHSLLD1Book Readingading552aimbr1CHIno !co|no ! CHI Target_Child , MOT Mother , BRO Robbie Brother , BAB  Baby Brother , INV Investigator 3;10.163female 22-JUL-1984 SES of Mot is lower 07-JUN-1988 Boston , MA , USA Home Visit 1Book Reading 1 (The Very Hungry Caterpillar) br11EngNAHSLLD1Book Readingading554aimbr1CHIno [<] !co|no ! CHI Target_Child , MOT Mother , BRO Robbie Brother , BAB  Baby Brother , INV Investigator 3;10.163female 22-JUL-1984 SES of Mot is lower 07-JUN-1988 Boston , MA , USA Home Visit 1Book Reading 1 (The Very Hungry Caterpillar) br11EngNAHSLLD1Book Readingading750aimbr1CHIno .co|no . CHI Target_Child , MOT Mother , BRO Robbie Brother , BAB  Baby Brother , INV Investigator 3;10.163female 22-JUL-1984 SES of Mot is lower 07-JUN-1988 Boston , MA , USA Home Visit 1Book Reading 1 (The Very Hungry Caterpillar) br11EngNAHSLLD1Book Readingading\n\n\nWe summarize the results in a table.\n\nno_no <- no %>%\n  dplyr::group_by(transcript_id, gender_targetchild, age_years_targetchild) %>%\n  dplyr::summarise(nos = nrow(.))\nhead(no_no)\n\n# A tibble: 6 × 4\n# Groups:   transcript_id, gender_targetchild [6]\n  transcript_id gender_targetchild age_years_targetchild   nos\n  <chr>         <chr>              <chr>                 <int>\n1 aceet7        male               9                      4420\n2 acemt7        male               10                     4420\n3 acetp2        female             4                      4420\n4 admbr1        female             4                      4420\n5 admbr2        female             4                      4420\n6 admbr3        male               5                      4420\n\n\nWe can also extract the number of words uttered by children to check if the use of no shows a relative increase or decrease over time.\n\nno_words <- hslld %>%\n  dplyr::filter(speaker == \"CHI\") %>%\n  dplyr::group_by(transcript_id, gender_targetchild, age_years_targetchild) %>%\n  dplyr::mutate(nos = stringr::str_detect(utterance, \"\\\\b[Nn][Oo]\\\\b\")) %>%\n  dplyr::summarise(nos = sum(nos),\n                   words = sum(words)) %>%\n  # add relative frequency\n  dplyr::mutate(freq = round(nos/words*1000, 3))\n# inspect data\nhead(no_words)\n\n# A tibble: 6 × 6\n# Groups:   transcript_id, gender_targetchild [6]\n  transcript_id gender_targetchild age_years_targetchild   nos words  freq\n  <chr>         <chr>              <chr>                 <int> <int> <dbl>\n1 acebr1        female             3                         0   149   0  \n2 acebr2        female             4                         0   322   0  \n3 acebr5        female             7                         0   270   0  \n4 aceer1        female             3                         0     4   0  \n5 aceer2        female             4                         0    29   0  \n6 aceet7        male               9                         7   458  15.3\n\n\nWe can also visualize the trends using the ggplot function . To learn how to visualize data in R see this tutorial.\n\nno_words %>%\n  dplyr::mutate(age_years_targetchild = as.numeric(age_years_targetchild)) %>%\n  ggplot(aes(x = age_years_targetchild, y = freq)) +\n  geom_smooth() +\n  theme_bw() +\n  labs(x = \"Age of target child\", y = \"Relative frequency of NOs \\n (per 1,000 words)\")"
  },
  {
    "objectID": "corplingr.html#case-study-2-extracting-questions",
    "href": "corplingr.html#case-study-2-extracting-questions",
    "title": "Corpus Linguistics with R",
    "section": "Case study 2: extracting questions",
    "text": "Case study 2: extracting questions\nHere, we want to extract all questions uttered by mothers. We operationalize questions as utterances containing a question mark.\n\nquestions <- hslld %>%\n  dplyr::filter(speaker == \"MOT\") %>%\n  dplyr::filter(stringr::str_detect(utterance, \"\\\\?\"))\n# inspect data\nhead(questions)\n\n  id transcript_id speaker                                        utterance\n1  9        acebr1     MOT                                            Chi ?\n2 10        acebr1     MOT                   you wan(t) (t)a hear a story ?\n3 15        acebr1     MOT                      will you show me the moon ?\n4 17        acebr1     MOT           Chi you don't know where the moon is ?\n5 19        acebr1     MOT                               is that the moon ?\n6 21        acebr1     MOT okay where's the egg that's laying on the leaf ?\n                                                                                                                postag\n1                                                                                                         n:prop|Chi ?\n2                                                                 pro:per|you v|want inf|to v|hear det:art|a n|story ?\n3                                                          mod|will pro:per|you v|show pro:obj|me det:art|the n|moon ?\n4                            n:prop|Chi pro:per|you mod|do~neg|not v|know pro:int|where det:art|the n|moon cop|be&3S ?\n5                                                                             cop|be&3S comp|that det:art|the n|moon ?\n6 co|okay pro:int|where~cop|be&3S det:art|the n|egg pro:rel|that~aux|be&3S part|lay-PRESP prep|on det:art|the n|leaf ?\n  comment                                       participants age_targetchild\n1          CHI Target_Child , MOT Mother , EX1 Investigator          3;07.08\n2          CHI Target_Child , MOT Mother , EX1 Investigator          3;07.08\n3          CHI Target_Child , MOT Mother , EX1 Investigator          3;07.08\n4          CHI Target_Child , MOT Mother , EX1 Investigator          3;07.08\n5          CHI Target_Child , MOT Mother , EX1 Investigator          3;07.08\n6          CHI Target_Child , MOT Mother , EX1 Investigator          3;07.08\n  age_years_targetchild gender_targetchild dob_targetchild         comment_file\n1                     3             female    11-DEC-1984  SES of Mot is lower \n2                     3             female    11-DEC-1984  SES of Mot is lower \n3                     3             female    11-DEC-1984  SES of Mot is lower \n4                     3             female    11-DEC-1984  SES of Mot is lower \n5                     3             female    11-DEC-1984  SES of Mot is lower \n6                     3             female    11-DEC-1984  SES of Mot is lower \n          date         location homevisit_activity\n1 19-JUL-1988  Boston , MA USA        Home Visit 1\n2 19-JUL-1988  Boston , MA USA        Home Visit 1\n3 19-JUL-1988  Boston , MA USA        Home Visit 1\n4 19-JUL-1988  Boston , MA USA        Home Visit 1\n5 19-JUL-1988  Boston , MA USA        Home Visit 1\n6 19-JUL-1988  Boston , MA USA        Home Visit 1\n                                       activity homevisit words collection\n1 Book Reading 1 (The Very Hungry Caterpillar)        br1     1      EngNA\n2 Book Reading 1 (The Very Hungry Caterpillar)        br1     8      EngNA\n3 Book Reading 1 (The Very Hungry Caterpillar)        br1     6      EngNA\n4 Book Reading 1 (The Very Hungry Caterpillar)        br1     9      EngNA\n5 Book Reading 1 (The Very Hungry Caterpillar)        br1     4      EngNA\n6 Book Reading 1 (The Very Hungry Caterpillar)        br1    11      EngNA\n  corpus visit         situation\n1  HSLLD     1 Book Readingading\n2  HSLLD     1 Book Readingading\n3  HSLLD     1 Book Readingading\n4  HSLLD     1 Book Readingading\n5  HSLLD     1 Book Readingading\n6  HSLLD     1 Book Readingading\n\n\nWe could now check if the rate of questions changes over time.\n\nqmot <- hslld %>%\n  dplyr::filter(speaker == \"MOT\") %>%\n  dplyr::mutate(questions = ifelse(stringr::str_detect(utterance, \"\\\\?\") == T, 1,0),\n                utterances = 1) %>%\n  dplyr::group_by(age_years_targetchild) %>%\n  dplyr::summarise(utterances = sum(utterances),\n                questions = sum(questions),\n                percent = round(questions/utterances*100, 2))\n# inspect data\nhead(qmot)\n\n# A tibble: 6 × 4\n  age_years_targetchild utterances questions percent\n  <chr>                      <dbl>     <dbl>   <dbl>\n1 10                          4249       925    21.8\n2 11                           343       141    41.1\n3 12                            56        22    39.3\n4 3                          27209      9089    33.4\n5 4                          45068     14487    32.1\n6 5                          37634     10844    28.8\n\n\n\nqmot %>%\n  dplyr::mutate(age_years_targetchild = as.numeric(age_years_targetchild)) %>%\n  ggplot(aes(x = age_years_targetchild, y = percent)) +\n  geom_smooth() +\n  theme_bw() +\n  labs(x = \"Age of target child\", y = \"Percent \\n (questions)\")"
  },
  {
    "objectID": "corplingr.html#case-study-3-extracting-aux-parts",
    "href": "corplingr.html#case-study-3-extracting-aux-parts",
    "title": "Corpus Linguistics with R",
    "section": "Case study 3: extracting aux + parts",
    "text": "Case study 3: extracting aux + parts\nHere we want to extract all occurrences of an auxiliary plus a participle (e.g. is swimming) produced by mothers.\n\nauxv <- hslld %>%\n  dplyr::filter(speaker == \"MOT\") %>%\n  dplyr::filter(stringr::str_detect(postag, \"aux\\\\|\\\\S{1,} part\\\\|\"))\n# inspect data\nhead(auxv)\n\n   id transcript_id speaker                                        utterance\n1  21        acebr1     MOT okay where's the egg that's laying on the leaf ?\n2  56        acebr1     MOT                      and here he is coming out !\n3  68        acebr1     MOT                   looks like he's eating a lot .\n4 202        acebr1     MOT        see the dog's getting closer to the cat .\n5 204        acebr1     MOT              (be)cause he's getting closer [!] .\n6 205        acebr1     MOT                       he's gonna catch the cat .\n                                                                                                                postag\n1 co|okay pro:int|where~cop|be&3S det:art|the n|egg pro:rel|that~aux|be&3S part|lay-PRESP prep|on det:art|the n|leaf ?\n2                                                    coord|and adv|here pro:sub|he aux|be&3S part|come-PRESP adv|out !\n3                                            v|look-3S conj|like pro:sub|he~aux|be&3S part|eat-PRESP det:art|a n|lot .\n4                            v|see det:art|the n|dog~aux|be&3S part|get-PRESP adj|close-CP prep|to det:art|the n|cat .\n5                                                      conj|because pro:sub|he~aux|be&3S part|get-PRESP adj|close-CP .\n6                                                pro:sub|he~aux|be&3S part|go-PRESP~inf|to v|catch det:art|the n|cat .\n  comment                                       participants age_targetchild\n1          CHI Target_Child , MOT Mother , EX1 Investigator          3;07.08\n2          CHI Target_Child , MOT Mother , EX1 Investigator          3;07.08\n3          CHI Target_Child , MOT Mother , EX1 Investigator          3;07.08\n4          CHI Target_Child , MOT Mother , EX1 Investigator          3;07.08\n5          CHI Target_Child , MOT Mother , EX1 Investigator          3;07.08\n6          CHI Target_Child , MOT Mother , EX1 Investigator          3;07.08\n  age_years_targetchild gender_targetchild dob_targetchild         comment_file\n1                     3             female    11-DEC-1984  SES of Mot is lower \n2                     3             female    11-DEC-1984  SES of Mot is lower \n3                     3             female    11-DEC-1984  SES of Mot is lower \n4                     3             female    11-DEC-1984  SES of Mot is lower \n5                     3             female    11-DEC-1984  SES of Mot is lower \n6                     3             female    11-DEC-1984  SES of Mot is lower \n          date         location homevisit_activity\n1 19-JUL-1988  Boston , MA USA        Home Visit 1\n2 19-JUL-1988  Boston , MA USA        Home Visit 1\n3 19-JUL-1988  Boston , MA USA        Home Visit 1\n4 19-JUL-1988  Boston , MA USA        Home Visit 1\n5 19-JUL-1988  Boston , MA USA        Home Visit 1\n6 19-JUL-1988  Boston , MA USA        Home Visit 1\n                                       activity homevisit words collection\n1 Book Reading 1 (The Very Hungry Caterpillar)        br1    11      EngNA\n2 Book Reading 1 (The Very Hungry Caterpillar)        br1     6      EngNA\n3 Book Reading 1 (The Very Hungry Caterpillar)        br1     7      EngNA\n4 Book Reading 1 (The Very Hungry Caterpillar)        br1     9      EngNA\n5 Book Reading 1 (The Very Hungry Caterpillar)        br1     6      EngNA\n6 Book Reading 1 (The Very Hungry Caterpillar)        br1     6      EngNA\n  corpus visit         situation\n1  HSLLD     1 Book Readingading\n2  HSLLD     1 Book Readingading\n3  HSLLD     1 Book Readingading\n4  HSLLD     1 Book Readingading\n5  HSLLD     1 Book Readingading\n6  HSLLD     1 Book Readingading\n\n\nWe can now extract all the particle forms from the pos-tagged utterance\n\nauxv_verbs <- auxv %>%\n  dplyr::mutate(participle = gsub(\".*part\\\\|(\\\\w{1,})-.*\", \"\\\\1\", postag)) %>%\n  dplyr::pull(participle)\nhead(auxv_verbs)\n\n[1] \"lay\"  \"come\" \"eat\"  \"get\"  \"get\"  \"go\"  \n\n\n\nauxv_verbs_df <- auxv_verbs %>%\n  as.data.frame(.)  %>%\n  dplyr::rename(\"verb\" = colnames(.)[1]) %>%\n  dplyr::group_by(verb) %>%\n  dplyr::summarise(freq = n()) %>%\n  dplyr::arrange(-freq) %>%\n  head(20)\n# inspect\nhead(auxv_verbs_df)\n\n# A tibble: 6 × 2\n  verb     freq\n  <chr>   <int>\n1 go       1927\n2 call      308\n3 do        243\n4 eat       205\n5 get       184\n6 suppose   146\n\n\nWe can again visualize the results. In this case, we create a bar plot (see the geom_bar).\n\nauxv_verbs_df %>%\n  ggplot(aes(x = reorder(verb, -freq), y = freq)) +\n  geom_bar(stat = \"identity\") +\n  theme_bw() +\n  labs(x = \"Verb\", y = \"Frequency\") +\n  theme(axis.text.x = element_text(angle = 90))"
  },
  {
    "objectID": "corplingr.html#case-study-4-ratio-of-verbs-to-words",
    "href": "corplingr.html#case-study-4-ratio-of-verbs-to-words",
    "title": "Corpus Linguistics with R",
    "section": "Case study 4: ratio of verbs to words",
    "text": "Case study 4: ratio of verbs to words\nHere we extract all lexical verbs and words uttered by children by year and then see if the rate of verbs changes over time.\n\nnverbs <- hslld %>%\n  dplyr::filter(speaker == \"CHI\") %>%\n  dplyr::mutate(nverbs = stringr::str_count(postag, \"^v\\\\|| v\\\\|\"),\n  age_years_targetchild = as.numeric(age_years_targetchild)) %>%\n  dplyr::group_by(age_years_targetchild) %>%\n  dplyr::summarise(words = sum(words),\n                verbs = sum(nverbs)) %>%\n  dplyr::mutate(verb.word.ratio = round(verbs/words, 3))\n# inspect data\nnverbs\n\n# A tibble: 10 × 4\n   age_years_targetchild  words verbs verb.word.ratio\n                   <dbl>  <int> <int>           <dbl>\n 1                     3  56864  5424           0.095\n 2                     4 101992 10355           0.102\n 3                     5 112173 11935           0.106\n 4                     6   8796   934           0.106\n 5                     7  59755  5405           0.09 \n 6                     8   5523   588           0.106\n 7                     9  46321  4739           0.102\n 8                    10  20310  2169           0.107\n 9                    11   1441   160           0.111\n10                    12    173    13           0.075\n\n\nWe can also visualize the results to show any changes over time.\n\nnverbs %>%\n  ggplot(aes(x = age_years_targetchild, y = verb.word.ratio)) +\n  geom_line() +\n  coord_cartesian(ylim = c(0, 0.2)) +\n  theme_bw() +\n  labs(x = \"Age of target child\", y = \"Verb-Word Ratio\")"
  },
  {
    "objectID": "corplingr.html#case-study-5-type-token-ratio-over-time",
    "href": "corplingr.html#case-study-5-type-token-ratio-over-time",
    "title": "Corpus Linguistics with R",
    "section": "Case study 5: type-token ratio over time",
    "text": "Case study 5: type-token ratio over time\nHere we extract all tokens (words with repetition) and types (words without repetition) uttered by children by year and then see if the type-token ratio changes over time.\nIn a first step, we create a table with the age of the children in years, we then collapse all utterances of the children into one long utterance and then clean this long utterance by removing digits and superfluous white spaces.\n\n\n\n\nTIPA more accurate way of doing this would be to create one utterance for each child per home visit as this would give us a distribution of type-token ratios rather than a single value.\n\n\n\n\n\n\n\nutterance_tb <- hslld %>%\n  dplyr::filter(speaker == \"CHI\") %>%\n  dplyr::group_by(age_years_targetchild) %>%\n  dplyr::summarise(allutts = paste0(utterance, collapse = \" \")) %>%\n  dplyr::ungroup() %>%\n  dplyr::mutate(age_years_targetchild = as.numeric(age_years_targetchild),\n                # clean utterance\n                allutts = stringr::str_replace_all(allutts, \"\\\\W\", \" \"),\n                allutts = stringr::str_replace_all(allutts, \"\\\\d\", \" \"),\n                allutts = stringr::str_remove_all(allutts, \"xxx\"),\n                allutts = stringr::str_remove_all(allutts, \"zzz\"),\n                allutts = tolower(allutts)) %>%\n  # remove superfluous white spaces\n  dplyr::mutate(allutts = gsub(\" {2,}\", \" \", allutts)) %>%\n  dplyr::mutate(allutts = stringr::str_squish(allutts))\n# inspect data\nhead(utterance_tb)\n\n# A tibble: 6 × 2\n  age_years_targetchild allutts                                                 \n                  <dbl> <chr>                                                   \n1                    10 laughs mutters www i get one of th em right here sure j…\n2                    11 i m adding adding two long necks yeah yeah two giraffes…\n3                    12 yes mommy it s very good it s good yes it was great i l…\n4                     3 hm where he s yellow and green and pink and green and y…\n5                     4 this one no you can t read it yeah i ll reach for the s…\n6                     5 mommy a turtle bee uhuh a caterpillar i don t know how …\n\n\nExtract the number of tokens, the number of types and calculating the type-token ratio.\n\ntokens <- stringr::str_count(utterance_tb$allutts, \" \") +1\ntypes <- stringr::str_split(utterance_tb$allutts, \" \")\ntypes <- sapply(types, function(x){\n  x <- length(names(table(x)))\n})\nttr <- utterance_tb %>%\n  dplyr::mutate(tokens = tokens,\n                types = types) %>%\n  dplyr::select(-allutts) %>%\n  dplyr::mutate(TypeTokenRatio = round(types/tokens, 3))\n# inspect \nttr\n\n# A tibble: 10 × 4\n   age_years_targetchild tokens types TypeTokenRatio\n                   <dbl>  <dbl> <int>          <dbl>\n 1                    10  19748  2001          0.101\n 2                    11   1421   456          0.321\n 3                    12    173    92          0.532\n 4                     3  53017  2770          0.052\n 5                     4  96499  4081          0.042\n 6                     5 107182  4079          0.038\n 7                     6   8474  1158          0.137\n 8                     7  58121  3275          0.056\n 9                     8   5330   922          0.173\n10                     9  44964  2906          0.065\n\n\nPlot the type-token ratio against age of the target child.\n\nttr %>%\n  ggplot(aes(x = age_years_targetchild, y = TypeTokenRatio)) +\n  geom_line() +\n  coord_cartesian(ylim = c(0, 0.75)) +\n  theme_bw() +\n  labs(x = \"Age of target child\", y = \"Type-Token Ratio\")"
  },
  {
    "objectID": "corplingr.html#data-processing-1",
    "href": "corplingr.html#data-processing-1",
    "title": "Corpus Linguistics with R",
    "section": "Data processing",
    "text": "Data processing\nIn a first step, we load the load the data into R. The way that the corpus data is loaded in this example is somewhat awkward because the data is in a server directory rather than on a hard drive on a simple PC. If the corpus data is not stored in a directory of a server, then you should not use the code shown immediately below but code in the window following the code immediately below.\n\n\n\n\n# define path to corpus\ncorpuspath <- \"https://slcladal.github.io/data/ICEIrelandSample/\"\n# define corpusfiles\nfiles <- paste(corpuspath, \"S1A-00\", 1:20, \".txt\", sep = \"\")\nfiles <- gsub(\"[0-9]([0-9][0-9][0-9])\", \"\\\\1\", files)\n# load corpus files\ncorpus <- sapply(files, function(x){\n  x <- readLines(x)\n  x <- paste(x, collapse = \" \")\n  x <- tolower(x)\n})\n# inspect corpus\nstr(corpus)\n\n Named chr [1:20] \"<s1a-001 riding>  <i> <s1a-001$a> <#> well how did the riding go tonight <s1a-001$b> <#> it was good so it was \"| __truncated__ ...\n - attr(*, \"names\")= chr [1:20] \"https://slcladal.github.io/data/ICEIrelandSample/S1A-001.txt\" \"https://slcladal.github.io/data/ICEIrelandSample/S1A-002.txt\" \"https://slcladal.github.io/data/ICEIrelandSample/S1A-003.txt\" \"https://slcladal.github.io/data/ICEIrelandSample/S1A-004.txt\" ...\n\n\nIf the corpus data is stored on your own computer (on not on a serves as is the case in the present example), you need to adapt the path though as the code below only works on my computer. Just exchange the corpuspath with the path to the data on your computer (e.g. with \"D:\\\\Uni\\\\UQ\\\\LADAL\\\\SLCLADAL.github.io\\\\data\\\\ICEIrelandSample\").\nData processing and extraction\nNow that the corpus data is loaded, we can prepare the searches by defining the search patterns. We will use regular expressions to retrieve all variants of the swear words. The sequence \\\\b denotes word boundaries while the sequence [a-z]{0,3} means that the sequences ass can be followed by a string consisting of any character symbol that is maximally three characters long (so that the search would also retrieve asses). We separate the search patterns by | as this means or.\n\nsearchpatterns <- c(\"\\\\bass[ingedholes]{0,6}\\\\b|\\\\bbitch[a-z]{0,3}\\\\b|\\\\b[a-z]{0,}fuck[a-z]{0,3}\\\\b|\\\\bshit[a-z]{0,3}\\\\b|\\\\bcock[a-z]{0,3}\\\\b|\\\\bwanker[a-z]{0,3}\\\\b|\\\\bboll[io]{1,1}[a-z]{0,3}\\\\b|\\\\bcrap[a-z]{0,3}\\\\b|\\\\bbugger[a-z]{0,3}\\\\b|\\\\bcunt[a-z]{0,3}\\\\b\")\n\nAfter defining the search pattern(s), we extract the kwics (keyword(s) in context) of the swear words.\n\n# extract kwic\nkwicswears <- quanteda::kwic(corpus, searchpatterns,window = 10, valuetype = \"regex\")\n\n\n\n\n\ndocnamefromtoprekeywordpostpatternhttps://slcladal.github.io/data/ICEIrelandSample/S1A-003.txt1,3481,348suppose the worrying thing was then you realised it didbugger-allyou know < & > laughter < / & >\\bass[ingedholes]{0,6}\\b|\\bbitch[a-z]{0,3}\\b|\\b[a-z]{0,}fuck[a-z]{0,3}\\b|\\bshit[a-z]{0,3}\\b|\\bcock[a-z]{0,3}\\b|\\bwanker[a-z]{0,3}\\b|\\bboll[io]{1,1}[a-z]{0,3}\\b|\\bcrap[a-z]{0,3}\\b|\\bbugger[a-z]{0,3}\\b|\\bcunt[a-z]{0,3}\\bhttps://slcladal.github.io/data/ICEIrelandSample/S1A-005.txt525525was uh they just want my money and all thisshite< # > fuck them < # > i '\\bass[ingedholes]{0,6}\\b|\\bbitch[a-z]{0,3}\\b|\\b[a-z]{0,}fuck[a-z]{0,3}\\b|\\bshit[a-z]{0,3}\\b|\\bcock[a-z]{0,3}\\b|\\bwanker[a-z]{0,3}\\b|\\bboll[io]{1,1}[a-z]{0,3}\\b|\\bcrap[a-z]{0,3}\\b|\\bbugger[a-z]{0,3}\\b|\\bcunt[a-z]{0,3}\\bhttps://slcladal.github.io/data/ICEIrelandSample/S1A-005.txt529529want my money and all this shite < # >fuckthem < # > i ' m never joining them\\bass[ingedholes]{0,6}\\b|\\bbitch[a-z]{0,3}\\b|\\b[a-z]{0,}fuck[a-z]{0,3}\\b|\\bshit[a-z]{0,3}\\b|\\bcock[a-z]{0,3}\\b|\\bwanker[a-z]{0,3}\\b|\\bboll[io]{1,1}[a-z]{0,3}\\b|\\bcrap[a-z]{0,3}\\b|\\bbugger[a-z]{0,3}\\b|\\bcunt[a-z]{0,3}\\bhttps://slcladal.github.io/data/ICEIrelandSample/S1A-005.txt664664flick through them bits < # > it ' sshite< s1a-005 $ a > < # > all the\\bass[ingedholes]{0,6}\\b|\\bbitch[a-z]{0,3}\\b|\\b[a-z]{0,}fuck[a-z]{0,3}\\b|\\bshit[a-z]{0,3}\\b|\\bcock[a-z]{0,3}\\b|\\bwanker[a-z]{0,3}\\b|\\bboll[io]{1,1}[a-z]{0,3}\\b|\\bcrap[a-z]{0,3}\\b|\\bbugger[a-z]{0,3}\\b|\\bcunt[a-z]{0,3}\\bhttps://slcladal.github.io/data/ICEIrelandSample/S1A-005.txt1,0121,0125 sylls < / unclear > i ' ve tofuckingdeal with that guy because he ' s a mason\\bass[ingedholes]{0,6}\\b|\\bbitch[a-z]{0,3}\\b|\\b[a-z]{0,}fuck[a-z]{0,3}\\b|\\bshit[a-z]{0,3}\\b|\\bcock[a-z]{0,3}\\b|\\bwanker[a-z]{0,3}\\b|\\bboll[io]{1,1}[a-z]{0,3}\\b|\\bcrap[a-z]{0,3}\\b|\\bbugger[a-z]{0,3}\\b|\\bcunt[a-z]{0,3}\\bhttps://slcladal.github.io/data/ICEIrelandSample/S1A-005.txt1,0261,026guy because he ' s a mason < # >fuckthat < s1a-005 $ c > < # > <\\bass[ingedholes]{0,6}\\b|\\bbitch[a-z]{0,3}\\b|\\b[a-z]{0,}fuck[a-z]{0,3}\\b|\\bshit[a-z]{0,3}\\b|\\bcock[a-z]{0,3}\\b|\\bwanker[a-z]{0,3}\\b|\\bboll[io]{1,1}[a-z]{0,3}\\b|\\bcrap[a-z]{0,3}\\b|\\bbugger[a-z]{0,3}\\b|\\bcunt[a-z]{0,3}\\bhttps://slcladal.github.io/data/ICEIrelandSample/S1A-005.txt1,6001,600all < # > i ' m like dad youfuckingjoined this < & > laughter < / & >\\bass[ingedholes]{0,6}\\b|\\bbitch[a-z]{0,3}\\b|\\b[a-z]{0,}fuck[a-z]{0,3}\\b|\\bshit[a-z]{0,3}\\b|\\bcock[a-z]{0,3}\\b|\\bwanker[a-z]{0,3}\\b|\\bboll[io]{1,1}[a-z]{0,3}\\b|\\bcrap[a-z]{0,3}\\b|\\bbugger[a-z]{0,3}\\b|\\bcunt[a-z]{0,3}\\bhttps://slcladal.github.io/data/ICEIrelandSample/S1A-005.txt1,7831,783try again < # > it ' s all justbollocks< s1a-005 $ b > < # > it '\\bass[ingedholes]{0,6}\\b|\\bbitch[a-z]{0,3}\\b|\\b[a-z]{0,}fuck[a-z]{0,3}\\b|\\bshit[a-z]{0,3}\\b|\\bcock[a-z]{0,3}\\b|\\bwanker[a-z]{0,3}\\b|\\bboll[io]{1,1}[a-z]{0,3}\\b|\\bcrap[a-z]{0,3}\\b|\\bbugger[a-z]{0,3}\\b|\\bcunt[a-z]{0,3}\\bhttps://slcladal.github.io/data/ICEIrelandSample/S1A-005.txt2,9212,921visiting < / [ > and she was like ohfucking< s1a-005 $ b > < # > < [\\bass[ingedholes]{0,6}\\b|\\bbitch[a-z]{0,3}\\b|\\b[a-z]{0,}fuck[a-z]{0,3}\\b|\\bshit[a-z]{0,3}\\b|\\bcock[a-z]{0,3}\\b|\\bwanker[a-z]{0,3}\\b|\\bboll[io]{1,1}[a-z]{0,3}\\b|\\bcrap[a-z]{0,3}\\b|\\bbugger[a-z]{0,3}\\b|\\bcunt[a-z]{0,3}\\bhttps://slcladal.github.io/data/ICEIrelandSample/S1A-005.txt3,5993,599marching down the corridor going you ' ve been outfuckingand whoring haven't you you bastard < # > and\\bass[ingedholes]{0,6}\\b|\\bbitch[a-z]{0,3}\\b|\\b[a-z]{0,}fuck[a-z]{0,3}\\b|\\bshit[a-z]{0,3}\\b|\\bcock[a-z]{0,3}\\b|\\bwanker[a-z]{0,3}\\b|\\bboll[io]{1,1}[a-z]{0,3}\\b|\\bcrap[a-z]{0,3}\\b|\\bbugger[a-z]{0,3}\\b|\\bcunt[a-z]{0,3}\\b\n\n\nWe now clean the kwic so that it is easier to see the relevant information.\n\nkwicswearsclean <- kwicswears %>%\n  as.data.frame() %>%\n  dplyr::rename(\"File\" = colnames(.)[1], \n                \"StartPosition\" = colnames(.)[2], \n                \"EndPosition\" = colnames(.)[3], \n                \"PreviousContext\" = colnames(.)[4], \n                \"Token\" = colnames(.)[5], \n                \"FollowingContext\" = colnames(.)[6], \n                \"SearchPattern\" = colnames(.)[7]) %>%\n  dplyr::select(-StartPosition, -EndPosition, -SearchPattern) %>%\n  dplyr::mutate(File = str_remove_all(File, \".*/\"),\n                File = stringr::str_remove_all(File, \".txt\"))\n\n\n\n\n\nFilePreviousContextTokenFollowingContextS1A-003suppose the worrying thing was then you realised it didbugger-allyou know < & > laughter < / & >S1A-005was uh they just want my money and all thisshite< # > fuck them < # > i 'S1A-005want my money and all this shite < # >fuckthem < # > i ' m never joining themS1A-005flick through them bits < # > it ' sshite< s1a-005 $ a > < # > all theS1A-0055 sylls < / unclear > i ' ve tofuckingdeal with that guy because he ' s a masonS1A-005guy because he ' s a mason < # >fuckthat < s1a-005 $ c > < # > <S1A-005all < # > i ' m like dad youfuckingjoined this < & > laughter < / & >S1A-005try again < # > it ' s all justbollocks< s1a-005 $ b > < # > it 'S1A-005visiting < / [ > and she was like ohfucking< s1a-005 $ b > < # > < [S1A-005marching down the corridor going you ' ve been outfuckingand whoring haven't you you bastard < # > and\n\n\nWe now create another kwic but with much more context because we want to extract the speaker that has uttered the swear word. To this end, we remove everything that proceeds the $ symbol as the speakers are identified by characters that follow the $ symbol, remove everything that follows the > symbol which end the speaker identification sequence, remove remaining white spaces, and convert the remaining character to upper case.\n\n# extract kwic\nkwiclong <- kwic(corpus, searchpatterns,window = 1000, valuetype = \"regex\")\nkwiclong <- as.data.frame(kwiclong)\ncolnames(kwiclong) <- c(\"File\", \"StartPosition\", \"EndPosition\", \"PreviousContext\", \"Token\", \"FollowingContext\", \"SearchPattern\")\nkwiclong <- kwiclong %>%\n  dplyr::select(-StartPosition, -EndPosition, -SearchPattern) %>%\n  dplyr::mutate(File = str_remove_all(File, \".*/\"),\n         File = str_remove_all(File, \".txt\"),\n         Speaker = str_remove_all(PreviousContext, \".*\\\\$\"),\n         Speaker = str_remove_all(Speaker, \">.*\"),\n         Speaker = str_squish(Speaker),\n         Speaker = toupper(Speaker)) %>%\n  dplyr::select(Speaker)\n# inspect results\nhead(kwiclong)\n\n  Speaker\n1       A\n2       B\n3       B\n4       B\n5       B\n6       B\n\n\nWe now add the Speaker to our initial kwic. This way, we combine the swear word kwic with the speaker and as we already have the file, we can use the file plus speaker identification to check if the speaker was a man or a woman.\n\nswire <- cbind(kwicswearsclean, kwiclong)\n\n\n\n\n\nFilePreviousContextTokenFollowingContextSpeakerS1A-003suppose the worrying thing was then you realised it didbugger-allyou know < & > laughter < / & >AS1A-005was uh they just want my money and all thisshite< # > fuck them < # > i 'BS1A-005want my money and all this shite < # >fuckthem < # > i ' m never joining themBS1A-005flick through them bits < # > it ' sshite< s1a-005 $ a > < # > all theBS1A-0055 sylls < / unclear > i ' ve tofuckingdeal with that guy because he ' s a masonBS1A-005guy because he ' s a mason < # >fuckthat < s1a-005 $ c > < # > <BS1A-005all < # > i ' m like dad youfuckingjoined this < & > laughter < / & >BS1A-005try again < # > it ' s all justbollocks< s1a-005 $ b > < # > it 'AS1A-005visiting < / [ > and she was like ohfucking< s1a-005 $ b > < # > < [CS1A-005marching down the corridor going you ' ve been outfuckingand whoring haven't you you bastard < # > andB\n\n\nNow, we inspect the extracted swear word tokens to check if our search strings have indeed captured swear words.\n\n# convert tokens to lower case\nswire$Token <- tolower(swire$Token)\n# inspect tokens\ntable(swire$Token)\n\n\n       ass      assed      bitch    bitches     bitchy    bollock   bollocks \n         2          1          1          1          2          1          3 \n    bugger bugger-all       crap       fuck   fuck-all     fucked    fucking \n         2          2          9          8          1          1         16 \n     fucks       shit      shite     wanker \n         1          1          3          2 \n\n\nFUCK and its variants is by far the most common swear word in our corpus. However, we do not need the type of swear word to answer our research question and we thus summarize the table to show which speaker in which files has used how many swear words.\n\nswire <- swire %>%\n  dplyr::group_by(File, Speaker) %>%\n  dplyr::summarise(Swearwords = n())\n\n\n\n\n\nFileSpeakerSwearwordsS1A-003A1S1A-005A1S1A-005B10S1A-005C1S1A-010A2S1A-011A2S1A-011B3S1A-014B2S1A-014C3S1A-014D2\n\n\nNow that we extract how many swear words the speakers in the corpus have used, we can load the biodata of the speakers.\n\n# load bio data\nbio <- base::readRDS(url(\"https://slcladal.github.io/data/d01.rda\", \"rb\"))\n\n\n\n\n\nidtext.idsubfilespk.refzonedatesexageresidereligword.count1S1A-0011Anorthern ireland1990-1994male34-41belfastprotestant7652S1A-0011Bnorthern ireland1990-1994female34-41belfastprotestant1,2984S1A-0021Anorthern ireland2002-2005female26-33belfastcatholic3915S1A-0021Bnorthern ireland2002-2005female19-25belfastcatholic476S1A-0021Cnorthern ireland2002-2005male50+belfastcatholic2007S1A-0021Dnorthern ireland2002-2005female50+belfastcatholic4648S1A-0021Emixed between ni and roi2002-2005male34-41englandcatholic6399S1A-0021Fnorthern ireland2002-2005female26-33belfastcatholic30810S1A-0021Gnorthern ireland2002-2005female50+belfastcatholic7811S1A-0021Hmixed between ni and roi2002-2005male19-25englandcatholic98\n\n\n\nbio <- bio %>%\n  dplyr::rename(File = text.id, \n         Speaker = spk.ref,\n         Gender = sex,\n         Age = age,\n         Words = word.count) %>%\n  dplyr::select(File, Speaker, Gender, Age, Words)\n\n\n\n\n\nFileSpeakerGenderAgeWordsS1A-001Amale34-41765S1A-001Bfemale34-411,298S1A-002Afemale26-33391S1A-002Bfemale19-2547S1A-002Cmale50+200S1A-002Dfemale50+464S1A-002Emale34-41639S1A-002Ffemale26-33308S1A-002Gfemale50+78S1A-002Hmale19-2598\n\n\nIn a next step, we combine the table with the speaker information with the table showing the swear word use.\n\n# combine frequencies and biodata\nswire <- dplyr::left_join(bio, swire, by = c(\"File\", \"Speaker\")) %>%\n  # replace NA with 0\n  dplyr::mutate(Swearwords = ifelse(is.na(Swearwords), 0, Swearwords),\n                File = factor(File),\n                Speaker = factor(Speaker),\n                Gender = factor(Gender),\n                Age = factor(Age))\n# inspect data\nhead(swire)\n\n     File Speaker Gender   Age Words Swearwords\n1 S1A-001       A   male 34-41   765          0\n2 S1A-001       B female 34-41  1298          0\n3 S1A-002       A female 26-33   391          0\n4 S1A-002       B female 19-25    47          0\n5 S1A-002       C   male   50+   200          0\n6 S1A-002       D female   50+   464          0\n\n\n\n\n\n\nFileSpeakerGenderAgeWordsSwearwordsS1A-001Amale34-417650S1A-001Bfemale34-411,2980S1A-002Afemale26-333910S1A-002Bfemale19-25470S1A-002Cmale50+2000S1A-002Dfemale50+4640S1A-002Emale34-416390S1A-002Ffemale26-333080S1A-002Gfemale50+780S1A-002Hmale19-25980\n\n\nWe now clean the table by removing speakers for which we do not have any information on their age and gender. Also, we summarize the table to extract the mean frequencies of swear words (per 1,000 words) by age and gender.\n\n# clean data\nswire <- swire %>%\n  dplyr::filter(is.na(Gender) == F,\n         is.na(Age) == F) %>%\n  dplyr::group_by(Age, Gender) %>%\n  dplyr::mutate(SumWords = sum(Words),\n                SumSwearwords = sum(Swearwords),\n                FrequencySwearwords = round(SumSwearwords/SumWords*1000, 3)) \n\n\n\n\n\nFileSpeakerGenderAgeWordsSwearwordsSumWordsSumSwearwordsFrequencySwearwordsS1A-001Amale34-41765022,21320.09S1A-001Bfemale34-411,298015,01730.20S1A-002Afemale26-33391035,137130.37S1A-002Bfemale19-2547062,53500.00S1A-002Cmale50+200064,04400.00S1A-002Dfemale50+464038,68300.00S1A-002Emale34-41639022,21320.09S1A-002Ffemale26-33308035,137130.37S1A-002Gfemale50+78038,68300.00S1A-002Hmale19-259808,82600.00\n\n\nTabulating and visualizing the data\nWe now summarize and visualize the data and exclude speakers between the ages of 0 and 18 as there are too few speakers within that age range to be representative.\n\nswire %>%\n  dplyr::filter(Age != \"0-18\") %>%\n  dplyr::group_by(Age, Gender) %>%\n  dplyr::summarise(Swears_ptw = SumSwearwords/SumWords*1000) %>%\n  unique() %>%\n  tidyr::spread(Gender, Swears_ptw)\n\n# A tibble: 5 × 3\n# Groups:   Age [5]\n  Age   female   male\n  <fct>  <dbl>  <dbl>\n1 19-25  0     0     \n2 26-33  0.370 0.484 \n3 34-41  0.200 0.0900\n4 42-49  0     0     \n5 50+    0     0     \n\n\nNow that we have prepared our data, we can plot swear word use by gender.\n\nswire %>%\n  dplyr::filter(Age != \"0-18\") %>%\n  dplyr::group_by(Age, Gender) %>%\n  dplyr::summarise(Swears_ptw = SumSwearwords/SumWords*1000) %>%\n  unique() %>%\nggplot(aes(x = Age, y = Swears_ptw, group = Gender, fill = Gender)) +\n  geom_bar(stat = \"identity\", position = position_dodge()) +\n  theme_bw() +\n  scale_fill_manual(values = c(\"orange\", \"darkgrey\")) +\n  labs(y = \"Relative frequency \\n swear words per 1,000 words\")\n\n\n\n\nThe graph suggests that the genders do not differ in their use of swear words except for the age bracket from 26 to 41: men swear more among speakers aged between 26 and 33 while women swear more between 34 and 41 years of age."
  },
  {
    "objectID": "corplingr.html#statistical-analysis",
    "href": "corplingr.html#statistical-analysis",
    "title": "Corpus Linguistics with R",
    "section": "Statistical analysis",
    "text": "Statistical analysis\nWe now perform a statistical test, e.g. a Configural Frequency Analysis (CFA) to check if specifically which groups in the data significantly over and under-use swearwords.\n\ncfa_swear <- swire %>%\n  dplyr::group_by(Gender, Age) %>%\n  dplyr::summarise(Words = sum(Words),\n                   Swearwords = sum(Swearwords)) %>%\n  dplyr::mutate(Words = Words - Swearwords) %>%\n  tidyr::gather(Type, Frequency,Words:Swearwords) %>%\n  dplyr::filter(Age != \"0-18\")\n\nAfter transforming the data, it has the following format.\n\n\n\n\nGenderAgeTypeFrequencyfemale19-25Words62,535female26-33Words35,124female34-41Words15,014female42-49Words10,785female50+Words38,683male19-25Words8,826male26-33Words20,654male34-41Words22,211male42-49Words40,923male50+Words64,044female19-25Swearwords0female26-33Swearwords13female34-41Swearwords3female42-49Swearwords0female50+Swearwords0male19-25Swearwords0male26-33Swearwords10male34-41Swearwords2male42-49Swearwords0male50+Swearwords0\n\n\n\n# define configurations\nconfigs <- cfa_swear %>%\n  dplyr::select(Age, Gender, Type)\n# define counts\ncounts <- cfa_swear$Frequency\n\nNow that configurations and counts are separated, we can perform the configural frequency analysis.\n\n# perform cfa\ncfa(configs,counts)$table %>%\n  as.data.frame() %>%\n  dplyr::filter(p.chisq < .1,\n                stringr::str_detect(label, \"Swear\")) %>%\n  dplyr::select(-z, -p.z, -sig.z, -sig.chisq, -Q)\n\n                    label  n expected chisq        p.chisq\n1 26-33 female Swearwords 13    2.492 44.30 0.000000005565\n2 26-33 male   Swearwords 10    2.408 23.93 0.000082325635\n\n\nAfter filtering out significant over use of non-swear words from the results of the CFA, we find that men and women in the age bracket 26 to 33 use significantly more swear words and other groups in the data.\nIt has to be borne in mind, though, that this is merely a case study and that a more fine-grained analysis on a substantially larger data set were necessary to get a more reliable impression."
  },
  {
    "objectID": "dstats.html#mean",
    "href": "dstats.html#mean",
    "title": "Descriptive Statistics with R",
    "section": "Mean",
    "text": "Mean\nThe mean is used when the data is numeric and normally distributed. The mean is calculated by applying the formula shown below.\n\\[\\begin{equation}\n  \\bar{x}=\\frac{1}{n} \\sum_{i=1}^n x_i = \\frac{x_{1}+x_{2}+ \\dots + x_{n}}{n}\n\\end{equation}\\]\nTo calculate the mean, sum up all values and divide by the number of values. See the example below for clarification.\n\n\n\n\n\nConsider, we are interested in the mean length of sentences in a short text, then the first thing we could do would be to list the sentences and their length in a table.\n\n\n\n\n\n\n\n\nSentences of the first paragraph of Herman Melville’s Moby Dick and the number of words in each sentence.\n\n\nSentencesWordsCall me Ishmael3Some years ago -- never mind how long precisely -- having little or no money in my purse, and nothing particular to interest me on shore, I thought I would sail about a little and see the watery part of the world.40It is a way I have of driving off the spleen, and regulating the circulation.15Whenever I find myself growing grim about the mouth; whenever it is a damp, drizzly November in my soul; whenever I find myself involuntarily pausing before coffin warehouses, and bringing up the rear of every funeral I meet; and especially whenever my hypos get such an upper hand of me, that it requires a strong moral principle to prevent me from deliberately stepping into the street, and methodically knocking people's hats off--then, I account it high time to get to sea as soon as I can.87\n\n\nTo calculate the mean, we need to divide the sum of the number of words per sentence (145) by the number of sentences (7) (see the equation below).\n\\[\\begin{equation}\n\\frac{3+40+15+87}{4} = \\frac{145}{4} = 36.25\n\\label{eq:mittel2}\n\\end{equation}\\]\nThe mean sentences length in our example is 36.25 words\nIn R, the mean is calculated as follows.\n\n# create numeric vector\nfrequencies <- c(3, 40, 15, 87)\n# calculate mean\nmean(frequencies)\n\n[1] 36.25\n\n\nThe mean is the most common way to summarize numeric variables and it is very easy and intuitive to understand. A disadvantage of the mean is that it is very strongly affected by outliers which is why the median is the preferable measure of centrality when dealing with data that is not normal or that contains outliers.\n\n\n\n\nEXERCISE TIME!\n\n\n\n\n\n\n`\n\nCalculate the arithmetic mean: 1, 2, 3, 4, 5, 6\n\n\n\nAnswer\n\n::: {.cell}\n  (1 + 2 + 3 + 4 + 5 + 6)/6\n::: {.cell-output .cell-output-stdout} [1] 3.5 ::: :::\n\n\nCalculate the arithmetic mean for the following values using the mean function: 4, 3, 6, 2, 1, 5, 6, 8\n\n\n\nAnswer\n\n::: {.cell}\n  mean(c(4, 3, 6, 2, 1, 5, 6, 8))\n::: {.cell-output .cell-output-stdout} [1] 4.375 ::: :::\n\n\nCreate a vector out of the following values and calculate the arithmetic mean using the mean function: 1, 5, 5, 9\n\n\n\nAnswer\n\n::: {.cell}\n  vec <- c(1, 5, 5, 9)\n  mean(vec)\n::: {.cell-output .cell-output-stdout} [1] 5 ::: :::\n\n\n`"
  },
  {
    "objectID": "dstats.html#median",
    "href": "dstats.html#median",
    "title": "Descriptive Statistics with R",
    "section": "Median",
    "text": "Median\nThe median can be used for both numeric and ordinal variables. In contract to the mean, it is more robust and not as easily affected by outliers. While the mean is commonly associated with numeric data that is normally distributed, the median is typically used when dealing with non-normal numeric or ordinal variables, i.e. variables that are ordered but not truly numeric. The median is the central value in a de- or increasing ordering of values in a vector. In other words, 50 percent of values are above and 50 percent of values are below the median in a given vector.\nIf the vector contains an even number of elements, then the two central values are summed up and divided by 2. If the vector contains an uneven number of elements, the median represents the central value.\n\\[\\begin{equation}\nmedian_{x}=\n\\begin{cases}\nx_{\\frac{n+1}{2}} & n\\text{ uneven} \\\\\n\\frac{1}{2}\\bigl(x_{\\frac{n}{2}}+x_{\\frac{n+1}{2}}\\bigr) & n\\text{ even}\n\\end{cases}\n\\label{eq:median}\n\\end{equation}\\]\n\n\n\n\n\nLet’s have a look at an example. Consider you are interested in the age stratification of speakers in the private dialogue section of the Irish component of the International Corpus of English (ICE). When tabulating and plotting the age variable you get the following table and graph.\n\n\n\n\n\nNumber of speakers across age groups in the private dialogue section of the Irish component of the International Corpus of English (ICE).\n\n\nAgeCounts0-18919-2516026-337034-411542-49950+57\n\n\n\n\n\n\n\nThe age groups represent an order factor which means that there are categories with a natural order (here from old to young or vice versa). If we order speakers according to their age from young to old, we get a vector of length 320. If we then take the central value, i.e. the value of the 160th speaker, we get the median age in the private dialogue section of the Irish component of the International Corpus of English (ICE).\nIn R, the median is calculated as shown below.\n\n# create a vector consisting out of ranks\nranks <- c(rep(1, 9), rep(2, 160), rep(3, 70), rep(4, 15), rep(5, 9), rep(6, 57))\n# calculate median\nmedian(ranks)\n\n[1] 2\n\n\nIn our case, the median age is 19-25 because the 160th speaker belongs to the 2nd age group, i.e. the age group with speakers between 19 and 25 years old.\n\n\n\n\nEXERCISE TIME!\n\n\n\n\n\n\n`\n\nCalculate the median: 1, 2, 3, 4, 5, 6\n\n\n\nAnswer\n\n::: {.cell}\n  (3 + 4)/2\n::: {.cell-output .cell-output-stdout} [1] 3.5 ::: :::\n\n\nCalculate the median for the following values using the median function: 4, 3, 6, 2, 1, 5, 6, 8\n\n\n\nAnswer\n\n::: {.cell}\n  median(c(4, 3, 6, 2, 1, 5, 6, 8))\n::: {.cell-output .cell-output-stdout} [1] 4.5 ::: :::\n\n\nCreate a vector out of the following values and calculate the median using the median function: 1, 5, 5, 9\n\n\n\nAnswer\n\n::: {.cell}\n  vec <- c(1, 5, 5, 9)\n  median(vec)\n::: {.cell-output .cell-output-stdout} [1] 5 ::: :::\n\n\n`"
  },
  {
    "objectID": "dstats.html#mode",
    "href": "dstats.html#mode",
    "title": "Descriptive Statistics with R",
    "section": "Mode",
    "text": "Mode\nThe mode is typically used when dealing with categorical variables and it reports which level of a factor or a categorical variable is the most frequent.\n\n\n\n\n\nHere is an example to illustrate the mode. Consider you are interested where most speakers in the private dialogue section of the Irish component of the International Corpus of English are currently residing and you get the following distribution.\n\n\n\n\n\nNumber of speakers across counties of current residency in the private dialogue section of the Irish component of the International Corpus of English (ICE).\n\n\nCurrentResidenceSpeakersBelfast98Down20Dublin (city)110Limerick13Tipperary19\n\n\n\n\n\n\n\nThe tabulated and visualized data show that the mode is Dublin (City), because the largest group (110 speakers) of speakers in the corpus are speakers from the city of Dublin. This means that the average speaker in in the private dialogue section of the Irish component of the International Corpus of English (ICE) is from Dublin city.\nIn R the mode is calculated as shown below:\n\n# create a factor with the current residence of speakers\nCurrentResidence <- c(rep(\"Belfast\", 98),         # repeat \"Belfast\" 98 times\n                      rep(\"Down\", 20),            # repeat \"Down\" 20 times\n                      rep(\"Dublin (city)\", 110),  # repeat \"Dublin (city)\" 110 times\n                      rep(\"Limerick\", 13),        # repeat \"Limerick\" 13 times\n                      rep(\"Tipperary\", 19))       # repeat \"Tipperary\" 19 times\n# calculate mode\nnames(which.max(table(CurrentResidence)))         # extract which level occurs most frequently\n\n[1] \"Dublin (city)\"\n\n\nA word of warning is in order here as only the first(!) maximal value is provided by R even if several categories have the same frequency."
  },
  {
    "objectID": "dstats.html#geometric-mean",
    "href": "dstats.html#geometric-mean",
    "title": "Descriptive Statistics with R",
    "section": "Geometric mean",
    "text": "Geometric mean\nThe geometric mean represents a measure of central tendency that is used when dealing with dynamic processes where the later elements are dependent on the previous elements. The geometric mean is calculated according to the equation below.\n\\[\\begin{equation}\n\\bar{x}_{geometric} = \\sqrt[n]{x_1 \\times x_{i+1} \\times \\dots \\times x_n}\n\\end{equation}\\]\nImagine you have the option to buy two different stock packages and you have to buy one of them. Which one would you buy?\n\n\n\n\n\nPerformance of two stock packages.\n\n\nYearPackage1Package2Year 1+5%+20%Year 2-5%-20%Year 3+5%+20%Year 4-5%-20%\n\n\nIs one package better than the other? Did one package perform better than the other?\n\nPackage 1:\n\nReturn: \\(1.05 \\times .95 \\times 1.05 \\times .95 = .995\\) (0.5% loss)\nYear-over-year average: \\(.995^{1/4}\\) = ~0.125% loss per year\n\nPackage 2:\n\nReturn: \\(1.2 \\times .8 \\times 1.2 \\times .8 = 0.9216\\) (7.84% loss)\nYear-over-year average: \\(.9216^{1/4}\\) = ~2% loss per year.\n\n\nPackage 2 performs substantially worse because here, the changes in growth depend on the previous growth rates."
  },
  {
    "objectID": "dstats.html#harmonic-mean",
    "href": "dstats.html#harmonic-mean",
    "title": "Descriptive Statistics with R",
    "section": "Harmonic mean",
    "text": "Harmonic mean\nThe harmonic mean is a measure of central tendency that provides us with the average rate and is used when dealing with dynamic processes that involve velocities and distances. The harmonic mean is calculated according to the equation below.\n\\[\\begin{equation}\n\\bar{x}_{harmonic} =\n\\frac{n}{\\frac{1}{x_i} + \\frac{1}{x_{i+1}} + \\frac{1}{x_{i+\\dots}} + \\frac{1}{x_n}}\n\\end{equation}\\]\nThe harmonic mean is used when two rates contribute to the same workload (for instance when we download a file). Each installment is in a relay race and contributes the same amount to the issue. For example, we make a round trip to work and back. The way to work is 60 kilometers. On the way to work, we can only travel at 30 kph while we can go 60 kph on the way back. The distance is the same. Half of the results (distance traveled) comes from the first rate (30 kilometers per hour) and the other half from the second rate (60 kilometers per hour). The result is that is takes us 3 hours to get to work and back.\n\\[\\begin{equation}\n\\bar{x}_{harmonic} =\n\\frac{2}{\\frac{1}{30} + \\frac{1}{60}} = \\frac{2}{\\frac{2}{60} + \\frac{1}{60}} = \\frac{2}{\\frac{3}{60}} = \\frac{2}{1} \\times \\frac{60}{3} = \\frac{120}{3} = 40\n\\end{equation}\\]\nThe reason why using the arithmetic mean is inappropriate in such cases is the following: The idea behind the arithmetic mean is that we calculate a single value that can replace all values in a given distribution and the sum of the mean values is identical to the sum of the observed values. So, the average is a single element that replaces each element. In our example, we have to drive at 40 kilometers per hour (instead of 30) to work and 40 kilometers per hour (instead of 60) to get back from work in the same amount of time. If we went with 45 kilometers per hour, then the result would not be 3 hours but 2 hours and 40 minutes so that the result would not be the same."
  },
  {
    "objectID": "dstats.html#notes-on-measures-of-centrality",
    "href": "dstats.html#notes-on-measures-of-centrality",
    "title": "Descriptive Statistics with R",
    "section": "Notes on Measures of Centrality",
    "text": "Notes on Measures of Centrality\nAs suggested above, the mean is strongly affected by outliers (which is why in sch cases, the median is the more appropriate measure fo central tendency). To illustrate this, imagine you are interested whether the use of discourse particles differs across two corpora. The two corpora represent the speech of the same five speakers but in different situations and the speech thus represents different registers. In a first step, you calculate the relative frequency of discourse particle use and both corpora have a mean of 13.4 particles per 1,000 words. Given the mean, the two corpora do not seem to differ. However, when tabulating and plotting the use of particles by speaker and across these two corpora, it becomes immediately clear that the mean is not the appropriate measure of central tendency as the distributions are very dissimilar.\n\n\n\n\n\nRelative frequencies of discourse particles per speaker in two corpora.\n\n\nCorpusSpeakerFrequencyC1A11.4C1B5.2C1C27.1C1D9.6C1E13.7C2A0.2C2B0.0C2C1.1C2D65.3C2E0.4\n\n\n\n\n\n\n\n\n\n\nThe Figure above shows that the use of discourse particles is distributed rather evenly across speakers in Corpus 1 while the distribution is very uneven in corpus 2. In corpus 2, 4 out of 5 speakers use almost no discourse particles and only one speaker, speaker D, makes excessive use of discourse particles in corpus 2. The high usage frequency of discourse particles by speaker D in corpus 2 causes the mean of corpus 2 to be identical to the mean reported for corpus 1 although the distribution of usage rates differs drastically. This means that reporting the median in addition to the mean can be useful if the distribution of values is very uneven (or non-normal or skewed).\nTo exemplify, we will summarize the distribution of discourse particles in the two corpora: the use of discourse particles in corpus 1 (mean = 13.4, median = 11.4) is substantially different from the use of discourse particles in corpus 2 (mean = 13.4, median = 0.4)."
  },
  {
    "objectID": "dstats.html#range",
    "href": "dstats.html#range",
    "title": "Descriptive Statistics with R",
    "section": "Range",
    "text": "Range\nThe range is the simplest measure of variability and reports the lowest and highest value of a distribution. That is, the range provides minimum and maximum of a vector to show the span of values within a distribution.\nIn R, the range is extracted as shown below.\n\n# create a numeric vector\nMoscow <- c(-5, -12, 5, 12, 15, 18, 22, 23, 20, 16, 8, 1)\nmin(Moscow); max(Moscow) # extract range\n\n[1] -12\n\n\n[1] 23\n\n\nThe lowest temperature value for Moscow is -12 degrees Celsius and the highest value is 23 degrees Celsius. The range thus spans from -12 to 23."
  },
  {
    "objectID": "dstats.html#interquartile-range-iqr",
    "href": "dstats.html#interquartile-range-iqr",
    "title": "Descriptive Statistics with R",
    "section": "Interquartile range (IQR)",
    "text": "Interquartile range (IQR)\nThe interquartile range (IQR) denotes the range that encompasses the central 50 percent of data points and thus informs about how values are distributed. This means that the IQR spans from the first quartile that encompasses 25 percent of the data to the third quartile that encompasses 75 percent of the data.\nThe easiest way to extract the IQR in R is to apply the summary function to a vector as shown below and then subtract the value of the 1st quartile from the value of the 3rd quartile.\n\nsummary(Moscow) # extract IQR\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n -12.00    4.00   13.50   10.25   18.50   23.00 \n\n\nThe summary function reports that the minimum temperature is -12 degrees Celsius and that the maximum temperature is 23 degrees Celsius. Also, the lower 25 percent of the data fall within -12 and 4 degrees Celsius (from the minimum value to the 1st quartile) and the upper 25 percent fall within 18.5 and 23 degrees Celsius (from the 3rd quartile to the maximum value). The IQR range represents a range that encompasses the central 50% of the data and thus represents the value that can be calculated by subtracting the value of the 1st from the value of the 3rd quartile..\nThus, the IQR is 18.5 - 4 = 14.5\n."
  },
  {
    "objectID": "dstats.html#variance",
    "href": "dstats.html#variance",
    "title": "Descriptive Statistics with R",
    "section": "Variance",
    "text": "Variance\nThe variance is calculated according to the formula below. To calculate the variance, each value is subtracted from the mean and the result is squared. The squared values are then added and the resulting sum is divided by the number of values minus 1.\n\\(s = \\sigma^2 = \\frac{1}{n-1} \\sum_{i=1}^{n} (x_i - \\bar{x})^{2}\\)\nFor our example, the variance of temperatures for Moscow is 123.6591 and 9.477273 for Hamburg.\nIn R, the variance is calculated as shown below.\n\nsd(Moscow)^2\n\n[1] 123.659090909"
  },
  {
    "objectID": "dstats.html#standard-deviation",
    "href": "dstats.html#standard-deviation",
    "title": "Descriptive Statistics with R",
    "section": "Standard deviation",
    "text": "Standard deviation\nThe standard deviation (abbreviated with capital \\(sigma\\) \\(\\sigma\\)) is calculated according to first equation shown below or, alternatively, according to second equation shown below and it is the square root of the squared variance.\n\\(\\sigma = \\sqrt{s} = \\sqrt{\\frac{1}{n-1} \\sum_{i=1}^{n} (x_i - \\bar{x})^2}\\)\n\\(\\sigma = \\sqrt{\\frac{ \\sum_{i=1}^{n} (x_i - \\bar{x})^2}{n-1}}\\)\nFor our example, the first equation shown above provides a standard deviation of 11.12 for Moscow and a standard deviation of 3.08 for Hamburg.\nIn R, the standard deviation is calculated as shown below.\n\n# calculate standard deviation\nsd(Moscow) \n\n[1] 11.1202109202\n\n\nThe standard deviation of temperature values of Moscow is 11.12.\n\n\n\n\nEXERCISE TIME!\n\n\n\n\n\n\n`\n\nCalculate the mean, median, and mode as well as the standard deviation for the following two vectorsA: 1, 3, 6, 2, 1, 1, 6, 8, 4, 2, 3, 5, 0, 0, 2, 1, 2, 1, 0B: 3, 2, 5, 1, 1, 4, 0, 0, 2, 3, 0, 3, 0, 5, 4, 5, 3, 3, 4\n\n\n\nAnswer\n\n::: {.cell}\n  A <- c(1, 3, 6, 2, 1, 1, 6, 8, 4, 2, 3, 5, 0, 0, 2, 1, 2, 1, 0)\n  B <- c(3, 2, 5, 1, 1, 4, 0, 0, 2, 3, 0, 3, 0, 5, 4, 5, 3, 3, 4)\n  mean(A)\n  median(A)\n  max(A)\n  sd(A)\n  mean(B)\n  median(B)\n  max(B)\n  sd(B)\n:::\n\n\nFind a partner and discuss which measure of central tendency is appropriate when dealing with grades. Then, find another partner and see whether they have come to the same conclusion or discuss why if not. Finally, discuss the advantages and disadvantages of calculating the mean when dealing with grades.\n\n\n\nAnswer\n\nThe problem is that - strictly speaking - grades are ordinal and not numeric (i.e., interval- or ratio-scaled). This means that calculating the arithmetic mean is somewhat controversial. To resolve this issue, it is recommendable to either calculate the median (rather than the mean) or to be transparent about this issue and inform readers that the mean was used despite dealing with an ordinal variable.\n\n\nWhere are mean, median, and mode when dealing with normal data (i.e., when the data approximate a normal distribution)?\n\n\n\nAnswer\n\nWhen dealing with normally distributed data, the arithmetic mean, median, and mode would ideally be identical (which is extremely rare when working with empirical data) or, at least, very similar.\n\n\nGo and find a partner and discuss what it means - on a conceptual level rather than on a statistical/mathematical level - that two groups have different ranges for a certain feature (be careful, this is not as trivial as it may seem!).\n\n\n`"
  },
  {
    "objectID": "dstats.html#standard-error",
    "href": "dstats.html#standard-error",
    "title": "Descriptive Statistics with R",
    "section": "Standard Error",
    "text": "Standard Error\nThe standard error is a measure of variability and it reports the average distance from some parameters (most often from the mean). It is calculated as the standard deviation of the residuals of the parameter in question. To exemplify the standard error, we will have a look at reaction times which show how fast participants realized that a sequence of letters were either existing words or just a sequence of letters.\n\n\n\n\n\nReaction times while sober and drunk.\n\n\nRTStateGender429.276SoberMale435.473SoberMale394.535SoberMale377.325SoberMale430.294SoberMale289.102SoberFemale411.505SoberFemale366.191SoberFemale365.792SoberFemale334.034SoberFemale444.188DrunkMale540.866DrunkMale468.531DrunkMale476.011DrunkMale412.473DrunkMale520.845DrunkFemale435.682DrunkFemale463.421DrunkFemale536.036DrunkFemale494.936DrunkFemale\n\n\nThe standard error of the mean is calculated using the equation below.\n\\[\\begin{equation}\n\\sigma~{\\bar{x}}~ =\\frac{\\sigma}{\\sqrt{n}}\n\\end{equation}\\]\nThe standard error can be calculated manually (see below) by implementing the equation from above.\n\nsd(rts$RT, na.rm=TRUE) /  \n   sqrt(length(rts$RT[!is.na(rts$RT)]))  \n\n[1] 14.7692485022\n\n\nAn easier way to extract standard errors is to use the describe function from the psych package (see below)\n\n# describe data\npsych::describe(rts$RT, type=2)\n\n   vars  n   mean    sd median trimmed  mad   min    max  range skew kurtosis\nX1    1 20 431.33 66.05 432.88   432.9 60.4 289.1 540.87 251.76 -0.2    -0.13\n      se\nX1 14.77"
  },
  {
    "objectID": "dstats.html#confidence-intervals-for-simple-vectors",
    "href": "dstats.html#confidence-intervals-for-simple-vectors",
    "title": "Descriptive Statistics with R",
    "section": "Confidence Intervals for Simple Vectors",
    "text": "Confidence Intervals for Simple Vectors\nConfidence intervals (CIs) give a range that’s likely to include a population value with a certain degree of confidence. As such, CIs tell us how likely it is to get a value within a certain range if we drew another sample from the same population.\nOne easy method for extracting confidence intervals is to apply the CI function from the Rmisc package.\n\n# extract mean and confidence intervals\nRmisc::CI(rts$RT, ci=0.95)   \n\n        upper          mean         lower \n462.238192381 431.325800000 400.413407619 \n\n\n\n\n\nThe ´CI´ function provides the mean reaction time (431.3258) and the 95 percent confidence band. With 95 percent confidence, the mean reaction time will have a mean between 400.41 and 462.24 milliseconds (ms).\nAnother way to extract the mean and its confidence intervals is by using t.test function.\n\n# extract mean and confidence intervals\nstats::t.test(rts$RT, conf.level=0.95)  \n\n\n    One Sample t-test\n\ndata:  rts$RT\nt = 29.20431598, df = 19, p-value < 0.0000000000000002220446\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 400.413407619 462.238192381\nsample estimates:\nmean of x \n 431.3258 \n\n\nAnother alternative to extract the man ans the confidence interval from a range of values is to use the MeanCI function from the DescTools package.\n\n# extract mean and confidence intervals\nDescTools::MeanCI(rts$RT, conf.level=0.95)   \n\n         mean        lwr.ci        upr.ci \n431.325800000 400.413407619 462.238192381 \n\n\nThis method is particularly interesting because it uses bootstrapping or resampling the data. As such, it is an empirical method to extract the mean and the confidence intervals. The values will differ given how many samples are drawn and we can get very precise estimates using this method.\n\n# extract mean CIs\nDescTools::MeanCI(rts$RT, method=\"boot\", type=\"norm\", R=1000)\n\n         mean        lwr.ci        upr.ci \n431.325800000 402.065920019 459.599597681 \n\n\nBecause this is a data-driven approach, the results will vary, depending on the characteristics of the resampled data. To illustrate, compare the values provided above to the values generated below.\n\n# extract mean CIs\nDescTools::MeanCI(rts$RT, method=\"boot\", type=\"norm\", R=1000)\n\n         mean        lwr.ci        upr.ci \n431.325800000 403.272504877 458.846973023 \n\n\nAnother method for extracting the mean and the confidence intervals from a range of values using bootstrapping is to use the boot function from the boot package.\n\n# function to extract values\nBootFunction = function(x, index) {                        \n                  return(c(mean(x[index]),\n                           var(x[index]) / length(index)))\n}\n# apply function to data\nBootstrapped = boot(data=rts$RT,     \n                    statistic=BootFunction,\n                    R=1000)\n# extract values\nmean(Bootstrapped$t[,1])                                   \n\n[1] 431.4550746\n\n# alternative to extract values\nboot.ci(Bootstrapped, conf=0.95)                           \n\nBOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\nBased on 1000 bootstrap replicates\n\nCALL : \nboot.ci(boot.out = Bootstrapped, conf = 0.95)\n\nIntervals : \nLevel      Normal              Basic             Studentized     \n95%   (402.9, 459.5 )   (402.8, 460.4 )   (397.6, 463.3 )  \n\nLevel     Percentile            BCa          \n95%   (402.2, 459.9 )   (400.1, 457.6 )  \nCalculations and Intervals on Original Scale\n\n\nThe advantage of using bootstrapping methods lies in the fact that the data is (frequently) not distributed normally which is not an issue for the bootstrapping and it will thus provide more reliable results as it does not rely on distributional assumptions about the data."
  },
  {
    "objectID": "dstats.html#confidence-intervals-for-grouped-data",
    "href": "dstats.html#confidence-intervals-for-grouped-data",
    "title": "Descriptive Statistics with R",
    "section": "Confidence Intervals for Grouped Data",
    "text": "Confidence Intervals for Grouped Data\nTo extract the confidence intervals for grouped data, we can sue the summarySE function from the Rmisc package.\n\n# apply summarySE function to data\nRmisc::summarySE(data=rts,   \n                 # define variable representing frequencies\n                 measurevar=\"RT\", \n                 # define grouping variable\n                 groupvars=\"Gender\",\n                 # extract standard deviation, standard error, and confidence intervals\n                 conf.interval = 0.95)   \n\n  Gender  N       RT            sd            se            ci\n1 Female 10 421.7544 82.8522922089 26.2001952746 59.2689594071\n2   Male 10 440.8972 46.2804393809 14.6351599557 33.1070319225"
  },
  {
    "objectID": "dstats.html#confidence-intervals-for-nominal-data",
    "href": "dstats.html#confidence-intervals-for-nominal-data",
    "title": "Descriptive Statistics with R",
    "section": "Confidence Intervals for Nominal Data",
    "text": "Confidence Intervals for Nominal Data\nWe now turn to confidence intervals for nominal data [see also 4]. When dealing with nominal data, confidence intervals can be determined with the binom.test function in the in-built stats package. Alternative methods are available via the BinomCI and MultinomCI functions from the DescTools package. More advanced techniques for confidence intervals on nominal data are available via the PropCIs package.\n\nstats::binom.test(2, 20, 0.5,              # binom.test(x, n, p = 0.5, ...)\n                  alternative=\"two.sided\", # define sidedness\n                  conf.level=0.95)         # define confidence level\n\n\n    Exact binomial test\n\ndata:  2 and 20\nnumber of successes = 2, number of trials = 20, p-value =\n0.000402450562\nalternative hypothesis: true probability of success is not equal to 0.5\n95 percent confidence interval:\n 0.0123485271703 0.3169827140191\nsample estimates:\nprobability of success \n                   0.1 \n\n\nAnother way to use the BinomCI function is shown below.\n\n# extract CIs                  \nBinomCI(2, 20,                        # apply BinomCI function\n        conf.level = 0.95,            # define ci\n        method = \"modified wilson\")   # define method for ci extraction\n\n     est          lwr.ci         upr.ci\n[1,] 0.1 0.0177680755349 0.301033645228"
  },
  {
    "objectID": "dstats.html#confidence-intervals-for-multinomial-data",
    "href": "dstats.html#confidence-intervals-for-multinomial-data",
    "title": "Descriptive Statistics with R",
    "section": "Confidence Intervals for Multinomial Data",
    "text": "Confidence Intervals for Multinomial Data\nWe use the MultinomCI function to extract the confidence intervals form multinominal data.\n\nobserved = c(35,74,22,69)       # define multinominal vector\nMultinomCI(observed,            # apply MultinomCI function\n           conf.level=0.95,     # define ci\n           method=\"goodman\")    # define method for ci extraction\n\n       est          lwr.ci         upr.ci\n[1,] 0.175 0.1125321503865 0.261910646274\n[2,] 0.370 0.2811364343551 0.468640684309\n[3,] 0.110 0.0622433796475 0.187087976345\n[4,] 0.345 0.2584619757815 0.443195358010"
  },
  {
    "objectID": "dviz.html#scatter-plots-in-base",
    "href": "dviz.html#scatter-plots-in-base",
    "title": "Data Visualization with R",
    "section": "Scatter Plots in base",
    "text": "Scatter Plots in base\nThe most fundamental function to create plots in the base environment is to use the general “plot” function. Here, we use that function to create a simple scatter plot.\n\n# create simple scatter plot\nplot(Prepositions ~ Date,                 # plot Prepositions by Date\n     type = \"p\",                          # plot type p (points) \n     data = pdat,                     # data from data set pdat  \n     ylab = \"Prepositions (Frequency)\",   # add y-axis label \n     xlab = \"Date (year of composition)\", # add x-axis label \n     main = \"plot type 'p' (points)\"      # add title \n     )                                    # end drawing plot\n\n\n\n\nLet us go over the command. The first part of the call is plot which is the function for plotting data in base R. In the round brackets are the arguments in which we specify what the plot should look like. The Prepositions ~ Date part tells R which variables should be displayed and the type = \"p\" part tells R which type of plot we want (p stands for points, l for lines, b for both lines and points). The part data = pdat tells R which data set to take the data from, and ylab = \"Prepositions (Frequency)\" and xlab = \"Date (year of composition)\" informs R about the axes’ labels. The part main = \"plot type 'p' (points)\" informs R about what we want as the main title of the plot.\nIn a next step, we will change the title, add two regression lines to the scatterplot (in the first case a linear and in the second case a smoothed regression line) and we will change the points as well as the color of the points.\n\n# create simple scatter plot with regression lines (ablines)\nplot(Prepositions ~ Date,                 # plot Prepositions by Date\n     type = \"p\",                          # plot type p (points) \n     data = pdat,                     # data from data set iris  \n     ylab = \"Prepositions (Frequency)\",   # add y-axis label \n     xlab = \"Date (year of composition)\", # add x-axis label \n     main = \"Scatterplot\",                # add title \n     pch = 20,                            # use point symbol 20 (filled circles)\n     col = \"lightgrey\"                    # define symbol colour as light grey\n     )                                    # end drawing plot\nabline(                                   # add regression line (y~x) \n  lm(pdat$Prepositions ~ pdat$Date),      # draw regression line of linear model (lm) \n  col=\"red\"                               # define line colour as red\n  )                                       # end drawing line             \nlines(                                    # add line (x,y)\n  lowess(pdat$Prepositions ~ pdat$Date),  # draw smoothed lowess line (x,y) \n  col=\"blue\"                              # define line colour as blue\n  )                                       # end drawing line\n\n\n\n\nThe only things that are different in the main call are the pch argument with has changed the points into filled dots (this is what the 20 stands for) and the col argument which we have specified as lightgrey. The regression lines are added using the abline and the lines argument.\n\n\n\n\nEXERCISE TIME!\n\n\n\n\n\n\n`\n\nLoad the data set called data03 and create a simple scatterplot showing the Variable1 on the x-axis and Variable2 on the y-axis.Tipp: Use the code below to load the data.\n\n\ndata03  <- base::readRDS(url(\"https://slcladal.github.io/data/d03.rda\", \"rb\"))\n\n\n\nAnswer\n\n::: {.cell}\n  # load data03\n  data03 <- read.delim(\"https://slcladal.github.io/data/data03.txt\", \n    sep = \"\\t\", header = T)\n  # create simple scatter plot\n  plot(Variable2 ~ Variable1, \n       type = \"p\", \n       data = data03, \n       ylab = \"Variable1\", \n       xlab = \"Variable2\",       \n       main = \"Scatterplot Exercise\",   \n       pch = 20,        \n       col = \"darkgrey\" \n       )\n::: {.cell-output-display}  ::: :::\n\n\n`"
  },
  {
    "objectID": "dviz.html#scatter-plots-in-lattice",
    "href": "dviz.html#scatter-plots-in-lattice",
    "title": "Data Visualization with R",
    "section": "Scatter Plots in lattice",
    "text": "Scatter Plots in lattice\nWe now turn to data visualization in lattice. As the lattice package is already loaded, we can create a first simple scatter plot using the xyplot function form the lattice package. The scatter plot shows the relative frequency of prepositions by year of composition.\n# create simple scatter plot\nxyplot(Prepositions ~ Date,  \n       # add y-axis label \n       ylab = \"Prepositions (Frequency)\",   \n       # add x-axis label\n       xlab = \"Date (year of composition)\", \n       data = pdat)                                    \n\n\n\nSince the lattice package was created to plot multiple relationships with a single call, we will now make use of that feature and plot multiple relationships at once. In addition, we will add a grid to the plot to improve comparability of data points within the graph. Thus, the scatter plot shows the relative frequency of prepositions by year of composition and genre.\n# create scatter plots by species\nxyplot(Prepositions ~ Date | Genre,     \n       # add y-axis label\n       ylab = \"Prepositions (Frequency)\", \n       # add y-axis label\n       xlab = \"Date (year of composition)\", \n       # add grids to panels\n       grid = TRUE,\n       data = pdat\n       )    \n\n\n\nThe only new code in the chunk above is the “| Genre” part. This part means that the relationship between Prepositions and Date should be displayed by Genre So, the |-symbol can be translated into “by”. The splitting of the plot into different panels for Genre is then done automatically.\nLike in base, we can modify lattice-plots and specify, e.g. the symbols that are plotted or their color.\n# create scatter plots by species\nxyplot(Prepositions ~ Date | Genre,\n       ylab = \"Prepositions (Frequency)\",  \n       xlab = \"Date (year of composition)\", \n       grid = TRUE,   \n       # symbol type (20 = filled dots)\n       pch = 20,            \n       # color of symbols\n       col = \"black\",\n       data = pdat\n       ) \n\n\n\nNext, we will use the ggplot2 package to create a scatter plot."
  },
  {
    "objectID": "dviz.html#scatter-plots-in-ggplot2",
    "href": "dviz.html#scatter-plots-in-ggplot2",
    "title": "Data Visualization with R",
    "section": "Scatter Plots in ggplot2",
    "text": "Scatter Plots in ggplot2\nWe now turn to data visualization using ggplot. As the ggplot2 package is already loaded, we create a very basic scatterplot in ggplot2 using the geom_point function to show the advantages of creating visualizations in this environment.\n# create simple scatter plot\n# use data set \"pdat\"\nggplot(pdat,  \n       # define axes\n       aes(x= Date,        \n           y= Prepositions)) + \n  # define plot type\n  geom_point()                  \n\n\n\nLet’s go over the code above. The function call for plotting in “ggplot2” is simply “ggplot”. This function takes the data set as its first argument and then requires aesthetics. The aesthetics are defined within the “ggplot” function as the arguments of “aes”. The “aes” function takes the axes as the arguments (in the current case). Then, we need to define the type of plot that we want. As we want a scatter plot with points, we add the “geom_point()” function without any arguments (as we do not want to specify the size, colour, and shape of the points just yet).\nThe advantage of “ggplot2” is that is really easy to modify the plot by adding new layers and to change the basic outlook by modifying the theme which is what we will do in the code below.\n\nggplot(pdat,    \n       # define axes\n       aes(x=Date,             \n           y= Prepositions, \n           # define to color by Species\n           color = GenreRedux)) + \n  # define plot type\n  geom_point() +   \n  # define theme  as black and white (bw)\n  theme_bw()                   \n\n\n\n\nThe white background is created by specifying the theme as a black and white theme (theme_bw()) while the color of the dots is changed by specifying that the color should be applied by Species (color = GenreRedux). Then, the colors to be used are defined in the function scale_color_manual.\nWe can now specify the symbols in the scatter plot.\n\n# create scatter plot colored by genre\nggplot(pdat, aes(Date, Prepositions, color = GenreRedux, shape = GenreRedux)) +\n  geom_point() +\n  guides(shape=guide_legend(override.aes=list(fill=NA))) +\n  scale_shape_manual(name = \"Genre\", \n                     breaks = names(table(pdat$GenreRedux)), \n                     values = 1:5) +\n  scale_color_manual(name = \"Genre\", \n                     breaks = names(table(pdat$GenreRedux)), \n                     values = clrs5) +\n  theme_bw() +\n  theme(legend.position=\"top\")"
  },
  {
    "objectID": "dviz.html#extensions-of-dot-plots",
    "href": "dviz.html#extensions-of-dot-plots",
    "title": "Data Visualization with R",
    "section": "Extensions of dot plots",
    "text": "Extensions of dot plots\nIn addition, we can add regression lines with error bars by Species and, if we want to show separate windows for the plots, we can use the “facet_grid” or “facet_wrap” function and define by which variable we want to create different panels.\n\n# create scatter plot colored by genre in different panels\nggplot(pdat, aes(Date, Prepositions,  color = Genre)) +\n  facet_wrap(vars(Genre), ncol = 4) +\n  geom_point() + \n  geom_smooth(method = \"lm\", se = F) +\n  theme_bw() +\n  theme(legend.title = element_blank(), \n        axis.text.x = element_text(size=8, angle=90))\n\n\n\n\nIf we only want to show the lines, we simply drop the “geom_point” function.\n\n# create scatter plot colored by genre in different panels\nggplot(pdat, aes(x=Date, y= Prepositions,  color = Genre)) +\n  facet_wrap(vars(Genre), ncol = 4) +\n  geom_smooth(method = \"lm\", se = F) +\n  theme_bw() +\n  theme(legend.title = element_blank(), \n        axis.text.x = element_text(size=8, angle=90))\n\n\n\n\nAnother option is to plot density layers instead of plotting the data points.\n\n# create scatter density plot\nggplot(pdat, aes(x=Date, y= Prepositions,  color = GenreRedux)) +\n    facet_wrap(vars(GenreRedux), ncol = 5) +\n  theme_bw() +                  \n  geom_density_2d() +\n  theme(legend.position = \"top\",\n        legend.title = element_blank(), \n        axis.text.x = element_text(size=8, angle=90))\n\n\n\n\nAlthough these are not scatterplots, plots with dot-symbols are very flexible and can be extended to show properties of the distribution of values. One way to create such a plot is to plot means as dot-symbols and add error bars to provide information about the underlying distribution. The plot below illustrates such a plot and additionally shows how plots can be further customized.\n\n# scatter plot with error bars\nggplot(pdat, aes(x=reorder(Genre, Prepositions, mean), y= Prepositions,  group = Genre)) +                 \n  stat_summary(fun = mean, geom = \"point\", aes(group= Genre)) +          \n  stat_summary(fun.data = mean_cl_boot,       \n               # add error bars\n               geom = \"errorbar\", width = 0.2) + \n  # def. y-axis range\n  coord_cartesian(ylim = c(100, 200)) +              \n  # def. font size\n  theme_bw(base_size = 15) +         \n  # def. x- and y-axis\n  theme(axis.text.x = element_text(size=10, angle = 90),  \n        axis.text.y = element_text(size=10, face=\"plain\")) + \n  # def. axes labels\n  labs(x = \"Genre\", y = \"Prepositions (Frequency)\") +     \n  # def. to col.\n  scale_color_manual(guide = FALSE)          \n\n\n\n\nBalloon plots are an extension of scatter plots that are typically used to display data that represents * two categorical variables * one numeric variable.\n# ballon plot\npdat %>%\n  dplyr::mutate(DateRedux = factor(DateRedux)) %>%\n  dplyr::group_by(DateRedux, GenreRedux) %>%\n  dplyr::summarise(Prepositions = mean(Prepositions)) %>%\n  ggplot(aes(DateRedux, 100, \n             size = Prepositions,\n             fill = GenreRedux)) +\n  facet_grid(vars(GenreRedux)) +\n  geom_point(shape = 21) +\n  scale_size_area(max_size = 15) +\n  coord_cartesian(ylim = c(50, 150)) +\n  theme_bw() +\n  theme(axis.title.y=element_blank(),\n        axis.text.y=element_blank(),\n        axis.ticks.y=element_blank()) +\n  scale_fill_discrete(guide = \"none\")"
  },
  {
    "objectID": "dviz.html#smoothed-line-graphs",
    "href": "dviz.html#smoothed-line-graphs",
    "title": "Data Visualization with R",
    "section": "Smoothed line graphs",
    "text": "Smoothed line graphs\nAnother very useful function when creating line graphs with “ggplot” is “geom_smooth” which smoothes the lines to be drawn.\n\nggplot(pdat, aes(x=DateRedux, y= Prepositions, group= GenreRedux, color = GenreRedux)) +\n  # add geom layer with lines\n  geom_smooth()\n\n\n\n\nAs this smoothed line graph is extremely useful, we will customize it to show how to modify your graph.\n\n# define aesthetics\nggplot(pdat, aes(x=Date, y= Prepositions,  color = GenreRedux, linetype = GenreRedux)) +\n  # add geom layer with lines\n  geom_smooth(se = F) +  \n  # legend without background color\n  guides(color=guide_legend(override.aes=list(fill=NA))) +  \n  # def. legend position\n  theme(legend.position=\"top\") +  \n  # def. linetype\n  scale_linetype_manual(values=c(\"twodash\", \"dashed\", \"dotdash\", \"dotted\", \"solid\"), \n                        # def. legend header\n                        name=c(\"Genre\"),\n                        # def. linetypes\n                        breaks = names(table(pdat$GenreRedux)),\n                        # def. labels\n                        labels = names(table(pdat$GenreRedux))) + \n  # def. col.\n  scale_colour_manual(values=clrs5,\n                      # define legend header\n                      name=c(\"Genre\"),\n                      # define elements\n                      breaks=names(table(pdat$GenreRedux)),  \n                      # define labels\n                      labels = names(table(pdat$GenreRedux))) +\n  # add x-axis label\n  labs(x = \"Year\") +      \n  # customize x-axis tick positions\n  scale_x_continuous(breaks=seq(1100, 1900, 100), \n                     # add labels to x-axis tick pos.\n                     labels=seq(1100, 1900, 100)) +\n  # add y-axis label\n  scale_y_continuous(name=\"Relative frequency \\n(per 1,000 words)\",  \n                     # customize tick y-axis\n                     limits=c(100, 200)) + \n  # define theme  as black and white\n  theme_bw(base_size = 10)  \n\n\n\n\nAlthough the code for the customized smoothed line graph is much longer and requires addition specifications, it is a very nice way to portrait the development over time."
  },
  {
    "objectID": "dviz.html#ribbon-plots",
    "href": "dviz.html#ribbon-plots",
    "title": "Data Visualization with R",
    "section": "Ribbon plots",
    "text": "Ribbon plots\nRibbon plots show an area, typically between minimum and maximum values. In addition, ribbon plots commonly also show the mean as depicted below.\n# create dot plot\npdat %>%\n  dplyr::mutate(DateRedux = as.numeric(DateRedux)) %>%\n  dplyr::group_by(DateRedux) %>%\n  dplyr::summarise(Mean = mean(Prepositions),\n                   Min = min(Prepositions),\n                   Max = max(Prepositions)) %>%\n  ggplot(aes(x = DateRedux, y = Mean)) +  \n  geom_ribbon(aes(ymin = Min, ymax = Max), fill = \"gray80\") +\n  geom_line() +\n  scale_x_continuous(labels = names(table(pdat$DateRedux)))"
  },
  {
    "objectID": "dviz.html#line-graphs-for-likert-data",
    "href": "dviz.html#line-graphs-for-likert-data",
    "title": "Data Visualization with R",
    "section": "Line graphs for Likert data",
    "text": "Line graphs for Likert data\nA special case of line graphs is used when dealing with Likert-scaled variables. In such cases, the line graph displays the density of cumulative frequencies of responses. The difference between the cumulative frequencies of responses displays differences in preferences. We will only focus on how to create such graphs using the “ggplot” environment here as it has an inbuilt function (“ecdf”) which is designed to handle such data.\nIn a first step, we create a data set which consists of a Likert-scaled variable. The fictitious data created here consists of rating of students from three courses about how satisfied they were with their language-learning course. The response to the Likert item is numeric so that “strongly disagree/very dissatisfied” would get the lowest and “strongly agree/very satisfied” the highest numeric value.\n\nldat <- base::readRDS(url(\"https://slcladal.github.io/data/lid.rda\", \"rb\"))\n\nLet’s briefly inspect the data.\n\n\n\n\n\nFirst 15 rows of the ldat data.\n\n\nCourseSatisfactionChinese1Chinese1Chinese1Chinese1Chinese1Chinese1Chinese1Chinese1Chinese1Chinese1Chinese1Chinese1Chinese1Chinese1Chinese1\n\n\nNow that we have data resembling a Likert-scaled item from a questionnaire, we will display the data in a cumulative line graph.\n\n# create cumulative density plot\nggplot(ldat,aes(x = Satisfaction, color = Course)) + \n  geom_step(aes(y = ..y..), stat = \"ecdf\") +\n  labs(y = \"Cumulative Density\") + \n  scale_x_discrete(limits = 1:5, breaks = 1:5,\n        labels=c(\"very dissatisfied\", \"dissatisfied\", \"neutral\", \"satisfied\", \"very satisfied\")) + \n  scale_colour_manual(values = clrs3)  \n\n\n\n\nThe satisfaction of the German course was the lowest as the red line shows the highest density (frequency of responses) of “very dissatisfied” and “dissatisfied” ratings. The students in our fictitious data set were most satisfied with the Chinese course as the blue line is the lowest for “very dissatisfied” and “dissatisfied” ratings while the difference between the courses shrinks for “satisfied” and “very satisfied”. The Japanese language course is in-between the German and the Chinese course."
  },
  {
    "objectID": "dviz.html#histograms-in-base-r",
    "href": "dviz.html#histograms-in-base-r",
    "title": "Data Visualization with R",
    "section": "Histograms in base R",
    "text": "Histograms in base R\nTo create histograms in base R, we simply use the hist function and add the variable that we want to summarize as its argument.\n\nhist(pdat$Prepositions,\n     xlab = \"Prepositions (per 1,000 words)\",\n     main = \"\")"
  },
  {
    "objectID": "dviz.html#histograms-in-ggplot",
    "href": "dviz.html#histograms-in-ggplot",
    "title": "Data Visualization with R",
    "section": "Histograms in ggplot",
    "text": "Histograms in ggplot\nUsing ggplot, we specify the variable we want to summarize in the aesthetics and use the geom_histogram function to generate a histogram.\n\nggplot(pdat, aes(Prepositions)) +\n  geom_histogram()\n\n\n\n\nWe can simply add information about a second variable by specifying this variable as the basis for the coloring of the bars (which we do by specify the fill argument).\n\nggplot(pdat, aes(Prepositions, fill = Region)) +\n  geom_histogram()"
  },
  {
    "objectID": "dviz.html#bar-plots-in-base-r",
    "href": "dviz.html#bar-plots-in-base-r",
    "title": "Data Visualization with R",
    "section": "Bar plots in base R",
    "text": "Bar plots in base R\nIn base R, we use the barplot function to create barplots. The barplot function is very flexible but takes a table with frequency counts as its main argument. We can also specify axes labels, a title, the color of the bras, and the axes limits. We specify text, grids, and boxes separately after the barplot function call.\n\n# create simple scatter plot\nbarplot(table(pdat$DateRedux),        # plot Texts by DateRedux\n     ylab = \"Texts (Frequency)\",          # add y-axis label \n     xlab = \"Period of composition\",      # add x-axis label \n     main = \"bar plot in base R\",         # add title\n     col = clrs5,                         # add colors\n     ylim = c(0, 250)                     # define y-axis limits\n     )                                    # end drawing plot\ngrid()                                    # add grid\ntext(seq(0.7, 5.5, 1.2),                  # add label positions (x-axis)\n     table(pdat$DateRedux)+10,        # add label positions (y-axis)\n     table(pdat$DateRedux))           # add labels\nbox()                                     # add box\n\n\n\n\nTo create grouped bar plots, we tabulate the variables that we are interested in. In the this example, we group by Region as shown below.\n\n# create simple scatter plot\nbarplot(table(pdat$DateRedux, pdat$Region), # plot Texts by DateRedux\n        beside = T,                          # bars beside each other\n        ylab = \"Texts (Frequency)\",          # add y-axis label \n        xlab = \"Period of composition\",      # add x-axis label\n        main = \"grouped bar plot in base R\", # add title\n        col = clrs5,                         # add colors\n        ylim = c(0, 250)                     # define y-axis limits\n        )                                    # end drawing plot\ngrid()                                       # add grid\ntext(c(seq(1.5, 5.5, 1.0), seq(7.5, 11.5, 1.0)),    # add label positions (x-axis)\n     table(pdat$DateRedux, pdat$Region)+10, # add label positions (y-axis)\n     table(pdat$DateRedux, pdat$Region))    # add labels\nlegend(\"topleft\", names(table(pdat$DateRedux)), # add legend\n       fill = clrs5)                        # add colors\nbox()                                       # add box\n\n\n\n\nTo transpose the plot, i.e. showing the Frequencies on the x- rather than the y-axis, we set the argument horiz to TRUE.\n\n# create simple scatter plot\nbarplot(table(pdat$DateRedux),        # plot Texts by DateRedux\n     ylab = \"Texts (Frequency)\",          # add y-axis label \n     xlab = \"Period of composition\",      # add x-axis label\n     col = clrs5,                         # add colors\n     horiz = T,                           # horizontal bars\n     xlim = c(0, 250),                    # define x-axis limits\n     las = 2,                             # change direction of axis labels\n     cex.names = .5)                      # reduce font of axis labels\nbox()                                     # add box"
  },
  {
    "objectID": "dviz.html#bar-plots-in-ggplot",
    "href": "dviz.html#bar-plots-in-ggplot",
    "title": "Data Visualization with R",
    "section": "Bar plots in ggplot",
    "text": "Bar plots in ggplot\nThe creation of barplots in ggplot works just like other types of visualizations in this framework. We first define the data and the aesthetics and then use the geom_bar to create a barplot.\n\n# bar plot\nggplot(bdat, aes(DateRedux, Percent, fill = DateRedux)) +\n  geom_bar(stat=\"identity\") +          # determine type of plot\n  theme_bw() +                         # use black & white theme\n  # add and define text\n  geom_text(aes(y = Percent-5, label = Percent), color = \"white\", size=3) + \n  # add colors\n  scale_fill_manual(values = clrs5) +\n  # suppress legend\n  theme(legend.position=\"none\")\n\n\n\n\nCompared with the pie chart, it is much easier to grasp the relative size and order of the percentage values which shows that pie charts are unfit to show relationships between elements in a graph and, as a general rule of thumb, should be avoided.\nBar plot can be grouped to add another layer of information which is particularly useful when dealing with frequency counts across multiple categorical variables. To create grouped bar plots, we plot Region while including DateRedux as the fill argument. Also, we use the command position=position_dodge().\n\n# bar plot\nggplot(pdat, aes(Region, fill = DateRedux)) + \n  geom_bar(position = position_dodge(), stat = \"count\") +  \n  theme_bw() +\n  scale_fill_manual(values = clrs5)\n\n\n\n\nIf we leave out the position=position_dodge() argument, we get a stacked bar plot as shown below.\n\n# bar plot\nggplot(pdat, aes(DateRedux, fill = GenreRedux)) + \n  geom_bar(stat=\"count\") +  \n  theme_bw() +\n  scale_fill_manual(values = clrs5)    \n\n\n\n\nOne issue to consider when using stacked bar plots is the number of variable levels: when dealing with many variable levels, stacked bar plots tend to become rather confusing. This can be solved by either collapsing infrequent variable levels or choose a colour palette that reflects some other inherent piece of information such as formality (e.g. blue) versus informality (e.g. red).\nStacked bar plots can also be normalized so that changes in percentages become visible. This is done by exchanging position=position_dodge() with position=\"fill\".\n\n# bar plot\nggplot(pdat, aes(DateRedux, fill = GenreRedux)) + \n  geom_bar(stat=\"count\", position=\"fill\") +  \n  theme_bw() +\n  scale_fill_manual(values = clrs5) +\n  labs(y = \"Probability\")"
  },
  {
    "objectID": "dviz.html#bar-plots-for-likert-data",
    "href": "dviz.html#bar-plots-for-likert-data",
    "title": "Data Visualization with R",
    "section": "Bar plots for Likert data",
    "text": "Bar plots for Likert data\nBar plots are particularly useful when visualizing data obtained through Likert items. As this is a very common issue that empirical researchers face. There are two basic ways to display Likert items using bar plots: grouped bar plots and more elaborate scaled bar plots.\nAlthough we have seen above how to create grouped bar plots, we will repeat it here with the language course example used above when we used cumulative density line graphs to visualise how to display Likert data.\nIn a first step, we recreate the data set which we have used above. The data set consists of a Likert-scaled variable (Satisfaction) which represents rating of students from three courses about how satisfied they were with their language-learning course. The response to the Likert item is numeric so that “strongly disagree/very dissatisfied” would get the lowest and “strongly agree/very satisfied” the highest numeric value.\n\n# create likert data\nnlik <- ldat %>%\n  dplyr::group_by(Course, Satisfaction) %>%\n  dplyr::summarize(Frequency = n())\n# inspect data\nhead(nlik)\n\n# A tibble: 6 × 3\n# Groups:   Course [2]\n  Course  Satisfaction Frequency\n  <chr>          <int>     <int>\n1 Chinese            1        20\n2 Chinese            2        30\n3 Chinese            3        25\n4 Chinese            4        10\n5 Chinese            5        15\n6 German             1        40\n\n\nNow that we have data resembling a Likert-scaled item from a questionnaire, we will display the data in a cumulative line graph.\n\n# create grouped bar plot\nggplot(nlik, aes(Satisfaction, Frequency,  fill = Course)) +\n  geom_bar(stat=\"identity\", position=position_dodge()) +\n  # define colors\n  scale_fill_manual(values=clrs5) + \n  # add text and define colour\n  geom_text(aes(label=Frequency), vjust=1.6, color=\"white\", \n            # define text position and size\n            position = position_dodge(0.9),  size=3.5) +     \n    scale_x_discrete(limits=c(\"1\",\"2\",\"3\",\"4\",\"5\"), breaks=c(1,2,3,4,5),\n        labels=c(\"very dissatisfied\", \"dissatisfied\",  \"neutral\", \"satisfied\", \n                 \"very satisfied\")) + \n  theme_bw()\n\n\n\n\nAnother and very interesting way to display such data is by using the Likert package. In a first step, we need to activate the package, clean the data, and extract a subset for the data visualization example.\n\nsdat <- base::readRDS(url(\"https://slcladal.github.io/data/sdd.rda\", \"rb\"))\n\n\n\n\n\n\nFirst 15 rows of the sdat data.\n\n\nGroupRespondentHow.did.you.like.the.course.How.did.you.like.the.teacher.Was.the.content.intersting.Was.the.content.adequate.for.the.course.Were.there.enough.discussions.Was.the.use.of.online.materials.appropriate.Was.the.teacher.appropriately.prepared.Was.the.workload.of.the.course.appropriate.Was.the.course.content.enganging.Were.there.enough.interactive.exerceises.included.in.the.sessions.GermanG14444444444GermanG24534445313GermanG35342434431GermanG43333333333GermanG51111111111GermanG63132333333GermanG75342433445GermanG85555555555GermanG95133445531GermanG103333333333GermanG114241444511GermanG123333333333GermanG135555555555GermanG143533433424GermanG154231444322\n\n\nAs you can see, we need to clean and adapt the column names. To do this, we will\n\nadd an identifier which shows which question we are dealing with (e.g. Q 1: question text)\nremove the dots between words with spaces\nadd a question mark at the end of questions\nremove superfluous white spaces\n\n\n# clean column names\ncolnames(sdat)[3:ncol(sdat)] <- paste0(\"Q \", str_pad(1:10, 2, \"left\", \"0\"), \": \", colnames(sdat)[3:ncol(sdat)]) %>%\n  stringr::str_replace_all(\"\\\\.\", \" \") %>%\n  stringr::str_squish() %>%\n  stringr::str_replace_all(\"$\", \"?\")\n# inspect column names\ncolnames(sdat)\n\n [1] \"Group\"                                                                   \n [2] \"Respondent\"                                                              \n [3] \"Q 01: How did you like the course?\"                                      \n [4] \"Q 02: How did you like the teacher?\"                                     \n [5] \"Q 03: Was the content intersting?\"                                       \n [6] \"Q 04: Was the content adequate for the course?\"                          \n [7] \"Q 05: Were there enough discussions?\"                                    \n [8] \"Q 06: Was the use of online materials appropriate?\"                      \n [9] \"Q 07: Was the teacher appropriately prepared?\"                           \n[10] \"Q 08: Was the workload of the course appropriate?\"                       \n[11] \"Q 09: Was the course content enganging?\"                                 \n[12] \"Q 10: Were there enough interactive exerceises included in the sessions?\"\n\n\nNow, that we have nice column names, we will replace the numeric values (1 to 5) with labels ranging from disagree to agree and convert our data into a data frame.\n\nlbs <- c(\"disagree\", \"somewhat disagree\", \"neither agree nor disagree\",  \"somewhat agree\", \"agree\")\nsurvey <- sdat %>%\n  dplyr::mutate_if(is.character, factor) %>%\n  dplyr::mutate_if(is.numeric, factor, levels = 1:5, labels = lbs) %>%\n  drop_na() %>%\n  as.data.frame()\n\n\n\n\n\n\nFirst 15 rows of the survey data.\n\n\nGroupRespondentQ 01: How did you like the course?Q 02: How did you like the teacher?Q 03: Was the content intersting?Q 04: Was the content adequate for the course?Q 05: Were there enough discussions?Q 06: Was the use of online materials appropriate?Q 07: Was the teacher appropriately prepared?Q 08: Was the workload of the course appropriate?Q 09: Was the course content enganging?Q 10: Were there enough interactive exerceises included in the sessions?GermanG1somewhat agreesomewhat agreesomewhat agreesomewhat agreesomewhat agreesomewhat agreesomewhat agreesomewhat agreesomewhat agreesomewhat agreeGermanG2somewhat agreeagreeneither agree nor disagreesomewhat agreesomewhat agreesomewhat agreeagreeneither agree nor disagreedisagreeneither agree nor disagreeGermanG3agreeneither agree nor disagreesomewhat agreesomewhat disagreesomewhat agreeneither agree nor disagreesomewhat agreesomewhat agreeneither agree nor disagreedisagreeGermanG4neither agree nor disagreeneither agree nor disagreeneither agree nor disagreeneither agree nor disagreeneither agree nor disagreeneither agree nor disagreeneither agree nor disagreeneither agree nor disagreeneither agree nor disagreeneither agree nor disagreeGermanG5disagreedisagreedisagreedisagreedisagreedisagreedisagreedisagreedisagreedisagreeGermanG6neither agree nor disagreedisagreeneither agree nor disagreesomewhat disagreeneither agree nor disagreeneither agree nor disagreeneither agree nor disagreeneither agree nor disagreeneither agree nor disagreeneither agree nor disagreeGermanG7agreeneither agree nor disagreesomewhat agreesomewhat disagreesomewhat agreeneither agree nor disagreeneither agree nor disagreesomewhat agreesomewhat agreeagreeGermanG8agreeagreeagreeagreeagreeagreeagreeagreeagreeagreeGermanG9agreedisagreeneither agree nor disagreeneither agree nor disagreesomewhat agreesomewhat agreeagreeagreeneither agree nor disagreedisagreeGermanG10neither agree nor disagreeneither agree nor disagreeneither agree nor disagreeneither agree nor disagreeneither agree nor disagreeneither agree nor disagreeneither agree nor disagreeneither agree nor disagreeneither agree nor disagreeneither agree nor disagreeGermanG11somewhat agreesomewhat disagreesomewhat agreedisagreesomewhat agreesomewhat agreesomewhat agreeagreedisagreedisagreeGermanG12neither agree nor disagreeneither agree nor disagreeneither agree nor disagreeneither agree nor disagreeneither agree nor disagreeneither agree nor disagreeneither agree nor disagreeneither agree nor disagreeneither agree nor disagreeneither agree nor disagreeGermanG13agreeagreeagreeagreeagreeagreeagreeagreeagreeagreeGermanG14neither agree nor disagreeagreeneither agree nor disagreeneither agree nor disagreesomewhat agreeneither agree nor disagreeneither agree nor disagreesomewhat agreesomewhat disagreesomewhat agreeGermanG15somewhat agreesomewhat disagreeneither agree nor disagreedisagreesomewhat agreesomewhat agreesomewhat agreeneither agree nor disagreesomewhat disagreesomewhat disagree\n\n\nNow, we can use the plot and the likert function to visualize the survey data.\n\n# load package\nlibrary(likert)\n# generate plot\nplot(likert(survey[,3:12]), ordered = F, wrap= 60)\n\n\n\n\nTo save this plot, you can use the save_plot function from the cowplot package as shown below.\n\nsurvey_p1 <- plot(likert(survey[,3:12]), ordered = F, wrap= 60)\n# save plot\ncowplot::save_plot(here(\"images\", \"stu_p1.png\"), # where to save the plot\n                   survey_p1,        # object to plot\n                   base_asp = 1.5,  # ratio of space fro questions vs space for plot\n                   base_height = 8) # size! higher for smaller font size\n\nAn additional and very helpful feature is that the likert package enables grouping the data as shown below. The display columns 3 to 8 and use column 1 for grouping.\n\n# create plot\nplot(likert(survey[,3:8], grouping = survey[,1]))"
  },
  {
    "objectID": "dviz.html#comparative-bar-plots-with-negative-values",
    "href": "dviz.html#comparative-bar-plots-with-negative-values",
    "title": "Data Visualization with R",
    "section": "Comparative bar plots with negative values",
    "text": "Comparative bar plots with negative values\nAnother frequent task is to evaluate the divergence of values from a reference, for instance when dealing with language learners where native speakers serve as a reference or target. To illustrate how such data can be visualized, we load the scales package as we want to create a bar plot in which we show the divergence of learners from native speakers regarding certain features and how that divergence changes over time. Then, we create an example data set which mirrors the format we expect for the actual data.\n\n# create a vector with values called Test1\nTest1 <- c(11.2, 13.5, 200, 185, 1.3, 3.5) \n# create a vector with values called Test2\nTest2 <- c(12.2, 14.7, 210, 175, 1.9, 3.0)   \n# create a vector with values called Test3\nTest3 <- c(13.2, 15.1, 177, 173, 2.4, 2.9)    \n# combine vectors in a data frame\ntestdata <- data.frame(Test1, Test2, Test3)     \n# add rownames\nrownames(testdata) <- c(\"Feature1_Student\",     \n                        \"Feature1_Reference\", \n                        \"Feature2_Student\", \n                        \"Feature2_Reference\", \n                        \"Feature3_Student\", \n                        \"Feature3_Reference\")\n# inspect data\ntestdata                                        \n\n                   Test1 Test2 Test3\nFeature1_Student    11.2  12.2  13.2\nFeature1_Reference  13.5  14.7  15.1\nFeature2_Student   200.0 210.0 177.0\nFeature2_Reference 185.0 175.0 173.0\nFeature3_Student     1.3   1.9   2.4\nFeature3_Reference   3.5   3.0   2.9\n\n\nWe can now determine how the learners deviate from the native speakers.\n\n# determine divergence from reference\n# row 1 (student) minus row 2 (reference)\nFeatureA <- t(testdata[1,] - testdata[2,]) \n# row 3 (student) minus row 4 (reference)\nFeatureB <- t(testdata[3,] - testdata[4,])  \n# row 5 (student) minus row 6 (reference)\nFeatureC <- t(testdata[5,] - testdata[6,])  \n# create data frame\nplottable <- data.frame(rep(rownames(FeatureA), 3), \n                  c(FeatureA, FeatureB, FeatureC), \n                  c(rep(\"FeatureA\", 3), \n                    rep(\"FeatureB\", 3), \n                    rep(\"FeatureC\", 3)))\n# def. col. names\ncolnames(plottable) <- c(\"Test\", \"Value\", \"Feature\")\n# inspect data\nplottable                                         \n\n   Test Value  Feature\n1 Test1  -2.3 FeatureA\n2 Test2  -2.5 FeatureA\n3 Test3  -1.9 FeatureA\n4 Test1  15.0 FeatureB\n5 Test2  35.0 FeatureB\n6 Test3   4.0 FeatureB\n7 Test1  -2.2 FeatureC\n8 Test2  -1.1 FeatureC\n9 Test3  -0.5 FeatureC\n\n\nFinally, we graphically display the divergence using a bar plot.\n\n# create plot\nggplot(plottable, \n       aes(Test, Value)) + # def. x/y-axes\n  # separate plots for each feature\n  facet_grid(vars(Feature), scales = \"free_y\") +\n  # create bars\n  geom_bar(stat = \"identity\", aes(fill = Test)) +  \n  # black and white theme\n  theme_bw() +\n  # suppress legend   \n  guides(fill=FALSE) + \n  # def. colours   \n  geom_bar(stat=\"identity\", fill=rep(clrs5[1:3], 3)) + \n  # axes titles\n  labs(x = \"\", y = \"Score\")"
  },
  {
    "objectID": "fixreg.html#multiple-linear-regression",
    "href": "fixreg.html#multiple-linear-regression",
    "title": "Fixed-Effects Regression Modelling in R",
    "section": "Multiple Linear Regression",
    "text": "Multiple Linear Regression\nIn contrast to simple linear regression, which estimates the effect of a single predictor, multiple linear regression estimates the effect of various predictor (see the equation below). A multiple linear regression can thus test the effects of various predictors simultaneously.\n\\[\\begin{equation}\n\nf_{(x)} = \\alpha + \\beta_{1}x_{i} + \\beta_{2}x_{i+1} + \\dots + \\beta_{n}x_{i+n} + \\epsilon\n\n\\end{equation}\\]\nThere exists a wealth of literature focusing on multiple linear regressions and the concepts it is based on. For instance, there are [1], [2], [3], [4], [5], [6], [7], [8] and [9] to name just a few. Introductions to regression modeling in R are [10], [11], [6], or [7].\nThe model diagnostics we are dealing with here are partly identical to the diagnostic methods discussed in the section on simple linear regression. Because of this overlap, diagnostics will only be described in more detail if they have not been described in the section on simple linear regression.\n\n\n\n\nEXCURSION\n\n\n\n\n\n\n`\n\n\nA note on sample size and power\n\nAlthough there appears to be a general assumption that 25 data points per variable level are sufficient, this is merely a general rule of thumb that is actually often incorrect. Such rules of thumb are inadequate because the required sample size depends on the number of variables in a given model, the size of the effect, the variance of the effect, and the desired power - in other words, the minimum necessary sample size relates to statistical power (see here for a tutorial on power). If a model contains many variables, then this requires a larger sample size than a model which only uses very few predictors and if the effect that we want to find is small, then this also requires a larger sample than if the effect we are interested in is large. Finally, effects that are very robust and do not exhibit substantive variability require a much smaller sample size compared with effects that are spurious and vary notably Since the sample size depends on the effect size and variance as well as the number of variables, there is no one-size-fits-all answer to what the best sample size is.\nAnother, slightly better but still incorrect, rule of thumb is that the more data, the better. This is not correct because models based on many cases are prone to report almost any effect as significant (although with a tiny effect size). A discussion about this phenomenon led to a discussion between Adam Kilgariff and Stefan Gries (see here for Adam’s initial discussion note and see here for Stefan’s response) resulting in a wider acknowledgement that effect sizes are more important than mere p-values. Also, given that there are procedures that can correct for overfitting, larger data sets are still preferable to data sets that are simply too small to warrant reliable results. In conclusion, it remains true that the sample size depends on the effect under investigation.\n\n\n`\n\nDespite there being no ultimate rule of thumb, [5], based on [12], provide data-driven suggestions for the minimal size of data required for regression models that aim to find medium sized effects (k = number of predictors; categorical variables with more than two levels should be transformed into dummy variables):\n\nIf one is merely interested in the overall model fit (something I have not encountered), then the sample size should be at least 50 + k (k = number of predictors in model).\nIf one is only interested in the effect of specific variables, then the sample size should be at least 104 + k (k = number of predictors in model).\nIf one is only interested in both model fit and the effect of specific variables, then the sample size should be at least the higher value of 50 + k or 104 + k (k = number of predictors in model).\n\nYou will see in the R code below that there is already a function that tests whether the sample size is sufficient.\n\nExample: Gifts and Availability\nThe example we will go through here is taken from [5]. In this example, the research question is if the money that men spend on presents for women depends on the women’s attractiveness and their relationship status. To answer this research question, we will implement a multiple linear regression and start by loading the data and inspect its structure and properties.\n\n# load data\nmlrdata  <- base::readRDS(url(\"https://slcladal.github.io/data/mld.rda\", \"rb\"))\n\n\n\n\n\n\nFirst 15 rows of the mlrdata.\n\n\nstatusattractionmoneyRelationshipNotInterested86.33RelationshipNotInterested45.58RelationshipNotInterested68.43RelationshipNotInterested52.93RelationshipNotInterested61.86RelationshipNotInterested48.47RelationshipNotInterested32.79RelationshipNotInterested35.91RelationshipNotInterested30.98RelationshipNotInterested44.82RelationshipNotInterested35.05RelationshipNotInterested64.49RelationshipNotInterested54.50RelationshipNotInterested61.48RelationshipNotInterested55.51\n\n\nThe data set consist of three variables stored in three columns. The first column contains the relationship status of the present giver (in this study this were men), the second whether the man is interested in the woman (the present receiver in this study), and the third column represents the money spend on the present. The data set represents 100 cases and the mean amount of money spend on a present is 88.38 dollars. In a next step, we visualize the data to get a more detailed impression of the relationships between variables.\n\n# create plots\np1 <- ggplot(mlrdata, aes(status, money)) +\n  geom_boxplot() + \n  theme_bw()\n# plot 2\np2 <- ggplot(mlrdata, aes(attraction, money)) +\n  geom_boxplot() +\n  theme_bw()\n# plot 3\np3 <- ggplot(mlrdata, aes(x = money)) +\n  geom_histogram(aes(y=..density..)) +            \n  theme_bw() +         \n  geom_density(alpha=.2, fill = \"gray50\") \n# plot 4\np4 <- ggplot(mlrdata, aes(status, money)) +\n  geom_boxplot(aes(fill = factor(status))) + \n  scale_fill_manual(values = c(\"grey30\", \"grey70\")) + \n  facet_wrap(~ attraction) + \n  guides(fill = \"none\") +\n  theme_bw()\n# show plots\nvip::grid.arrange(grobs = list(p1, p2, p3, p4), widths = c(1, 1), layout_matrix = rbind(c(1, 2), c(3, 4)))\n\n\n\n\nThe upper left figure consists of a boxplot which shows how much money was spent by relationship status. The figure suggests that men spend more on women if they are not in a relationship. The next figure shows the relationship between the money spend on presents and whether or not the men were interested in the women.\nThe boxplot in the upper right panel suggests that men spend substantially more on women if the men are interested in them. The next figure depicts the distribution of the amounts of money spend on the presents for the women. In addition, the figure indicates the existence of two outliers (dots in the boxplot)\nThe histogram in the lower left panel shows that, although the mean amount of money spent on presents is 88.38 dollars, the distribution peaks around 50 dollars indicating that on average, men spend about 50 dollars on presents. Finally, we will plot the amount of money spend on presents against relationship status by attraction in order to check whether the money spent on presents is affected by an interaction between attraction and relationship status.\nThe boxplot in the lower right panel confirms the existence of an interaction (a non-additive term) as men only spend more money on women if the men single and they are interested in the women. If men are not interested in the women, then the relationship has no effect as they spend an equal amount of money on the women regardless of whether they are in a relationship or not.\nWe will now start to implement the regression model. In a first step, we create two saturated models that contain all possible predictors (main effects and interactions). The two models are identical but one is generated with the lm and the other with the glm function as these functions offer different model parameters in their output.\n\nm1.mlr = lm(                      # generate lm regression object\n  money ~ 1 + attraction*status,  # def. regression formula (1 = intercept)\n  data = mlrdata)                 # def. data\nm1.glm = glm(                     # generate glm regression object\n  money ~ 1 + attraction*status,  # def. regression formula (1 = intercept)\n  family = gaussian,              # def. linkage function\n  data = mlrdata)                 # def. data\n\nAfter generating the saturated models we can now start with the model fitting. Model fitting refers to a process that aims at find the model that explains a maximum of variance with a minimum of predictors [see 5]. Model fitting is therefore based on the principle of parsimony which is related to Occam’s razor according to which explanations that require fewer assumptions are more likely to be true.\n\n\nAutomatic Model Fitting\nIn this section, we will use a backward elimination procedure that uses decreases in AIC (Akaike Information Criterion) as the criterion to minimize the model in a step-wise manner. This procedure aims at finding the model with the lowest AIC values by evaluating - step-by-step - whether the removal of a predictor (term) leads to a lower AIC value.\nWe use this method here to show how to implement it but a better option would be to use the glmulti function from the glmulti package [see 13] to find the best model. The glmulti function is better as it checks all possible models and the reports the model(s) with the best fit. This is advantages over forward or backward elimination as these procedures will often lead to different suggestions for the best model (based on the order in which predictors are added or removed). The tutorial on mixed-effects regression modelling shows how to implement\nThe reason for avoiding automated model fitting is that the algorithm only checks if the AIC has decreased but not if the model is stable or reliable. Thus, automated model fitting has the problem that you can never be sure that the way that lead you to the final model is reliable and that all models were indeed stable. Imagine you want to climb down from a roof top and you have a ladder. The problem is that you do not know if and how many steps are broken. This is similar to using automated model fitting. In other sections, we will explore better methods to fit models (manual step-wise step-up and step-down procedures, for example).\nThe AIC is calculated using the equation below. The lower the AIC value, the better the balance between explained variance and the number of predictors. AIC values can and should only be compared for models that are fit on the same data set with the same (number of) cases (LL stands for logged likelihood or LogLikelihood and k represents the number of predictors in the model (including the intercept); the LL represents a measure of how good the model fits the data).\n\\[\\begin{equation}\nAkaike Information Criterion (AIC) = -2LL + 2k\n\\end{equation}\\]\nAn alternative to the AIC is the BIC (Bayesian Information Criterion). Both AIC and BIC penalize models for including variables in a model. The penalty of the BIC is bigger than the penalty of the AIC and it includes the number of cases in the model (LL stands for logged likelihood or LogLikelihood, k represents the number of predictors in the model (including the intercept), and N represents the number of cases in the model).\n\\[\\begin{equation}\nBayesian Information Criterion (BIC) = -2LL + 2k * log(N)\n\\end{equation}\\]\nInteractions are evaluated first and only if all insignificant interactions have been removed would the procedure start removing insignificant main effects (that are not part of significant interactions). Other model fitting procedures (forced entry, step-wise step up, hierarchical) are discussed during the implementation of other regression models. We cannot discuss all procedures here as model fitting is rather complex and a discussion of even the most common procedures would to lengthy and time consuming at this point. It is important to note though that there is not perfect model fitting procedure and automated approaches should be handled with care as they are likely to ignore violations of model parameters that can be detected during manual - but time consuming - model fitting procedures. As a general rule of thumb, it is advisable to fit models as carefully and deliberately as possible. We will now begin to fit the model.\n\n# automated AIC based model fitting\nstep(m1.mlr, direction = \"both\")\n\nStart:  AIC=592.52\nmoney ~ 1 + attraction * status\n\n                    Df   Sum of Sq         RSS         AIC\n<none>                             34557.56428 592.5211556\n- attraction:status  1 24947.25481 59504.81909 644.8642395\n\n\n\nCall:\nlm(formula = money ~ 1 + attraction * status, data = mlrdata)\n\nCoefficients:\n                         (Intercept)               attractionNotInterested  \n                             99.1548                              -47.6628  \n                        statusSingle  attractionNotInterested:statusSingle  \n                             57.6928                              -63.1788  \n\n\nThe automated model fitting procedure informs us that removing predictors has not caused a decrease in the AIC. The saturated model is thus also the final minimal adequate model. We will now inspect the final minimal model and go over the model report.\n\nm2.mlr = lm(                       # generate lm regression object\n  money ~ (status + attraction)^2, # def. regression formula\n  data = mlrdata)                  # def. data\nm2.glm = glm(                      # generate glm regression object\n  money ~ (status + attraction)^2, # def. regression formula\n  family = gaussian,               # def. linkage function\n  data = mlrdata)                  # def. data\n# inspect final minimal model\nsummary(m2.mlr)\n\n\nCall:\nlm(formula = money ~ (status + attraction)^2, data = mlrdata)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-45.0760 -14.2580   0.4596  11.9315  44.1424 \n\nCoefficients:\n                                         Estimate   Std. Error  t value\n(Intercept)                           99.15480000   3.79459947 26.13050\nstatusSingle                          57.69280000   5.36637403 10.75080\nattractionNotInterested              -47.66280000   5.36637403 -8.88175\nstatusSingle:attractionNotInterested -63.17880000   7.58919893 -8.32483\n                                                   Pr(>|t|)    \n(Intercept)                          < 0.000000000000000222 ***\nstatusSingle                         < 0.000000000000000222 ***\nattractionNotInterested                 0.00000000000003751 ***\nstatusSingle:attractionNotInterested    0.00000000000058085 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 18.9729973 on 96 degrees of freedom\nMultiple R-squared:  0.852041334,   Adjusted R-squared:  0.847417626 \nF-statistic: 184.276619 on 3 and 96 DF,  p-value: < 0.0000000000000002220446\n\n\nThe first element of the report is called Call and it reports the regression formula of the model. Then, the report provides the residual distribution (the range, median and quartiles of the residuals) which allows drawing inferences about the distribution of differences between observed and expected values. If the residuals are distributed non-normally, then this is a strong indicator that the model is unstable and unreliable because mathematical assumptions on which the model is based are violated.\nNext, the model summary reports the most important part: a table with model statistics of the fixed-effects structure of the model. The table contains the estimates (coefficients of the predictors), standard errors, t-values, and the p-values which show whether a predictor significantly correlates with the dependent variable that the model investigates.\nAll main effects (status and attraction) as well as the interaction between status and attraction is reported as being significantly correlated with the dependent variable (money). An interaction occurs if a correlation between the dependent variable and a predictor is affected by another predictor.\nThe top most term is called intercept and has a value of 99.15 which represents the base estimate to which all other estimates refer. To exemplify what this means, let us consider what the model would predict that a man would spend on a present if he interested in the woman but he is also in a relationship. The amount he would spend (based on the model would be 99.15 dollars (which is the intercept). This means that the intercept represents the predicted value if all predictors take the base or reference level. And since being in relationship but being interested are the case, and because the interaction does not apply, the predicted value in our example is exactly the intercept (see below).\n\n#intercept  Single  NotInterested  Single:NotInterested\n99.15     + 57.69  + 0           + 0     # 156.8 single + interested\n\n[1] 156.84\n\n99.15     + 57.69  - 47.66       - 63.18 # 46.00 single + not interested\n\n[1] 46\n\n99.15     - 0      + 0           - 0     # 99.15 relationship + interested\n\n[1] 99.15\n\n99.15     - 0      - 47.66       - 0     # 51.49 relationship + not interested\n\n[1] 51.49\n\n\nNow, let us consider what a man would spend if he is in a relationship and he is not attracted to the women. In that case, the model predicts that the man would spend only 51.49 dollars on a present: the intercept (99.15) minus 47.66 because the man is not interested (and no additional subtraction because the interaction does not apply).\nWe can derive the same results easier using the predict function.\n\n# make prediction based on the model for original data\nprediction <- predict(m2.mlr, newdata = mlrdata)\n# inspect predictions\ntable(round(prediction,2))\n\n\n 46.01  51.49  99.15 156.85 \n    25     25     25     25 \n\n\nBelow the table of coefficients, the regression summary reports model statistics that provide information about how well the model performs. The difference between the values and the values in the coefficients table is that the model statistics refer to the model as a whole rather than focusing on individual predictors.\nThe multiple R2-value is a measure of how much variance the model explains. A multiple R2-value of 0 would inform us that the model does not explain any variance while a value of .852 mean that the model explains 85.2 percent of the variance. A value of 1 would inform us that the model explains 100 percent of the variance and that the predictions of the model match the observed values perfectly. Multiplying the multiple R2-value thus provides the percentage of explained variance. Models that have a multiple R2-value equal or higher than .05 are deemed substantially significant [see 14]. It has been claimed that models should explain a minimum of 5 percent of variance but this is problematic as it is not uncommon for models to have very low explanatory power while still performing significantly and systematically better than chance. In addition, the total amount of variance is negligible in cases where one is interested in very weak but significant effects. It is much more important for model to perform significantly better than minimal base-line models because if this is not the case, then the model does not have any predictive and therefore no explanatory power.\nThe adjusted R2-value considers the amount of explained variance in light of the number of predictors in the model (it is thus somewhat similar to the AIC and BIC) and informs about how well the model would perform if it were applied to the population that the sample is drawn from. Ideally, the difference between multiple and adjusted R2-value should be very small as this means that the model is not overfitted. If, however, the difference between multiple and adjusted R2-value is substantial, then this would strongly suggest that the model is unstable and overfitted to the data while being inadequate for drawing inferences about the population. Differences between multiple and adjusted R2-values indicate that the data contains outliers that cause the distribution of the data on which the model is based to differ from the distributions that the model mathematically requires to provide reliable estimates. The difference between multiple and adjusted R2-value in our model is very small (85.2-84.7=.05) and should not cause concern.\nBefore continuing, we will calculate the confidence intervals of the coefficients.\n\n# extract confidence intervals of the coefficients\nconfint(m2.mlr)\n\n                                              2.5 %         97.5 %\n(Intercept)                           91.6225795890 106.6870204110\nstatusSingle                          47.0406317400  68.3449682600\nattractionNotInterested              -58.3149682600 -37.0106317400\nstatusSingle:attractionNotInterested -78.2432408219 -48.1143591781\n\n# create and compare baseline- and minimal adequate model\nm0.mlr <- lm(money ~1, data = mlrdata)\nanova(m0.mlr, m2.mlr)\n\nAnalysis of Variance Table\n\nModel 1: money ~ 1\nModel 2: money ~ (status + attraction)^2\n  Res.Df          RSS Df   Sum of Sq         F                 Pr(>F)    \n1     99 233562.28650                                                    \n2     96  34557.56428  3 199004.7222 184.27662 < 0.000000000000000222 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nNow, we compare the final minimal adequate model to the base-line model to test whether then final model significantly outperforms the baseline model.\n\n# compare baseline- and minimal adequate model\nAnova(m0.mlr, m2.mlr, type = \"III\")\n\nAnova Table (Type III tests)\n\nResponse: money\n                 Sum Sq Df    F value                 Pr(>F)    \n(Intercept) 781015.8300  1 2169.64133 < 0.000000000000000222 ***\nResiduals    34557.5643 96                                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe comparison between the two model confirms that the minimal adequate model performs significantly better (makes significantly more accurate estimates of the outcome variable) compared with the baseline model.\n\n\nOutlier Detection\nAfter implementing the multiple regression, we now need to look for outliers and perform the model diagnostics by testing whether removing data points disproportionately decreases model fit. To begin with, we generate diagnostic plots.\n\n# generate plots\nautoplot(m2.mlr) + \n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +\n  theme_bw()\n\n\n\n\nThe plots do not show severe problems such as funnel shaped patterns or drastic deviations from the diagonal line in Normal Q-Q plot (have a look at the explanation of what to look for and how to interpret these diagnostic plots in the section on simple linear regression) but data points 52, 64, and 83 are repeatedly indicated as potential outliers.\n\n# determine a cutoff for data points that have D-values higher than 4/(n-k-1)\ncutoff <- 4/((nrow(mlrdata)-length(m2.mlr$coefficients)-2))\n# start plotting\npar(mfrow = c(1, 2))           # display plots in 3 rows/2 columns\nqqPlot(m2.mlr, main=\"QQ Plot\") # create qq-plot\n\n[1] 52 83\n\nplot(m2.mlr, which=4, cook.levels = cutoff); par(mfrow = c(1, 1))\n\n\n\n\nThe graphs indicate that data points 52, 64, and 83 may be problematic. We will therefore statistically evaluate whether these data points need to be removed. In order to find out which data points require removal, we extract the influence measure statistics and add them to out data set.\n\n# extract influence statistics\ninfl <- influence.measures(m2.mlr)\n# add infl. statistics to data\nmlrdata <- data.frame(mlrdata, infl[[1]], infl[[2]])\n# annotate too influential data points\nremove <- apply(infl$is.inf, 1, function(x) {\n  ifelse(x == TRUE, return(\"remove\"), return(\"keep\")) } )\n# add annotation to data\nmlrdata <- data.frame(mlrdata, remove)\n# number of rows before removing outliers\nnrow(mlrdata)\n\n[1] 100\n\n# remove outliers\nmlrdata <- mlrdata[mlrdata$remove == \"keep\", ]\n# number of rows after removing outliers\nnrow(mlrdata)\n\n[1] 98\n\n\nThe difference in row in the data set before and after removing data points indicate that two data points which represented outliers have been removed.\n\n\n\nNOTEIn general, outliers should not simply be removed unless there are good reasons for it (this could be that the outliers represent measurement errors). If a data set contains outliers, one should rather switch to methods that are better at handling outliers, e.g. by using weights to account for data points with high leverage. One alternative would be to switch to a robust regression (see here). However, here we show how to proceed by removing outliers as this is a common, though potentially problematic, method of dealing with outliers.\n\n\n\n\n\n\n\n\nRerun Regression\nAs we have decided to remove the outliers which means that we are now dealing with a different data set, we need to rerun the regression analysis. As the steps are identical to the regression analysis performed above, the steps will not be described in greater detail.\n\n# recreate regression models on new data\nm0.mlr = lm(money ~ 1, data = mlrdata)\nm0.glm = glm(money ~ 1, family = gaussian, data = mlrdata)\nm1.mlr = lm(money ~ (status + attraction)^2, data = mlrdata)\nm1.glm = glm(money ~ status * attraction, family = gaussian,\n             data = mlrdata)\n# automated AIC based model fitting\nstep(m1.mlr, direction = \"both\")\n\nStart:  AIC=570.29\nmoney ~ (status + attraction)^2\n\n                    Df   Sum of Sq         RSS         AIC\n<none>                             30411.31714 570.2850562\n- status:attraction  1 21646.86199 52058.17914 620.9646729\n\n\n\nCall:\nlm(formula = money ~ (status + attraction)^2, data = mlrdata)\n\nCoefficients:\n                         (Intercept)                          statusSingle  \n                          99.1548000                            55.8535333  \n             attractionNotInterested  statusSingle:attractionNotInterested  \n                         -47.6628000                           -59.4613667  \n\n\n\n# create new final models\nm2.mlr = lm(money ~ (status + attraction)^2, data = mlrdata)\nm2.glm = glm(money ~ status * attraction, family = gaussian,\n             data = mlrdata)\n# inspect final minimal model\nsummary(m2.mlr)\n\n\nCall:\nlm(formula = money ~ (status + attraction)^2, data = mlrdata)\n\nResiduals:\n         Min           1Q       Median           3Q          Max \n-35.76416667 -13.50520000  -0.98948333  10.59887500  38.77166667 \n\nCoefficients:\n                                         Estimate   Std. Error  t value\n(Intercept)                           99.15480000   3.59735820 27.56323\nstatusSingle                          55.85353333   5.14015367 10.86612\nattractionNotInterested              -47.66280000   5.08743275 -9.36873\nstatusSingle:attractionNotInterested -59.46136667   7.26927504 -8.17982\n                                                   Pr(>|t|)    \n(Intercept)                          < 0.000000000000000222 ***\nstatusSingle                         < 0.000000000000000222 ***\nattractionNotInterested               0.0000000000000040429 ***\nstatusSingle:attractionNotInterested  0.0000000000013375166 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 17.986791 on 94 degrees of freedom\nMultiple R-squared:  0.857375902,   Adjusted R-squared:  0.852824069 \nF-statistic: 188.358387 on 3 and 94 DF,  p-value: < 0.0000000000000002220446\n\n\n\n# extract confidence intervals of the coefficients\nconfint(m2.mlr)\n\n                                              2.5 %         97.5 %\n(Intercept)                           92.0121609656 106.2974390344\nstatusSingle                          45.6476377202  66.0594289465\nattractionNotInterested              -57.7640169936 -37.5615830064\nstatusSingle:attractionNotInterested -73.8946826590 -45.0280506744\n\n\n\n# compare baseline with final model\nanova(m0.mlr, m2.mlr)\n\nAnalysis of Variance Table\n\nModel 1: money ~ 1\nModel 2: money ~ (status + attraction)^2\n  Res.Df          RSS Df   Sum of Sq         F                 Pr(>F)    \n1     97 213227.06081                                                    \n2     94  30411.31714  3 182815.7437 188.35839 < 0.000000000000000222 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n# compare baseline with final model\nAnova(m0.mlr, m2.mlr, type = \"III\")\n\nAnova Table (Type III tests)\n\nResponse: money\n                 Sum Sq Df    F value                 Pr(>F)    \n(Intercept) 760953.2107  1 2352.07181 < 0.000000000000000222 ***\nResiduals    30411.3171 94                                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nAdditional Model Diagnostics\nAfter rerunning the regression analysis on the updated data set, we again create diagnostic plots in order to check whether there are potentially problematic data points.\n\n# generate plots\nautoplot(m2.mlr) + \n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +\n  theme_bw()\n\n\n\n\n\n# determine a cutoff for data points that have\n# D-values higher than 4/(n-k-1)\ncutoff <- 4/((nrow(mlrdata)-length(m2.mlr$coefficients)-2))\n# start plotting\npar(mfrow = c(1, 2))           # display plots in 1 row/2 columns\nqqPlot(m2.mlr, main=\"QQ Plot\") # create qq-plot\n\n84 88 \n82 86 \n\nplot(m2.mlr, which=4, cook.levels = cutoff); par(mfrow = c(1, 1))\n\n\n\n\nAlthough the diagnostic plots indicate that additional points may be problematic, but these data points deviate substantially less from the trend than was the case with the data points that have already been removed. To make sure that retaining the data points that are deemed potentially problematic by the diagnostic plots, is acceptable, we extract diagnostic statistics and add them to the data.\n\n# add model diagnostics to the data\nmlrdata <- mlrdata %>%\n  dplyr::mutate(residuals = resid(m2.mlr),\n                standardized.residuals = rstandard(m2.mlr),\n                studentized.residuals = rstudent(m2.mlr),\n                cooks.distance = cooks.distance(m2.mlr),\n                dffit = dffits(m2.mlr),\n                leverage = hatvalues(m2.mlr),\n                covariance.ratios = covratio(m2.mlr),\n                fitted = m2.mlr$fitted.values)\n\nWe can now use these diagnostic statistics to create more precise diagnostic plots.\n\n# plot 5\np5 <- ggplot(mlrdata,\n             aes(studentized.residuals)) +\n  theme(legend.position = \"none\")+\n  geom_histogram(aes(y=..density..),\n                 binwidth = .2,\n                 colour=\"black\",\n                 fill=\"gray90\") +\n  labs(x = \"Studentized Residual\", y = \"Density\") +\n  stat_function(fun = dnorm,\n                args = list(mean = mean(mlrdata$studentized.residuals, na.rm = TRUE),\n                            sd = sd(mlrdata$studentized.residuals, na.rm = TRUE)),\n                colour = \"red\", size = 1) +\n  theme_bw(base_size = 8)\n# plot 6\np6 <- ggplot(mlrdata, aes(fitted, studentized.residuals)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", colour = \"Red\")+\n  theme_bw(base_size = 8)+\n  labs(x = \"Fitted Values\",\n       y = \"Studentized Residual\")\n# plot 7\np7 <- qplot(sample = mlrdata$studentized.residuals, stat=\"qq\") +\n  theme_bw(base_size = 8) +\n  labs(x = \"Theoretical Values\",\n       y = \"Observed Values\")\nvip::grid.arrange(p5, p6, p7, nrow = 1)\n\n\n\n\nThe new diagnostic plots do not indicate outliers that require removal. With respect to such data points the following parameters should be considered:\n\nData points with standardized residuals > 3.29 should be removed [5]\nIf more than 1 percent of data points have standardized residuals exceeding values > 2.58, then the error rate of the model is unacceptable [5].\nIf more than 5 percent of data points have standardized residuals exceeding values > 1.96, then the error rate of the model is unacceptable [5]\nIn addition, data points with Cook’s D-values > 1 should be removed [5]\nAlso, data points with leverage values higher than \\(3(k + 1)/N\\) or \\(2(k + 1)/N\\) (k = Number of predictors, N = Number of cases in model) should be removed [5]\nThere should not be (any) autocorrelation among predictors. This means that independent variables cannot be correlated with itself (for instance, because data points come from the same subject). If there is autocorrelation among predictors, then a Repeated Measures Design or a (hierarchical) mixed-effects model should be implemented instead.\nPredictors cannot substantially correlate with each other (multicollinearity) (see the subsection on (multi-)collinearity in the section of multiple binomial logistic regression for more details about (multi-)collinearity). If a model contains predictors that have variance inflation factors (VIF) > 10 the model is unreliable [15] and predictors causing such VIFs should be removed. Indeed, even VIFs of 2.5 can be problematic [14] Indeed, [16] propose that variables with VIFs exceeding 3 should be removed!\n\n\n\n\nNOTEHowever, (multi-)collinearity is only an issue if one is interested in interpreting regression results! If the interpretation is irrelevant because what is relevant is prediction(!), then it does not matter if the model contains collinear predictors! See [6] for a more elaborate explanation.\n\n\n\n\n\n\n\nThe mean value of VIFs should be ~ 1 [17].\n\nThe following code chunk evaluates these criteria.\n\n# 1: optimal = 0\n# (listed data points should be removed)\nwhich(mlrdata$standardized.residuals > 3.29)\n\nnamed integer(0)\n\n# 2: optimal = 1\n# (listed data points should be removed)\nstdres_258 <- as.vector(sapply(mlrdata$standardized.residuals, function(x) {\nifelse(sqrt((x^2)) > 2.58, 1, 0) } ))\n(sum(stdres_258) / length(stdres_258)) * 100\n\n[1] 0\n\n# 3: optimal = 5\n# (listed data points should be removed)\nstdres_196 <- as.vector(sapply(mlrdata$standardized.residuals, function(x) {\nifelse(sqrt((x^2)) > 1.96, 1, 0) } ))\n(sum(stdres_196) / length(stdres_196)) * 100\n\n[1] 6.12244897959\n\n# 4: optimal = 0\n# (listed data points should be removed)\nwhich(mlrdata$cooks.distance > 1)\n\nnamed integer(0)\n\n# 5: optimal = 0\n# (data points should be removed if cooks distance is close to 1)\nwhich(mlrdata$leverage >= (3*mean(mlrdata$leverage)))\n\nnamed integer(0)\n\n# 6: checking autocorrelation:\n# Durbin-Watson test (optimal: high p-value)\ndwt(m2.mlr)\n\n lag  Autocorrelation D-W Statistic p-value\n   1 -0.0143324675649  1.9680423527   0.622\n Alternative hypothesis: rho != 0\n\n# 7: test multicollinearity 1\nvif(m2.mlr)\n\n                        statusSingle              attractionNotInterested \n                                2.00                                 1.96 \nstatusSingle:attractionNotInterested \n                                2.96 \n\n# 8: test multicollinearity 2\n1/vif(m2.mlr)\n\n                        statusSingle              attractionNotInterested \n                      0.500000000000                       0.510204081633 \nstatusSingle:attractionNotInterested \n                      0.337837837838 \n\n# 9: mean vif should not exceed 1\nmean(vif(m2.mlr))\n\n[1] 2.30666666667\n\n\nExcept for the mean VIF value (2.307) which should not exceed 1, all diagnostics are acceptable. We will now test whether the sample size is sufficient for our model. With respect to the minimal sample size and based on [12], [5] offer the following rules of thumb for an adequate sample size (k = number of predictors; categorical predictors with more than two levels should be recoded as dummy variables):\n\nif you are interested in the overall model: 50 + 8k (k = number of predictors)\nif you are interested in individual predictors: 104 + k\nif you are interested in both: take the higher value!\n\n\n\nEvaluation of Sample Size\nAfter performing the diagnostics, we will now test whether the sample size is adequate and what the values of R would be based on a random distribution in order to be able to estimate how likely a \\(\\beta\\)-error is given the present sample size [see 5]. Beta errors (or \\(\\beta\\)-errors) refer to the erroneous assumption that a predictor is not significant (based on the analysis and given the sample) although it does have an effect in the population. In other words, \\(\\beta\\)-error means to overlook a significant effect because of weaknesses of the analysis. The test statistics ranges between 0 and 1 where lower values are better. If the values approximate 1, then there is serious concern as the model is not reliable given the sample size. In such cases, unfortunately, the best option is to increase the sample size.\n\n# load functions\nsource(\"https://slcladal.github.io/rscripts/SampleSizeMLR.r\")\nsource(\"https://slcladal.github.io/rscripts/ExpR.r\")\n# check if sample size is sufficient\nsmplesz(m2.mlr)\n\n[1] \"Sample too small: please increase your sample by  9  data points\"\n\n# check beta-error likelihood\nexpR(m2.mlr)\n\n[1] \"Based on the sample size expect a false positive correlation of 0.0309 between the predictors and the predicted\"\n\n\nThe function smplesz reports that the sample size is insufficient by 9 data points according to [12]. The likelihood of \\(\\beta\\)-errors, however, is very small (0.0309). As a last step, we summarize the results of the regression analysis.\n\n# tabulate model results\nsjPlot::tab_model(m0.glm, m2.glm)\n\n\n\n\n \nmoney\nmoney\n\n\nPredictors\nEstimates\nCI\np\nEstimates\nCI\np\n\n\n(Intercept)\n88.12\n78.72 – 97.52\n<0.001\n99.15\n92.10 – 106.21\n<0.001\n\n\nstatus [Single]\n\n\n\n55.85\n45.78 – 65.93\n<0.001\n\n\nattraction[NotInterested]\n\n\n\n-47.66\n-57.63 – -37.69\n<0.001\n\n\nstatus [Single] *attraction[NotInterested]\n\n\n\n-59.46\n-73.71 – -45.21\n<0.001\n\n\nObservations\n98\n98\n\n\nR2\n0.000\n0.857\n\n\n\n\n\n\nAdditionally, we can inspect the summary of the regression model as shown below to extract additional information.\n\nsummary(m2.mlr)\n\n\nCall:\nlm(formula = money ~ (status + attraction)^2, data = mlrdata)\n\nResiduals:\n         Min           1Q       Median           3Q          Max \n-35.76416667 -13.50520000  -0.98948333  10.59887500  38.77166667 \n\nCoefficients:\n                                         Estimate   Std. Error  t value\n(Intercept)                           99.15480000   3.59735820 27.56323\nstatusSingle                          55.85353333   5.14015367 10.86612\nattractionNotInterested              -47.66280000   5.08743275 -9.36873\nstatusSingle:attractionNotInterested -59.46136667   7.26927504 -8.17982\n                                                   Pr(>|t|)    \n(Intercept)                          < 0.000000000000000222 ***\nstatusSingle                         < 0.000000000000000222 ***\nattractionNotInterested               0.0000000000000040429 ***\nstatusSingle:attractionNotInterested  0.0000000000013375166 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 17.986791 on 94 degrees of freedom\nMultiple R-squared:  0.857375902,   Adjusted R-squared:  0.852824069 \nF-statistic: 188.358387 on 3 and 94 DF,  p-value: < 0.0000000000000002220446\n\n\nAlthough [5] suggest that the main effects of the predictors involved in the interaction should not be interpreted, they are interpreted here to illustrate how the results of a multiple linear regression can be reported.\nWe can use the reports package [18] to summarize the analysis.\n\nreport::report(m2.mlr)\n\nWe fitted a linear model (estimated using OLS) to predict money with status and attraction (formula: money ~ (status + attraction)^2). The model explains a statistically significant and substantial proportion of variance (R2 = 0.86, F(3, 94) = 188.36, p < .001, adj. R2 = 0.85). The model's intercept, corresponding to status = Relationship and attraction = Interested, is at 99.15 (95% CI [92.01, 106.30], t(94) = 27.56, p < .001). Within this model:\n\n  - The effect of status [Single] is statistically significant and positive (beta = 55.85, 95% CI [45.65, 66.06], t(94) = 10.87, p < .001; Std. beta = 1.19, 95% CI [0.97, 1.41])\n  - The effect of attraction [NotInterested] is statistically significant and negative (beta = -47.66, 95% CI [-57.76, -37.56], t(94) = -9.37, p < .001; Std. beta = -1.02, 95% CI [-1.23, -0.80])\n  - The interaction effect of attraction [NotInterested] on status [Single] is statistically significant and negative (beta = -59.46, 95% CI [-73.89, -45.03], t(94) = -8.18, p < .001; Std. beta = -1.27, 95% CI [-1.58, -0.96])\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using the Wald approximation.\n\n\nWe can use this output to write up a final report:\nA multiple linear regression was fitted to the data using an automated, step-wise, AIC-based (Akaike’s Information Criterion) procedure. The model fitting arrived at a final minimal model. During the model diagnostics, two outliers were detected and removed. Further diagnostics did not find other issues after the removal.\nThe final minimal adequate regression model is based on 98 data points and performs highly significantly better than a minimal baseline model (multiple R2: .857, adjusted R2: .853, F-statistic (3, 94): 154.4, AIC: 850.4, BIC: 863.32, p<.001\\(***\\)). The final minimal adequate regression model reports attraction and status as significant main effects. The relationship status of men correlates highly significantly and positively with the amount of money spend on the women’s presents (SE: 5.14, t-value: 10.87, p<.001\\(***\\)). This shows that men spend 156.8 dollars on presents if they are single while they spend 99,15 dollars if they are in a relationship. Whether men are attracted to women also correlates highly significantly and positively with the money they spend on women (SE: 5.09, t-values: -9.37, p<.001\\(***\\)). If men are not interested in women, they spend 47.66 dollar less on a present for women compared with women the men are interested in.\nFurthermore, the final minimal adequate regression model reports a highly significant interaction between relationship status and attraction (SE: 7.27, t-value: -8.18, p<.001\\(***\\)): If men are single but they are not interested in a women, a man would spend only 59.46 dollars on a present compared to all other constellations."
  },
  {
    "objectID": "fixreg.html#robust-regression",
    "href": "fixreg.html#robust-regression",
    "title": "Fixed-Effects Regression Modelling in R",
    "section": "Robust Regression",
    "text": "Robust Regression\nRobust regression represents an alternative to linear regression models when the data contains outliers that should not be removed. As such, robust regressions can handle overly influential data points (outliers) and they allow us to retain outliers rather than removing them by adding weights [19]. Thus, robust regressions are used when there are outliers present in the data and we can thus not use traditional models but we do not have a good reason for removing the outliers.\n\nRobust regressions allow us to handle overly influential data points (outliers) by using weights. Thus, robust regressions enable us to retain all data points.\n\nWe begin by loading a data set (the mlrdata set which have used for multiple linear regression).\n\n# load data\nrobustdata  <- base::readRDS(url(\"https://slcladal.github.io/data/mld.rda\", \"rb\"))\n\n\n\n\n\n\nFirst 15 rows of the robustdata.\n\n\nstatusattractionmoneyRelationshipNotInterested86.33RelationshipNotInterested45.58RelationshipNotInterested68.43RelationshipNotInterested52.93RelationshipNotInterested61.86RelationshipNotInterested48.47RelationshipNotInterested32.79RelationshipNotInterested35.91RelationshipNotInterested30.98RelationshipNotInterested44.82RelationshipNotInterested35.05RelationshipNotInterested64.49RelationshipNotInterested54.50RelationshipNotInterested61.48RelationshipNotInterested55.51\n\n\nWe first fit an ordinary linear model (and although we know from the section on multiple regression that the interaction between status and attraction is significant, we will disregard this for now as this will help to explain the weighing procedure which is the focus of this section).\n\n# create model\nslm <- lm(money ~ status+attraction, data = robustdata)\n# inspect model\nsummary(slm)\n\n\nCall:\nlm(formula = money ~ status + attraction, data = robustdata)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-60.87070 -15.78645  -2.61010  13.88770  59.93710 \n\nCoefficients:\n                            Estimate   Std. Error   t value\n(Intercept)             114.94950000   4.28993616  26.79515\nstatusSingle             26.10340000   4.95359160   5.26959\nattractionNotInterested -79.25220000   4.95359160 -15.99894\n                                      Pr(>|t|)    \n(Intercept)             < 0.000000000000000222 ***\nstatusSingle                     0.00000082576 ***\nattractionNotInterested < 0.000000000000000222 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 24.767958 on 97 degrees of freedom\nMultiple R-squared:  0.745229335,   Adjusted R-squared:  0.739976331 \nF-statistic: 141.867286 on 2 and 97 DF,  p-value: < 0.0000000000000002220446\n\n\nWe now check whether the model is well fitted using diagnostic plots.\n\n# generate plots\nautoplot(slm) + \n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) \n\n\n\n\nThe diagnostic plots indicate that there are three outliers in the data (data points 52, 83 and possibly 64). Therefore, we need to evaluate if the outliers severely affect the fit of the model.\n\nrobustdata[c(52, 64, 83),]\n\n   status    attraction  money\n52 Single NotInterested   0.93\n64 Single NotInterested  84.28\n83 Single    Interested 200.99\n\n\nWe can now calculate Cook’s distance and standardized residuals check if the values of the potentially problematic points have unacceptably high values (-2 < ok < 2).\n\nCooksDistance <- cooks.distance(slm)\nStandardizedResiduals <- stdres(slm)\na <- cbind(robustdata, CooksDistance, StandardizedResiduals)\na[CooksDistance > 4/100, ]\n\n         status    attraction  money   CooksDistance StandardizedResiduals\n1  Relationship NotInterested  86.33 0.0444158829857         2.07565427025\n52       Single NotInterested   0.93 0.0641937458854        -2.49535435377\n65       Single NotInterested  12.12 0.0427613629698        -2.03662765573\n67       Single NotInterested  13.28 0.0407877963315        -1.98907421786\n83       Single    Interested 200.99 0.0622397126545         2.45708203516\n84       Single    Interested 193.69 0.0480020776213         2.15782333134\n88       Single    Interested 193.78 0.0481663678409         2.16151282221\n\n\nWe will calculate the absolute value and reorder the table so that it is easier to check the values.\n\nAbsoluteStandardizedResiduals <- abs(StandardizedResiduals)\na <- cbind(robustdata, CooksDistance, StandardizedResiduals, AbsoluteStandardizedResiduals)\nasorted <- a[order(-AbsoluteStandardizedResiduals), ]\nasorted[1:10, ]\n\n         status    attraction  money   CooksDistance StandardizedResiduals\n52       Single NotInterested   0.93 0.0641937458854        -2.49535435377\n83       Single    Interested 200.99 0.0622397126545         2.45708203516\n88       Single    Interested 193.78 0.0481663678409         2.16151282221\n84       Single    Interested 193.69 0.0480020776213         2.15782333134\n1  Relationship NotInterested  86.33 0.0444158829857         2.07565427025\n65       Single NotInterested  12.12 0.0427613629698        -2.03662765573\n67       Single NotInterested  13.28 0.0407877963315        -1.98907421786\n78       Single    Interested 188.76 0.0394313968241         1.95572122040\n21 Relationship NotInterested  81.90 0.0369837409025         1.89404933081\n24 Relationship NotInterested  81.56 0.0364414260634         1.88011125419\n   AbsoluteStandardizedResiduals\n52                 2.49535435377\n83                 2.45708203516\n88                 2.16151282221\n84                 2.15782333134\n1                  2.07565427025\n65                 2.03662765573\n67                 1.98907421786\n78                 1.95572122040\n21                 1.89404933081\n24                 1.88011125419\n\n\nAs Cook’s distance and the standardized residuals do have unacceptable values, we re-calculate the linear model as a robust regression and inspect the results\n\n# create robust regression model\nrmodel <- robustbase::lmrob(money ~ status + attraction, data = robustdata)\n# inspect model\nsummary(rmodel)\n\n\nCall:\nrobustbase::lmrob(formula = money ~ status + attraction, data = robustdata)\n \\--> method = \"MM\"\nResiduals:\n         Min           1Q       Median           3Q          Max \n-61.14269797 -15.20405742  -1.48712073  14.43502548  62.42342866 \n\nCoefficients:\n                            Estimate   Std. Error   t value\n(Intercept)             113.18405742   3.89777743  29.03810\nstatusSingle             25.38251392   5.08841085   4.98830\nattractionNotInterested -76.49387337   5.06626478 -15.09867\n                                      Pr(>|t|)    \n(Intercept)             < 0.000000000000000222 ***\nstatusSingle                      0.0000026725 ***\nattractionNotInterested < 0.000000000000000222 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nRobust residual standard error: 22.349751 \nMultiple R-squared:  0.740716956,   Adjusted R-squared:  0.735370914 \nConvergence in 11 IRWLS iterations\n\nRobustness weights: \n 10 weights are ~= 1. The remaining 90 ones are summarized as\n       Min.     1st Qu.      Median        Mean     3rd Qu.        Max. \n0.415507115 0.856134372 0.947485761 0.889078637 0.986192098 0.998890516 \nAlgorithmic parameters: \n           tuning.chi                    bb            tuning.psi \n1.5476399999999999046 0.5000000000000000000 4.6850610000000001421 \n           refine.tol               rel.tol             scale.tol \n0.0000001000000000000 0.0000001000000000000 0.0000000001000000000 \n            solve.tol           eps.outlier                 eps.x \n0.0000001000000000000 0.0010000000000000000 0.0000000000018189894 \n    warn.limit.reject     warn.limit.meanrw \n0.5000000000000000000 0.5000000000000000000 \n     nResample         max.it       best.r.s       k.fast.s          k.max \n           500             50              2              1            200 \n   maxit.scale      trace.lev            mts     compute.rd fast.s.large.n \n           200              0           1000              0           2000 \n                  psi           subsampling                   cov \n           \"bisquare\"         \"nonsingular\"         \".vcov.avar1\" \ncompute.outlier.stats \n                 \"SM\" \nseed : int(0) \n\n\nThe output shows that both status and attraction are significant but, as we have seen above, the effect that really matters is the interaction between status and attraction.\nWe will briefly check the weights to understand the process of weighing better. The idea of weighing is to downgrade data points that are too influential while not punishing data points that have a good fit and are thus less influential. This means that the problematic data points should have lower weights than other data points (the maximum is 1 - so points can only be made “lighter”).\n\nhweights <- data.frame(status = robustdata$status, resid = rmodel$resid, weight = rmodel$rweights)\nhweights2 <- hweights[order(rmodel$rweights), ]\nhweights2[1:15, ]\n\n         status          resid         weight\n83       Single  62.4234286579 0.415507114929\n52       Single -61.1426979713 0.434323488879\n88       Single  55.2134286579 0.521220438134\n84       Single  55.1234286579 0.522529018160\n78       Single  50.1934286579 0.593234264142\n65       Single -49.9526979713 0.596626236587\n1  Relationship  49.6398159519 0.601024799801\n67       Single -48.7926979713 0.612874510784\n21 Relationship  45.2098159519 0.661914443015\n24 Relationship  44.8698159519 0.666467525203\n39 Relationship -43.8940574189 0.679427923638\n79       Single  40.8234286579 0.719104302439\n58       Single -40.5226979713 0.722893398846\n89       Single  39.9734286579 0.729766935693\n95       Single  39.8234286579 0.731633318260\n\n\nThe values of the weights support our assumption that those data points that were deemed too influential are made lighter as they now only have weights of 0.415507114929 and 0.434323488879 respectively. This was, however, not the focus of this sections as this section merely served to introduce the concept of weights and how they can be used in the context of a robust linear regression."
  },
  {
    "objectID": "fixreg.html#logistic-regression",
    "href": "fixreg.html#logistic-regression",
    "title": "Fixed-Effects Regression Modelling in R",
    "section": "Logistic Regression",
    "text": "Logistic Regression\nLogistic regression is a multivariate analysis technique that builds on and is very similar in terms of its implementation to linear regression but logistic regressions take dependent variables that represent nominal rather than numeric scaling [20]. The difference requires that the linear regression must be modified in certain ways to avoid producing non-sensical outcomes. The most fundamental difference between logistic and linear regressions is that logistic regression work on the probabilities of an outcome (the likelihood), rather than the outcome itself. In addition, the likelihoods on which the logistic regression works must be logged (logarithmized) in order to avoid produce predictions that produce values greater than 1 (instance occurs) and 0 (instance does not occur). You can check this by logging the values from -10 to 10 using the plogis function as shown below.\n\nround(plogis(-10:10), 5)\n\n [1] 0.00005 0.00012 0.00034 0.00091 0.00247 0.00669 0.01799 0.04743 0.11920\n[10] 0.26894 0.50000 0.73106 0.88080 0.95257 0.98201 0.99331 0.99753 0.99909\n[19] 0.99966 0.99988 0.99995\n\n\nIf we visualize these logged values, we get an S-shaped curve which reflects the logistic function.\n\n\n\n\n\nTo understand what this mean, we will use a very simple example. In this example, we want to see whether the height of men affect their likelihood of being in a relationship. The data we use represents a data set consisting of two variables: height and relationship.\n\n\n\n\n\nThe left panel of the Figure above shows that a linear model would predict values for the relationship status, which represents a factor (0 = Single and 1 = In a Relationship), that are nonsensical because values above 1 or below 0 do not make sense. In contrast to a linear regression, which predicts actual values, such as the frequencies of prepositions in a certain text, a logistic regression predicts probabilities of events (for example, being in a relationship) rather than actual values. The center panel shows the predictions of a logistic regression and we see that a logistic regression also has an intercept and a (very steep) slope but that the regression line also predicts values that are above 1 and below 0. However, when we log the predicted values we these predicted values are transformed into probabilities with values between 0 and 1. And the logged regression line has a S-shape which reflects the logistic function. Furthermore, we can then find the optimal line (the line with the lowest residual deviance) by comparing the sum of residuals - just as we did for a simple linear model and that way, we find the regression line for a logistic regression.\n\nExample: EH in Kiwi English\nTo exemplify how to implement a logistic regression in R [see 21, 22] for very good and thorough introductions to this topic], we will analyze the use of the discourse particle eh in New Zealand English and test which factors correlate with its occurrence. The data set represents speech units in a corpus that were coded for the speaker who uttered a given speech unit, the gender, ethnicity, and age of that speaker and whether or not the speech unit contained an eh. To begin with, we clean the current work space, set option, install and activate relevant packages, load customized functions, and load the example data set.\n\n# load data\nblrdata  <- base::readRDS(url(\"https://slcladal.github.io/data/bld.rda\", \"rb\"))\n\n\n\n\n\n\nFirst 15 rows of the blrdata.\n\n\nIDGenderAgeEthnicityEH<S1A-001#M>MenYoungPakeha0<S1A-001#M>MenYoungPakeha1<S1A-001#M>MenYoungPakeha0<S1A-001#M>MenYoungPakeha0<S1A-001#M>MenYoungPakeha1<S1A-001#M>MenYoungPakeha1<S1A-001#M>MenYoungPakeha0<S1A-001#M>MenYoungPakeha0<S1A-001#M>MenYoungPakeha0<S1A-001#M>MenYoungPakeha1<S1A-001#M>MenYoungPakeha0<S1A-001#M>MenYoungPakeha0<S1A-001#M>MenYoungPakeha1<S1A-001#M>MenYoungPakeha1<S1A-001#M>MenYoungPakeha0\n\n\nThe summary of the data show that the data set contains 25,821 observations of five variables. The variable ID contains strings that represent a combination file and speaker of a speech unit. The second variable represents the gender, the third the age, and the fourth the ethnicity of speakers. The fifth variable represents whether or not a speech unit contained the discourse particle eh.\nNext, we factorize the variables in our data set. In other words, we specify that the strings represent variable levels and define new reference levels because as a default R will use the variable level which first occurs in alphabet ordering as the reference level for each variable, we redefine the variable levels for Age and Ethnicity.\n\nblrdata <- blrdata %>%\n  # factorize variables\n  dplyr::mutate(Age = factor(Age),\n                Gender = factor(Gender),\n                Ethnicity = factor(Ethnicity),\n                ID = factor(ID),\n                EH = factor(EH)) %>%\n  # relevel Age (Reference = Young) and Ethnicity (Reference= Pakeha))\n  dplyr::mutate(Age = relevel(Age, \"Young\"),\n                Ethnicity = relevel(Ethnicity, \"Pakeha\"))\n\nAfter preparing the data, we will now plot the data to get an overview of potential relationships between variables.\n\nblrdata %>%\n  dplyr::mutate(EH = ifelse(EH == \"0\", 0, 1)) %>%\n  ggplot(aes(Age, EH, color = Gender)) +\n  facet_wrap(~Ethnicity) +\n  stat_summary(fun = mean, geom = \"point\") +\n  stat_summary(fun.data = mean_cl_boot, geom = \"errorbar\", width = 0.2) +\n  theme_set(theme_bw(base_size = 10)) +\n  theme(legend.position = \"top\") +\n  labs(x = \"\", y = \"Observed Probabilty of eh\") +\n  scale_color_manual(values = c(\"gray20\", \"gray70\"))\n\n\n\n\nWith respect to main effects, the Figure above indicates that men use eh more frequently than women, that young speakers use it more frequently compared with old speakers, and that speakers that are descendants of European settlers (Pakeha) use eh more frequently compared with Maori (the native inhabitants of New Zealand).\nThe plots in the lower panels do not indicate significant interactions between use of eh and the Age, Gender, and Ethnicity of speakers. In a next step, we will start building the logistic regression model.\n\n\nModel Building\nAs a first step, we need to define contrasts and use the datadist function to store aspects of our variables that can be accessed later when plotting and summarizing the model. Contrasts define what and how variable levels should be compared and therefore influences how the results of the regression analysis are presented. In this case, we use treatment contrasts which are in-built. Treatment contrasts mean that we assess the significance of levels of a predictor against a baseline which is the reference level of a predictor. [5] and [6] provide very good and accessible explanations of contrasts and how to manually define contrasts if you would like to know more.\n\n# set contrasts\noptions(contrasts  =c(\"contr.treatment\", \"contr.poly\"))\n# create distance matrix\nblrdata.dist <- datadist(blrdata)\n# include distance matrix in options\noptions(datadist = \"blrdata.dist\")\n\nNext, we generate a minimal model that predicts the use of eh solely based on the intercept.\n\n# baseline glm model\nm0.glm = glm(EH ~ 1, family = binomial, data = blrdata)\n\n\n\nModel Fitting\nWe will now start with the model fitting procedure. In the present case, we will not use a manual step-wise procedure as above but we will use the the glmulti function from the glmulti package [see 13] to select the best model. The glmulti function computes all possible models (when we choose exhaustive screen by setting the method to \"h\") and then reports those models that have the best information criterion values, i.e. values which provide information about the most parsimonious models (models that explain a maxi,um amount of variance with a minimum number of predictors).\n\nblr.glmulti <- glmulti(y = EH ~ Gender + Age + Ethnicity, # formula\n                       crit = aicc,                     # information crit. (aic, bic, aicc, qaic, qaicc)\n                       data = blrdata,                  # data\n                       family = binomial,               # model type\n                       method = \"h\",                    # screening type\n                       fitfunc = glm,                   # fit function\n                       level = 2)                       # 2 = with intercations (1 = without)\n\nInitialization...\nTASK: Exhaustive screening of candidate set.\nFitting...\nCompleted.\n\n# inspect\nprint(blr.glmulti)\n\nglmulti.analysis\nMethod: h / Fitting: glm / IC used: aicc\nLevel: 2 / Marginality: FALSE\nFrom 18 models:\nBest IC: 32145.5418216606\nBest model:\n[1] \"EH ~ 1 + Gender + Age\"\nEvidence weight: 0.293712170021837\nWorst IC: 33009.7548480748\n4 models within 2 IC units.\n7 models to reach 95% of evidence weight.\n\n\nAs the results inform us that there are 4 additional models that perform similarly well (4 models within 2 IC units.) we inspect these 4 models.\n\nweightable(blr.glmulti)[1:4,] %>%\n  regulartable() %>%\n  autofit()\n\n\n\nmodelaiccweightsEH ~ 1 + Gender + Age32,145.54182170.293712170022EH ~ 1 + Gender + Age + Ethnicity + Ethnicity:Age32,146.47456180.184238336451EH ~ 1 + Gender + Age + Ethnicity32,147.28142690.123075457179EH ~ 1 + Gender + Age + Age:Gender32,147.41820150.114940009116\n\n\nThe results show that a model with only Gender and Age is the most parsimonious model and we decide to consider this model our final minimal adequate model.\n\n# baseline glm model\nlr.glm <- glm(EH ~ Age + Gender, family = binomial, data = blrdata)\nlr.lrm <- lrm(EH ~ Age + Gender, data = blrdata, x = T, y = T, linear.predictors = T)\n# inspect\nsummary(lr.glm)\n\n\nCall:\nglm(formula = EH ~ Age + Gender, family = binomial, data = blrdata)\n\nDeviance Residuals: \n         Min            1Q        Median            3Q           Max  \n-1.080454338  -0.915641628  -0.770357585   1.277555296   1.837151604  \n\nCoefficients:\n                 Estimate    Std. Error   z value               Pr(>|z|)    \n(Intercept) -0.2323829796  0.0222629000 -10.43813 < 0.000000000000000222 ***\nAgeOld      -0.8305365551  0.0335166085 -24.77985 < 0.000000000000000222 ***\nGenderWomen -0.4201134305  0.0272517594 -15.41601 < 0.000000000000000222 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 33007.75469  on 25820  degrees of freedom\nResidual deviance: 32139.54089  on 25818  degrees of freedom\nAIC: 32145.54089\n\nNumber of Fisher Scoring iterations: 4\n\n\n\n\nModel Diagnostics\nWe will now check for incomplete information, complete separation, and (multi-)collinearity.\nIncomplete information means that the data does not contain all combinations of the predictor or the dependent variable. This is important because if the data does not contain cases of all combinations, it will be unable to provide accurate results.\nComplete separation is a related phenomenon where a model will assume that it has found a perfect predictor (because one level of a predictor correlates purely with hits or with fails but not both hits and fails of the dependent variable). In such cases the model overestimates the effect of that predictor and the results of that model are no longer reliable. For example, if eh was only used by young speakers in the data, the model would jump on that fact and say:\n\nHa! If there is an old speaker, that means that that speaker will never ever and under no circumstances say eh - I can therefore ignore all other factors!\n\nMulticollinearity means that predictors in a model can be predicted by other predictors in the model (this means that they share variance with other predictors). If this is the case, the model results are unreliable because the presence of absence of one predictor has substantive effects on at least one other predictor.\nTo check whether the final minimal model contains predictors that correlate with each other, we extract variance inflation factors (VIF). If a model contains predictors that have variance inflation factors (VIF) > 10 the model is unreliable [15]. [6] shows that a VIF of 10 means that that predictor is explainable (predictable) from the other predictors in the model with an R2 of .9 (a VIF of 5 means that predictor is explainable (predictable) from the other predictors in the model with an R2 of .8).Indeed, predictors with VIF values greater than 4 are usually already problematic but, for large data sets, even VIFs greater than 2 can lead inflated standard errors (Jaeger 2013). Also, VIFs of 2.5 can be problematic [14] and [16] proposes that variables with VIFs exceeding 3 should be removed.\n\n\n\nNOTEHowever, (multi-)collinearity is only an issue if one is interested in interpreting regression results! If the interpretation is irrelevant because what is relevant is prediction(!), then it does not matter if the model contains collinear predictors! See [6] or the excursion below for a more elaborate explanation.\n\n\n\n\n\n\n\n\n\n\nEXCURSION\n\n\n\n\n\n\n`\nWhat is multicollinearity?\n\n\nAnswer\n\n\nDuring the workshop on mixed-effects modeling, we talked about (multi-)collinearity and someone asked if collinearity reflected shared variance (what I thought) or predictability of variables (what the other person thought). Both answers are correct! We will see below why…\n\n\n\n\n(Multi-)collinearity reflects the predictability of predictors based on the values of other predictors!\n\n\n\n\nTo test this, I generate a data set with 4 independent variables a, b, c, and d as well as two potential response variables r1 (which is random) and r2 (where the first 50 data points are the same as in r1 but for the second 50 data points I have added a value of 50 to the data points 51 to 100 from r1). This means that the predictors a and d should both strongly correlate with r2.\n::: {.cell}\n  # load packages\n  library(dplyr)\n  library(rms)\n  # create data set\n  # responses\n  # 100 random numbers\n  r1 <- rnorm(100, 50, 10)\n  # 50 smaller + 50 larger numbers\n  r2 <- c(r1[1:50], r1[51:100] + 50)\n  # predictors\n  a <- c(rep(\"1\", 50), rep (\"0\", 50))\n  b <- rep(c(rep(\"1\", 25), rep (\"0\", 25)), 2)\n  c <- rep(c(rep(\"1\", 10), rep(\"0\", 10)), 5)\n  d <- c(rep(\"1\", 47), rep (\"0\", 3), rep (\"0\", 47), rep (\"1\", 3))\n  # create data set\n  df <- data.frame(r1, r2, a, b, c, d)\n:::\n::: {.cell} ::: {.cell-output-display}\n\n  \n\nFirst 10 rows of the mlrdata.\n\n\n  r1r2abcd55.380512013355.3805120133111150.516993394250.5169933942111167.513573493267.5135734932111160.173397372860.1733973728111172.367816127872.3678161278111152.430900278852.4309002788111155.690423627755.6904236277111144.236648488244.2366484882111155.598733581655.5987335816111158.312076173758.31207617371111\n::: :::\nHere are the visualizations of r1 and r2\n::: {.cell} ::: {.cell-output-display}  :::\n::: {.cell-output-display}  ::: :::\nFit first model\nNow, I fit a first model. As the response is random, we do not expect any of the predictors to have a significant effect and we expect the R2 to be rather low.\n::: {.cell}\n  m1 <- lm(r1 ~ a + b + c + d, data = df)\n  # inspect model\n  summary(m1)\n::: {.cell-output .cell-output-stdout} ```\nCall: lm(formula = r1 ~ a + b + c + d, data = df)\nResiduals: Min 1Q Median 3Q Max -25.076373713 -7.774694324 0.933388851 6.630369791 22.021652947\nCoefficients: Estimate Std. Error t value Pr(>|t|)\n(Intercept) 46.077236782 2.059313248 22.37505 < 0.0000000000000002 ** a1 5.957479586 4.593681927 1.29689 0.197812\nb1 5.148440925 2.098541787 2.45334 0.015977 \nc1 -0.213502165 2.188893729 -0.09754 0.922504\nd1 -5.232737413 4.515342995 -1.15888 0.249410\n— Signif. codes: 0 ‘’ 0.001 ’’ 0.01 ’’ 0.05 ‘.’ 0.1 ’ ’ 1\nResidual standard error: 10.4927089 on 95 degrees of freedom Multiple R-squared: 0.075627922, Adjusted R-squared: 0.0367069924 F-statistic: 1.94311705 on 4 and 95 DF, p-value: 0.109586389 ``` ::: :::\nWe now check for (multi-)collinearity using the vif function from the rms package [23]. Variables a and d should have high variance inflation factor values (vif-values) because they overlap very much!\n::: {.cell}\n  # extract vifs\n  rms::vif(m1)\n::: {.cell-output .cell-output-stdout} a1            b1            c1            d1    4.79166666667 1.00000000000 1.08796296296 4.62962962963 ::: :::\nVariables a and d do indeed have high vif-values.\nFit second model\nWe now fit a second model to the response which has higher values for the latter part of the response. Both a and d strongly correlate with the response. But because a and d are collinear, d should not be reported as being significant by the model. The R2 of the model should be rather high (given the correlation between the response r2 and a and d).\n::: {.cell}\n  m2 <- lm(r2 ~ a + b + c + d, data = df)\n  # inspect model\n  summary(m2)\n::: {.cell-output .cell-output-stdout} ```\nCall: lm(formula = r2 ~ a + b + c + d, data = df)\nResiduals: Min 1Q Median 3Q Max -25.076373713 -7.774694324 0.933388851 6.630369791 22.021652947\nCoefficients: Estimate Std. Error t value Pr(>|t|)\n(Intercept) 96.077236782 2.059313248 46.65499 < 0.000000000000000222  a1 -44.042520414 4.593681927 -9.58763 0.0000000000000012574  b1 5.148440925 2.098541787 2.45334 0.015977 *\nc1 -0.213502165 2.188893729 -0.09754 0.922504\nd1 -5.232737413 4.515342995 -1.15888 0.249410\n— Signif. codes: 0 ‘’ 0.001 ’’ 0.01 ’’ 0.05 ‘.’ 0.1 ’ ’ 1\nResidual standard error: 10.4927089 on 95 degrees of freedom Multiple R-squared: 0.851726565, Adjusted R-squared: 0.845483473 F-statistic: 136.427041 on 4 and 95 DF, p-value: < 0.0000000000000002220446 ``` ::: :::\nAgain, we extract the vif-values.\n::: {.cell}\n  # extract vifs\n  rms::vif(m2)\n::: {.cell-output .cell-output-stdout} a1            b1            c1            d1    4.79166666667 1.00000000000 1.08796296296 4.62962962963 ::: :::\nThe vif-values are identical which shows that what matters is if the variables are predictable. To understand how we arrive at vif-values, we inspect the model matrix.\n::: {.cell}\n  # inspect model matrix\n  mm <- model.matrix(m2)\n:::\n::: {.cell} ::: {.cell-output-display}\n\n  \n\nFirst 10 rows of the mlrdata.\n\n\n  (Intercept)a1b1c1d1111111111111111111111111111111111111111111111111111110111101111011110111101\n::: :::\nWe now fit a linear model in which we predict d from the other predictors in the model matrix.\n::: {.cell}\n  mt <- lm(mm[,5] ~ mm[,1:4])\n  summary(mt)$r.squared\n::: {.cell-output .cell-output-stdout} [1] 0.784 ::: :::\nThe R2 shows that the values of d are explained to 78.4 percent by the values of the other predictors in the model.\nNow, we can write a function [taken from 6] that converts this R2 value\n::: {.cell}\n  R2.to.VIF <- function(some.modelmatrix.r2) {\n  return(1/(1-some.modelmatrix.r2)) } \n  R2.to.VIF(0.784)\n::: {.cell-output .cell-output-stdout} [1] 4.62962962963 ::: :::\nThe function outputs the vif-value of d. This shows that the vif-value of d represents its predictability from the other predictors in the model matrix which represents the amount of shared variance between d and the other predictors in the model.\n\n\n`\n\n\nWe start by We will now check for incomplete information and complete separation.\n\n# check incomplete information and complete separation\nifelse(min(ftable(blrdata$Age, blrdata$Gender, blrdata$EH)) == 0, \"not possible\", \"possible\")\n\n[1] \"possible\"\n\n\nAs the table does not contain any 0s, we can rule out incomplete information and complete separation. We continue to check for (multi-)collinearity.\n\nifelse(max(vif(lr.glm)) <= 3,  \"vifs ok\", \"WARNING: high vifs!\") # VIFs ok\n\n[1] \"vifs ok\"\n\n\nIn addition, predictors with 1/VIF values \\(<\\) .1 must be removed (data points with values above .2 are considered problematic) [24] and the mean value of VIFs should be \\(~\\) 1 [17].\n\nmean(vif(lr.glm))\n\n[1] 1.00481494539\n\n\nAs the variance inflation factors are below 3, we do not need to worry about (multi-)collinearity.\n\n\nModel Evaluation\nNow, that we have confirmed that our model is not suffering from incomplete information, complete separation, and (multi-)collinearity we can check if the final minimal model significantly improves model fit compared to the null model. For this, we calculate a Model Likelihood Ratio Test.\n\n# check if adding Age significantly improves model fit\nanova(lr.glm, m0.glm, test = \"Chi\")\n\nAnalysis of Deviance Table\n\nModel 1: EH ~ Age + Gender\nModel 2: EH ~ 1\n  Resid. Df  Resid. Dev Df     Deviance               Pr(>Chi)    \n1     25818 32139.54089                                           \n2     25820 33007.75469 -2 -868.2138011 < 0.000000000000000222 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nAnova(lr.glm, test = \"LR\")\n\nAnalysis of Deviance Table (Type II tests)\n\nResponse: EH\n          LR Chisq Df             Pr(>Chisq)    \nAge    668.6350712  1 < 0.000000000000000222 ***\nGender 237.3199140  1 < 0.000000000000000222 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe final model significantly improves model fit which means that we have now arrived at the final minimal adequate model. To elaborate, the code above provides three values: a \\(\\chi\\)2, the degrees of freedom, and a p-value. The p-value is lower than .05 and the results of the Model Likelihood Ratio Test therefore confirm that the final minimal adequate model performs significantly better than the initial minimal base-line model.\nIn a next step, we calculate pseudo-R2 values which represent the amount of residual variance that is explained by the final minimal adequate model. We cannot use the ordinary R2 because the model works on the logged probabilities rather than the values of the dependent variable.\n\n# calculate pseudo R^2\n# number of cases\nmodelChi <- lr.glm$null.deviance - lr.glm$deviance\nncases <- length(fitted(lr.glm))\nR2.hl <- modelChi/lr.glm$null.deviance\nR.cs <- 1 - exp ((lr.glm$deviance - lr.glm$null.deviance)/ncases)\nR.n <- R.cs /( 1- ( exp (-(lr.glm$null.deviance/ ncases))))\n# function for extracting pseudo-R^2\nlogisticPseudoR2s <- function(LogModel) {\n  dev <- LogModel$deviance\n    nullDev <- LogModel$null.deviance\n    modelN <-  length(LogModel$fitted.values)\n    R.l <-  1 -  dev / nullDev\n    R.cs <- 1- exp ( -(nullDev - dev) / modelN)\n    R.n <- R.cs / ( 1 - ( exp (-(nullDev / modelN))))\n    cat(\"Pseudo R^2 for logistic regression\\n\")\n    cat(\"Hosmer and Lemeshow R^2  \", round(R.l, 3), \"\\n\")\n    cat(\"Cox and Snell R^2        \", round(R.cs, 3), \"\\n\")\n    cat(\"Nagelkerke R^2           \", round(R.n, 3),    \"\\n\") }\nlogisticPseudoR2s(lr.glm)\n\nPseudo R^2 for logistic regression\nHosmer and Lemeshow R^2   0.026 \nCox and Snell R^2         0.033 \nNagelkerke R^2            0.046 \n\n\nThe low pseudo-R2 values show that our model has very low explanatory power. For instance, the value of Hosmer and Lemeshow R2 (0.026) “is the proportional reduction in the absolute value of the log-likelihood measure and as such it is a measure of how much the badness of fit improves as a result of the inclusion of the predictor variables” [5]. In essence, all the pseudo-R2 values are measures of how substantive the model is (how much better it is compared to a baseline model). Next, we extract the confidence intervals for the coefficients of the model.\n\n# extract the confidence intervals for the coefficients\nconfint(lr.glm)\n\n                      2.5 %          97.5 %\n(Intercept) -0.276050866670 -0.188778707810\nAgeOld      -0.896486392279 -0.765095825382\nGenderWomen -0.473530977637 -0.366703827307\n\n\nDespite having low explanatory and predictive power, the age of speakers and their gender are significant as the confidence intervals of the coefficients do not overlap with 0.\n\n\nEffect Size\nIn a next step, we compute odds ratios and their confidence intervals. Odds Ratios represent a common measure of effect size and can be used to compare effect sizes across models. Odds ratios rang between 0 and infinity. Values of 1 indicate that there is no effect. The further away the values are from 1, the stronger the effect. If the values are lower than 1, then the variable level correlates negatively with the occurrence of the outcome (the probability decreases) while values above 1 indicate a positive correlation and show that the variable level causes an increase in the probability of the outcome (the occurrence of EH).\n\nexp(lr.glm$coefficients) # odds ratios\n\n   (Intercept)         AgeOld    GenderWomen \n0.792642499264 0.435815384592 0.656972294902 \n\nexp(confint(lr.glm))     # confidence intervals of the odds ratios\n\nWaiting for profiling to be done...\n\n\n                     2.5 %         97.5 %\n(Intercept) 0.758774333456 0.827969709653\nAgeOld      0.408000698619 0.465289342309\nGenderWomen 0.622799290871 0.693014866732\n\n\nThe odds ratios confirm that older speakers use eh significantly less often compared with younger speakers and that women use eh less frequently than men as the confidence intervals of the odds rations do not overlap with 1. In a next step, we calculate the prediction accuracy of the model.\n\n\nPrediction Accuracy\nIn order to calculate the prediction accuracy of the model, we generate a variable called Prediction that contains the predictions of pour model and which we add to the data. Then, we use the confusionMatrix function from the caret package [25] to extract the prediction accuracy.\n\n# create variable with contains the prediction of the model\nblrdata <- blrdata %>%\n  dplyr::mutate(Prediction = predict(lr.glm, type = \"response\"),\n                Prediction = ifelse(Prediction > .5, 1, 0),\n                Prediction = factor(Prediction, levels = c(\"0\", \"1\")),\n                EH = factor(EH, levels = c(\"0\", \"1\")))\n# create a confusion matrix with compares observed against predicted values\ncaret::confusionMatrix(blrdata$Prediction, blrdata$EH)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction     0     1\n         0 17114  8707\n         1     0     0\n                                                    \n               Accuracy : 0.66279385                \n                 95% CI : (0.656990096, 0.668560948)\n    No Information Rate : 0.66279385                \n    P-Value [Acc > NIR] : 0.5029107                 \n                                                    \n                  Kappa : 0                         \n                                                    \n Mcnemar's Test P-Value : < 0.00000000000000022     \n                                                    \n            Sensitivity : 1.00000000                \n            Specificity : 0.00000000                \n         Pos Pred Value : 0.66279385                \n         Neg Pred Value :        NaN                \n             Prevalence : 0.66279385                \n         Detection Rate : 0.66279385                \n   Detection Prevalence : 1.00000000                \n      Balanced Accuracy : 0.50000000                \n                                                    \n       'Positive' Class : 0                         \n                                                    \n\n\nWe can see that out model has never predicted the use of eh which is common when dealing with rare phenomena. This is expected as the event s so rare that the probability of it not occurring substantively outweighs the probability of it occurring. As such, the prediction accuracy of our model is not significantly better compared to the prediction accuracy of the baseline model which is the no-information rate (NIR)) (p = 0.5029).\nWe can use the plot_model function from the sjPlot package [26] to visualize the effects.\n\n# predicted probability\nefp1 <- plot_model(lr.glm, type = \"pred\", terms = c(\"Age\"), axis.lim = c(0, 1)) \n# predicted percentage\nefp2 <- plot_model(lr.glm, type = \"pred\", terms = c(\"Gender\"), axis.lim = c(0, 1)) \ngrid.arrange(efp1, efp2, nrow = 1)\n\n\n\n\nAnd we can also combine the visualization of the effects in a single plot as shown below.\n\nsjPlot::plot_model(lr.glm, type = \"pred\", terms = c(\"Age\", \"Gender\"), axis.lim = c(0, 1)) +\n  theme(legend.position = \"top\") +\n  labs(x = \"\", y = \"Predicted Probabilty of eh\", title = \"\") +\n  scale_color_manual(values = c(\"gray20\", \"gray70\"))\n\n\n\n\n\n\nOutlier detection\nIn order to detect potential outliers, we will calculate diagnostic parameters and add these to our data set.\n\ninfl <- influence.measures(lr.glm) # calculate influence statistics\nblrdata <- data.frame(blrdata, infl[[1]], infl[[2]]) # add influence statistics\n\nIn a next step, we use these diagnostic parameters to check if there are data points which should be removed as they unduly affect the model fit.\n\n\nSample Size\nWe now check whether the sample size is sufficient for our analysis [12].\n\nif you are interested in the overall model: 50 + 8k (k = number of predictors)\nif you are interested in individual predictors: 104 + k\nif you are interested in both: take the higher value!\n\n\n# function to evaluate sample size\nsmplesz <- function(x) {\n  ifelse((length(x$fitted) < (104 + ncol(summary(x)$coefficients)-1)) == TRUE,\n    return(\n      paste(\"Sample too small: please increase your sample by \",\n      104 + ncol(summary(x)$coefficients)-1 - length(x$fitted),\n      \" data points\", collapse = \"\")),\n    return(\"Sample size sufficient\")) }\n# apply unction to model\nsmplesz(lr.glm)\n\n[1] \"Sample size sufficient\"\n\n\nAccording to rule of thumb provided in [12], the sample size is sufficient for our analysis.\n\n\nSummarizing Results\nAs a final step, we summarize our findings in tabulated form.\n\nsjPlot::tab_model(lr.glm)\n\n\n\n\n \nEH\n\n\nPredictors\nOdds Ratios\nCI\np\n\n\n(Intercept)\n0.79\n0.76 – 0.83\n<0.001\n\n\nAge [Old]\n0.44\n0.41 – 0.47\n<0.001\n\n\nGender [Women]\n0.66\n0.62 – 0.69\n<0.001\n\n\nObservations\n25821\n\n\nR2 Tjur\n0.032\n\n\n\n\n\n\n\nA more detailed summary table can be retrieved as follows:\n\n# load function\nsource(\"https://slcladal.github.io/rscripts/blrsummary.r\")\n# calculate accuracy \npredict.acc <- caret::confusionMatrix(blrdata$Prediction, blrdata$EH)\npredict.acc <- predict.acc[3]$overall[[1]]\n# create summary table\nblrsummarytb <- blrsummary(lr.glm, lr.lrm, predict.acc) \n\n\n\n\n\n\nResults of the binomial logistic regression analysis.\n\n\nStatisticsEstimateVIFOddsRatioCI(2.5%)CI(97.5%)Std. Errorz valuePr(>|z|)Significance(Intercept)-0.230.790.760.830.02-10.440p < .001***AgeOld-0.8310.440.410.470.03-24.780p < .001***GenderWomen-0.4210.660.620.690.03-15.420p < .001***Model statisticsValueNumber of cases in model25821Observed misses0 :17114Observed successes1 :8707Null deviance33007.75Residual deviance32139.54R2 (Nagelkerke)0.046R2 (Hosmer & Lemeshow)0.026R2 (Cox & Snell)0.033C0.602Somers' Dxy0.203AIC32145.54Prediction accuracy0.66%Model Likelihood Ratio TestModel L.R.: 868.21df: 2p-value: 0sig: p < .001***\n\n\nR2 (Hosmer & Lemeshow)\nHosmer and Lemeshow’s R2 “is the proportional reduction in the absolute value of the log-likelihood measure and as such it is a measure of how much the badness of fit improves as a result of the inclusion of the predictor variables. It can vary between 0 (indicating that the predictors are useless at predicting the outcome variable) and 1 (indicating that the model predicts the outcome variable perfectly)” [5].\nR2 (Cox & Snell)\n“Cox and Snell’s R2 (1989) is based on the deviance of the model (-2LL(new») and the deviance of the baseline model (-2LL(baseline), and the sample size, n […]. However, this statistic never reaches its theoretical maximum of 1.\nR2 (Nagelkerke)\nSince R2 (Cox & Snell) never reaches its theoretical maximum of 1, Nagelkerke (1991) suggested Nagelkerke’s R2 [5].\nSomers’ Dxy\nSomers’ Dxy is a rank correlation between predicted probabilities and observed responses ranges between 0 (randomness) and 1 (perfect prediction). Somers’ Dxy should have a value higher than .5 for the model to be meaningful [10].\nC\nC is an index of concordance between the predicted probability and the observed response. When C takes the value 0.5, the predictions are random, when it is 1, prediction is perfect. A value above 0.8 indicates that the model may have some real predictive capacity [10].\nAkaike information criteria (AIC)\nAkaike information criteria (AlC = -2LL + 2k) provide a value that reflects a ratio between the number of predictors in the model and the variance that is explained by these predictors. Changes in AIC can serve as a measure of whether the inclusion of a variable leads to a significant increase in the amount of variance that is explained by the model. “You can think of this as the price you pay for something: you get a better value of R2, but you pay a higher price, and was that higher price worth it? These information criteria help you to decide. The BIC is the same as the AIC but adjusts the penalty included in the AlC (i.e., 2k) by the number of cases: BlC = -2LL + 2k x log(n) in which n is the number of cases in the model” [5].\nWe can use the reports package [18] to summarize the analysis.\n\nreport::report(lr.glm)\n\nWe fitted a logistic model (estimated using ML) to predict EH with Age and Gender (formula: EH ~ Age + Gender). The model's explanatory power is weak (Tjur's R2 = 0.03). The model's intercept, corresponding to Age = Young and Gender = Men, is at -0.23 (95% CI [-0.28, -0.19], p < .001). Within this model:\n\n  - The effect of Age [Old] is statistically significant and negative (beta = -0.83, 95% CI [-0.90, -0.77], p < .001; Std. beta = -0.83, 95% CI [-0.90, -0.77])\n  - The effect of Gender [Women] is statistically significant and negative (beta = -0.42, 95% CI [-0.47, -0.37], p < .001; Std. beta = -0.42, 95% CI [-0.47, -0.37])\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using \n\n\nWe can use this output to write up a final report:\nWe fitted a logistic model (estimated using ML) to predict the use of the utterance-final discourse particle eh with Age and Gender (formula: EH ~ Age + Gender). The model’s explanatory power is weak (Tjur’s R2 = 0.03). The model’s intercept, corresponding to Age = Young and Gender = Men, is at -0.23 (95% CI [-0.28, -0.19], p < .001). Within this model:\n\nThe effect of Age [Old] is statistically significant and negative (beta = -0.83, 95% CI [-0.90, -0.77], p < .001; Std. beta = -0.83, 95% CI [-0.90, -0.77])\nThe effect of Gender [Women] is statistically significant and negative (beta = -0.42, 95% CI [-0.47, -0.37], p < .001; Std. beta = -0.42, 95% CI [-0.47, -0.37])\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using"
  },
  {
    "objectID": "fixreg.html#ordinal-regression",
    "href": "fixreg.html#ordinal-regression",
    "title": "Fixed-Effects Regression Modelling in R",
    "section": "Ordinal Regression",
    "text": "Ordinal Regression\nOrdinal regression is very similar to multiple linear regression but takes an ordinal dependent variable [27]. For this reason, ordinal regression is one of the key methods in analysing Likert data.\nTo see how an ordinal regression is implemented in R, we load and inspect the ´ordinaldata´ data set. The data set consists of 400 observations of students that were either educated at this school (Internal = 1) or not (Internal = 0). Some of the students have been abroad (Exchange = 1) while other have not (Exchange = 0). In addition, the data set contains the students’ final score of a language test (FinalScore) and the dependent variable which the recommendation of a committee for an additional, very prestigious program. The recommendation has three levels (very likely, somewhat likely, and unlikely) and reflects the committees’ assessment of whether the student is likely to succeed in the program.\n\n# load data\nordata  <- base::readRDS(url(\"https://slcladal.github.io/data/ord.rda\", \"rb\")) %>%\n  dplyr::rename(Recommend = 1, \n              Internal = 2, \n              Exchange = 3, \n              FinalScore = 4) %>%\n  dplyr::mutate(FinalScore = round(FinalScore, 2))\n\n\n\n\n\n\nFirst 15 rows of the ordata.\n\n\nRecommendInternalExchangeFinalScorevery likely003.26somewhat likely103.21unlikely113.94somewhat likely002.81somewhat likely002.53unlikely012.59somewhat likely002.56somewhat likely002.73unlikely003.00somewhat likely103.50unlikely113.65somewhat likely002.84very likely013.90somewhat likely002.68unlikely103.57\n\n\nIn a first step, we need to relevel the ordinal variable to represent an ordinal factor (or a progression from “unlikely” over “somewhat likely” to “very likely”. And we will also factorize Internal and Exchange to make it easier to interpret the output later on.\n\n# relevel data\nordata <- ordata %>%\n  dplyr::mutate(Recommend = factor(Recommend, \n                           levels=c(\"unlikely\", \"somewhat likely\", \"very likely\"),\n                           labels=c(\"unlikely\",  \"somewhat likely\",  \"very likely\"))) %>%\n  dplyr::mutate(Exchange = ifelse(Exchange == 1, \"Exchange\", \"NoExchange\")) %>%\n  dplyr::mutate(Internal = ifelse(Internal == 1, \"Internal\", \"External\"))\n\nNow that the dependent variable is releveled, we check the distribution of the variable levels by tabulating the data. To get a better understanding of the data we create frequency tables across variables rather than viewing the variables in isolation.\n\n## three way cross tabs (xtabs) and flatten the table\nftable(xtabs(~ Exchange + Recommend + Internal, data = ordata))\n\n                           Internal External Internal\nExchange   Recommend                                 \nExchange   unlikely                       25        6\n           somewhat likely                12        4\n           very likely                     7        3\nNoExchange unlikely                      175       14\n           somewhat likely                98       26\n           very likely                    20       10\n\n\nWe also check the mean and standard deviation of the final score as final score is a numeric variable and cannot be tabulated (unless we convert it to a factor).\n\nsummary(ordata$FinalScore); sd(ordata$FinalScore)\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n1.900000 2.720000 2.990000 2.998925 3.270000 4.000000 \n\n\n[1] 0.397940933861\n\n\nThe lowest score is 1.9 and the highest score is a 4.0 with a mean of approximately 3. Finally, we inspect the distributions graphically.\n\n# visualize data\nggplot(ordata, aes(x = Recommend, y = FinalScore)) +\n  geom_boxplot(size = .75) +\n  geom_jitter(alpha = .5) +\n  facet_grid(Exchange ~ Internal, margins = TRUE) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1))\n\n\n\n\nWe see that we have only few students that have taken part in an exchange program and there are also only few internal students overall. With respect to recommendations, only few students are considered to very likely succeed in the program. We can now start with the modeling by using the polr function. To make things easier for us, we will only consider the main effects here as this tutorial only aims to how to implement an ordinal regression but not how it should be done in a proper study - then, the model fitting and diagnostic procedures would have to be performed accurately, of course.\n\n# fit ordered logit model and store results 'm'\nm <- polr(Recommend ~ Internal + Exchange + FinalScore, data = ordata, Hess=TRUE)\n# summarize model\nsummary(m)\n\nCall:\npolr(formula = Recommend ~ Internal + Exchange + FinalScore, \n    data = ordata, Hess = TRUE)\n\nCoefficients:\n                          Value  Std. Error     t value\nInternalInternal   1.0476639460 0.265789134 3.941710973\nExchangeNoExchange 0.0586810767 0.297858822 0.197009698\nFinalScore         0.6157435926 0.260631275 2.362508462\n\nIntercepts:\n                            Value       Std. Error  t value    \nunlikely|somewhat likely    2.261997623 0.882173604 2.564118460\nsomewhat likely|very likely 4.357441880 0.904467838 4.817685824\n\nResidual Deviance: 717.024871356 \nAIC: 727.024871356 \n\n\nThe results show that having studied here at this school increases the chances of receiving a positive recommendation but that having been on an exchange has a negative but insignificant effect on the recommendation. The final score also correlates positively with a positive recommendation but not as much as having studied here.\n\n## store table\n(ctable <- coef(summary(m)))\n\n                                      Value     Std. Error        t value\nInternalInternal            1.0476639460469 0.265789134041 3.941710972596\nExchangeNoExchange          0.0586810766831 0.297858821982 0.197009698396\nFinalScore                  0.6157435925546 0.260631274948 2.362508462104\nunlikely|somewhat likely    2.2619976233665 0.882173604268 2.564118459704\nsomewhat likely|very likely 4.3574418799106 0.904467837679 4.817685824069\n\n\nAs the regression report does not provide p-values, we have to calculate them separately (after having calculated them, we add them to the coefficient table).\n\n## calculate and store p values\np <- pnorm(abs(ctable[, \"t value\"]), lower.tail = FALSE) * 2\n## combined table\n(ctable <- cbind(ctable, \"p value\" = p))\n\n                                      Value     Std. Error        t value\nInternalInternal            1.0476639460469 0.265789134041 3.941710972596\nExchangeNoExchange          0.0586810766831 0.297858821982 0.197009698396\nFinalScore                  0.6157435925546 0.260631274948 2.362508462104\nunlikely|somewhat likely    2.2619976233665 0.882173604268 2.564118459704\nsomewhat likely|very likely 4.3574418799106 0.904467837679 4.817685824069\n                                        p value\nInternalInternal            0.00008090242989074\nExchangeNoExchange          0.84381994829785212\nFinalScore                  0.01815172703306605\nunlikely|somewhat likely    0.01034382345525988\nsomewhat likely|very likely 0.00000145232812832\n\n\nAs predicted, Exchange does not have a significant effect but FinalScore and Internal both correlate significantly with the likelihood of receiving a positive recommendation.\n\n# extract profiled confidence intervals\nci <- confint(m)\n# calculate odds ratios and combine them with profiled CIs\nexp(cbind(OR = coef(m), ci))\n\n                              OR          2.5 %        97.5 %\nInternalInternal   2.85098328212 1.695837799597 4.81711408266\nExchangeNoExchange 1.06043698872 0.595033205649 1.91977108408\nFinalScore         1.85103250193 1.113625249822 3.09849059342\n\n\nThe odds ratios show that internal students are 2.85 times or 285 percent as likely as non-internal students to receive positive evaluations and that a 1-point increase in the test score lead to a 1.85 times or 185 percent increase in the chances of receiving a positive recommendation. The effect of an exchange is slightly negative but, as we have seen above, not significant."
  },
  {
    "objectID": "fixreg.html#poisson-regression",
    "href": "fixreg.html#poisson-regression",
    "title": "Fixed-Effects Regression Modelling in R",
    "section": "Poisson Regression",
    "text": "Poisson Regression\nThis section is based on this tutorials on how to perform a Poisson regression in R.\n\n\n\nPoisson regressions are used to analyze data where the dependent variable represents counts.\n\n\n\n\n\n\nThis applied particularly to counts that are based on observations of something that is measured in set intervals. For instances the number of pauses in two-minute-long conversations. Poisson regressions are particularly appealing when dealing with rare events, i.e. when something only occurs very infrequently. In such cases, normal linear regressions do not work because the instances that do occur are automatically considered outliers. Therefore, it is useful to check if the data conform to a Poisson distribution.\nHowever, the tricky thing about Poisson regressions is that the data has to conform to the Poisson distribution which is, according to my experience, rarely the case, unfortunately. The Gaussian Normal Distribution is very flexible because it is defined by two parameters, the mean (mu, i.e. \\(\\mu\\)) and the standard deviation (sigma, i.e. \\(\\sigma\\)). This allows the normal distribution to take very different shapes (for example, very high and slim (compressed) or very wide and flat). In contrast, the Poisson is defined by only one parameter (lambda, i.e. \\(\\lambda\\)) which mean that if we have a mean of 2, then the standard deviation is also 2 (actually we would have to say that the mean is \\(\\lambda\\) and the standard deviation is also \\(\\lambda\\) or \\(\\lambda\\) = \\(\\mu\\) = \\(\\sigma\\)). This is much trickier for natural data as this means that the Poisson distribution is very rigid.\n\n\n\n\n\nAs we can see, as \\(\\lambda\\) takes on higher values, the distribution becomes wider and flatter - a compressed distribution with a high mean can therefore not be Poisson-distributed. We will now start by loading the data.\n\n# load data\npoissondata  <- base::readRDS(url(\"https://slcladal.github.io/data/prd.rda\", \"rb\"))\n\n\n\n\n\n\nFirst 15 rows of the poissondata.\n\n\nIdPausesLanguageAlcohol450German411080Russian41150German44670German421530German40510Russian421640German461330German4020German33530German4610German401280English38160German441060German37890German40\n\n\nWe will clean the data by factorizing Id which is currently considered a numeric variable rather than a factor.\n\n# process data\npoissondata <- poissondata %>%\n  mutate(Id = factor(Id, levels = 1:200, labels = 1:200))\n# inspect data\nstr(poissondata)\n\n'data.frame':   200 obs. of  4 variables:\n $ Id      : Factor w/ 200 levels \"1\",\"2\",\"3\",\"4\",..: 45 108 15 67 153 51 164 133 2 53 ...\n $ Pauses  : int  0 0 0 0 0 0 0 0 0 0 ...\n $ Language: chr  \"German\" \"Russian\" \"German\" \"German\" ...\n $ Alcohol : int  41 41 44 42 40 42 46 40 33 46 ...\n\n\nFirst, we check if the conditions for a Poisson regression are met.\n\n# output the results\ngf = vcd::goodfit(poissondata$Pauses, \n                  type= \"poisson\", \n                  method= \"ML\")\n# inspect results\nsummary(gf)\n\n\n     Goodness-of-fit test for poisson distribution\n\n                           X^2 df            P(> X^2)\nLikelihood Ratio 33.0122916717  5 0.00000374234139957\n\n\nIf the p-values is smaller than .05, then data is not Poisson distributed which means that it differs significantly from a Poisson distribution and is very likely over-dispersed. We will check the divergence from a Poisson distribution visually by plotting the observed counts against the expected counts if the data were Poisson distributed.\n\nplot(gf,main=\"Count data vs Poisson distribution\")\n\n\n\n\nAlthough the goodfit function reported that the data differs significantly from the Poisson distribution, the fit is rather good. We can use an additional Levene’s test to check if variance homogeneity is given.\n\n\n\nNOTEThe use of Levene’s test to check if the model is heteroscedastic is generally not recommended as it is too lax when dealing with few observations (because in such cases it does not have the power to identify heteroscedasticity) while it is too harsh when dealing with many observations (when heteroscedasticity typically is not a severe problem).\n\n\n\n\n\n\n\n# check homogeneity\nleveneTest(poissondata$Pauses, poissondata$Language, center = mean)\n\nLevene's Test for Homogeneity of Variance (center = mean)\n       Df  F value        Pr(>F)    \ngroup   2 17.15274 0.00000013571 ***\n      197                           \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe Levene’s test indicates that variance homogeneity is also violated. Since both the approximation to a Poisson distribution and variance homogeneity are violated, we should switch either to a quasi-Poisson model or a negative binomial model. However, as we are only interested in how to implement a Poisson model here, we continue despite the fact that this could not be recommended if we were actually interested in accurate results based on a reliable model.\nIn a next step, we summarize Progression by inspecting the means and standard deviations of the individual variable levels.\n\n# extract mean and standard deviation\nwith(poissondata, tapply(Pauses, Language, function(x) {\n  sprintf(\"M (SD) = %1.2f (%1.2f)\", mean(x), sd(x))\n}))\n\n               English                 German                Russian \n\"M (SD) = 1.00 (1.28)\" \"M (SD) = 0.24 (0.52)\" \"M (SD) = 0.20 (0.40)\" \n\n\nNow, we visualize the data.\n\n# plot data\nggplot(poissondata, aes(Pauses, fill = Language)) +\n  geom_histogram(binwidth=.5, position=\"dodge\") +\n  scale_fill_manual(values=c(\"gray30\", \"gray50\", \"gray70\"))\n\n\n\n\n\n# calculate Poisson regression\nm1.poisson <- glm(Pauses ~ Language + Alcohol, family=\"poisson\", data=poissondata)\n# inspect model\nsummary(m1.poisson)\n\n\nCall:\nglm(formula = Pauses ~ Language + Alcohol, family = \"poisson\", \n    data = poissondata)\n\nDeviance Residuals: \n         Min            1Q        Median            3Q           Max  \n-2.204338080  -0.843641817  -0.510586515   0.255772098   2.679576560  \n\nCoefficients:\n                     Estimate    Std. Error  z value         Pr(>|z|)    \n(Intercept)     -4.1632652529  0.6628774832 -6.28060 0.00000000033728 ***\nLanguageGerman  -0.7140499158  0.3200148750 -2.23130         0.025661 *  \nLanguageRussian -1.0838591456  0.3582529824 -3.02540         0.002483 ** \nAlcohol          0.0701523975  0.0105992050  6.61865 0.00000000003625 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 287.6722345  on 199  degrees of freedom\nResidual deviance: 189.4496199  on 196  degrees of freedom\nAIC: 373.5045031\n\nNumber of Fisher Scoring iterations: 6\n\n\nIn addition to the Estimates for the coefficients, we could also calculate the confidence intervals for the coefficients (LL stands for lower limit and UL for upper limit in the table below).\n\n# calculate model\ncov.m1 <- sandwich::vcovHC(m1.poisson, type=\"HC0\")\n# extract standard error\nstd.err <- sqrt(diag(cov.m1))\n# extract robust estimates\nr.est <- cbind(Estimate= coef(m1.poisson), \n               \"Robust SE\" = std.err,\n               \"Pr(>|z|)\" = 2 * pnorm(abs(coef(m1.poisson)/std.err),\n                                      lower.tail=FALSE),\nLL = coef(m1.poisson) - 1.96 * std.err,\nUL = coef(m1.poisson) + 1.96 * std.err)\n# inspect data\nr.est\n\n                        Estimate       Robust SE                 Pr(>|z|)\n(Intercept)     -4.1632652529178 0.6480942940026 0.0000000001328637743948\nLanguageGerman  -0.7140499157783 0.2986422497410 0.0168031204011183932234\nLanguageRussian -1.0838591456208 0.3210481575684 0.0007354744824167306532\nAlcohol          0.0701523974937 0.0104351647012 0.0000000000178397516955\n                              LL               UL\n(Intercept)     -5.4335300691629 -2.8930004366727\nLanguageGerman  -1.2993887252708 -0.1287111062859\nLanguageRussian -1.7131135344549 -0.4546047567867\nAlcohol          0.0496994746793  0.0906053203082\n\n\nWe can now calculate the p-value of the model.\n\nwith(m1.poisson, cbind(res.deviance = deviance, df = df.residual,\n  p = pchisq(deviance, df.residual, lower.tail=FALSE)))\n\n     res.deviance  df              p\n[1,] 189.44961991 196 0.618227445717\n\n\nNow, we check, if removing Language leads to a significant decrease in model fit.\n\n# remove Language from the model\nm2.poisson <- update(m1.poisson, . ~ . -Language)\n# check if dropping Language causes a significant decrease in model fit\nanova(m2.poisson, m1.poisson, test=\"Chisq\")\n\nAnalysis of Deviance Table\n\nModel 1: Pauses ~ Alcohol\nModel 2: Pauses ~ Language + Alcohol\n  Resid. Df  Resid. Dev Df   Deviance   Pr(>Chi)    \n1       198 204.0213018                             \n2       196 189.4496199  2 14.5716819 0.00068517 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWe now calculate robust coefficients using the msm package [28].\n\n# get estimates\ns <- msm::deltamethod(list(~ exp(x1), ~ exp(x2), ~ exp(x3), ~ exp(x4)), \n                                                coef(m1.poisson), cov.m1)\n# exponentiate old estimates dropping the p values\nrexp.est <- exp(r.est[, -3])\n# replace SEs with estimates for exponentiated coefficients\nrexp.est[, \"Robust SE\"] <- s\n# display results\nrexp.est\n\n                       Estimate       Robust SE               LL\n(Intercept)     0.0155566784084 0.0100821945101 0.00436765044896\nLanguageGerman  0.4896571063601 0.1462322998451 0.27269843575906\nLanguageRussian 0.3382875026084 0.1086065794408 0.18030353649622\nAlcohol         1.0726716412682 0.0111935052470 1.05095521026088\n                             UL\n(Intercept)     0.0554097096208\nLanguageGerman  0.8792279322820\nLanguageRussian 0.6346987787641\nAlcohol         1.0948368101199\n\n\n\n# extract predicted values\n(s1 <- data.frame(Alcohol = mean(poissondata$Alcohol),\n  Language = factor(1:3, levels = 1:3, labels = names(table(poissondata$Language)))))\n\n  Alcohol Language\n1  52.645  English\n2  52.645   German\n3  52.645  Russian\n\n# show results\npredict(m1.poisson, s1, type=\"response\", se.fit=TRUE)\n\n$fit\n             1              2              3 \n0.624944591447 0.306008560283 0.211410945109 \n\n$se.fit\n              1               2               3 \n0.0862811728183 0.0883370633684 0.0705010813453 \n\n$residual.scale\n[1] 1\n\n\n\n## calculate and store predicted values\npoissondata$Predicted <- predict(m1.poisson, type=\"response\")\n## order by program and then by math\npoissondata <- poissondata[with(poissondata, order(Language, Alcohol)), ]\n\n\n## create the plot\nggplot(poissondata, aes(x = Alcohol, y = Predicted, colour = Language)) +\n  geom_point(aes(y = Pauses), alpha=.5, \n             position=position_jitter(h=.2)) +\n  geom_line(size = 1) +\n  labs(x = \"Alcohol (ml)\", y = \"Expected number of pauses\") +\n  scale_color_manual(values=c(\"gray30\", \"gray50\", \"gray70\"))\n\n\n\n\nThat’s it for this tutorial. We hope that you have enjoyed this tutorial and learned how to perform regression analysis including model fitting and model diagnostics as well as reporting regression results."
  },
  {
    "objectID": "gutenberg.html#preparation-and-session-set-up",
    "href": "gutenberg.html#preparation-and-session-set-up",
    "title": "Downloading Texts from Project Gutenberg using R",
    "section": "Preparation and session set up",
    "text": "Preparation and session set up\nThis tutorial is based on R. If you have not installed R or are new to it, you will find an introduction to and more information how to use R here. For this tutorials, we need to install certain packages from an R library so that the scripts shown below are executed without errors. Before turning to the code below, please install the packages by running the code below this paragraph. If you have already installed the packages mentioned below, then you can skip ahead and ignore this section. To install the necessary packages, simply run the following code - it may take some time (between 1 and 5 minutes to install all of the libraries so you do not need to worry if it takes some time).\n\n# install libraries\ninstall.packages(\"tidyverse\")\ninstall.packages(\"gutenbergr\")\ninstall.packages(\"DT\")\ninstall.packages(\"flextable\")\n# install klippy for copy-to-clipboard button in code chunks\ninstall.packages(\"remotes\")\nremotes::install_github(\"rlesur/klippy\")\n\nNow that we have installed the packages, we activate them as shown below.\n\n# set options\noptions(stringsAsFactors = F)         # no automatic data transformation\noptions(\"scipen\" = 100, \"digits\" = 4) # suppress math annotation\n# activate packages\nlibrary(tidyverse)\nlibrary(gutenbergr)\nlibrary(DT)\nlibrary(flextable)\n# activate klippy for copy-to-clipboard button\nklippy::klippy()\n\n\n\n\nOnce you have installed R and RStudio and initiated the session by executing the code shown above, you are good to go."
  },
  {
    "objectID": "gviz.html",
    "href": "gviz.html",
    "title": "Introduction to Geospatial Data Visualization with R",
    "section": "",
    "text": "Introduction\nThis tutorial introduces geospatial data visualization in R.\n\n\n\n\n\nThis tutorial is aimed at beginners and intermediate users of R with the aim of showcasing how to visualize geospatial data, i.e. how to generate maps in R, and how to prepare data for geospatial visualizations using R. The aim is not to provide a fully-fledged analysis but rather to show and exemplify selected useful methods associated with generating maps. Very recommendable and detailed resources for geospatial data visualization using R can be found here, here, or here. If you are interested in cartography, here is a cheat sheet for cartography with R.\n\n\n\nThe entire R Notebook for the tutorial can be downloaded here. If you want to render the R Notebook on your machine, i.e. knitting the document to html or a pdf, you need to make sure that you have R and RStudio installed and you also need to download the bibliography file and store it in the same folder where you store the Rmd file. \n\n\n\n\n\n\nPreparation and session set up\nThis tutorial is based on R. If you have not installed R or are new to it, you will find an introduction to and more information how to use R here. For this tutorials, we need to install certain packages from an R library so that the scripts shown below are executed without errors. Before turning to the code below, please install the packages by running the code below this paragraph. If you have already installed the packages mentioned below, then you can skip ahead and ignore this section. To install the necessary packages, simply run the following code - it may take some time (between 1 and 5 minutes to install all of the libraries so you do not need to worry if it takes some time).\n\n\n\nNOTEThe installation of the packages will be relatively data intensive due to some of the required packages containing shp-files (shape files) - which renders these packages to be larger big in comparison to other R packages. It is thus recommendable to be logged into an institutional network that has a decent connectivity and download rate (e.g., a university network).\n\n\n\n\n\n\ninstall.packages(\"sf\")\ninstall.packages(\"raster\")\ninstall.packages(\"dplyr\")\ninstall.packages(\"spData\")\ninstall.packages(\"tmap\")  \ninstall.packages(\"leaflet\")\ninstall.packages(\"ggplot2\")\ninstall.packages(\"spDataLarge\",\n                 repos = \"https://nowosad.github.io/drat/\", type = \"source\")\ninstall.packages(\"ggspatial\")\ninstall.packages(\"rnaturalearth\")\ninstall.packages(\"rnaturalearthdata\")\ninstall.packages(\"ggmap\")\ninstall.packages(\"leaflet\")\ninstall.packages(\"maptools\")\ninstall.packages(\"rgdal\")\ninstall.packages(\"scales\")\ninstall.packages(\"maps\")\ninstall.packages(\"here\")\ninstall.packages(\"rgeos\")\n# install klippy for copy-to-clipboard button in code chunks\ninstall.packages(\"remotes\")\nremotes::install_github(\"rlesur/klippy\")\n\nNow that we have installed the packages, we activate them as shown below.\n\nlibrary(sf)\nlibrary(raster)\nlibrary(dplyr)\nlibrary(spData)\nlibrary(spDataLarge)\nlibrary(tmap)  \nlibrary(ggplot2)\nlibrary(ggspatial)\nlibrary(rnaturalearth)\nlibrary(ggmap)\nlibrary(leaflet)\nlibrary(maptools)\nlibrary(rgdal)\nlibrary(scales)\nlibrary(rgeos)\n# activate klippy for copy-to-clipboard button\nklippy::klippy()\n\n\n\n\nOnce you have installed R and RStudio and also initiated the session by executing the code shown above, you are good to go.\n\n\nCreating Basic Maps\nWe will start by generating maps of the world using a in-build world data set that is part of the spData package and the plot function for generating the map.\n\n# Load the world data package\ndata(world)\nplot(world)\n\n\n\n\nWe see that the world data set contains information on various factors, such as information about regions (e.g., continent or subregion), country names (name_long), population size (pop) or life expectancy (lifeExp). We can use this information to show a specific map as shown below.\n\nplot(world[\"lifeExp\"])\n\n\n\n\nWe can use the world data set and filter for specific features, e.g., we can visualize only a single continent by filtering for the continent we are interested in. In addition, we define the x-axis and y-axis limits so that we zoom in on the region of interest.\n\n# extract europe (exclude russia and iceland)\nworld_eur <- world %>%\n  dplyr::filter(continent == \"Europe\", \n                name_long != \"Russian Federation\", \n                name_long != \"Iceland\") %>%\n  dplyr::select(name_long, geom)\n# plot\nplot(world_eur,\n     xlim = c(5, 10),\n     ylim = c(30, 70),\n     main = \"\")\n\n\n\n\nWe can also overlay information such as population size over continents and countries as shown below.\n\n# plot world map\nplot(world[\"continent\"], reset = FALSE)\n# define size bases on population size\ncex <- sqrt(world$pop) / 10000\n# center world map\nworld_cents <- sf::st_centroid(world, of_largest = TRUE)\n# plot\nplot(sf::st_geometry(world_cents), \n     add = TRUE, \n     cex = cex)\n\n\n\n\nOverlaying is interesting because it allows us to highlight certain regions or countries as shown below.\n\n# extract map of europe\nworld_eur <- world %>%\n  dplyr::filter(continent == \"Europe\")\n# extract germany\nger <- world %>%\n  dplyr::filter(name_long == \"Germany\")\n# plot germany\nplot(sf::st_geometry(ger), expandBB = c(.2, .2, .2, .2), col = \"gray\", lwd = 3)\n# plot europe\nplot(world_eur[0], add = TRUE)\n\n\n\n\nWe can also add information to the world data set and use the added information to generate customized plots.\n\n# countries I have been to\ncountries <- c(\"United States\", \"Norway\", \"France\", \"United Arab Emirates\", \n             \"Qatar\", \"Sweden\", \"Poland\", \"Austria\", \"Hungary\", \"Romania\", \n             \"Germany\", \"Bulgaria\", \"Greece\", \"Turkey\", \"Croatia\", \n             \"Switzerland\", \"Belgium\", \"Netherlands\", \"Spain\", \"Ireland\", \n             \"Australia\", \"China\", \"Italy\", \"Denmark\", \"United Kingdom\", \n             \"Slovenia\", \"Finland\", \"Slovakia\", \"Czech Republic\", \"Japan\", \n             \"Saudi Arabia\", \"Serbia\")\n# data frame with countries I have visited\nvisited <- world %>%\n  dplyr::filter(name_long %in% countries)\n# plot world\nplot(world[0], col = \"lightgray\")\n# overlay countries I have visited in orange \nplot(sf::st_geometry(visited), add = TRUE, col = \"orange\")\n\n\n\n\n\n\nCreating Maps with ggplot2\nSo far, we have used the base plot function to generate maps. However, it is also possible to use ggplot2 to generate maps and the easiest way is to use borders to draw a map.\n\n# plot map\nggplot() +\n  borders()\n\n\n\n\nAnother option is to add geom_sf to a ggplot2 object as shown below. A nice feature is that we can add perspective and projection.\n\n# plot map\nggplot(data = world) +\n  geom_sf(fill = \"white\") +\n  coord_sf(crs = \"+proj=laea +lat_1=-28 +lat_2=-36 +lat_0=-32 +lon_0=135 +x_0=1000000 +y_0=2000000 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs\")\n\n\n\n\nOr have a map that shows the world in a bit of an unusual perspective due to the Labert projection.\n\n# plot map\nggplot(data = world) +\n  geom_sf(fill = \"beige\") +\n  coord_sf(crs = \"+proj=lcc +lat_1=-28 +lat_2=-36 +lat_0=-32 +lon_0=135 +x_0=1000000 +y_0=2000000 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs\") +\n  theme(panel.grid.major = element_line(color = \"gray50\", \n                                         size = 0.25),\n        panel.background = element_rect(fill = \"aliceblue\"))\n\n\n\n\nThe nice thing about ggplot2 is, of course, that it is very easy to add layers and create very pretty visualizations.\n\n# plot map\nggplot(data = world) +\n  geom_sf() + \n  theme_bw() +\n  # adding axes title\n  labs(x = \"Longitude\", y = \"Latitude\") +\n  # adding title and subtitle\n  ggtitle(\"A map of Australia\") +\n  # defining coordinates\n  coord_sf(xlim = c(100.00, 160.00), \n           ylim = c(-45.00, -10.00), \n           expand = T) +\n  # add distance measure\n  annotation_scale(location = \"bl\", width_hint = 0.5) +\n  # add compass \n  annotation_north_arrow(location = \"br\")\n\n\n\n\nAgain, we can customize the map according to what we want. In addition, we load a map with a higher resolution using the ne_countries function from the rnaturalearth package.\n\n# load data\nworld <- rnaturalearth::ne_countries(returnclass = \"sf\") \n# add to prevent errors\nsf::sf_use_s2(FALSE)\n# extract locations\nworld_points<- st_centroid(world)\n# extract labels\nworld_points <- cbind(world, sf::st_coordinates(sf::st_centroid(world$geometry)))\n# generate annotated world map\nggplot(data = world) +\n  # land is gray\n  geom_sf(fill= \"gray90\") +\n  # axes labels\n  labs(x = \"Longitude\", y = \"Latitude\") +\n  # define zoom\n  coord_sf(xlim = c(100.00, 180.00), \n           ylim = c(-45.00, -10.00), expand = T) +\n  # add scale bar\n  annotation_scale(location = \"bl\", width_hint = 0.5) +\n  # add compass\n  annotation_north_arrow(location = \"br\", which_north = \"true\", \n                         style = north_arrow_fancy_orienteering) +\n  # define theme (add grid lines)\n  theme(panel.grid.major = element_line(color = \"gray60\", \n                                         linetype = \"dashed\", \n                                         size = 0.25),\n        # define background color\n         panel.background = element_rect(fill = \"aliceblue\")) +\n  # add text\n  geom_text(data= world_points,aes(x=X, y=Y, label=name),\n            color = \"gray20\", fontface = \"italic\", check_overlap = T, size = 3)\n\n\n\n\nWe can explore other designs and maps and show different regions of the world.\n\n# load data\neurope <- ne_countries(scale = \"medium\", continent='europe', returnclass = \"sf\") \n# plot map\nggplot(data = europe) +\n  # add map and define filling\n  geom_sf(mapping = aes(fill = ifelse(name_long == \"Germany\", \"0\", \"1\"))) +\n  # simply black and white background\n  theme_bw() +\n  # adding axes title\n  labs(x = \"Longitude\", y = \"Latitude\") +\n  # adding title and subtitle\n  ggtitle(\"A map of central Europe\") +\n  # defining coordinates\n  coord_sf(xlim = c(-10, 30), \n           ylim = c(40, 60)) +\n  # add distance measure\n  annotation_scale(location = \"bl\", width_hint = 0.5) +\n  # add compass \n  annotation_north_arrow(location = \"br\",\n                         # make compass fancy\n                         style = north_arrow_fancy_orienteering) +\n  theme(legend.position = \"none\",\n        # add background color\n        panel.background = element_rect(fill = \"lightblue\"),\n        # remove grid lines\n        panel.grid.major = element_blank(), \n        panel.grid.minor = element_blank()) +\n  # define fill colors\n  scale_fill_manual(name = \"Country\", values = c(\"darkgray\", \"beige\")) +\n  # add text\n  geom_sf_text(aes(label = name_long), size=2.5, color = \"gray20\")\n\n\n\n\n\n\nAdding External Information\nWhile being very useful, displaying basic maps is usually less relevant because, typically, we want to add different layers to a map. In order to add layers to a map, we need to combine the existing data with some data that we would like to lay over the existing map.\nWe will now lad external information which contains the locations of Australian airports.\n\n# load data\nairports <- base::readRDS(url(\"https://slcladal.github.io/data/apd.rda\", \"rb\")) %>%\n  dplyr::mutate(ID = as.character(ID)) %>%\n  dplyr::filter(Country == \"Australia\")\n# inspect\nhead(airports)\n\n    ID                           Name          City   Country Latitude\n1 3317   Brisbane Archerfield Airport      Brisbane Australia -27.5703\n2 3318     Northern Peninsula Airport      Amberley Australia -10.9508\n3 3319          Alice Springs Airport Alice Springs Australia -23.8067\n4 3320 Brisbane International Airport      Brisbane Australia -27.3842\n5 3321             Gold Coast Airport   Coolangatta Australia -28.1644\n6 3322   Cairns International Airport        Cairns Australia -16.8858\n  Longitude\n1   153.008\n2   142.459\n3   133.902\n4   153.117\n5   153.505\n6   145.755\n\n\nNext, we load an additional data set about the route volume of Australian airports (how many routes go in and out of these airports).\n\n# read in routes data\nroutes <- base::readRDS(url(\"https://slcladal.github.io/data/ard.rda\", \"rb\")) %>%\n  dplyr::rename(ID = destinationAirportID) %>%\n  dplyr::group_by(ID) %>%\n  dplyr::summarise(flights = n())\n# inspect\nhead(routes)\n\n# A tibble: 6 × 2\n  ID     flights\n  <chr>    <int>\n1 \"\\\\N\"      221\n2 \"1\"          5\n3 \"10\"         2\n4 \"100\"       44\n5 \"1001\"       4\n6 \"1004\"       7\n\n\nWe can now merge the airports and the route volume data sets to combine the information about the location with the information about the number of routes that end at each airport.\n\n# combine tables\narrivals <- dplyr::left_join(airports, routes, by = \"ID\") %>%\n  na.omit()\n# inspect\nhead(arrivals)\n\n    ID                           Name          City   Country Latitude\n2 3318     Northern Peninsula Airport      Amberley Australia -10.9508\n3 3319          Alice Springs Airport Alice Springs Australia -23.8067\n4 3320 Brisbane International Airport      Brisbane Australia -27.3842\n5 3321             Gold Coast Airport   Coolangatta Australia -28.1644\n6 3322   Cairns International Airport        Cairns Australia -16.8858\n7 3323            Charleville Airport  Charlieville Australia -26.4133\n  Longitude flights\n2   142.459       1\n3   133.902      13\n4   153.117     144\n5   153.505      28\n6   145.755      54\n7   146.262       4\n\n\nNow that we have that data (which contains geolocations (longitudes and latitudes), we can visualize the location of the airports on a map and add information about the route volume of the airports in the form of, e.g., points that we plot over the airport - the bigger the point, the higher the route volume. In addition, we add the locations as texts and also make these labels correspond to the route volume.\n\n# create a layer of borders\nggplot(arrivals, aes(x=Longitude, y= Latitude)) +   \n  borders(\"world\", colour=\"gray20\", fill=\"wheat1\")  +\n  geom_point(color=\"blue\", alpha = .3, size = log(arrivals$flights)) +\n  scale_x_continuous(name=\"Longitude\", limits=c(110, 160)) +\n  scale_y_continuous(name=\"Latitude\", limits=c(-45, -10)) +\n  theme(panel.background = element_rect(fill = \"azure1\", colour = \"azure1\")) +\n  geom_text(aes(x=Longitude, y= Latitude, label=City),\n            color = \"gray20\", check_overlap = T, size = log(arrivals$flights))\n\n\n\n\n\n\nCreating Maps with ggmap\nIf you simply want to show specific locations on existing maps, then the ggmap package is an easy way to take on this talk.\n\n# define box\nsbbox <- ggmap::make_bbox(lon = c(152.8, 153.4), lat = c(-27.1, -27.7), f = .1)\n# get map\nbrisbane = ggmap::get_map(location=sbbox, zoom=10, maptype=\"terrain\")\n# create map\nbrisbanemap = ggmap::ggmap(brisbane)\n# display map\nbrisbanemap +\n  geom_point(data = arrivals, mapping = aes(x = Longitude, y = Latitude), \n               color = \"red\", size = 2) +\n  geom_text(data = arrivals, \n            mapping = aes(x = Longitude+0.1,\n                          y = Latitude,\n                          label = \"Brisbane Airport\"),\n            size = 3, color = \"gray20\", \n            fontface = \"bold\", \n            check_overlap = T) +\n  labs(x = \"Longitude\", y = \"Latitude\")\n\n\n\n\n\n\nInteractive Maps with leaflet and maptools\nThe leaflet package offers very easy-to use options for generating interactive maps (here is the link to the leaflet cheat sheet provided by RStudio). The interactivity is achieved by the leaflet function from the leaflet package which creates a leaflet-map with html-widgets which can be used, e.g., in html rendered R Notebooks or Shiny applications. The advantage of using this function lies in the fact that it offers very detailed maps which enable zooming in on specific locations.\n\n# generate basic leaflet map\nm <- leaflet() %>% \n  leaflet::setView(lng = 153.05, lat = -27.45, zoom = 12)%>% \n  leaflet::addTiles()\n# show map\nm\n\n\n\n\n\nAnother option for interactive geospatial visualizations is provided by the maptools package which comes with a SpatialPolygonsDataFrame of the world and the population by country (in 2005). To make the visualization a bit more appealing, we will calculate the population density, add this variable to the data which underlies the visualization, and then display the information interactively. In this case, this means that you can use mouse-over or hoover effects so that you see the population density in each country if you put the cursor on that country (given the information is available for that country).\nWe start by loading the required package from the library, adding population density to the data, and removing data points without meaningful information (e.g. we set values like Inf to NA).\n\n# load data\ndata(wrld_simpl)\n# calculate population density and add it to the data \nwrld_simpl@data$PopulationDensity <- round(wrld_simpl@data$POP2005/wrld_simpl@data$AREA,2)\nwrld_simpl@data$PopulationDensity <- ifelse(wrld_simpl@data$PopulationDensity == \"Inf\", NA, wrld_simpl@data$PopulationDensity)\nwrld_simpl@data$PopulationDensity <- ifelse(wrld_simpl@data$PopulationDensity == \"NaN\", NA, wrld_simpl@data$PopulationDensity)\n# inspect\nhead(wrld_simpl@data, 10)\n\n    FIPS ISO2 ISO3 UN                NAME   AREA  POP2005 REGION SUBREGION\nATG   AC   AG  ATG 28 Antigua and Barbuda     44    83039     19        29\nDZA   AG   DZ  DZA 12             Algeria 238174 32854159      2        15\nAZE   AJ   AZ  AZE 31          Azerbaijan   8260  8352021    142       145\nALB   AL   AL  ALB  8             Albania   2740  3153731    150        39\nARM   AM   AM  ARM 51             Armenia   2820  3017661    142       145\nAGO   AO   AO  AGO 24              Angola 124670 16095214      2        17\nASM   AQ   AS  ASM 16      American Samoa     20    64051      9        61\nARG   AR   AR  ARG 32           Argentina 273669 38747148     19         5\nAUS   AS   AU  AUS 36           Australia 768230 20310208      9        53\nBHR   BA   BH  BHR 48             Bahrain     71   724788    142       145\n         LON     LAT PopulationDensity\nATG  -61.783  17.078           1887.25\nDZA    2.632  28.163            137.94\nAZE   47.395  40.430           1011.14\nALB   20.068  41.143           1151.00\nARM   44.563  40.534           1070.09\nAGO   17.544 -12.296            129.10\nASM -170.730 -14.318           3202.55\nARG  -65.167 -35.377            141.58\nAUS  136.189 -24.973             26.44\nBHR   50.562  26.019          10208.28\n\n\nWe can now display the data and use color coding to indicate the different population densities.\n\n# define colors\nqpal <- colorQuantile(rev(viridis::viridis(10)),\n                      wrld_simpl$PopulationDensity, n=10)\n# generate visualization\nl <- leaflet(wrld_simpl, options =\n               leafletOptions(attributionControl = FALSE, minzoom=1.5)) %>%\n  addPolygons(\n    label=~stringr::str_c(\n      NAME, ' ',\n      formatC(PopulationDensity, big.mark = ',', format='d')),\n    labelOptions= labelOptions(direction = 'auto'),\n    weight=1, color='#333333', opacity=1,\n    fillColor = ~qpal(PopulationDensity), fillOpacity = 1,\n    highlightOptions = highlightOptions(\n      color='#000000', weight = 2,\n      bringToFront = TRUE, sendToBack = TRUE)\n    ) %>%\n  addLegend(\n    \"topright\", pal = qpal, values = ~PopulationDensity,\n    title = htmltools::HTML(\"Population density <br> (2005)\"),\n    opacity = 1 )\n# display visualization\nl\n\n\n\n\n\n\n\nAdding Shapes to Maps\nThe following code will download a shape file and associated metadata to a temporary directory. Note that just downloading the .shp is not enough, the associated .shx and .dbf files are also needed.\n\n# Download shapefile to a temporary location so we can read it in\ntmpdir <- tempdir()\nurls <- list(\n  'https://slcladal.github.io/data/shapes/AshmoreAndCartierIslands.shp',\n  'https://slcladal.github.io/data/shapes/AshmoreAndCartierIslands.shx',\n  'https://slcladal.github.io/data/shapes/AshmoreAndCartierIslands.dbf'\n)\n\nfor (url in urls) {\n  dest <- paste(tmpdir, \"/\", basename(url), sep=\"\")\n  download.file(url, dest)\n}\n\naustralia <- rgdal::readOGR(tmpdir, layer='AshmoreAndCartierIslands', stringsAsFactors = F)\n\nOGR data source with driver: ESRI Shapefile \nSource: \"/tmp/RtmpKV4VJ8\", layer: \"AshmoreAndCartierIslands\"\nwith 15 features\nIt has 14 fields\n\n\nWe can now generate a first map based on the shape file we downloaded.\n\n# plot australia based on shp file\nggplot() + \n  geom_polygon(data = australia, \n               aes(x = long, y = lat, group = group),\n               colour = \"black\", fill = \"gray90\") +\n  theme_void()\n\n\n\n\nNext, we convert the data set into a tidy format.\n\n# convert the data into tidy format\naustralia_tidy <- broom::tidy(australia, region = \"name\")\n# inspect data\nhead(australia_tidy)\n\n# A tibble: 6 × 7\n   long   lat order hole  piece group                         id                \n  <dbl> <dbl> <int> <lgl> <fct> <fct>                         <chr>             \n1  123. -12.2     1 FALSE 1     Ashmore and Cartier Islands.1 Ashmore and Carti…\n2  123. -12.2     2 FALSE 1     Ashmore and Cartier Islands.1 Ashmore and Carti…\n3  123. -12.2     3 FALSE 1     Ashmore and Cartier Islands.1 Ashmore and Carti…\n4  123. -12.2     4 FALSE 1     Ashmore and Cartier Islands.1 Ashmore and Carti…\n5  123. -12.2     5 FALSE 1     Ashmore and Cartier Islands.1 Ashmore and Carti…\n6  123. -12.2     6 FALSE 1     Ashmore and Cartier Islands.1 Ashmore and Carti…\n\n\nNow, we extract the names of the states and territories as well as the longitudes and latitudes where we want to display the labels. Then, we display the information on the map.\n\n# extract names of states and their long and lat\naustralia_states <- australia_tidy %>%\n  dplyr::group_by(id) %>%\n  dplyr::summarise(long = mean(long),\n                   lat = mean(lat))\n\nWe can now generate a map based on the shp-file. In addition, we define colors for the states and territories and customize the map.\n\n# define colors\nclrs <- viridis_pal()(15)\n# plot map\np <- ggplot() +\n  # plot map\n  geom_polygon(data = australia_tidy, \n               aes(x = long, y = lat, group = group, fill = id, alpha = .75), \n               asp = 1, colour = \"gray50\") +\n  # add text\n  geom_text(data = australia_states, aes(x = long, y = lat, label = id), \n            size = 3, color = \"gray20\", fontface = \"bold\", \n            check_overlap = T) +\n  geom_text(data= world_points,aes(x=X, y=Y, label=name),\n            color = \"gray20\", fontface = \"bold\", check_overlap = T, size = 5) +\n  # color states\n  scale_fill_manual(values = clrs) +\n  # define theme and axes\n  theme_void() +\n  scale_x_continuous(name = \"Longitude\", limits = c(110, 160)) +\n  scale_y_continuous(name = \"Latitude\", limits = c(-45, -10)) +\n  theme(panel.grid.major = element_line(color = \"gray60\", \n                                         linetype = \"dashed\", \n                                         size = 0.25),\n        # define background color\n        panel.background = element_rect(fill = \"aliceblue\"),\n        legend.position = \"none\")+ \n  # add compass\n  annotation_north_arrow(location = \"tl\", which_north = \"true\", \n                         style = north_arrow_fancy_orienteering)\n# show plot\np\n\n\n\n\nYou can create customized polygons by defining longitudes and latitudes. In fact, you can generate very complex polygons like this. However, in this example, we only create a very basic one as a poof-of-concept.\n\n# create data frame with longitude and latitude values\nlat <- c(-25, -27.5, -25, -30, -30, -35, -25)\nlong <- c(150, 140, 130, 135, 140, 147.5, 150)\nmypolygon <- as.data.frame(cbind(long, lat))\n# inspect data\nmypolygon\n\n   long   lat\n1 150.0 -25.0\n2 140.0 -27.5\n3 130.0 -25.0\n4 135.0 -30.0\n5 140.0 -30.0\n6 147.5 -35.0\n7 150.0 -25.0\n\n\nWe can now plot out polygon over the map produced above.\n\np + \n  geom_polygon(data=mypolygon,\n               aes(x = long, y = lat),\n               alpha = 0.2, \n               colour = \"gray20\", \n               fill = \"red\") +\n  ggplot2::annotate(\"text\", \n                    x = mean(long),\n                    y = mean(lat),\n                    label = \"My Polygon Area\", \n                    colour=\"white\", \n                    size=3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe will end this introduction here but if you want to want to learn more, check out the detailed resources for geospatial data visualization using R can be found here or here.\n\n\nCitation & Session Info\nSchweinberger, Martin. 2022. Introduction to Geospatial Data Visualization with R. Brisbane: The University of Queensland. url: https://slcladal.github.io/gviz.html (Version 2022.08.31).\n@manual{schweinberger2022gviz,\n  author = {Schweinberger, Martin},\n  title = {Introduction to Geospatial Data Visualization with R},\n  note = {https://slcladal.github.io/gviz.html},\n  year = {2022},\n  organization = \"The University of Queensland, Australia. School of Languages and Cultures},\n  address = {Brisbane},\n  edition = {2022.08.31}\n}\n\nsessionInfo()\n\nR version 4.2.1 (2022-06-23)\nPlatform: x86_64-pc-linux-gnu (64-bit)\nRunning under: Ubuntu 22.04.1 LTS\n\nMatrix products: default\nBLAS:   /usr/lib/x86_64-linux-gnu/blas/libblas.so.3.10.0\nLAPACK: /usr/lib/x86_64-linux-gnu/lapack/liblapack.so.3.10.0\n\nlocale:\n [1] LC_CTYPE=en_AU.UTF-8       LC_NUMERIC=C              \n [3] LC_TIME=en_AU.UTF-8        LC_COLLATE=en_AU.UTF-8    \n [5] LC_MONETARY=en_AU.UTF-8    LC_MESSAGES=en_AU.UTF-8   \n [7] LC_PAPER=en_AU.UTF-8       LC_NAME=C                 \n [9] LC_ADDRESS=C               LC_TELEPHONE=C            \n[11] LC_MEASUREMENT=en_AU.UTF-8 LC_IDENTIFICATION=C       \n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] rgeos_0.5-9         scales_1.2.0        rgdal_1.5-32       \n [4] maptools_1.1-4      leaflet_2.1.1       ggmap_3.0.0        \n [7] rnaturalearth_0.1.0 ggspatial_1.1.5     ggplot2_3.3.6      \n[10] tmap_3.3-3          spDataLarge_2.0.6   spData_2.0.1       \n[13] dplyr_1.0.9         raster_3.5-21       sp_1.5-0           \n[16] sf_1.0-7           \n\nloaded via a namespace (and not attached):\n [1] bitops_1.0-7            RColorBrewer_1.1-3      httr_1.4.3             \n [4] backports_1.4.1         tools_4.2.1             utf8_1.2.2             \n [7] R6_2.5.1                KernSmooth_2.23-20      DBI_1.1.3              \n[10] colorspace_2.0-3        withr_2.5.0             gridExtra_2.3          \n[13] rnaturalearthdata_0.1.0 tidyselect_1.1.2        curl_4.3.2             \n[16] compiler_4.2.1          leafem_0.2.0            cli_3.3.0              \n[19] labeling_0.4.2          classInt_0.4-7          proxy_0.4-27           \n[22] stringr_1.4.0           digest_0.6.29           foreign_0.8-82         \n[25] rmarkdown_2.14          base64enc_0.1-3         dichromat_2.0-0.1      \n[28] jpeg_0.1-9              pkgconfig_2.0.3         htmltools_0.5.2        \n[31] fastmap_1.1.0           maps_3.4.0              htmlwidgets_1.5.4      \n[34] rlang_1.0.4             rstudioapi_0.13         farver_2.1.1           \n[37] generics_0.1.3          jsonlite_1.8.0          crosstalk_1.2.0        \n[40] magrittr_2.0.3          s2_1.0.7                Rcpp_1.0.8.3           \n[43] munsell_0.5.0           fansi_1.0.3             viridis_0.6.2          \n[46] abind_1.4-5             lifecycle_1.0.1         terra_1.5-34           \n[49] stringi_1.7.8           leafsync_0.1.0          yaml_2.3.5             \n[52] tmaptools_3.1-1         plyr_1.8.7              grid_4.2.1             \n[55] parallel_4.2.1          crayon_1.5.1            lattice_0.20-45        \n[58] stars_0.5-5             knitr_1.39              klippy_0.0.0.9500      \n[61] pillar_1.7.0            rjson_0.2.21            codetools_0.2-18       \n[64] wk_0.6.0                XML_3.99-0.10           glue_1.6.2             \n[67] evaluate_0.15           leaflet.providers_1.9.0 png_0.1-7              \n[70] vctrs_0.4.1             RgoogleMaps_1.4.5.3     gtable_0.3.0           \n[73] purrr_0.3.4             tidyr_1.2.0             assertthat_0.2.1       \n[76] xfun_0.31               broom_1.0.0             lwgeom_0.2-8           \n[79] e1071_1.7-11            class_7.3-20            viridisLite_0.4.0      \n[82] tibble_3.1.7            units_0.8-0             ellipsis_0.3.2         \n\n\n\nBack to top\nBack to HOME"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Language Technology and Data Analysis Laboratory (LADAL)",
    "section": "",
    "text": "WELCOME\n\n\n\n\n\nWelcome to the website of the Language Technology and Data Analysis Laboratory (LADAL). LADAL (pronounced lah’dahl) is a free, open-source, collaborative support infrastructure for digital and computational humanities established 2019 by the School of Languages and Cultures at the University of Queensland. LADAL aims at assisting anyone interested in working with language data in matters relating to data processing, visualization, and analysis and offers guidance on matters relating to language technology and digital research tools. To this end, LADAL offers introductions to topics and concepts related to digital and computational humanities, online tutorials, interactive Jupyter notebooks, and events including workshops and webinar series.\n\n\n\n\n\nLADAL is part of the Australian Text Analytics Platform (ATAP). The aim of ATAP is to provide researchers with a Notebook environment – in other words a tool set - that is more powerful and customisable than standard packages, while being accessible to a large number of researchers who do not have strong coding skills.\nThe Australian Text Analytics Platform (ATAP) projects received investment (https://doi.org/10.47486/PL074) from the Australian Research Data Commons (ARDC). The ARDC is funded by the National Collaborative Research Infrastructure Strategy (NCRIS).\n\n\n\n\n\n\nThe Australian Text Analytics Platform (ATAP) projects received investment (https://doi.org/10.47486/PL074) from the Australian Research Data Commons (ARDC). The ARDC is funded by the National Collaborative Research Infrastructure Strategy (NCRIS).\n\n\n\n\nSince going public January 1, 2021, LADAL has received more than 200,000 unique page views of more than 120,000 users in more than 170,000 sessions! The majority of LADAL users access the LADAL website from the USA (app. 28%), Great Britain (app. 7%), Germany (app. 6%), India (5%), Australia (5%), and China (app. 2.5%). The highest number of unique users was April 27, 2021 with 824 unique users.\n\n\n\n\n\n\n\n\n\n\n\n\n\nGet in Touch!\nTo get in touch with us here at LADAL, maybe because you are interested in becoming a contributor or because you found an error on our site, you can simply write an email to ladal@uq.edu.au, reach out to us on Twitter (@slcladal), send us a message on Facebook (see our Facebook page or you can sign up to the LADAL email list. To subscribe to our email list, simply write an email to ladal@uq.edu.au with the subject email list.\n\n\nGoals\nThe LADAL aims to help develop computational and digital skills by providing information and practical, hands-on tutorials on data and text analytics as well as on statistical methods relevant for language research. In addition, the LADAL provides self-guided study materials relevant for computational Natural Language Processing. In order to be attractive to both beginners and people with advanced skills, the LADAL website covers topics and introduces methods relevant for people coming with different degrees of prior knowledge and experience - ranging from introductions to concepts of quantitative reasoning to step-by-step guides on advanced statistical modeling.\nSince the primary concern of the LADAL is to introduce computational methods that are relevant to research involving natural language, the focus of this website is placed on linguistic data and methods relevant for text analytics. As such, the LADAL provides resources for (computational) text analytics and offers introductions to quantitative reasoning, research designs, and computational methods including data visualization and statistics. The areas covered on the LADAL website are\n\n\n\n\n\n\n\nintroductions to quantitative reasoning and basic concepts in empirical language studies.\nintroductions to R as programming environment for processing natural language data.\ntutorials on data visualization and data analytics (statistics and machine learning).\ntutorials on text analysis, text mining, distant reading, and corpus linguistics.\n\n\nThe resources and events offered by LADAL also aim at promoting and informing about Best Practices in data handling such as transparency, reproducibility and the FAIR principles (data should be findable, accessible, interoperable, and reusable).\n\n\n\n\n\n\n\nUser Stories\nBelow are selected user stories of people that have used LADAL resources in their research, training, or teaching.\n\n\n\n\nUSER STORIESWe are currently looking for user stories (also known as testimonials) to see and show what people use LADAL resources for. If you have used LADAL resources - be it by simply copying some code, attending a workshop, learning about a method using a tutorial, or in any other way - we would be extremely grateful, if you would send us your user story! To submit your user story, simply write up a short paragraph describing how you have used LADAL resources and what you have used them for and send it to ladal@uq.edu.au. We really appreciate any feedback from you about this!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPaula Rautionaho (University researcher, University of Eastern Finland)  I learned about the LADAL website at a conference and since then I’ve been going through the contents bit by bit, starting with R and Data science basics and the many tutorials available. The website is great for acquiring basic knowledge, and I’ve also used it to find information on specific methods that I need for my research. The way the code is explained in detail and exemplified through actual studies, and the fact that the code is downloadable from the site, are extremely useful and helpful. What I usually do is download the string of code, go through each line to understand what’s going on and then modify it to my needs. The website also helps in understanding the output of statistical analyses, which for me is what sets this resource apart from many others.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLaura Janda (Professor of Russian, The Arctic University of Norway, Tromsø)  LADAL is a tremendously valuable resource that I recommend to all my students in my Quantitative Methods in Linguistics course. Given the broad portfolio of various courses that I teach plus my numerous other commitments, combined with the rapid pace of developments in both R itself and its application to linguistic analyses, it is not possible for me to keep apace with all of the developments all of the time. It is very important to have an authoritative and comprehensive resource that represents current best practices in the field, and that is exactly what LADAL is.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRobert Daugs (Postdoctoral Researcher, University of Kiel)  It’s great to see how the content has evolved and it seems that whenever I come across a method I hear about in another talk or read in a paper and wish to implement in my own research (or just try it out), a corresponding script with meaty instructions is already available at LADAL. The tutorial I probably came back to more than once actually covers mixed-effects regression modeling. Given this method’s value in corpus-based, variationist linguistics and elsewhere, I think it’s great that this tutorial made the cut and I’m looking forward to any further updates the LADAL crowd might have planned for this.\nThanks for such a wonderful, open access resource!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTouba Warsi (Analyst - School of Medicine, University of California San Diego)  I was very happy to find the text analysis tutorial (https://slcladal.github.io/textanalysis.html) for text analysis in R. Up-to-date and super helpful! I use it for mining open-ended student survey comments with the hope of identifying themes, glean feedback without resorting to human coding of the comments.\nPlease keep up the great work!\n\n\n\n\n\n\n\n\n\n\n\nAudience\nThe LADAL resources are aimed at researchers in HASS (Humanities, Arts, and the Social Sciences) and we aspire to attract complete novices as well as expert users. And, while the focus of the LADAL website is placed on handling data that represents natural language, anyone who has an interest in quantitative methods, data visualization, statistics, or R is welcome to explore this webpage.\n\n\n\n\n\n\nAt LADAL, we aim to reach out and make our resources available to the research community and anyone interested Language Technology, Data Analysis, and using computational means to extract, process, visualize and analyze language data. To this end, we offer workshops, give presentations and talks, organize webinars (see, e.g., the LADAL Webinar Series 2021).\n\n\n\n\n\nIn addition, we provide resources on the LADAL website and on the LADAL YouTube channel, we announce updates on Twitter (@slcladal) as well as on our NEWS site and via our Facebook page. To get in touch, you can contact us on Twitter or send us an email via ladal@uq.edu.au.\n\n\n\n\nProgramming\n\n\n\n\n\nThe LADAL primarily uses the programming language R because R is extremely flexible, relatively easy to learn, free and open source, and R has a substantive and very friendly user community. R is not merely a software package but a fully-fledged programming environment which allows complex Natural Language Processing, statistics and data visualizations and it can also be used to create websites or apps, and has direct pipelines for version control (Git). This website as well as the self-guided study materials offered by the LADAL use are written in R-markdown - a way to combine R-code with text. The flexibility of R makes it a sensible choice for researchers that strive for high quality and extreme flexibility while following best practices that enable complete replicability and full transparency. If you want to learn more about R and why we use it, please check out our Why R? page.\n\n\n\n\n\nAs computation is becoming ever more prevalent across disciplines as well as in both the social and economic domains, the LADAL offers a resource space for R that make it accessible to lay users as well as expert programmers. That said, we will expand the resources provided by the LADAL to other tools and environments and include tutorials based on Python in the future.\n\n\n\n\n\n\n\nLicensing\nThe LADAL website was created by Martin Schweinberger. It was freely released under GNU General Public License, Version 3, June 2007.\n\n\n\nCitation\nIf you use (parts of) LADAL tutorials for your own research or in your teaching materials, please cite the individual subpages as shown at the bottom of each page or reference it as:\nSchweinberger, Martin. 2022. The Language Technology and Data Analysis Laboratory (LADAL). Brisbane: The University of Queensland, School of Languages and Cultures. url: https://slcladal.github.io/index.html (Version 2022.08.31).\n@manual{uqslc2022ladal,\n  author = {Schweinberger, Martin},\n  title = {The Language Technology and Data Analysis Laboratory (LADAL)},\n  note = {https://slcladal.github.io/index.html},\n  year = {2022},\n  organization = {The University of Queensland, School of Languages and Cultures},\n  address = {Brisbane},\n  edition = {2022.08.31}\n}\n\nDisclaimer\n\n\n\n\n\n\nThe content of this website is free and comes with ABSOLUTELY NO WARRANTY. You are welcome to redistribute the content of LADAL resources given you adhere to the licensing. The content of this website is distributed under the terms of the GNU General Public License, Version 3, June 2007.\n\n\nShare and Enjoy!\n\nBack to top"
  },
  {
    "objectID": "introquant.html#errare-humanum-est",
    "href": "introquant.html#errare-humanum-est",
    "title": "Introduction to Quantitative Reasoning",
    "section": "Errare Humanum Est",
    "text": "Errare Humanum Est\nAll of the above indicates that we need empirical evidence to determine the rules and laws governing empirical reality but it does not tell us why we need science. As we have established above, only empirical evidence can lead the way to determining what the characteristics of the world we live in are. Science comes in as we, as human beings, are not very good at seeing and understanding the world as it is (in fact, we are not able to do so!). To elaborate, when it comes things we are scared of, we rather rely on emotional narratives rather than things that really do us harm. For instance, we are more scared of what is sometimes referred to as stranger danger, i.e. that someone unknown will harm us or people dear to us than people we know while most murders and sexual exploits are committed by people, we know such as our family and friends. Another example would be represented by the movie Jaws while mosquitoes or even cows kill more people (mosquitoes are in fact among the most dangerous animals for humans due to the diseases they transmit).\nBesides being more strongly influenced by emotional narratives there are other in-build biases such as our drive to stick to opinions rather than correcting them or switching sides once these are proven incorrect. In other words, we seek confirmation for our believes rather than challenging them. For example,"
  },
  {
    "objectID": "introquant.html#the-monty-hall-problem",
    "href": "introquant.html#the-monty-hall-problem",
    "title": "Introduction to Quantitative Reasoning",
    "section": "The Monty Hall Problem",
    "text": "The Monty Hall Problem\n\n\n\n\n\n\n\n\nAnother reason why we need science, i.e. a methodological approach to evaluating evidence, is that we are simply bad with numbers. Take the famous Monty Hall example:\nMonty Hall the host of the TV game show Let’s make a deal in which every participant was given the choice between three doors - behind two of them there was a goat while the other door hid a prize.\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe game in the show went like this:\n\n\n\n\n\n1. Every participant would choose one of three doors.\n\n\n\n2. After the participant had chosen a door, Monty Hall would show the participant a goat behind one of the doors not chosen by the participant.\n\n\n\n3. Monty would then offer the participant the option to switch doors (to the other door not initially selected).\n\n\n\n\nNow, what do you think - should you switch doors at that point or doesn’t make a difference?\nWell, in fact, you should switch doors and it does make a difference but it is really difficult for us humans to wrap our head around it. So, the chances of winning the prize actually increase when you switch (you can check it out yourselves when you go to this website which simulates many scenarios and confirms that after switching your chance of winning increases to 2 in 3.\n\n\n\n\n\n\n\nBut let’s go over why you should switch together!\n\n\n\n\n\nInitially, all three doors had a chance of 1 in 3 of hiding the prize. When you chose your door initially, you thus had a 1 in 3 chance of winning the prize while the other doors together had a 2 in 3 chance of winning.\n\n\n\nWhen Monty Hall opens one door (he will always open a door behind which there is a goat), the 2 in 3 chance concentrate on the door that is left unopened.\n\n\n\n\n\n\n\n\n\n\n\nTo make it clearer: image, Monty Hall gave you 20 doors to choose from.\n\n\n\n\n\nYou pick one (with a 1 in 20 chance of winning) while the other 19 doors combined have a 19 out of 20 chance to win.\n\n\n\nNext, Monty Hall opens 18 out of the 19 doors you have not initially selected and asks you whether you want to switch to the one unopened door.\n\n\n\n\nYou would definitely switch as, in this case, as it is more intuitive for us to see that the chances of winning concentrate on the door that Monty leaves unopened and which we did not initially select."
  },
  {
    "objectID": "introquant.html#the-birthday-conundrum",
    "href": "introquant.html#the-birthday-conundrum",
    "title": "Introduction to Quantitative Reasoning",
    "section": "The Birthday Conundrum",
    "text": "The Birthday Conundrum\nAnother interesting way to show that we are not good with numbers relates to the so-called Birthday Conundrum. The birthday conundrum deals with exponential growth and it can best be exemplified by the following scenario: imagine you are in a class consisting of 23 students.\n\n\n\n\nThink Break!\n\n\n\n\n\n\n`\n\nHow likely is it that 2 students have the same birthday?\n\n\n`\n\nIn a room of just 23 people there’s a 50-50 chance of two people having the same birthday. In a room of 75 there’s a 99.9 percent chance of two people matching. The vast majority of people will intuitively come up with much higher numbers of students that are required to find a pair of students that have the same birthday. But why is that?\nWell, we are quite good with our numeric intuition when it comes to simple addition, subtraction, and multiplication but we are really bad when it comes to exponential growth.\nLet us go back to our problem: What are the chances that two students in a class of 23 have their birthday on the same day?\nWe could list all possible couples and count all the ways they could fit but that would be really labor-intensive. In fact, it would be the same as asking What’s the chance to get one or more heads in 23-coin flips?.\nIs there an easier way to solve the coin-flip and the birthday problem?\nYes, there is. We can turn the problem on its head: instead of determining the likelihood of each and every path to get heads, we simply calculate the chance to get only tails (or all separate birthdays).\nIf there’s a 1% chance of getting only tails (it is, in fact, \\(.5^23\\) to be precise but let’s stick with 1 percent for the sake of simplicity here), there’s a 99% chance of having at least one head. We do not know if it’s only 1 head or 2 or 15 or 23: there is at least once head, and that is what matters here. If we subtract the probability of the negative of our problem from 1, we get the probability of the scenario that we are interested in.\nThe same principle applies to birthdays. Instead of finding all possibilities, we find the chance that everyone is different (the negative scenario). We then determine the probability of the positive and have the chance of at least two people having the same birthday. It can be 1 pair, or 2 or 20, but at least two people have the same birthday.\nSo let’s go over how to calculate the actual probability. If we have a class of 23 students, then there are 253 pairs (22+21+20+19+18+17+16+15+14+13+12+11+10+9+8+7+6+5+4+3+2+1).\nAn easier way to calculate this is show below.\n\\[\\begin{equation}\n\\frac{23 \\cdot 22}{2} = 253\n\\end{equation}\\]\nThe chance of 2 people having different birthdays is:\n\\[\\begin{equation}\n1 - \\frac{1}{365} = \\frac{364}{365} = .997260\n\\end{equation}\\]\nBut we need to multiply this probability with 253 - the number of possible pairs in our class.\n\\[\\begin{equation}\n(\\frac{364}{365})^{253} = .4995\n\\end{equation}\\]\nThe chance of getting a single miss, i.e. that two specific people have the same birthday, is very high (99.7260%), but when that chance is multiplied given all the possible pairs, the odds of two people having the same birthday decrease very fast. In fact, in a class of 73 people, the chance that two people have the same birthday is higher than 99%.\n\n\n\n\nEXERCISE TIME!\n\n\n\n\n\n\n`\n\nWhat is the probability of two people having the same birthday in a class of 73 students?\n\n\n\nAnswer\n\n::: {.cell}\n  ((364/365)^23)*100 \n::: {.cell-output .cell-output-stdout} [1] 93.88494 ::: :::\nThe result shows the probability of 2 students having the same birthday in a class of 73 students (in percent).\n\n\n`"
  },
  {
    "objectID": "introquant.html#fast-and-slow-thinking",
    "href": "introquant.html#fast-and-slow-thinking",
    "title": "Introduction to Quantitative Reasoning",
    "section": "Fast and Slow Thinking",
    "text": "Fast and Slow Thinking\nA similar point can be made by another example: assume that a ball and a bat together cost 1.10 AUD. The bat costs 1 AUD more than the ball. What does the ball cost?\nMost students initially think that the ball costs 10 cents but this is not the correct answer. Do you know why? Well, if the ball costs 10 cents and the bat costs 1 AUD more, then the bat would cost 1.10 AUD and both together would cost 1.20 AUD and not 1.10 AUD. The correct answer is of course 5 cents (0.05 AUD + 1.05AUD = 1.10 AUD).\nAccording to the psychologist Daniel Kahnemann, one possible explanation for this bias is that humans have two different ways of thinking: fast thinking which is very intuitive and quick and slow thinking which takes longer and is more deliberate. While fast thinking is typically a very economical way to decide, slow and deliberate is more precise but more expensive as it takes more time and effort. Science is essentially a method to approach problems, that we would normally use fast thinking to resolve, with slow and deliberate thought."
  },
  {
    "objectID": "introquant.html#randomness",
    "href": "introquant.html#randomness",
    "title": "Introduction to Quantitative Reasoning",
    "section": "Randomness",
    "text": "Randomness\nA very powerful cognitive bias that underlies superstitious beliefs or behaviors is that we are very bad at dealing with randomness. In other words, we are prone to see causes or patterns although there may be none. An experiment involving pigeon performed by the psychologist B. F. Skinner. Skinner provided pigeon at random intervals with food pallets. After a while, Skinner noticed that the pigeon exhibited unusual behaviors such as twirling around in circles or picking in a corner. According to Skinner, the behavior that pigeon performed while receiving food was positively enforced. In other words, a pigeon thought that when it performed a certain behavior, this would cause food to fall from the feeder. Something similar can be observed among athletes who stick to certain behaviors such as not shaving etc. because they didn’t shave when they last won. The underlying mechanism is that we assign a causal relationship between some behavior and a certain outcome although the behavior and the outcome may be completely unrelated.\n\n\n\n\n\nPart 1 of a video about Skinner’s experiments with Pigeons.\n\n\n\n\n\n\n\n\n\nPart 2 of a video about Skinner’s experiments with Pigeons.\n\n\n\n\nA similar cognitive bias underlies many ghost sightings. We are prone to see faces or human figures in random patterns. For example, the famous picture of a face on Mars or faces on toasts. Similarly, if you hear leaves turning over at night, it is likely that you think that someone is following you, as you assign noises rather to agents such as people rather than natural forces.\nBruce Hood, a psychologist at Cardiff University, offers a very interesting evolutionary explanation for this phenomenon. To elaborate, imagine a professor offered you 10 AUD to wear a jumper he brought along for only a minute or so. Most people would take the offer and earn the 10 AUD. However, the professor adds that the jumper belonged to a brutal psychopathic serial killer and asks again whether you would wear the jumper. While some students would still wear the jumper, some students would not wear it given this information and even students who would, state that they would feel less comfortable. The underlying mechanism is that we assume that the jumper is not merely a piece of cloth but that it has changed somehow and acquired something by having been worn by a brutal psychopathic serial killer. While this is completely natural, it is irrational as the jumper is, in fact, merely a piece of cloth. Hood explains that people often assign value to material things which goes to say that people treat objects not only as material consisting of atoms but as things that have something like an essence or a soul. The underlying mechanism, he hypothesizes, adds an evolutionary advantage as people who had this belief were less likely to get close to items or people suffering from diseases and were thus less likely to get infected. This goes to show that irrational thinking or responses can be grounded in rational behavior.\nIn addition to being generally bad with numbers and assuming agents rather than natural causes, there is another bias, called confirmation bias, which is an inbuilt hindrance to accurate knowledge. Let’s turn to another example to illustrate this. This example is called the Wason Selection Task after Peter Wason who came up with this test. Imaging you are presented with cards that have letters on one side and numbers on the other. Four of these cards are placed on a table before you. Card 1 is an A, card 2 is a K, card 3 is a 2, and the fourth card is a 7. You are told that whenever a vowel is on one side of the card, the other side is an even number. Which of these four cards do you have to turn over to determine whether this rule holds true?\n\n\n\n\n\n\nCard 1Card 2Card 3Card 4AK27\n\nWhat have you guessed? The most common answer is cards 1 and 2 while the correct answer is actually cards 1 and 4 because the rule does not say whether there are even or odd numbers behind consonants; thus, turning over cards 2 and 3 does not help you in determining whether the rule holds true or not - in fact, cards 2 and 3 are irrelevant for the problem.\nLet us now turn to another example to further illustrate cognitive bias: Imaging that I have a rule in my mind and write down 3 numbers which are generated in accordance with my rule. Your task is to find out what the rule is that I have in mind. To help you finding out about that, you are allowed to propose a fourth number and I have to answer whether the proposed number is aligned with my rule or not. After proposing a number, you may then propose what the rule is and I have to tell you whether the proposed rule is the rule I had in mind or not. The numbers I write down are 1, 2 and 4. What do you think is the rule I have in mind and which number would you propose?\n\n\n\n\n\n\nNumber 1Number 2Nuber 3Number 4124?\n\nTypically, students first propose 8 (which is in accordance with my rule) and the rule that is typically proposed first is Double the previous number. Unfortunately, this is not the rule according to which I generated the numbers. The next guess is typically 16 (which is also in accordance with my rule) and students propose the rule Square the previous number (which is again incorrect). It is only when students propose numbers that contradict their hypothesized rule that they get closer to finding the actual rule I have in mind.\nActually, the rule I have in mind is very simple as it is The current number must always be bigger than the previous number. This is to show that we intuitively test whether our hypothesized rule is correct, rather than testing whether it is false. A well-reasoned proposal would thus be numbers like 3 or 7 which conflict with the hypothesized rule rather than numbers which comply with it.\nThis goes to show that we, as humans, aim to support ideas we already have rather than testing our beliefs. Science, however, does exactly the opposite: in the scientific process, ideas, hypotheses and theories are challenged. Support for Ideas or theories comes from failed attempts to disprove them rather than from findings which support them.\nWhy have we had a look at these examples and quizzes? Basically, the intention here was to convince you that we as humans to not necessarily come to rational conclusions but that there are in-built mechanisms, which systematically lead us astray and cause us to misjudge phenomena. It is important to understand that these biases often have a rational cause but that they are (a) part of human nature and (b) that they are constantly at work and thus constantly lead us astray. And it is here where the scientific method comes in as the scientific method is simply a procedure which prevents us from coming to erroneous conclusions."
  },
  {
    "objectID": "introquant.html#the-anthropocentric-bias",
    "href": "introquant.html#the-anthropocentric-bias",
    "title": "Introduction to Quantitative Reasoning",
    "section": "The Anthropocentric Bias",
    "text": "The Anthropocentric Bias\nAnother bias that we as humans are rarely aware of is the anthropological bias. The anthropological bias refers to conceptualizing the world around us in a way that assumes that the world is just as we as humans see the world. Another name for this bias is Experiential Realism. To exemplify what this means, think about what the world would look like for a bee of we were the size of microbes. Bees see ultraviolet - a wavelength of light that we, as humans, cannot directly perceive. This means that the world looks very different for bees. many flowers have evolved to be seen by bees as they rely on bees to spread their pollen. For a bee, a summer meadow looks something like the dark night’s sky does for us: a dark blue background with islands of light that are the flower what reflect ultraviolet radiation (see below).\n\n\n\n\n\n\n\n\n\nVyvyan Evans and Melanie Green describe the anthropological bias and its origin as follows:\n\nHowever, the parts of this external reality to which we have access are largely constrained by the ecological niche we have adapted to and the nature of our embodiment. In other words, language does not directly reflect the world. Rather, it reflects our unique human construal of the world: our ‘world view’ as it appears to us through the lens of our embodiment.\n\n[1]\n\n\nThere are other examples which show that our perception of the world depends on our senses and how they function. have a look at the experiment below.\n\n\n\n\nExperiment Time!\n\n\n\n\n\n\n`\n\nLook at the dot between the red and green squares for about 30 seconds straight (try to really focus on that white dot!). Once the 30 seconds are over, immediately look at the white dot between the mirrored images of the sand dunes. What happens?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n`\n\nThe left sand dunes appear greenish while the right sand dunes look red. This is because the neuronal networks which are responsible for make use see green have run out of neurotransmitters. As the neurons cannot fire anymore, we see the sand dunes without red (which makes the dune appear green) and without green (which makes the dune appear red).\nThis example should just draw your attention to the fact that our perception of the world rests upon our senses and our brain.\nAnother factor which influences the way we perceive the world has to do with something that is called Gestalt and has been studied in subfield of psychology (Gestalt-psychology or Gestalt theory). Gestalt theory has to do with how we perceive shapes by grouping or lumping (unrelated) elements together. Consider the Figure below. Do you see the triangle?\n\n\n\n\n\n\n\n\n\nWe cannot but see a triangle, although there is no triangle. We simply fill in the blanks and groups the triangles together so that they form a Gestalt - a shape. This becomes obvious if we re-arrange the triangles because, now, you see Pacman!"
  },
  {
    "objectID": "introquant.html#patterns-and-matters-of-habit",
    "href": "introquant.html#patterns-and-matters-of-habit",
    "title": "Introduction to Quantitative Reasoning",
    "section": "Patterns and Matters of Habit",
    "text": "Patterns and Matters of Habit\nRelated to but different from the phenomena with Gestalt theory is another interesting way in which our mind manipulates the input to our senses. Have a look at the upside-down faces of Margaret Thatcher below. Does one face strike you as odd?\n\n\n\n\n\n\n\n\n\nProbably the picture of Margaret Thatcher on the right strikes you as somewhat odd but have a look at what happens if we turn the pictures around.\n\n\n\n\n\n\n\n\n\nMost people are somewhat shocked how distorted the picture on the right really is. This is because our brain compensates the skewness of the Margaret Thatcher’s face in the right picture while it does not normalize the picture when shown in the way that we normally see people. The interesting point is that our brain automatically adjusts the picture of a face based on out previous experience with faces. One could say that our perception - especially of human faces, autocorrects aspects of our visual input based on patterns and expectations build from previous input.\n\n\n\n\n\nThe autocorrection effect also plays on another bias which has to do with face recognition. Because reading and interpreting faces is and has been so important and informative for us, we are prone not only to see or find patterns in randomness but more specifically we are prone to seeing faces or constructing faces from ambiguous input. To see what I mean, have a look at the picture below.\nAlthough the picture does not show a face, we interpret the various elements in the picture to form - in combination - a face. Of course, the face is not emerging from a merely chaotic assemblies of elements but the elements in the picture are arranged to create that effect. And although the arrangement is deliberate in the picture above, it still goes to show that humans are particularly available for suggestive, ambiguous input to be interpreted as human faces. One reason for this is that the context, here understood as the combinatorial effect of various unrelated elements, influences our perception and interpretation of visual input.\nLet me clarify this with another example. What symbol do you see in the red circle?\n\n\n\n\n\n\n\n\n\nGiven that context, with the letters A and C to the left and right of the symbol, the most natural interpretation of the symbol is, of course, a B. However, if the context changes (as you can see below), the interpretation changes with it although the symbol and all its features remain the same.\n\n\n\n\n\n\n\n\n\nThis goes to show that our perception and interpretation of the world around us is not solely based on the input or the element itself but rather that categorization is context-dependent."
  },
  {
    "objectID": "introquant.html#locical-fallacies-and-biases",
    "href": "introquant.html#locical-fallacies-and-biases",
    "title": "Introduction to Quantitative Reasoning",
    "section": "Locical Fallacies and Biases",
    "text": "Locical Fallacies and Biases\nIn this section, we will have a look at things we do or ways we argue that will prevent us from finding out what is really going on. These logical fallacies are very common and noone is exempt from them - we should however, be aware of them and aim to avoid them if we do not want to arrive at wrong conclusions or make erroneous judgments.\nLogical fallacies can be defines as flawed, deceptive, or false arguments that can be proven wrong with reasoning and we will briefly inspect some of the the most common logical fallacies below.\nConfirmation Bias and Cherry Picking\nAs creatures of habit, we focus on certain things while we ignore others. This is quite helpful to orient ourselves in a highly complex world but it is unfortunately quite detrimental to the scientific endeavor. This is particularly so as we tend to ignore things or ideas that do not match our expectations. This is called confirmation bias or cherry picking and it is very common - even in science. In order to avoid this logical fallacy, rather than finding evidence that supports our assumptions and views, we need to actively look for evidence that shows that our assumptions are wrong. But while this is the better methodological approach towards finding real answers, it goes against how we intuitively operate. Unfortunately, seeking to confirm one’s view is not only human but leads to deeply misguided conclusions.\nAd Hominem\nAn ad hominem is when, instead of providing a (factual) counterargument or pointing out problems or inconsistencies in an argument, the person bringing forward the argument is attached - often using slurs such as racist, lefty, Nazi, or the like. As such, the ad hominem fallacy represents the tendency to use personal attacks rather than logic to counter an argument. Ad hominem are also sometimes referred to as mudslinging in public discourse and they are manipulative in that they guide the attention away form the arguments to the personal and emotional level without addressing core issues.\nAppeal to Authority\nAn appeal to authority is a fallacy when someone refers to an authoritative figure as a justification or in support of an argument rather than explaining what this person has argued, found out, or stated. However, this fallacy is tricky because an authority’s stance can represent evidence, but it becomes a fallacy if the person rather than what they showed is used as a justification. An authority should only be referred to as a stand-in for research or a study which that person represents or has conducted - the person itself is typically irrelevant to the topic or the argument.\nAn example of this fallacy would be if someone said X has said that Y is the case and assumes that Y is true because the authority, X, has said so. Instead, the study or research conducted by X is the cause for Y being likely true, rather than X merely saying so.\nStraw Man\nThe straw man fallacy occurs when the position of someone is misportrayed so that it is easier to argue against it and show it to be a flawed stance. The purpose of this fallacy is to make one’s own position appear superior or stronger than it actually is.\nThe straw man fallacy derives its name from actual harmless, lifeless straw men - a scarecrow - because, instead of arguing against the position an opponent is actually holding, an easily defeated puppet version of the opponent’s position is attacked that the opponent was never arguing for in the first place.\nArgument from Ignorance\nThe argument from ignorance occurs when it is argues that a proposition must be true because it has not been proven to be false or if there is no evidence against it. An example of the argument from ignorance would be if someone posed No one has ever been able to prove that extraterrestrials exist, so they must not be real or Noone knows how the world came into being, therefore God must be the cause. An appeal to ignorance doesn’t prove anything and only shifts the burden for proof away from the person making a claim.\nFalse Dichotomy\nA false dilemma or false dichotomy presents limited options — typically by focusing on two extremes — when in fact more possibilities exist. The phrase “America: Love it or leave it” is an example of a false dilemma.\nThe false dilemma fallacy is a manipulative tool designed to polarize the audience, promoting one side and demonizing another. It’s common in political discourse as a way of strong-arming the public into supporting controversial legislation or policies.\nSlippery Slope\nA slippery slope argument assumes that a certain course of action will necessarily lead to a chain of future events. The slippery slope fallacy takes a benign premise or starting point and suggests that it will lead to unlikely or ridiculous outcomes with no supporting evidence.\nYou may have used this fallacy on your parents as a teenager: “But you have to let me go to the party! If I don’t go to the party, I’ll be a loser with no friends. Next thing you know, I’ll end up alone and jobless, living in your basement when I’m 30!”\nCircular Argument\nCircular arguments occur when a person’s argument repeats what they already assumed before without arriving at a new conclusion. For example, if someone says, “According to my brain, my brain is reliable,” that’s a circular argument.\nCircular arguments often use a claim as both a premise and a conclusion. This fallacy only appears to be an argument when in fact it’s just restating one’s assumptions.\nRed Herring\nWhen it comes to fallacies, a red herring is an argument that uses confusion or distraction to shift attention away from a topic and toward a false conclusion. Red herrings usually contain an unimportant fact, idea, or event that has little relevance to the real issue. Red herrings are a very common and they are used when someone wants to shift the focus away from a topic or an argument to something that is easier or safer to address. As such, red herrings are related to shifting from rational to emotional - similar to ad hominem attacks.\nSunk Cost\nThe so-called sunk cost fallacy occurs when someone continues doing something because of the effort they have already put in - regardless of whether the additional costs outweigh the potential benefits. Sunk cost is a term borrowed from economy where it refers to expenses that can no longer be recovered. An example would be if you continue watching a TV show only because you have already watched some episodes although you are actually not enjoying watching the show - you just to go through with it so the initial costs of watching the TV shows was not “in vein”.\nFalling pray to cognitive biases or logical fallacies means that we are constantly deceiving ourselves and therefore need to protect ourselves from our own wrong judgments. To protect ourselves from these natural (systematic) delusions that have been discussed so far, we have developed skeptical or critical thinking skills. Among these skills, the hypothetic-deductive method of science is known as the scientific method . However, these skills are not just there - they have to be learned and trained! Such skeptical skills form the basis of science because science, in its essence, is the search for answers about how the world really works. And in order to avoid being led astray, we have to safeguard us from our own biases by following a methodological and careful approach. The next section will focus on the relationship between science and why it must be methodological.\n\n\n\n\nEXERCISE TIME!\n\n\n\n\n\n\n`\n\nGiven what you have learned in this chapter, can you come up with explanations for a belief in ghosts?\n\n\n\nAnswer\n\nExperiences of ghosts (or what people experience as ghosts) can be caused by many factors, for example, pareidolia which can cause people to interpret random images, or patterns of light and shadow, as faces.\n\n\nSomeone tells you that his grandfather smoked a pack of cigarettes and drank a bottle of whiskey every day and thus claims that smoking and drinking does not harm your health. What is problematic about the conclusion and why is anecdotal evidence not appropriate?\n\n\n\nAnswer\n\nWhen aiming to answer how one factor (e.g., smoking) affects another factor (e.g., developing cancer), we are typically interested in general patterns or trends rather than unusual or unrepresentative cases. As such, individual anecdotes are not useful to answer questions asking for the relationship between factors, because we do not know how common or prototypical the anecdotal case is. Even many anecdotes do not qualify as data as anecdotes are likely biased and not representative. To determine the relationship between factors, we would need systematic and unbiased observation .\n\n\n`"
  },
  {
    "objectID": "introquant.html#clever-hans",
    "href": "introquant.html#clever-hans",
    "title": "Introduction to Quantitative Reasoning",
    "section": "Clever Hans",
    "text": "Clever Hans\nSo, what is science? Well, let’s start with the example of Clever Hans to illustrate how science is applied to phenomena.\nClever Hans was horse who responded to questions requiring mathematical calculations by tapping his hoof. If asked by his master, William Von Osten, what is the sum of 3 plus 2, the horse would tap his hoof five times. It appeared the animal was responding to human language and was capable of grasping mathematical concepts. It was 1891 when Hans became public but only in 1904 it was discovered by Oskar Pfungst that the horse was responding to subtle physical cues. Yet, more than a dozen scientists observed Hans and were convinced there was no signaling or trickery. But the scientists were wrong.\n\n\n\n\n\nClever Hans.\n\n\n\n\nPfungst noted that when the correct answer was not known to anyone present, Clever Hans didn’t know it either. And when the horse couldn’t see the person who did know the answer, the horse didn’t respond correctly. This led Pfungst to conclude that the horse was getting visual cues, albeit subtle ones. It turned out that Von Osten and others were cuing Hans unconsciously by tensing their muscles until Hans produced the correct answer. The horse truly was clever, not because he understood human language but because he could perceive very subtle muscle movements.\nThe fact that it took a methodological and very careful approach to find out why the hoarse appeared to be able to do math is very telling. So, we use science because it effectively protects us from being deceived by others (which would be bad) and, more importantly, by ourselves (which is much worse). The consistent application of the scientific method not only brings insight into how the world really works but it allows us to find ways in which the world around us can then be used to our advantage (fire, wheel, agriculture, magnetism, steam engine, telephone, microwave, nuclear fusion, etc.),\nSo, what is science?\n\nScience is an unbiased, fundamentally methodological enterprise that aims at building and organizing knowledge about the empirical world in the form of falsifiable explanations and predictions by means of observation and experimentation. (MS)\n\n\nScience is the effort to understand how the universe works through the scientific method, with observable evidence as the basis of that understanding.\n\n\n\n\n\n\n\n\n\n\nWhich varieties of science are there?\n\nEmpirical Science(s) examine phenomena of reality through the scientific method (cf.), with the aim of explaining and / or predicting them (for example, functional linguistics, biology, astronomy, sociology, …).\nFormal Science(s) examine systems of abstract constructs using axiomatically set or derived rules (for example, mathematics, formal logic, theoretical computer science, system theory, chaos theory, formal linguistics, …)."
  },
  {
    "objectID": "introquant.html#popper-falsification-and-scientific-progress",
    "href": "introquant.html#popper-falsification-and-scientific-progress",
    "title": "Introduction to Quantitative Reasoning",
    "section": "Popper, falsification, and scientific progress",
    "text": "Popper, falsification, and scientific progress\n\n\n\n\n\nSir Karl Raimund Popper was a Austrian-British academic and public figure who vigorously defended liberal democracy and the principles of social criticism that he believed made a flourishing open society possible. In the context of scientific thinking, Popper is important because he is arguably the most important philosopher of science - probably best known for his realization that nothing can be proven in the empirical sciences, but that hypotheses can and need to be falsified. This means that in contrast to the formal sciences, where statements can be proven once certain axioms are accepted, any knowledge that is produced by the empirical sciences is always, and has to be preliminary which implies that the empirical sciences represent an ongoing process.\n\n\n\n\nWhat does this mean and why can empirical sciences not prove?\n\n\n\n\n\n\nWell, logically speaking, no number of observations of something can confirm a scientific theory. However, a single counterexample can be enough to show that a theory is false. Imagine someone states that all swans are white, any number of white swans cannot prove that that statement is correct, but a single black swan immediately falsifies the statement.\nIt is important to note here that to say that something is falsifiable does not mean that it is false or wrong or fake: it simply means that it can, in principle, be shown to be false by observation or by experiment.\nAccording to Popper, falsifiability is the defining criterion of what is and what is not science: a theory can only be considered scientific if, and only if, it is falsifiable. This led him to refute claims that both psychoanalysis and Marxism are scientific because these theories are not falsifiable.\nThere is an apparent progress of scientific knowledge, meaning that our understanding of the universe appears to be improving over time. In Popper’s view, this advance of scientific knowledge is an evolutionary process: competing theories are systematically subjected to attempts at falsification which leads to error elimination - so that falsification performs similar function for science that natural selection performs for biological evolution. Theories that better survive the process of refutation are not more true, but rather, more fit. Consequently, just as a species’ biological fitness does not ensure continued survival, neither does rigorous testing protect a scientific theory from refutation in the future. Yet, as it appears that the engine of biological evolution has, over many generations, produced adaptive traits equipped to deal with more and more complex problems of survival, likewise, the evolution of theories through the scientific method may progress. For Popper, it is in the interplay between the tentative theories (conjectures) and error elimination (refutation) that scientific knowledge advances - in a process very much akin to the interplay between genetic variation and natural selection."
  },
  {
    "objectID": "introquant.html#what-is-empirical-linguistics",
    "href": "introquant.html#what-is-empirical-linguistics",
    "title": "Introduction to Quantitative Reasoning",
    "section": "What is (empirical) Linguistics?",
    "text": "What is (empirical) Linguistics?\nLinguistics is commonly defined as the the scientific study of language or individual languages and linguists try to uncover the systems behind language, to describe these systems, and to theoretically explain and model them [1]. Linguistics who work empirically conduct research on linguistic phenomena based on observations of reality. As such, (empirical) linguistics is descriptive rather than prescriptive in nature.\n\n\n\nHow does the scientific method work in linguistics?\n\n\n\n\n\n\nIn empirical research typically follows the scheme described below. This scheme is also referred to as the scientific circle and we will use the example of having lost your keys to exemplify how it works.\nSo, let’s imagine you lost your key: in a first step we make an observation (keys are not here). In a second step, we ask ourselves where the keys may be (research question). In a third step, we come up with an idea where the keys may be (hypothesis). Then, we think about where we have lost the keys before (literature review). Next, we look for the keys where we expect them to be (empirical testing), then, we evaluate the result of the test (was hypothesis correct), and finally, we either have found the keys (hypothesis was correct) or not (keys are still missing) which causes us to come up with another idea and we need to go through the same steps again.\nA slightly more elaborate depiction of this scenario with the equivalent steps in the scientific circle is listed below.\n\nMake an observation (e.g., My keys are gone!)\nFormulate a research question (e.g., Where are my keys?)\nDeduce a test hypothesis (H1) based on observation (e.g. My keys are on the table next to the TV!)\nFormulate null hypothesis (H0) (e.g., My keys are not on the table next to the TV!)\nDetermining the level of significance at which the H0 is rejected\nFormulate potential results: what results are possible and what do they mean for the H0 and H1? (My keys are not on the table next to the TV!: H0 cannot be rejected, formulate new H1)\nDesign experiment/study/research (e.g., I will go over to the TV and see if my keys on the table next to the TV.)\nConduct experiment/study/research (e.g., Actually go over to the TV and see if my keys on the table next to the TV.)\nStatistical analysis\nInterpretation of the results (e.g., My keys are not on the table next to the TV so I must have lost them elsewhere!)\nIn case H0 could not be rejected: Formulate new H1. (e.g., My keys are on the kitchen table!)\n\n\n\n\n\nThink Break!\n\n\n\n\n\n\n`\n\nApply the scientific circle to a study of the existence of the Loch Ness monster.\nYou want to investigate whether the speech of young or old people is more fluent: how could you go about testing this?\n\n\n`\n\nWe will stop here with our introduction to quantitative reasoning. If you are interested in learning more, we highly recommend that you continue with our tutorial on basic concepts in quantitative research."
  },
  {
    "objectID": "introreg.html#example-1-preposition-use-across-real-time",
    "href": "introreg.html#example-1-preposition-use-across-real-time",
    "title": "Introduction to Regression Analysis",
    "section": "Example 1: Preposition Use across Real-Time",
    "text": "Example 1: Preposition Use across Real-Time\nWe will now turn to our first example. In this example, we will investigate whether the frequency of prepositions has changed from Middle English to Late Modern English. The reasoning behind this example is that Old English was highly synthetic compared with Present-Day English which comparatively analytic. In other words, while Old English speakers used case to indicate syntactic relations, speakers of Present-Day English use word order and prepositions to indicate syntactic relationships. This means that the loss of case had to be compensated by different strategies and maybe these strategies continued to develop and increase in frequency even after the change from synthetic to analytic had been mostly accomplished. And this prolonged change in compensatory strategies is what this example will focus on.\nThe analysis is based on data extracted from the Penn Corpora of Historical English (see http://www.ling.upenn.edu/hist-corpora/), that consists of 603 texts written between 1125 and 1900. In preparation of this example, all elements that were part-of-speech tagged as prepositions were extracted from the PennCorpora.\nThen, the relative frequencies (per 1,000 words) of prepositions per text were calculated. This frequency of prepositions per 1,000 words represents our dependent variable. In a next step, the date when each letter had been written was extracted. The resulting two vectors were combined into a table which thus contained for each text, when it was written (independent variable) and its relative frequency of prepositions (dependent or outcome variable).\nA regression analysis will follow the steps described below:\n\nExtraction and processing of the data\nData visualization\nApplying the regression analysis to the data\nDiagnosing the regression model and checking whether or not basic model assumptions have been violated.\n\nIn a first step, we load functions that we may need (which in this case is a function that we will use to summarize the results of the analysis).\n\n# load functions\nsource(\"https://slcladal.github.io/rscripts/slrsummary.r\")\n\nAfter preparing our session, we can now load and inspect the data to get a first impression of its properties.\n\n# load data\nslrdata  <- base::readRDS(url(\"https://slcladal.github.io/data/sld.rda\", \"rb\"))\n\n\n\n\n\n\nFirst 15 rows of slrdata.\n\n\nDateGenreTextPrepositionsRegion1,736Sciencealbin166.01North1,711Educationanon139.86North1,808PrivateLetterausten130.78North1,878Educationbain151.29North1,743Educationbarclay145.72North1,908Educationbenson120.77North1,906Diarybenson119.17North1,897Philosophyboethja132.96North1,785Philosophyboethri130.49North1,776Diaryboswell135.94North1,905Travelbradley154.20North1,711Educationbrightland149.14North1,762Sermonburton159.71North1,726Sermonbutler157.49North1,835PrivateLettercarlyle124.16North\n\n\nInspecting the data is very important because it can happen that a data set may not load completely or that variables which should be numeric have been converted to character variables. If unchecked, then such issues could go unnoticed and cause trouble.\nWe will now plot the data to get a better understanding of what the data looks like.\n\np1 <- ggplot(slrdata, aes(Date, Prepositions)) +\n  geom_point() +\n  theme_bw() +\n  labs(x = \"Year\") +\n  labs(y = \"Prepositions per 1,000 words\") +\n  geom_smooth()\np2 <- ggplot(slrdata, aes(Date, Prepositions)) +\n  geom_point() +\n  theme_bw() +\n  labs(x = \"Year\") +\n  labs(y = \"Prepositions per 1,000 words\") +\n  geom_smooth(method = \"lm\") # with linear model smoothing!\n# display plots\nggpubr::ggarrange(p1, p2, ncol = 2, nrow = 1)\n\n\n\n\nBefore beginning with the regression analysis, we will center the Date variable using the scale function. We center the values of Date by subtracting each value from the mean of Date. This can be useful when dealing with numeric variables because if we did not center Date, we would get estimated values for year 0 (a year when English did not even exist yet). If a variable is centered, the regression provides estimates of the model refer to the mean of that numeric variable. In other words, centering can be very helpful, especially with respect to the interpretation of the results that regression models report.\n\n# center date\nslrdata %>%\n  dplyr::mutate(Date = scale(Date)) -> slrdata \n\nWe will now begin the regression analysis by generating a first regression model and inspect its results.\n\n# create initial model\nm1.lm <- lm(Prepositions ~ Date, data = slrdata)\n# inspect results\nsummary(m1.lm)\n\n\nCall:\nlm(formula = Prepositions ~ Date, data = slrdata)\n\nResiduals:\n        Min          1Q      Median          3Q         Max \n-69.1012471 -13.8549421   0.5779091  13.3208913  62.8580401 \n\nCoefficients:\n                 Estimate    Std. Error   t value             Pr(>|t|)    \n(Intercept) 132.190093110   0.838637480 157.62483 < 0.0000000000000002 ***\nDate          2.000732730   0.839419427   2.38347             0.017498 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 19.4339648 on 535 degrees of freedom\nMultiple R-squared:  0.010507008,   Adjusted R-squared:  0.00865748837 \nF-statistic: 5.68093894 on 1 and 535 DF,  p-value: 0.017498081\n\n\n\nInterpreting Output\nThe summary output starts by repeating the regression equation. Then, the model provides the distribution of the residuals. The residuals should be distributed normally with the absolute values of the Min and Max as well as the 1Q (first quartile) and 3Q (third quartile) being similar or ideally identical. In our case, the values are very similar which suggests that the residuals are distributed evenly and follow a normal distribution. The next part of the report is the coefficients table. The estimate for the intercept is the value of y at x = 0. The estimate for Date represents the slope of the regression line and tells us that with each year, the predicted frequency of prepositions increase by .01732 prepositions. The t-value is the Estimate divided by the standard error (Std. Error). Based on the t-value, the p-value can be calculated manually as shown below.\n\n# use pt function (which uses t-values and the degrees of freedom)\n2*pt(-2.383, nrow(slrdata)-1)\n\n[1] 0.0175196401501\n\n\nThe R2-values tell us how much variance is explained by our model. The baseline value represents a model that uses merely the mean. 0.0105 means that our model explains only 1.05 percent of the variance (0.010 x 100) - which is a tiny amount. The problem of the multiple R2 is that it will increase even if we add variables that explain almost no variance. Hence, multiple R2 encourages the inclusion of junk variables.\n\\[\\begin{equation}\nR^2 = R^2_{multiple} = 1 - \\frac{\\sum (y_i - \\hat{y_i})^2}{\\sum (y_i - \\bar y)^2}\n\\end{equation}\\]\nThe adjusted R2-value takes the number of predictors into account and, thus, the adjusted R2 will always be lower than the multiple R2. This is so because the adjusted R2 penalizes models for having predictors. The equation for the adjusted R2 below shows that the amount of variance that is explained by all the variables in the model (the top part of the fraction) must outweigh the inclusion of the number of variables (k) (lower part of the fraction). Thus, the adjusted R2 will decrease when variables are added that explain little or even no variance while it will increase if variables are added that explain a lot of variance.\n\\[\\begin{equation}\nR^2_{adjusted} = 1 - (\\frac{(1 - R^2)(n - 1)}{n - k - 1})\n\\end{equation}\\]\nIf there is a big difference between the two R2-values, then the model contains (many) predictors that do not explain much variance which is not good. The F-statistic and the associated p-value tell us that the model, despite explaining almost no variance, is still significantly better than an intercept-only base-line model (or using the overall mean to predict the frequency of prepositions per text).\nWe can test this and also see where the F-values comes from by comparing the fit of the baseline or null model which only used the overall mean as the sole predictor with the fit of our regression model with used Date as a predictor for preposition use.\n\n# create intercept-only base-line model\nm0.lm <- lm(Prepositions ~ 1, data = slrdata)\n# compare the base-line and the more saturated model\nanova(m1.lm, m0.lm, test = \"F\")\n\nAnalysis of Variance Table\n\nModel 1: Prepositions ~ Date\nModel 2: Prepositions ~ 1\n  Res.Df         RSS Df   Sum of Sq       F   Pr(>F)  \n1    535 202058.2576                                  \n2    536 204203.8289 -1 -2145.57126 5.68094 0.017498 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe F- and p-values are exactly those reported by the summary which shows where the F-values comes from and what it means; namely it denote the difference between the base-line and the more saturated model.\nThe degrees of freedom associated with the residual standard error are the number of cases in the model minus the number of predictors (including the intercept). The residual standard error is square root of the sum of the squared residuals of the model divided by the degrees of freedom. Have a look at he following to clear this up:\n\n# DF = N - number of predictors (including intercept)\nDegreesOfFreedom <- nrow(slrdata)-length(coef(m1.lm))\n# sum of the squared residuals\nSumSquaredResiduals <- sum(resid(m1.lm)^2)\n# Residual Standard Error\nsqrt(SumSquaredResiduals/DegreesOfFreedom); DegreesOfFreedom\n\n[1] 19.4339647585\n\n\n[1] 535\n\n\n\n\nModel Diagnostics\nWe will now check if mathematical assumptions have been violated (homogeneity of variance) or whether the data contains outliers. We check this using diagnostic plots.\n\n# generate data\ndf2 <- data.frame(id = 1:length(resid(m1.lm)),\n                 residuals = resid(m1.lm),\n                 standard = rstandard(m1.lm),\n                 studend = rstudent(m1.lm))\n# generate plots\np1 <- ggplot(df2, aes(x = id, y = residuals)) + \n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +\n  geom_point() +\n  labs(y = \"Residuals\", x = \"Index\")\np2 <- ggplot(df2, aes(x = id, y = standard)) + \n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +\n  geom_point() +\n  labs(y = \"Standardized Residuals\", x = \"Index\")\np3 <- ggplot(df2, aes(x = id, y = studend)) + \n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +\n  geom_point() +\n  labs(y = \"Studentized Residuals\", x = \"Index\")\n# display plots\nggpubr::ggarrange(p1, p2, p3, ncol = 3, nrow = 1)\n\n\n\n\nThe left graph shows the residuals of the model (i.e., the differences between the observed and the values predicted by the regression model). The problem with this plot is that the residuals are not standardized and so they cannot be compared to the residuals of other models. To remedy this deficiency, residuals are normalized by dividing the residuals by their standard deviation. Then, the normalized residuals can be plotted against the observed values (center panel). In this way, not only are standardized residuals obtained, but the values of the residuals are transformed into z-values, and one can use the z-distribution to find problematic data points. There are three rules of thumb regarding finding problematic data points through standardized residuals [6]:\n\nPoints with values higher than 3.29 should be removed from the data.\nIf more than 1% of the data points have values higher than 2.58, then the error rate of our model is too high.\nIf more than 5% of the data points have values greater than 1.96, then the error rate of our model is too high.\n\nThe right panel shows the * studentized residuals* (adjusted predicted values: each data point is divided by the standard error of the residuals). In this way, it is possible to use Student’s t-distribution to diagnose our model.\nAdjusted predicted values are residuals of a special kind: the model is calculated without a data point and then used to predict this data point. The difference between the observed data point and its predicted value is then called the adjusted predicted value. In summary, studentized residuals are very useful because they allow us to identify influential data points.\nThe plots show that there are two potentially problematic data points (the top-most and bottom-most point). These two points are clearly different from the other data points and may therefore be outliers. We will test later if these points need to be removed.\nWe will now generate more diagnostic plots.\n\n# generate plots\nautoplot(m1.lm) + \n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) \n\n\n\n\nThe diagnostic plots are very positive and we will go through why this is so for each panel. The graph in the upper left panel is useful for finding outliers or for determining the correlation between residuals and predicted values: when a trend becomes visible in the line or points (e.g., a rising trend or a zigzag line), then this would indicate that the model would be problematic (in such cases, it can help to remove data points that are too influential (outliers)).\nThe graphic in the upper right panel indicates whether the residuals are normally distributed (which is desirable) or whether the residuals do not follow a normal distribution. If the points lie on the line, the residuals follow a normal distribution. For example, if the points are not on the line at the top and bottom, it shows that the model does not predict small and large values well and that it therefore does not have a good fit.\nThe graphic in the lower left panel provides information about homoscedasticity. Homoscedasticity means that the variance of the residuals remains constant and does not correlate with any independent variable. In unproblematic cases, the graphic shows a flat line. If there is a trend in the line, we are dealing with heteroscedasticity, that is, a correlation between independent variables and the residuals, which is very problematic for regressions.\nThe graph in the lower right panel shows problematic influential data points that disproportionately affect the regression (this would be problematic). If such influential data points are present, they should be either weighted (one could generate a robust rather than a simple linear regression) or they must be removed. The graph displays Cook’s distance, which shows how the regression changes when a model without this data point is calculated. The cook distance thus shows the influence a data point has on the regression as a whole. Data points that have a Cook’s distance value greater than 1 are problematic [6].\nThe so-called leverage is also a measure that indicates how strongly a data point affects the accuracy of the regression. Leverage values range between 0 (no influence) and 1 (strong influence: suboptimal!). To test whether a specific data point has a high leverage value, we calculate a cut-off point that indicates whether the leverage is too strong or still acceptable. The following two formulas are used for this:\n\\[\\begin{equation}\nLeverage = \\frac{3(k + 1)}{n} |  \\frac{2(k + 1)}{n}\n\\end{equation}\\]\nWe will look more closely at leverage in the context of multiple linear regression. We can thus turn to summarized our regression analysis.\nModel Diagnostics Made Easy\nA very handy way to inspect the quality of regression models is offered by the performance package as the check_model function in the performance package extracts and provides summaries of model diagnostics.\n\n# create summary table\nperformance::check_model(m1.lm)  \n\n\n\n\nUsing the performance package to inspect model diagnostics is really easy and neat. However, extracting the diagnostics using the check_model function is not always advisable, for instance, when dealing with only a categorical (instead of a numeric predictor). The decision what diagnostics to inspect and how to interpret them thus depends on the model and what the diagnostics are supposed to check. If you are unsure, I suggest you extract all diagnostics and see if there are apparent issues (indicated by high values or deviations from the expected values).\nIn addition, we can use the performance package to validate(or inspect) the performance of the model.\n\n# create summary table\nperformance::model_performance(m1.lm)  \n\n# Indices of model performance\n\nAIC      |      BIC |    R2 | R2 (adj.) |   RMSE |  Sigma\n---------------------------------------------------------\n4714.518 | 4727.376 | 0.011 |     0.009 | 19.398 | 19.434\n\n\n\n\nSummarizing Results\nCommonly, the results of regression analyses are summarized in tabular format and additionally described in prose. We therefore start the write-up with a tabular summary summarizing the results of the regression analysis in a table.\n\n# create summary table\nslrsummary(m1.lm)  \n\n\n\n\n\n\nResults of a simple linear regression analysis.\n\n\nParametersEstimatePearson's rStd. Errort valuePr(>|t|)P-value sig.(Intercept)132.190.84157.620p < .001***Date20.10.842.380.0175p < .05*Model statisticsValueNumber of cases in model537Residual standard error on 535 DF19.43Multiple R-squared0.0105Adjusted R-squared0.0087F-statistic (1, 535)5.68Model p-value0.0175\n\n\nAn alternative but less informative summary table of the results of a regression analysis can be generated using the tab_model function from the sjPlot package [13] (as is shown below).\n\n# generate summary table\nsjPlot::tab_model(m1.lm) \n\n\n\n\n \nPrepositions\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n132.19\n130.54 – 133.84\n<0.001\n\n\nDate\n2.00\n0.35 – 3.65\n0.017\n\n\nObservations\n537\n\n\nR2 / R2 adjusted\n0.011 / 0.009\n\n\n\n\n\n\n\nTypically, the results of regression analyses are presented in such tables as they include all important measures of model quality and significance, as well as the magnitude of the effects.\nIn addition, the results of simple linear regressions should be summarized in writing.\nWe can use the reports package [14] to summarize the analysis.\n\nreport::report(m1.lm)\n\nWe fitted a linear model (estimated using OLS) to predict Prepositions with Date (formula: Prepositions ~ Date). The model explains a statistically significant and very weak proportion of variance (R2 = 0.01, F(1, 535) = 5.68, p = 0.017, adj. R2 = 8.66e-03). The model's intercept, corresponding to Date = 0, is at 132.19 (95% CI [130.54, 133.84], t(535) = 157.62, p < .001). Within this model:\n\n  - The effect of Date is statistically significant and positive (beta = 2.00, 95% CI [0.35, 3.65], t(535) = 2.38, p = 0.017; Std. beta = 0.10, 95% CI [0.02, 0.19])\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using the Wald approximation.\n\n\nWe can use this output to write up a final report:\nA simple linear regression has been fitted to the data. A visual assessment of the model diagnostic graphics did not indicate any problematic or disproportionately influential data points (outliers) and performed significantly better compared to an intercept-only base line model but only explained .87 percent of the variance (adjusted R2: .0087, F-statistic (1, 535): 5,68, p-value: 0.0175*). The final minimal adequate linear regression model is based on 537 data points and confirms a significant and positive correlation between the year in which the text was written and the relative frequency of prepositions (coefficient estimate: .02 (standardized : 0.10, 95% CI [0.02, 0.19]), SE: 0.01, t-value535: 2.38, p-value: .0175*). Standardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using the Wald approximation."
  },
  {
    "objectID": "introreg.html#example-2-teaching-styles",
    "href": "introreg.html#example-2-teaching-styles",
    "title": "Introduction to Regression Analysis",
    "section": "Example 2: Teaching Styles",
    "text": "Example 2: Teaching Styles\nIn the previous example, we dealt with two numeric variables, while the following example deals with a categorical independent variable and a numeric dependent variable. The ability for regressions to handle very different types of variables makes regressions a widely used and robust method of analysis.\nIn this example, we are dealing with two groups of students that have been randomly assigned to be exposed to different teaching methods. Both groups undergo a language learning test after the lesson with a maximum score of 20 points.\nThe question that we will try to answer is whether the students in group A have performed significantly better than those in group B which would indicate that the teaching method to which group A was exposed works better than the teaching method to which group B was exposed.\nLet’s move on to implementing the regression in R. In a first step, we load the data set and inspect its structure.\n\n# load data\nslrdata2  <- base::readRDS(url(\"https://slcladal.github.io/data/sgd.rda\", \"rb\"))\n\n\n\n\n\n\nFirst 15 rows of the slrdata2 data.\n\n\nGroupScoreA15A12A11A18A15A15A9A19A14A13A11A12A18A15A16\n\n\nNow, we graphically display the data. In this case, a boxplot represents a good way to visualize the data.\n\n# extract means\nslrdata2 %>%\n  dplyr::group_by(Group) %>%\n  dplyr::mutate(Mean = round(mean(Score), 1), SD = round(sd(Score), 1)) %>%\n  ggplot(aes(Group, Score)) + \n  geom_boxplot(fill=c(\"orange\", \"darkgray\")) +\n  geom_text(aes(label = paste(\"M = \", Mean, sep = \"\"), y = 1)) +\n  geom_text(aes(label = paste(\"SD = \", SD, sep = \"\"), y = 0)) +\n  theme_bw(base_size = 15) +\n  labs(x = \"Group\") +                      \n  labs(y = \"Test score (Points)\", cex = .75) +   \n  coord_cartesian(ylim = c(0, 20)) +  \n  guides(fill = FALSE)                \n\n\n\n\nThe data indicate that group A did significantly better than group B. We will test this impression by generating the regression model and creating the model and extracting the model summary.\n\n# generate regression model\nm2.lm <- lm(Score ~ Group, data = slrdata2) \n# inspect results\nsummary(m2.lm)                             \n\n\nCall:\nlm(formula = Score ~ Group, data = slrdata2)\n\nResiduals:\n        Min          1Q      Median          3Q         Max \n-6.76666667 -1.93333333  0.15000000  2.06666667  6.23333333 \n\nCoefficients:\n                Estimate   Std. Error  t value               Pr(>|t|)    \n(Intercept) 14.933333333  0.534571121 27.93517 < 0.000000000000000222 ***\nGroupB      -3.166666667  0.755997730 -4.18873            0.000096692 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.92796662 on 58 degrees of freedom\nMultiple R-squared:  0.232249929,   Adjusted R-squared:  0.219012859 \nF-statistic:  17.545418 on 1 and 58 DF,  p-value: 0.0000966923559\n\n\nThe model summary reports that Group A performed significantly better compared with Group B. This is shown by the fact that the p-value (the value in the column with the header (Pr(>|t|)) is smaller than .001 as indicated by the three * after the p-values). Also, the negative Estimate for Group B indicates that Group B has lower scores than Group A. We will now generate the diagnostic graphics.3\n\npar(mfrow = c(1, 3))        # plot window: 1 plot/row, 3 plots/column\nplot(resid(m2.lm))     # generate diagnostic plot\nplot(rstandard(m2.lm)) # generate diagnostic plot\nplot(rstudent(m2.lm)); par(mfrow = c(1, 1))  # restore normal plot window\n\n\n\n\nThe graphics do not indicate outliers or other issues, so we can continue with more diagnostic graphics.\n\npar(mfrow = c(2, 2)) # generate a plot window with 2x2 panels\nplot(m2.lm); par(mfrow = c(1, 1)) # restore normal plot window\n\n\n\n\nThese graphics also show no problems which is why we can now summarize the results.\n\n# tabulate results\nsjPlot::tab_model(m2.lm)\n\n\n\n\n \nScore\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n14.93\n13.86 – 16.00\n<0.001\n\n\nGroup [B]\n-3.17\n-4.68 – -1.65\n<0.001\n\n\nObservations\n60\n\n\nR2 / R2 adjusted\n0.232 / 0.219\n\n\n\n\n\n\nWe can use the reports package [14] to summarize the analysis.\n\nreport::report(m2.lm)\n\nWe fitted a linear model (estimated using OLS) to predict Score with Group (formula: Score ~ Group). The model explains a statistically significant and moderate proportion of variance (R2 = 0.23, F(1, 58) = 17.55, p < .001, adj. R2 = 0.22). The model's intercept, corresponding to Group = A, is at 14.93 (95% CI [13.86, 16.00], t(58) = 27.94, p < .001). Within this model:\n\n  - The effect of Group [B] is statistically significant and negative (beta = -3.17, 95% CI [-4.68, -1.65], t(58) = -4.19, p < .001; Std. beta = -0.96, 95% CI [-1.41, -0.50])\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using the Wald approximation.\n\n\nWe can use this output to write up a final report:\nA simple linear regression was fitted to the data. A visual assessment of the model diagnostics did not indicate any problematic or disproportionately influential data points (outliers). The final linear regression model is based on 60 data points, performed significantly better than an intercept-only base line model (F (1, 58): 17.55, p-value <. 001\\(***\\)), and reported that the model explained 21.9 percent of variance which confirmed a good model fit. According to this final model, group A scored significantly better on the language learning test than group B (coefficient: -3.17, 95% CI [-4.68, -1.65], Std. : -0.96, 95% CI [-1.41, -0.50], SE: 0.48, t-value58: -4.19, p-value <. 001\\(***\\)). Standardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using the Wald approximation.\nThat’s it for this tutorial. We hope that you have enjoyed this tutorial and learned how to perform regression analysis including model fitting and model diagnostics as well as reporting regression results."
  },
  {
    "objectID": "introta.html#word",
    "href": "introta.html#word",
    "title": "Introduction to Text Analysis",
    "section": "Word",
    "text": "Word\nWhat a word is is actually very tricky. For instance, How many words are there in this sentence?\n\nThe cat sat on the mat One answer is that there are six words; that is, there are six groups of characters which are separated according to typographical convention. But there is another answer: There are five words, that is five distinct sequences of characters and one of those sequences (the) occurs twice. The terms commonly used to make this distinction are type and token. Tokens are instances of types, therefore if we count tokens, we count without considering repetition, while if we count types, we do consider repetition. In our example, there are five types (the, cat, sat, on, mat) but six tokens, because there are two tokens of one of the types (the).\n\nThere is a further distinction we may need to make which we can see if we consider another question: are cat and cats the same word? They are distinct types, and therefore must also be distinct as tokens. But we have an intuition that at some level they are related, that there is some more abstract item which underlies both of them. This concept is usually referred to as a lemma."
  },
  {
    "objectID": "introta.html#concordancing",
    "href": "introta.html#concordancing",
    "title": "Introduction to Text Analysis",
    "section": "Concordancing",
    "text": "Concordancing\nIn Text Analysis, concordancing refers to the extraction of words from a given text or texts [4]. Commonly, concordances are displayed in the form of key-word in contexts (KWIC) where the search term is shown with some preceding and following context. Thus, such displays are referred to as key word in context concordances. A more elaborate tutorial on how to perform concordancing with R is available here. If you do not want to use coding to extract concordances, a highly recommendable tool for extracting concordances (and many other TA tasks) is AntConc.\nConcordancing is helpful for seeing how the term is used in the data, for inspecting how often a given word occurs in a text or a collection of texts, for extracting examples, and it also represents a basic procedure and often the first step in more sophisticated analyses of language data."
  },
  {
    "objectID": "introta.html#corpus-pl.-corpora",
    "href": "introta.html#corpus-pl.-corpora",
    "title": "Introduction to Text Analysis",
    "section": "Corpus (pl. Corpora)",
    "text": "Corpus (pl. Corpora)\nA corpus is a machine readable and electronically stored collection of natural language texts representing writing or speech chosen to be characteristic of a variety or the state of a language [see 5]. Corpora are great for extracting examples of natural examples and testing research hypotheses as it is easy to obtain information on frequencies, grammatical patterns, or collocations and they are commonly publicly available so the research results can be contrasted, compared and repeated.\nThere are four main types of corpora:\n\nMonitor corpora: large collections of texts from different genres/modes that aim at representing a language or language variety, e.g., International Corpus of English (ICE), Corpus of Contemporary Corpus of American English (COCA), that are, e.g., used to analyse the use of certain linguistic phenomena or to investigate collocations of certain words/topics\nLearner corpora: Contain data from language learners - these can be either L1 learners, e.g., Child Language Data Exchange System (CHILDES), and/or L2 learners, e.g., the International Corpus of Learner English (ICLE)) - to study, e.g., how L1 and/or L2 speakers learn/acquire (aspects of) a language and to see how learners differ from native speakers.\nHistorical or diachronic corpora: Contain data from different points in time that allow to analyse the development of a language or language variety (e.g., Penn Parsed Corpora of Historical English,The Helsinki Corpus of English Texts) to study, e.g., how language changes or how genres develop over time.\nSpecialized corpora: Contain data representing a specific genre/text type (e.g., British Academic Written English Corpus (BAWE)) to study, e.g., (linguistic) features of a genre (e.g. academic writing) or language in class rooms."
  },
  {
    "objectID": "introta.html#collocations",
    "href": "introta.html#collocations",
    "title": "Introduction to Text Analysis",
    "section": "Collocations",
    "text": "Collocations\nCollocations are words that are attracted to each other (and that co-occur or co-locate together), e.g., Merry Christmas, Good Morning, No worries, or Fuck off. Collocations are important because any word in any given language has collocations, i.e., others words that are attracted to that word or words that that word is attracted to allow us to anticipate what word comes next and collocations are context/text type specific. It is important to note that collocations to not have to appear/occur right next to each other but that other words can be in between. There are various different statistical measures are used to define the strength of the collocations, like the Mutual Information (MI) score and log-likelihood (see here for an over view of different association strengths measures)."
  },
  {
    "objectID": "introta.html#document-classification",
    "href": "introta.html#document-classification",
    "title": "Introduction to Text Analysis",
    "section": "Document Classification",
    "text": "Document Classification\nDocument or Text Classification (also referred to as text categorization) generally refers to process of grouping texts or documents based on similarity. This similarity can be based on word frequencies or other linguistics features but also on text external features such as genre labels or polarity scores."
  },
  {
    "objectID": "introta.html#document-term-matrix",
    "href": "introta.html#document-term-matrix",
    "title": "Introduction to Text Analysis",
    "section": "Document-Term Matrix",
    "text": "Document-Term Matrix\nDocument-Term Matrices (DTM) and Term- Document Matrices (TDM) contain the frequencies of words per document. DTM and TDM differ in whether the words or the documents are represented as rows. Thus, the words (terms) are listed as row names and the documents represent the column names while the matrix itself contains the frequencies of the words in the documents."
  },
  {
    "objectID": "introta.html#frequency-analysis",
    "href": "introta.html#frequency-analysis",
    "title": "Introduction to Text Analysis",
    "section": "Frequency Analysis",
    "text": "Frequency Analysis\nFrequency Analysis is a suit of methods which extract and compare frequencies of different words (tokens and/or types), collocations, phrases, sentences, etc. These frequencies are the often tabulated to show lists of words, phrases, etc. descending by frequency, visualized to show distributions, and/or compared and analyzed statistically to find differences between texts or collections fo texts."
  },
  {
    "objectID": "introta.html#keyword-analysis",
    "href": "introta.html#keyword-analysis",
    "title": "Introduction to Text Analysis",
    "section": "Keyword Analysis",
    "text": "Keyword Analysis\nKeyword Analysis refers to a suit of methods that allow to detect words that are characteristic of on text or collection of texts compared to another text/collection of texts. There are various keyness measures such as Log-Likelihood or the term frequency–inverse document frequency (tf-idf)."
  },
  {
    "objectID": "introta.html#lemma-lemmatization",
    "href": "introta.html#lemma-lemmatization",
    "title": "Introduction to Text Analysis",
    "section": "Lemma (Lemmatization)",
    "text": "Lemma (Lemmatization)\nLemma refers to the base form of a word (example: walk, walked, and walking are word forms of the lemma WALK). Lemmatization refers to a annotation process in which word forms are associated with their base form (lemma). Lemmatization is a very common and sometimes useful processing step for further analyses. In contrast to stemming - which is a related process - lemmatization also takes into account semantic differences (differences in the word meaning), while stemming only takes the orthography of words into consideration."
  },
  {
    "objectID": "introta.html#n-gram",
    "href": "introta.html#n-gram",
    "title": "Introduction to Text Analysis",
    "section": "N-Gram",
    "text": "N-Gram\nN-grams are combinations/sequences of words, e.g. the sentence I really like pizza! has the bi-grams (2-grams): I really, really like, and like pizza and the tri-grams (3-grams) I really like and really like pizza. N-grams play an important part in natural language processing (e.g. part-of-speech tagging), language learning, psycholinguistics models of language production, and genre analysis."
  },
  {
    "objectID": "introta.html#natural-language-processing",
    "href": "introta.html#natural-language-processing",
    "title": "Introduction to Text Analysis",
    "section": "Natural Language Processing",
    "text": "Natural Language Processing\nNatural Language Processing (NLP) is an interdisciplinary field in computer science that has specialized on processing natural language data using computational and mathematical methods. Many methods used in Text Analysis have been developed in NLP."
  },
  {
    "objectID": "introta.html#network-analysis",
    "href": "introta.html#network-analysis",
    "title": "Introduction to Text Analysis",
    "section": "Network Analysis",
    "text": "Network Analysis\nNetwork Analysis is the most common way to visualize relationships between entities. Networks, also called graphs, consist of nodes (typically represented as dots) and edges (typically represented as lines) and they can be directed or undirected networks.\nIn directed networks, the direction of edges is captured. For instance, the exports of countries. In such cases the lines are directed and typically have arrows to indicate direction. The thickness of lines can also be utilized to encode information such as frequency of contact."
  },
  {
    "objectID": "introta.html#part-of-speech-tagging",
    "href": "introta.html#part-of-speech-tagging",
    "title": "Introduction to Text Analysis",
    "section": "Part-of-Speech Tagging",
    "text": "Part-of-Speech Tagging\nPart-of-Speech (PoS) Tagging identifies the word classes of words (e.g., noun, adjective, verb, etc.) in a text and adds part-of-speech tags to each word. There are various part-of-speech tagsets, e.g. the Penn Treebank is the most frequently used tagset used for English. A more detailed tutorial on how to perform part-of-speech tagging in R can be found here."
  },
  {
    "objectID": "introta.html#project-gutenberg",
    "href": "introta.html#project-gutenberg",
    "title": "Introduction to Text Analysis",
    "section": "Project Gutenberg",
    "text": "Project Gutenberg\nThe Project Gutenberg is a excellent resource for accessing digitized literary texts. The Project Gutenberg library contains over 60,000 ebooks that are out of copyright in the US. A tutorial on how to download texts form the Project Gutenberg library using the GutenbergR package can be found here."
  },
  {
    "objectID": "introta.html#regular-expression",
    "href": "introta.html#regular-expression",
    "title": "Introduction to Text Analysis",
    "section": "Regular Expression",
    "text": "Regular Expression\nRegular Expressions - often simply referred to as regex - are symbols or sequence of symbols utilized to search for patterns in textual data. Regular Expressions are very useful and widely used in Text Analysis and often different programming languages will have very similar but slightly different Regular Expressions. A tutorial on how to use regular expression in R can be found here and here is a link to a regex in R cheat sheet."
  },
  {
    "objectID": "introta.html#semantic-analysis",
    "href": "introta.html#semantic-analysis",
    "title": "Introduction to Text Analysis",
    "section": "Semantic Analysis",
    "text": "Semantic Analysis\nSemantic Analysis refers to a suit of methods that allow to analyze the semantic (semantics) fo texts. Such analyses often rely on semantic tagsets that are based on word meaning or meaning families/categories. Two examples of such semantic tagsets are the URCEL tagset and the Historical Thesaurus Semantic Tagger [6] developed at the University of Glasgow."
  },
  {
    "objectID": "introta.html#sentiment-analysis",
    "href": "introta.html#sentiment-analysis",
    "title": "Introduction to Text Analysis",
    "section": "Sentiment Analysis",
    "text": "Sentiment Analysis\nSentiment Analysis is a computational approach to determine if words or texts are associated with (positive or negative) polarity or emotions.Commonly, sentiments analyses are based on sentiment dictionaries (words are annotated based on whether they occur in a list of words associated with, e.g., positive polarity or emotion, e.g., fear, anger, or joy. A tutorial on how to perform sentiment analysis in R can be found here."
  },
  {
    "objectID": "introta.html#string",
    "href": "introta.html#string",
    "title": "Introduction to Text Analysis",
    "section": "String",
    "text": "String\nIn computational approaches, a string is a specific type of data that represents text and is often encoded in specific format, e.g., Latin1 or UTF8. Strings may also be present in other data types such as lists or data frames. A tutorial on how to work with strings in R can be found here."
  },
  {
    "objectID": "introta.html#term-frequencyinverse-document-frequency-tf-idf",
    "href": "introta.html#term-frequencyinverse-document-frequency-tf-idf",
    "title": "Introduction to Text Analysis",
    "section": "Term Frequency–Inverse Document Frequency (tf-idf)",
    "text": "Term Frequency–Inverse Document Frequency (tf-idf)\nTerm Frequency–Inverse Document Frequency is a statistical measure of keyness which reflects how characteristic a word is of a specific text. Term Frequency–Inverse Document Frequency is based on the frequencies of words in a text compared to the frequency of documents in which it occurs"
  },
  {
    "objectID": "introta.html#topic-modeling",
    "href": "introta.html#topic-modeling",
    "title": "Introduction to Text Analysis",
    "section": "Topic Modeling",
    "text": "Topic Modeling\nTopic modelling is a machine learning method seeks to answer the question: given a collection of documents, can we identify what they are about?\nTopic model algorithms look for patterns of co-occurrences of words in documents. We assume that, if a document is about a certain topic, one would expect words that are related to that topic to appear in the document more often than in documents that deal with other topics. Topic model commonly use Latent Dirichlet Allocation (LDA) to find topics in textual data.\nThere are two basic types of Topic models\n\nsupervised or seeded topics models where the researchers provides seed terms around which the LDS looks for topics (collections of correlating terms)\nunsupervised or unseeded topic models which try to find a predefined number of topics (collections of correlating terms)\n\nA tutorial on how to work with strings in R can be found here."
  },
  {
    "objectID": "introviz.html#basics-of-data-visualization",
    "href": "introviz.html#basics-of-data-visualization",
    "title": "Introduction to Data Visualization in R",
    "section": "Basics of data visualization",
    "text": "Basics of data visualization\nOn a very general level, graphs should be used to inform the reader about properties and relationships between variables. This implies that…\n\ngraphs, including axes, must be labeled properly to allow the reader to understand the visualization with ease.\nthere should not be more dimensions in the visualization than there are in the data.\nall elements within a graph should be unambiguous.\nvariable scales should be portrayed accurately (for instance, lines - which imply continuity - should not be used for categorically scaled variables).\ngraphs should be as intuitive as possible and should not mislead the reader."
  },
  {
    "objectID": "introviz.html#graphics-philosophies",
    "href": "introviz.html#graphics-philosophies",
    "title": "Introduction to Data Visualization in R",
    "section": "Graphics philosophies",
    "text": "Graphics philosophies\nThe three main frameworks in which to create graphics are basic framework, the lattice framework, and the ggplot or tidyverse framework. These frameworks reflect the changing nature of R as a programming language (or as a programming environment). The so-called base R consists of about 30 packages that are always loaded automatically when you open R - it is, so to say - the default version of using R when nothing else is loaded. The base R framework is the oldest way to generate visualizations in R that was used when other packages did not exists yet. However, base R can and is still used to create visualizations although most visualizations are now generated using the ggplot or tidyverse framework. The lattice framework followed the base R framework and offered some advantages such as handy ways to split up visualizations. However, lattice was replaced by the ggplot or tidyverse framework because the latter are much more flexible, offer full control, and follow an easy to understand syntax.\nWe will briefly elaborate on these three frameworks before moving on.\n\nThe base R framework\nThe base R framework is the oldest of the three and is included in what is called the base R - a collection of about 30 packages that are automatically activated/loaded when you start R. The idea behind the “base” environment is that the creation of graphics is seen in analogy to a painter who paints on an empty canvass. Each line or element is added to the graph consecutively which oftentimes leads to code that is very comprehensible but also very long.\n\n\nThe lattice framework\nThe lattice environment was a follow-up to the base framework and it complements it insofar as it made it much easier to display various variables and variable levels simultaneously. The philosophy of the lattice-package is quite different from the philosophy of base: whereas everything had to be specified in base, the graphs created in the lattice environment require only very little code but are therefore very easily created when one is satisfied with the design but very labor intensive when it comes to customizing graphs. However, lattice is very handy when summarizing relationships between multiple variable and variable levels.\n\n\nThe ggplot framework\nThe ggplot environment was written by Hadley Wickham and it combines the positive aspects of both the base and the lattice package. It was first publicized in the gplot and ggplot1 packages but the latter was soon repackaged and improved in the now most widely used package for data visualization: the ggplot2 package. The ggplot environment implements a philosophy of graphic design described in builds on The Grammar of Graphics by Leland Wilkinson [1].\nThe philosophy of ggplot2 is to consider graphics as consisting out of basic elements (called aesthetics and they include, for instance, the data set to be plotted and the axes) and layers that overlaid onto the aesthetics. The idea of the ggplot2 package can be summarized as taking “care of many of the fiddly details that make plotting a hassle (like drawing legends) as well as providing a powerful model of graphics that makes it easy to produce complex multi-layered graphics.”\nThus, ggplots typically start with the function call (ggplot) followed by the specification of the data, then the aesthetics (aes), and then a specification of the type of plot that is created (geom_line for line graphs, geom_box for box plots, geom_bar for bar graphs, geom_text for text, etc.). In addition, ggplot allows to specify all elements that the graph consists of (e.g. the theme and axes). The underlying principle is that a visualization is build up by adding layers as shown below.\n\n\n\n\n\nAs the ggplot framework has become the dominant way to create visualizations in R, we will only focus on this framework in the following practical examples."
  },
  {
    "objectID": "introviz.html#preparation-and-session-set-up",
    "href": "introviz.html#preparation-and-session-set-up",
    "title": "Introduction to Data Visualization in R",
    "section": "Preparation and session set up",
    "text": "Preparation and session set up\nThis tutorial is based on R. If you have not installed R or are new to it, you will find an introduction to and more information how to use R here. For this tutorials, we need to install certain packages from an R library so that the scripts shown below are executed without errors. Before turning to the code below, please install the packages by running the code below this paragraph. If you have already installed the packages mentioned below, then you can skip ahead and ignore this section. To install the necessary packages, simply run the following code - it may take some time (between 1 and 5 minutes to install all of the libraries so you do not need to worry if it takes some time).\n\n# install packages\ninstall.packages(\"knitr\")\ninstall.packages(\"lattice\")\ninstall.packages(\"tidyverse\")\ninstall.packages(\"vcd\")\ninstall.packages(\"SnowballC\")\ninstall.packages(\"tidyr\")\ninstall.packages(\"gridExtra\")\ninstall.packages(\"DT\")\ninstall.packages(\"flextable\")\n# install klippy for copy-to-clipboard button in code chunks\ninstall.packages(\"remotes\")\nremotes::install_github(\"rlesur/klippy\")\n\nNow that we have installed the packages, we activate them as shown below.\n\n# set options\noptions(stringsAsFactors = F)          # no automatic data transformation\noptions(\"scipen\" = 100, \"digits\" = 12) # suppress math annotation\n# activate packages\nlibrary(knitr)\nlibrary(lattice)\nlibrary(tidyverse)\nlibrary(tidyr)\nlibrary(gridExtra)\nlibrary(knitr)\nlibrary(DT)\nlibrary(flextable)\n# activate klippy for copy-to-clipboard button\nklippy::klippy()\n\n\n\n\nOnce you have installed R and RStudio and initiated the session by executing the code shown above, you are good to go."
  },
  {
    "objectID": "isle6_reprows.html#folder-structure",
    "href": "isle6_reprows.html#folder-structure",
    "title": "ISLE 6 workshop: Replication and Reproducibility in English Corpus Linguistics",
    "section": "Folder structure",
    "text": "Folder structure\n\n\n\n\n\nThere are various ways in which you can organize your folders. All of these ways to organize your folders have different advantages and problems but they all have in common that they rely on a tree-structure - more general folders contain more specialized ones. For example, if you want to find any file with as few clicks as possible, an alphabetical folder structure would be a good solution. Organized in this way, everything that starts with a certain letter will be stored by its initial letter (e.g. everything starting with a t such as travel under T or everything related to your courses under C). However, organizing your data alphabetically is not intuitive and completely unrelated topics will be located in the same folder.\nA more common and intuitive way to organize your data is to separate your data into meaningful aspects of your life such as Work (containing, e.g., teaching and research), Living (including rent, finances, and insurances), and Media (including Movies, Music, and Audiobooks)."
  },
  {
    "objectID": "isle6_reprows.html#naming-conventions",
    "href": "isle6_reprows.html#naming-conventions",
    "title": "ISLE 6 workshop: Replication and Reproducibility in English Corpus Linguistics",
    "section": "Naming conventions",
    "text": "Naming conventions\nA File Naming Convention (FNC) is a framework or protocol if you like for naming your files in a way that describes what files contain and importantly, how they relate to other files. It is essential prior to collecting data to establish an agreed FNC.\n\n\n\n\n\nFolders (as well as files) should be labeled in a meaningful way. This means that you avoid names like Stuff or Document for folders and doc2 or homework for files.\nNaming files consistently, logically and in a predictable manner will prevent against unorganized files, misplaced or lost data. It could also prevent possible backlogs or project delays. A file naming convention will ensure files are:\n\nEasier to process - All team members won’t have to over think the file naming process\nEasier to facilitate access, retrieval and storage of files\nEasier to browse through files saving time and effort\nHarder to lose!\nCheck for obsolete or duplicate records\n\nThe University of Edinburgh has a comprehensive and easy to follow list (with examples and explanations) of 13 Rules for file naming conventions. You can also use the recommendations of the\nAustralian National Data Services (ANDS) guide of file wrangling. Some of the suggestions are summarized below.\n\n\n\n\n\nLike the different conventions for folders, there are different conventions for files. As a general rule, any special character symbols, such as +, !, “, -, ., ö, ü, ä, %, &, (, ), [, ], &, $, =, ?, ’, #, or / but also including white spaces, should be avoided (while _ is also a special character, it is still common practice to include them in file names for readability). One reason for this is that you will may encounter problems when sharing files if you avoid white spaces and special characters. Also, some software applications automatically replace or collapse white spaces. A common practice to avoid this issue is to capitalize initial letters in file names which allows you avoid white spaces. An example would be TutorialIntroComputerSkills or Tutorial_IntroComputerSkills.\nWhen you want to include time-stamps in file names, the best way to do this is to use the YYYYMMDD format (rather than DDMMYYYY or even D.M.YY). The reason is that if dates are added this way, the files can be easily sorted in ascending or descending order. To elaborate on the examples shown before, we may use TutorialIntroComputerSkills20200413 or Tutorial_IntroComputerSkills_20200413"
  },
  {
    "objectID": "isle6_reprows.html#documentation-and-the-bus-factor",
    "href": "isle6_reprows.html#documentation-and-the-bus-factor",
    "title": "ISLE 6 workshop: Replication and Reproducibility in English Corpus Linguistics",
    "section": "Documentation and the Bus Factor",
    "text": "Documentation and the Bus Factor\nDocumentation is the idea of documenting your work so that outsiders (or yourself after a long time) can understand what you did and how you did it. As a general rule, you should document where to find what as if you informed someone else how to find stuff on your computer.\n\n\n\n\n\nDocumentation can include where your results and data are saved but it can also go far beyond this. Documentation does not have to be complex and it can come in different forms, depending on your needs and your work. Documenting what you do can also include photos, word documents with descriptions, or websites that detail how you work.\nThe idea behind documentation is to keep a log of the contents of folders so that you at a later point in time or someone else can continue your work in case you or someone else is run over by a bus (hence the Bus Factor).\nIn fact, documentation is all about changing the Bus Factor - how many people on a project would need to be hit by a bus to make a project fail. Many times, projects can have a bus factor of one. Adding documentation means when someone goes on leave, needs to take leave suddenly or finishes their study, their work is preserved for your project.\nIf you work in a collaborative project, it is especially useful to have a log of where one can find relevant information and who to ask for help with what. Ideally you want to document anything that someone coming on board would need to know. Thus, if you have not created a log for on-boarding, the perfect person to create a log would be the last person who joined the project. Although there is no fixed rule, it is recommendable to store the log either as a ReadMe document or in a ReadMe folder on the top level of the project."
  },
  {
    "objectID": "isle6_reprows.html#keep-copies",
    "href": "isle6_reprows.html#keep-copies",
    "title": "ISLE 6 workshop: Replication and Reproducibility in English Corpus Linguistics",
    "section": "Keep copies",
    "text": "Keep copies\nKeeping a copy of all your data (working, raw and completed) in the cloud is incredibly important. This ensures that if you have a computer failure, accidentally delete your data or your data is corrupted, your research is recoverable and restorable.\nThe 3-2-1 backup rule\nTry to have at least three copies of your project that are in different locations. The rule is: keep at least three (3) copies of your data, and store backup-copies on two (2) different storage media , with one (1) of them located offsite. Although this is just a personal preference, I always safe my projects\n\n\n\n\n\n\non my personal notebook\non at least one additional hard drive (that you keep in a secure location)\nin an online repository (for example, UQ’s Research Data Management system (RDM) OneDrive, MyDrive, GitHub, or GitLab)\n\nUsing online repositories ensures that you do not lose any data in case your computer crashes (or in case you spill lemonade over it - don’t ask…) but it comes at the cost that your data can be more accessible to (criminal or other) third parties. Thus, if you are dealing with sensitive data, I suggest to store it on an additional external hard drive and do not keep cloud-based back-ups. If you trust tech companies with your data (or think that they are not interested in stealing your data), cloud-based solutions such as OneDrive, Google’s MyDrive, or Dropbox are ideal and easy to use options (however, UQ’s RDM is a safer option).\nThe UQ library also offers additional information on complying with ARC and NHMRC data management plan requirements and that UQ RDM meets these requirements for sensitive data (see here)."
  },
  {
    "objectID": "isle6_reprows.html#avoid-duplicates",
    "href": "isle6_reprows.html#avoid-duplicates",
    "title": "ISLE 6 workshop: Replication and Reproducibility in English Corpus Linguistics",
    "section": "Avoid duplicates",
    "text": "Avoid duplicates\n\n\n\n\n\nMany of the files on our computers have several duplicates or copies on the same machine. Optimally, each file should only be stored once (on one machine). Thus, to minimize the superfluous files on your computer, we can delete any duplicates of files.\nYou can, of course, do that manually but a better way to do this is to use programs that detect files that are identical in name, file size, and date of creation. One of these programs is the Douplicate Cleaner. A tutorial on how to use it can be found here."
  },
  {
    "objectID": "isle6_reprows.html#how-to-organize-projects",
    "href": "isle6_reprows.html#how-to-organize-projects",
    "title": "ISLE 6 workshop: Replication and Reproducibility in English Corpus Linguistics",
    "section": "How to organize projects",
    "text": "How to organize projects\nThis section focuses on how to organize your projects and how to use your computer in doing so.\nProject folder design\nHaving a standard folder structure can keep your files neat and tidy and save you time looking for data. It can also help if you are sharing files with colleagues and having a standard place to put working data and documentation.\nStore your projects in a separate folder. For instance, if you are creating a folder for a research project, create the project folder within a separate project folder that is within a research folder. If you are creating a folder for a course, create the course folder within a courses folder within a teaching folder, etc.\n\n\n\n\n\nWhenever you create a folder for a new project, try to have a set of standard folders. For example, when I create research project folders, I always have folders called archive, data, docs, and images. When I create course folders, I always have folders called slides, assignments, exam, studentmaterials, and correspondence. However, you are, of course, free to modify and come up or create your own basic project design. Also, by prefixing the folder names with numbers, you can force your files to be ordered by the steps in your workflow.\n\nHaving different sub folders allows you to avoid having too many files and many different file types in a single folder. Folders with many different files and file types tend to be chaotic and can be confusing. In addition, I have one ReadMe file on the highest level (which only contains folders except for this one single file) in which I describe very briefly what the folder is about and which folders contain which documents as well as some basic information about the folder (e.g. why and when I created it). This ReadMe file is intended both for me as a reminder what this folder is about and what it contains but also for others in case I hand the project over to someone else who continues the project or for someone who takes over my course and needs to use my materials.\n\nIf you work in a team or share files and folders regularly, it makes sense to develop a logical structure for your team, you need to consider the following points:\n\nCheck to make sure there are no pre-existing folder structure agreements\nName folders appropriately and in a meaningful manner. Don’t use staff names and consider using the type of work\nConsistency - make sure you use the agreed structure/hierarchy\nStructure folders hierarchically - start with a limited number of folders for the broader topics, and then create more specific folders within these\nSeparate ongoing and completed work - as you start to create lots of folders and files, it is a good idea to start thinking about separating your older documents from those you are currently working on\nBackup – ensure folders and files are backed up and retrievable in the event of a disaster. UQ like most universities, have safe storage solutions.\nClean up folders and files post project."
  },
  {
    "objectID": "isle6_reprows.html#data-as-publications",
    "href": "isle6_reprows.html#data-as-publications",
    "title": "ISLE 6 workshop: Replication and Reproducibility in English Corpus Linguistics",
    "section": "Data as publications",
    "text": "Data as publications\nMore recently, regarding data as a form of publications has gain a lot of traction. This has the advantage that it rewards researchers who put a lot of work into compiling data and it has created an incentive for making data available, e.g. for replication. The UQ RDM and UQ eSpace can help with the process of publishing a dataset.\n\n\n\n\n\nThere are many platforms where data can be published and made available in a sustainable manner. Below are listed just some options that are recommendable:\n\n\nUQ Research Data Manager\n\nThe UQ Research Data Manager (RDM) system is a robust, world-leading system designed and developed here at UQ. The UQ RDM provides the UQ research community with a collaborative, safe and secure large-scale storage facility to practice good stewardship of research data. The European Commission report “Turning FAIR into Reality” cites UQ’s RDM as an exemplar of, and approach to, good research data management practice. The disadvantage of RDM is that it is not available to everybody but restricted to UQ staff, affiliates, and collaborators.\n\n\n\nOpen Science Foundation\n\nThe Open Science Foundation (OSF) is a free, global open platform to support your research and enable collaboration.\n\n\n\nTROLLing\n\nTROLLing | DataverseNO (The Tromsø Repository of Language and Linguistics) is a repository of data, code, and other related materials used in linguistic research. The repository is open access, which means that all information is available to everyone. All postings are accompanied by searchable metadata that identify the researchers, the languages and linguistic phenomena involved, the statistical methods applied, and scholarly publications based on the data (where relevant).\n\n\n\nGit\n\nGitHub offers the distributed version control using Git. While GitHub is not designed to host research data, it can be used to share small collections of research data and make them available to the public. The size restrictions and the fact that GitHub is a commercial enterprise owned by Microsoft are disadvantages of this as well as alternative, but comparable platforms such as GitLab."
  },
  {
    "objectID": "isle6_reprows.html#installing-r-and-rstudio",
    "href": "isle6_reprows.html#installing-r-and-rstudio",
    "title": "ISLE 6 workshop: Replication and Reproducibility in English Corpus Linguistics",
    "section": "Installing R and RStudio",
    "text": "Installing R and RStudio\n\nYou have NOT yet installed R on your computer?\n\nYou have a Windows computer? Then click here for downloading and installing R\nYou have a Mac? Then click here for downloading and installing R\n\nYou have NOT yet installed RStudio on your computer?\n\nClick here for downloading and installing RStudio.\n\n\nYou can find a more elaborate explanation of how to download and install R and RStudio here that was created by the UQ library."
  },
  {
    "objectID": "isle6_reprows.html#rproj",
    "href": "isle6_reprows.html#rproj",
    "title": "ISLE 6 workshop: Replication and Reproducibility in English Corpus Linguistics",
    "section": "Rproj",
    "text": "Rproj\nIf you’re using RStudio, you have the option of creating a new R project. A project is simply a working directory designated with a .RProj file. When you open a project (using File/Open Project in RStudio or by double–clicking on the .RProj file outside of R), the working directory will automatically be set to the directory that the .RProj file is located in. Using Rprojects makes working in and with R much easier and you can always return exactly to where you left of when you closed the Rproject.\n\n\n\n\n\nI recommend creating a new R Project whenever you are starting a new research project. Once you’ve created a new R project, you should immediately create folders in the directory which will contain your R code, data files, notes, and other material relevant to your project (you can do this outside of R on your computer, or in the Files window of RStudio). For example, you could create a folder called scripts that contains all of your R code (the scripts), a folder called data that contains all your data, a folder called images for your visualizations, etc.\nBefore working with Rprojects, I set my working directories with setwd() but this is not optimal because it takes an absolute file path as an input then sets it as the current working directory of the R process. However, this makes scripts break easily and it makes it more difficult to share my analyses and projects with others. Hence, the setwd() approach makes it very difficult to share work and make it transparent and reproducible."
  },
  {
    "objectID": "isle6_reprows.html#solving-dependency-issues-renv",
    "href": "isle6_reprows.html#solving-dependency-issues-renv",
    "title": "ISLE 6 workshop: Replication and Reproducibility in English Corpus Linguistics",
    "section": "Solving dependency issues: renv",
    "text": "Solving dependency issues: renv\n\n\n\n\n\nThe renv package is a new way to make Rprojects independent and thus to remove outside dependencies from Rprojects. The renv package creates a new library within your project so that your Rproject is independent of your personal library and when you share your project, you automatically also share the packages that you have used. The idea behind renv is to be a robust, stable replacement for the packrat package which was rather unsatisfactory to work with (speaking from experience here).\nI have used renv and, while it took some time to generate the local library on first use, it was very easy to use, did not cause any issues and is overall very recommendable to get rid of outside dependencies and thus make Rprojects easier to share for transparency and reproducibility.\nUnderlying the philosophy of renv is that all existing work flows should just work as they did before – renv helps to isolate your project’s R dependencies (like package versioning).\nYou can get more information about renv and how it works, as well as how you can use it here."
  },
  {
    "objectID": "isle6_reprows.html#version-control-with-git",
    "href": "isle6_reprows.html#version-control-with-git",
    "title": "ISLE 6 workshop: Replication and Reproducibility in English Corpus Linguistics",
    "section": "Version Control with Git",
    "text": "Version Control with Git\n\n\n\n\n\nGetting started with Git\nTo connect your Rproject with GitHub, you need to have Git installed (if you have not yet downloaded and installed Git, simply search for download git in your favorite search engine and follow the instructions) and you need to have a GitHub account. If you do not have a GitHub account, here is a video tutorial showing how you can do this. If you have trouble with this, you can also check out Happy Git and GitHub for the useR at happygitwithr.com.\n\n\n\n\n\nJust as a word of warning: when I set up my connection to Git and GitHUb things worked very differently, so things may be a bit different on your machine. In any case, I highly recommend this YouTube tutorial which shows how to connect to Git and GitHub using the usethis package or this, slightly older, YouTube tutorial on how to get going with Git and GitHub when working with RStudio.\nOld school\nWhile many people use the usethis package to connect RStudio to GitHub, I still use a somewhat old school way to connect my projects with GitHub. I have decided to show you how to connect RStudio to GitHub using this method, as I actually think it is easier once you have installed Git and created a gitHub account.\nBefore you can use Git with R, you need to tell RStudio that you want to use Git. To do this, go to Tools, then Global Options and then to Git/SVN and make sure that the box labeled Enable version control interface for RStudio projects. is checked. You need to then browse to the Git executable file (for Window’s users this is called the Git.exe file).\n\n\n\n\n\n\n\nNow, we can connect our project to Git (not to GitHub yet). To do this, go to Tools, then to Project Options... and in the Git/SVN tab, change Version Control System to Git (from None). Once you have accepted these changes, a new tab called Git appears in the upper right pane of RStudio (you may need to / be asked to restart RStudio at this point). You can now commit files and changes in files to Git.\n\n\n\n\n\nTo commit files, go to the Git tab and check the boxes next to the files you would like to commit (this is called staging; meaning that these files are now ready to be committed). Then, click on Commit and enter a message in the pop-up window that appears. Finally, click on the commit button under the message window.\nConnecting your Rproj with GitHub\nTo connect your Rproject and GitHub, we go to our GitHub page and create a new GitHub repository (repo) that we call test (or whatever you want to call your repository). To create a new repository on GitHub, simply click on New Repository after you have clicked on the New icon. I recommend that you check Add a Readme in which you can describe what the repo contains, but you do not have to do this.\nOnce you have created a new GitHub repo, we need to connect this repo to our computer. To do this, we need to clone the repo which we do by clicking on the green Code icon. When you click on the green Code icon, a dropdown menu appears and you copy the url in the section clone with HTTPS.\nThen, we go to the Terminal (in-between Console and Jobs) and we include the path we got from the git repository after the command git remote add origin. We then use the command git branch -M main (to merge a master and a main branch) and then, finally, push our files into the remote GitHub repo by using the command git push -u origin main.\n\n# initiate the upstream tracking of the project on the GitHub repo\ngit remote add origin https://github.com/MartinSchweinberger/test.git\n# set main as main branch (rather than master)\ngit branch -M main\n# push content to main\ngit push -u origin main\n\nWe can then commit changes and push them to the remote GitHub repo.\nYou can then go to your GitHub repo and check if the documents that we pushed are now in the remote repo.\nFrom now on, you can simply commit all changes that you make to the GitHub repo associated with that Rproject. Other projects can, of course, be connected and push to other GitHub repos."
  },
  {
    "objectID": "isle6_reprows.html#solving-path-issues-here",
    "href": "isle6_reprows.html#solving-path-issues-here",
    "title": "ISLE 6 workshop: Replication and Reproducibility in English Corpus Linguistics",
    "section": "Solving path issues: here",
    "text": "Solving path issues: here\nThe goal of the here package is to enable easy file referencing in project-oriented workflows. In contrast to using setwd(), which is fragile and dependent on the way you organize your files, here uses the top-level directory of a project to easily build paths to files.\nThis makes your projects more robust as the paths will still work if you put your project into another location or on another computer. Also, moving between Mac and Windows (which would require different kind of path specifications) is no longer an issue with the here package.\n\n# define path\nexample_path_full <- \"D:\\\\Uni\\\\Konferenzen\\\\ISLE\\\\ISLE_2021\\\\isle6_reprows/repro.Rmd\"\n# show path\nexample_path_full\n\n[1] \"D:\\\\Uni\\\\Konferenzen\\\\ISLE\\\\ISLE_2021\\\\isle6_reprows/repro.Rmd\"\n\n\nWith the here package, the path starts in folder where the Rproj file is. As the Rmd file is in the same folder, we only need to specify the Rmd file and the here package will add the rest.\n\n# load package\nlibrary(here)\n# define path using here\nexample_path_here <- here::here(\"repro.Rmd\")\n#show path\nexample_path_here\n\n[1] \"/home/sam/programming/SLCLADAL.github.io/repro.Rmd\""
  },
  {
    "objectID": "isle6_reprows.html#reproducible-randomness-set.seed",
    "href": "isle6_reprows.html#reproducible-randomness-set.seed",
    "title": "ISLE 6 workshop: Replication and Reproducibility in English Corpus Linguistics",
    "section": "Reproducible randomness: set.seed",
    "text": "Reproducible randomness: set.seed\nThe set.seed function in R sets the seed of R‘s random number generator, which is useful for creating simulations or random objects that can be reproduced. This means that when you call a function that uses some form of randomness (e.g. when using random forests), using the set.seed function allows you to replicate results.\nBelow is an example of what I mean. First, we generate a random sample from a vector.\n\nnumbers <- 1:10\nrandomnumbers1 <- sample(numbers, 10)\nrandomnumbers1\n\n [1] 10  2  6  9  5  4  3  1  8  7\n\n\nWe now draw another random sample using the same sample call.\n\nrandomnumbers2 <- sample(numbers, 10)\nrandomnumbers2\n\n [1]  9  5  7  6  3  1  8 10  2  4\n\n\nAs you can see, we now have a different string of numbers although we used the same call. However, when we set the seed and then generate a string of numbers as show below, we create a reproducible random sample.\n\nset.seed(123)\nrandomnumbers3 <- sample(numbers, 10)\nrandomnumbers3\n\n [1]  3 10  2  8  6  9  1  7  5  4\n\n\nTo show that we can reproduce this sample, we call the same seed and then generate another random sample which will be the same as the previous one because we have set the seed.\n\nset.seed(123)\nrandomnumbers4 <- sample(numbers, 10)\nrandomnumbers4\n\n [1]  3 10  2  8  6  9  1  7  5  4"
  },
  {
    "objectID": "kwics.html#loading-and-processing-textual-data",
    "href": "kwics.html#loading-and-processing-textual-data",
    "title": "Concordancing with R",
    "section": "Loading and processing textual data",
    "text": "Loading and processing textual data\nFor this tutorial, we will use Charles Darwin’s On the Origin of Species by means of Natural Selection which we download from the Project Gutenberg archive [see 13]. Thus, Darwin’s Origin of Species forms the basis of our analysis. You can use the code below to download this text into R (but you have to have access to the internet to do so).\n\norigin <- gutenberg_works(\n  # define id of darwin's origin in project gutenberg\n  gutenberg_id == \"1228\") %>%\n  # download text\n  gutenberg_download(meta_fields = \"gutenberg_id\", \n                     mirror = \"http://mirrors.xmission.com/gutenberg/\") %>%\n  # remove empty rows\n  dplyr::filter(text != \"\")\n\n\n\n\n\n\nFirst 10 text elements of Charles Darwin’s Origin.\n\n\ngutenberg_idtext1,228Click on any of the filenumbers below to quickly view each ebook.1,2281228    1859, First Edition1,22822764   1860, Second Edition1,2282009    1872, Sixth Edition, considered the definitive edition.1,228On1,228the Origin of Species1,228BY MEANS OF NATURAL SELECTION,1,228OR THE1,228PRESERVATION OF FAVOURED RACES IN THE STRUGGLE FOR LIFE.1,228By Charles Darwin, M.A.,\n\n\nThe table above shows that Darwin’s Origin of Species requires formatting so that we can use it. Therefore, we collapse it into a single object (or text) and remove superfluous white spaces.\n\norigin <- origin$text %>%\n  # collapse lines into a single  text\n  paste0(collapse = \" \") %>%\n  # remove superfluous white spaces\n  str_squish()\n\n\n\n\n\n\nFirst 1000 characters of Charles Darwin’s Origin\n\n\n.Click on any of the filenumbers below to quickly view each ebook. 1228 1859, First Edition 22764 1860, Second Edition 2009 1872, Sixth Edition, considered the definitive edition. On the Origin of Species BY MEANS OF NATURAL SELECTION, OR THE PRESERVATION OF FAVOURED RACES IN THE STRUGGLE FOR LIFE. By Charles Darwin, M.A., Fellow Of The Royal, Geological, Linnæan, Etc., Societies; Author Of ‘Journal Of Researches During H.M.S. Beagle’s Voyage Round The World.’ LONDON: JOHN MURRAY, ALBEMARLE STREET. 1859. “But with regard to the material world, we can at least go so far as this—we can perceive that events are brought about not by insulated interpositions of Divine power, exerted in each particular case, but by the establishment of general laws.” W. WHEWELL: _Bridgewater Treatise_. “To conclude, therefore, let no man out of a weak conceit of sobriety, or an ill-applied moderation, think or maintain, that a man can search too far or be too well studied in the book of God’s word, or in the \n\n\nThe result confirms that the entire text is now combined into a single character object."
  },
  {
    "objectID": "kwics.html#creating-simple-concordances",
    "href": "kwics.html#creating-simple-concordances",
    "title": "Concordancing with R",
    "section": "Creating simple concordances",
    "text": "Creating simple concordances\nNow that we have loaded the data, we can easily extract concordances using the kwic function from the quanteda package. The kwic function takes the text (x) and the search pattern (pattern) as it main arguments but it also allows the specification of the context window, i.e. how many words/elements are show to the left and right of the key word (we will go over this later on).\n\nkwic_natural <- kwic(\n  # define text\n  origin, \n  # define search pattern\n  pattern = \"selection\")\n\n\n\n\n\n\nFirst 10 concordances for the keyword natural in Charles Darwin’s Origin.\n\n\ndocnamefromtoprekeywordpostpatterntext14444Species BY MEANS OF NATURALSELECTION, OR THE PRESERVATION OFselectiontext1275275EXISTENCE . 4 . NATURALSELECTION. 5 . LAWS OFselectiontext1411411and Origin . Principle ofSelectionanciently followed , its Effectsselectiontext1421421Effects . Methodical and UnconsciousSelection. Unknown Origin of ourselectiontext1436436favourable to Man's power ofSelection. CHAPTER 2 . VARIATIONselectiontext1522522EXISTENCE . Bears on naturalselection. The term used inselectiontext1616616. CHAPTER 4 . NATURALSELECTION. Natural Selection : itsselectiontext1619619. NATURAL SELECTION . NaturalSelection: its power compared withselectiontext1626626its power compared with man'sselection, its power on charactersselectiontext1647647on both sexes . SexualSelection. On the generality ofselection\n\n\nYou will see that you get a warning stating that you should use token f´before extracting concordances. This can be done as shown below. Also, we can specify the package from which we want to use a function by adding the package name plus :: before the function (see below)\n\nkwic_natural <- quanteda::kwic(\n  # define and tokenize text\n  quanteda::tokens(origin), \n  # define search pattern\n  pattern = \"selection\")\n\n\n\n\n\n\nFirst 10 concordances for the keyword natural in Charles Darwin’s Origin.\n\n\ndocnamefromtoprekeywordpostpatterntext14444Species BY MEANS OF NATURALSELECTION, OR THE PRESERVATION OFselectiontext1275275EXISTENCE . 4 . NATURALSELECTION. 5 . LAWS OFselectiontext1411411and Origin . Principle ofSelectionanciently followed , its Effectsselectiontext1421421Effects . Methodical and UnconsciousSelection. Unknown Origin of ourselectiontext1436436favourable to Man's power ofSelection. CHAPTER 2 . VARIATIONselectiontext1522522EXISTENCE . Bears on naturalselection. The term used inselectiontext1616616. CHAPTER 4 . NATURALSELECTION. Natural Selection : itsselectiontext1619619. NATURAL SELECTION . NaturalSelection: its power compared withselectiontext1626626its power compared with man'sselection, its power on charactersselectiontext1647647on both sexes . SexualSelection. On the generality ofselection\n\n\nWe can easily extract the frequency of the search term (selection) using the nrow or the length functions which provide the number of rows of a tables (nrow) or the length of a vector (length).\n\nnrow(kwic_natural)\n\n[1] 412\n\n\n\nlength(kwic_natural$keyword)\n\n[1] 412\n\n\nThe results show that there are 414 instances of the search term (selection) but we can also find out how often different variants (lower case versus upper case) of the search term were found using the table function. This is especially useful when searches involve many different search terms (while it is, admittedly, less useful in the present example).\n\ntable(kwic_natural$keyword)\n\n\nselection Selection SELECTION \n      369        39         4 \n\n\nTo get a better understanding of the use of a word, it is often useful to extract more context. This is easily done by increasing size of the context window. To do this, we specify the window argument of the kwic function. In the example below, we set the context window size to 10 words/elements rather than using the default (which is 5 word/elements).\n\nkwic_natural_longer <- kwic(\n  # define text\n  origin, \n  # define search pattern\n  pattern = \"selection\", \n  # define context window size\n  window = 10)\n\n\n\n\n\n\nFirst 10 concordances for the keyword natural in Charles Darwin’s Origin with extended context (10 elements).\n\n\ndocnamefromtoprekeywordpostpatterntext14444. On the Origin of Species BY MEANS OF NATURALSELECTION, OR THE PRESERVATION OF FAVOURED RACES IN THE STRUGGLEselectiontext1275275. 3 . STRUGGLE FOR EXISTENCE . 4 . NATURALSELECTION. 5 . LAWS OF VARIATION . 6 . DIFFICULTIESselectiontext1411411Domestic Pigeons , their Differences and Origin . Principle ofSelectionanciently followed , its Effects . Methodical and Unconscious Selectionselectiontext1421421Selection anciently followed , its Effects . Methodical and UnconsciousSelection. Unknown Origin of our Domestic Productions . Circumstances favourableselectiontext1436436our Domestic Productions . Circumstances favourable to Man's power ofSelection. CHAPTER 2 . VARIATION UNDER NATURE . Variability .selectiontext1522522CHAPTER 3 . STRUGGLE FOR EXISTENCE . Bears on naturalselection. The term used in a wide sense . Geometricalselectiontext1616616most important of all relations . CHAPTER 4 . NATURALSELECTION. Natural Selection : its power compared with man's selectionselectiontext1619619all relations . CHAPTER 4 . NATURAL SELECTION . NaturalSelection: its power compared with man's selection , its powerselectiontext1626626SELECTION . Natural Selection : its power compared with man'sselection, its power on characters of trifling importance , itsselectiontext1647647power at all ages and on both sexes . SexualSelection. On the generality of intercrosses between individuals of theselection\n\n\n\n\n\n\nEXERCISE TIME!\n\n\n\n\n\n\n`\n\nExtract the first 10 concordances for the word nature.\n\n\n\nAnswer\n\n::: {.cell}\n  kwic_nature <- kwic(x = origin, pattern = \"nature\")\n::: {.cell-output .cell-output-stderr} Warning: 'kwic.character()' is deprecated. Use 'tokens()' first. :::\n  # inspect\n  kwic_natural %>%\n  as.data.frame() %>%\n  head(10)\n::: {.cell-output .cell-output-stdout} docname from  to                                  pre   keyword   1    text1   44  44          Species BY MEANS OF NATURAL SELECTION   2    text1  275 275              EXISTENCE . 4 . NATURAL SELECTION   3    text1  411 411            and Origin . Principle of Selection   4    text1  421 421 Effects . Methodical and Unconscious Selection   5    text1  436 436         favourable to Man's power of Selection   6    text1  522 522         EXISTENCE . Bears on natural selection   7    text1  616 616                . CHAPTER 4 . NATURAL SELECTION   8    text1  619 619        . NATURAL SELECTION . Natural Selection   9    text1  626 626        its power compared with man's selection   10   text1  647 647               on both sexes . Sexual Selection                                  post   pattern   1          , OR THE PRESERVATION OF selection   2                     . 5 . LAWS OF selection   3  anciently followed , its Effects selection   4           . Unknown Origin of our selection   5           . CHAPTER 2 . VARIATION selection   6                . The term used in selection   7         . Natural Selection : its selection   8         : its power compared with selection   9         , its power on characters selection   10           . On the generality of selection ::: :::\n\n\nHow many instances are there of the word nature?\n\n\n\nAnswer\n\n::: {.cell}\n  kwic_nature %>%\n  as.data.frame() %>%\n  nrow()\n::: {.cell-output .cell-output-stdout} [1] 261 ::: :::\n\n\nExtract concordances for the word origin and show the first 5 concordance lines.\n\n\n\nAnswer\n\n::: {.cell}\n  kwic_origin <- kwic(x = origin, pattern = \"origin\")\n::: {.cell-output .cell-output-stderr} Warning: 'kwic.character()' is deprecated. Use 'tokens()' first. :::\n  # inspect\n  kwic_origin %>%\n  as.data.frame() %>%\n  head(5)\n::: {.cell-output .cell-output-stdout} docname from  to                                 pre keyword   1   text1   37  37         definitive edition . On the  Origin   2   text1  351 351         DETEAILED CONTENTS . ON THE  ORIGIN   3   text1  391 391     between Varieties and Species .  Origin   4   text1  407 407     Pigeons , their Differences and  Origin   5   text1  424 424 and Unconscious Selection . Unknown  Origin                                   post pattern   1             of Species BY MEANS OF  origin   2        OF SPECIES . INTRODUCTION .  origin   3     of Domestic Varieties from one  origin   4 . Principle of Selection anciently  origin   5      of our Domestic Productions .  origin ::: :::\n\n\n`"
  },
  {
    "objectID": "kwics.html#extracting-more-than-single-words",
    "href": "kwics.html#extracting-more-than-single-words",
    "title": "Concordancing with R",
    "section": "Extracting more than single words",
    "text": "Extracting more than single words\nWhile extracting single words is very common, you may want to extract more than just one word. To extract phrases, all you need to so is to specify that the pattern you are looking for is a phrase, as shown below.\n\nkwic_naturalselection <- kwic(origin, pattern = phrase(\"natural selection\"))\n\n\n\n\n\n\nFirst 10 concordances for the keyphrase natural selection in Charles Darwin’s Origin with extended context (10 elements).\n\n\ndocnamefromtoprekeywordpostpatterntext14344of Species BY MEANS OFNATURAL SELECTION, OR THE PRESERVATION OFnatural selectiontext1274275FOR EXISTENCE . 4 .NATURAL SELECTION. 5 . LAWS OFnatural selectiontext1521522FOR EXISTENCE . Bears onnatural selection. The term used innatural selectiontext1615616relations . CHAPTER 4 .NATURAL SELECTION. Natural Selection : itsnatural selectiontext16186194 . NATURAL SELECTION .Natural Selection: its power compared withnatural selectiontext1666667Circumstances favourable and unfavourable toNatural Selection, namely , intercrossing ,natural selectiontext1685686action . Extinction caused byNatural Selection. Divergence of Character ,natural selectiontext1709710to naturalisation . Action ofNatural Selection, through Divergence of Characternatural selectiontext1753754and disuse , combined withnatural selection; organs of flight andnatural selectiontext1925926embraced by the theory ofNatural Selection. CHAPTER 7 . INSTINCTnatural selection\n\n\nOf course you can extend this to longer sequences such as entire sentences. However, you may want to extract more or less concrete patterns rather than words or phrases. To search for patterns rather than words, you need to include regular expressions in your search pattern.\n\n\n\n\nEXERCISE TIME!\n\n\n\n\n\n\n`\n\nExtract the first 10 concordances for the phrase natural habitat.\n\n\n\nAnswer\n\n::: {.cell}\n  kwic_naturalhabitat <- kwic(x = origin, pattern = phrase(\"natural habitat\"))\n::: {.cell-output .cell-output-stderr} Warning: 'kwic.character()' is deprecated. Use 'tokens()' first. :::\n  # inspect\n  kwic_naturalhabitat %>%\n  as.data.frame() %>%\n  head(10)\n::: {.cell-output .cell-output-stdout} [1] docname from    to      pre     keyword post    pattern   <0 rows> (or 0-length row.names) ::: :::\n\n\nHow many instances are there of the phrase natural habitat?\n\n\n\nAnswer\n\n::: {.cell}\n  kwic_naturalhabitat %>%\n  as.data.frame() %>%\n  nrow()\n::: {.cell-output .cell-output-stdout} [1] 0 ::: :::\n\n\nExtract concordances for the phrase the origin and show the first 5 concordance lines.\n\n\n\nAnswer\n\n::: {.cell}\n  kwic_theorigin <- kwic(x = origin, pattern = phrase(\"the origin\"))\n::: {.cell-output .cell-output-stderr} Warning: 'kwic.character()' is deprecated. Use 'tokens()' first. :::\n  # inspect\n  kwic_theorigin %>%\n  as.data.frame() %>%\n  head(5)\n::: {.cell-output .cell-output-stdout} docname from   to                           pre    keyword   1   text1   36   37   the definitive edition . On the Origin   2   text1  350  351 INDEX DETEAILED CONTENTS . ON THE ORIGIN   3   text1 1617 1618     . Concluding remarks . ON THE ORIGIN   4   text1 1679 1680        to throw some light on the origin   5   text1 1910 1911    conclusions that I have on the origin                            post    pattern   1      of Species BY MEANS OF the origin   2 OF SPECIES . INTRODUCTION . the origin   3 OF SPECIES . INTRODUCTION . the origin   4   of species - that mystery the origin   5      of species . Last year the origin ::: :::\n\n\n`"
  },
  {
    "objectID": "kwics.html#searches-using-regular-expressions",
    "href": "kwics.html#searches-using-regular-expressions",
    "title": "Concordancing with R",
    "section": "Searches using regular expressions",
    "text": "Searches using regular expressions\nRegular expressions allow you to search for abstract patterns rather than concrete words or phrases which provides you with an extreme flexibility in what you can retrieve. A regular expression (in short also called regex or regexp) is a special sequence of characters that stand for are that describe a pattern. You can think of regular expressions as very powerful combinations of wildcards or as wildcards on steroids. For example, the sequence [a-z]{1,3} is a regular expression that stands for one up to three lower case characters and if you searched for this regular expression, you would get, for instance, is, a, an, of, the, my, our, etc, and many other short words as results.\nThere are three basic types of regular expressions:\n\nregular expressions that stand for individual symbols and determine frequencies\nregular expressions that stand for classes of symbols\nregular expressions that stand for structural properties\n\nThe regular expressions below show the first type of regular expressions, i.e. regular expressions that stand for individual symbols and determine frequencies.\n\n\n\n\n\n\n\n\nRegular expressions that stand for individual symbols and determine frequencies.\n\n\nRegEx Symbol/SequenceExplanationExample?The preceding item is optional and will be matched at most oncewalk[a-z]? = walk, walks*The preceding item will be matched zero or more timeswalk[a-z]* = walk, walks, walked, walking+The preceding item will be matched one or more timeswalk[a-z]+ = walks, walked, walking{n}The preceding item is matched exactly n timeswalk[a-z]{2} = walked{n,}The preceding item is matched n or more timeswalk[a-z]{2,} = walked, walking{n,m}The preceding item is matched at least n times, but not more than m timeswalk[a-z]{2,3} = walked, walking\n\n\nThe regular expressions below show the second type of regular expressions, i.e. regular expressions that stand for classes of symbols.\n\n\n\n\n\n\n\n\nRegular expressions that stand for classes of symbols.\n\n\nRegEx Symbol/SequenceExplanation[ab]lower case a and b[AB]upper case a and b[12]digits 1 and 2[:digit:]digits: 0 1 2 3 4 5 6 7 8 9[:lower:]lower case characters: a–z[:upper:]upper case characters: A–Z[:alpha:]alphabetic characters: a–z and A–Z[:alnum:]digits and alphabetic characters[:punct:]punctuation characters: . , ; etc.[:graph:]graphical characters: [:alnum:] and [:punct:][:blank:]blank characters: Space and tab[:space:]space characters: Space, tab, newline, and other space characters[:print:]printable characters: [:alnum:], [:punct:] and [:space:]\n\n\nThe regular expressions that denote classes of symbols are enclosed in [] and :. The last type of regular expressions, i.e. regular expressions that stand for structural properties are shown below.\n\n\n\n\n\n\n\n\nRegular expressions that stand for structural properties.\n\n\nRegEx Symbol/SequenceExplanation\\\\wWord characters: [[:alnum:]_]\\\\WNo word characters: [^[:alnum:]_]\\\\sSpace characters: [[:blank:]]\\\\SNo space characters: [^[:blank:]]\\\\dDigits: [[:digit:]]\\\\DNo digits: [^[:digit:]]\\\\bWord edge\\\\BNo word edge<Word beginning>Word end^Beginning of a string$End of a string\n\n\nTo include regular expressions in your KWIC searches, you include them in your search pattern and set the argument valuetype to \"regex\". The search pattern \"\\\\bnatu.*|\\\\bselec.*\" retrieves elements that contain natu and selec followed by any characters and where the n in natu and the s in selec are at a word boundary, i.e. where they are the first letters of a word. Hence, our search would not retrieve words like unnatural or deselect. The | is an operator (like +, -, or *) that stands for or.\n\n# define search patterns\npatterns <- c(\"\\\\bnatu.*|\\\\bselec.*\")\nkwic_regex <- kwic(\n  # define text\n  origin, \n  # define search pattern\n  patterns, \n  # define valuetype\n  valuetype = \"regex\")\n\n\n\n\n\n\nFirst 10 concordances for the regular expression .*.\n\n\ndocnamefromtoprekeywordpostpatterntext14343of Species BY MEANS OFNATURALSELECTION , OR THE PRESERVATION\\bnatu.*|\\bselec.*text14444Species BY MEANS OF NATURALSELECTION, OR THE PRESERVATION OF\\bnatu.*|\\bselec.*text1264264. 2 . VARIATION UNDERNATURE. 3 . STRUGGLE FOR\\bnatu.*|\\bselec.*text1274274FOR EXISTENCE . 4 .NATURALSELECTION . 5 . LAWS\\bnatu.*|\\bselec.*text1275275EXISTENCE . 4 . NATURALSELECTION. 5 . LAWS OF\\bnatu.*|\\bselec.*text1411411and Origin . Principle ofSelectionanciently followed , its Effects\\bnatu.*|\\bselec.*text1421421Effects . Methodical and UnconsciousSelection. Unknown Origin of our\\bnatu.*|\\bselec.*text1436436favourable to Man's power ofSelection. CHAPTER 2 . VARIATION\\bnatu.*|\\bselec.*text1443443CHAPTER 2 . VARIATION UNDERNATURE. Variability . Individual Differences\\bnatu.*|\\bselec.*text1521521FOR EXISTENCE . Bears onnaturalselection . The term used\\bnatu.*|\\bselec.*\n\n\n\n\n\n\nEXERCISE TIME!\n\n\n\n\n\n\n`\n\nExtract the first 10 concordances for words containing exu.\n\n\n\nAnswer\n\n::: {.cell}\n  kwic_exu <- kwic(x = origin, pattern = \".*exu.*\", valuetype = \"regex\")\n::: {.cell-output .cell-output-stderr} Warning: 'kwic.character()' is deprecated. Use 'tokens()' first. :::\n  # inspect\n  kwic_exu %>%\n  as.data.frame() %>%\n  head(10)\n::: {.cell-output .cell-output-stdout} docname  from    to                               pre keyword   1    text1   646   646               and on both sexes .  Sexual   2    text1   806   806 variable than generic : secondary  sexual   3    text1 29294 29294               and on both sexes .  Sexual   4    text1 31953 31953      like every other structure . _Sexual   5    text1 32040 32040              words on what I call  Sexual   6    text1 32082 32082             few or no offspring .  Sexual   7    text1 32157 32157     chance of leaving offspring .  Sexual   8    text1 32330 32330         be given through means of  sexual   9    text1 32628 32628   having been chiefly modified by  sexual   10   text1 32726 32726        have been mainly caused by  sexual                                  post pattern   1     Selection . On the generality .*exu.*   2  characters variable . Species of .*exu.*   3     Selection . On the generality .*exu.*   4        Selection_ . - Inasmuch as .*exu.*   5        Selection . This depends , .*exu.*   6        selection is , therefore , .*exu.*   7  selection by always allowing the .*exu.*   8           selection , as the mane .*exu.*   9       selection , acting when the .*exu.*   10            selection ; that is , .*exu.* ::: :::\n\n\nHow many instances are there of words beginning with nonet?\n\n\n\nAnswer\n\n::: {.cell}\n  kwic_nonet <- kwic(x = origin, pattern = \"\\\\bnonet.*\", valuetype = \"regex\") %>%\n  as.data.frame() %>%\n  nrow()\n::: {.cell-output .cell-output-stderr} Warning: 'kwic.character()' is deprecated. Use 'tokens()' first. ::: :::\n\n\nExtract concordances for words ending with ption and show the first 5 concordance lines.\n\n\n\nAnswer\n\n::: {.cell}\n  kwic_ption <- kwic(x = origin, pattern = \"ption\\\\b\", valuetype = \"regex\")\n::: {.cell-output .cell-output-stderr} Warning: 'kwic.character()' is deprecated. Use 'tokens()' first. :::\n  # inspect\n  kwic_ption %>%\n  as.data.frame() %>%\n  head(5)\n::: {.cell-output .cell-output-stdout} docname from   to                          pre    keyword   1   text1 1605 1605    extended . Effects of its   adoption   2   text1 2641 2641          see them ; but this assumption   3   text1 3926 3926         or at the instant of conception   4   text1 3990 3990          prior to the act of conception   5   text1 4233 4233 under confinement , with the  exception                            post  pattern   1     on the study of Natural ption\\\\b   2           seems to me to be ption\\\\b   3   . Geoffroy St . Hilaire's ption\\\\b   4   . Several reasons make me ption\\\\b   5 of the plantigrades or bear ption\\\\b ::: :::\n\n\n`"
  },
  {
    "objectID": "kwics.html#piping-concordances",
    "href": "kwics.html#piping-concordances",
    "title": "Concordancing with R",
    "section": "Piping concordances",
    "text": "Piping concordances\nQuite often, we only want to retrieve patterns if they occur in a certain context. For instance, we might be interested in instances of selection but only if the preceding word is natural. Such conditional concordances could be extracted using regular expressions but they are easier to retrieve by piping. Piping is done using the %>% function from the dplyr package and the piping sequence can be translated as and then. We can then filter those concordances that contain natural using the filter function from the dplyr package. Note the the $ stands for the end of a string so that natural$ means that natural is the last element in the string that is preceding the keyword.\n\nkwic_pipe <- kwic(x = origin, pattern = \"selection\") %>%\n  dplyr::filter(stringr::str_detect(pre, \"natural$|NATURAL$\"))\n\n\n\n\n\n\nFirst 10 concordances for instances of selection that are preceeded by natural.\n\n\ndocnamefromtoprekeywordpostpatterntext14444Species BY MEANS OF NATURALSELECTION, OR THE PRESERVATION OFselectiontext1275275EXISTENCE . 4 . NATURALSELECTION. 5 . LAWS OFselectiontext1522522EXISTENCE . Bears on naturalselection. The term used inselectiontext1616616. CHAPTER 4 . NATURALSELECTION. Natural Selection : itsselectiontext1754754disuse , combined with naturalselection; organs of flight andselectiontext11,5971,597far the theory of naturalselectionmay be extended . Effectsselectiontext16,6176,617do occur ; but naturalselection, as will hereafter beselectiontext114,82714,827a process of \" naturalselection, \" as will hereafterselectiontext117,26917,269they afford materials for naturalselectionto accumulate , in theselectiontext117,81917,819and rendered definite by naturalselection, as hereafter will beselection\n\n\nPiping is a very useful helper function and it is very frequently used in R - not only in the context of text processing but in all data science related domains."
  },
  {
    "objectID": "kwics.html#arranging-concordances-and-adding-frequency-information",
    "href": "kwics.html#arranging-concordances-and-adding-frequency-information",
    "title": "Concordancing with R",
    "section": "Arranging concordances and adding frequency information",
    "text": "Arranging concordances and adding frequency information\nWhen inspecting concordances, it is useful to re-order the concordances so that they do not appear in the order that they appeared in the text or texts but by the context. To reorder concordances, we can use the arrange function from the dplyr package which takes the column according to which we want to re-arrange the data as it main argument.\nIn the example below, we extract all instances of natural and then arrange the instances according to the content of the post column in alphabetical.\n\nkwic_ordered <- kwic(x = origin, pattern = \"natural\") %>%\n  dplyr::arrange(post)\n\n\n\n\n\n\nFirst 10 re-ordered concordances for instances of natural.\n\n\ndocnamefromtoprekeywordpostpatterntext1176,207176,207, 190 . System ,natural, 413 . Tail :naturaltext1176,668176,668, 159 . Varieties :natural, 44 . struggle betweennaturaltext1175,731175,731. unconscious , 34 .natural, 80 . sexual ,naturaltext1147,280147,280and this would be strictlynatural, as it would connectnaturaltext1175,739175,739. sexual , 87 .natural, circumstances favourable to ,naturaltext1146,387146,387genealogical in order to benatural; but that the _amount_naturaltext1111,947111,947of old forms , bothnaturaland artificial , are boundnaturaltext156,94756,947parts having been accumulated bynaturaland sexual selection , andnaturaltext156,63056,630be taken advantage of bynaturaland sexual selection , innaturaltext1150,464150,464, or at least anaturalarrangement , would be possiblenatural\n\n\nArranging concordances according to alphabetical properties may, however, not be the most useful option. A more useful option may be to arrange concordances according to the frequency of co-occurring terms or collocates. In order to do this, we need to extract the co-occurring words and calculate their frequency. We can do this by combining the mutate, group_by, n() functions from the dplyr package with the str_remove_all function from the stringr package. Then, we arrange the concordances by the frequency of the collocates in descending order (that is why we put a - in the arrange function). In order to do this, we need to\n\ncreate a new variable or column which represents the word that co-occurs with, or, as in the example below, immediately follows the search term. In the example below, we use the mutate function to create a new column called post_word. We then use the str_remove_all function to remove everything except for the word that immediately follows the search term (we simply remove everything and including a white space).\ngroup the data by the word that immediately follows the search term.\ncreate a new column called post_word_freq which represents the frequencies of all the words that immediately follow the search term.\narrange the concordances by the frequency of the collocates in descending order.\n\n\nkwic_ordered_coll <- kwic(\n  # define text\n  x = origin, \n  # define search pattern\n  pattern = \"natural\") %>%\n  # extract word following the keyword\n  dplyr::mutate(post_word = str_remove_all(post, \" .*\")) %>%\n  # group following words\n  dplyr::group_by(post_word) %>%\n  # extract frequencies of the following words\n  dplyr::mutate(post_word_freq = n()) %>%\n  # arrange/order by the frequency of the following word\n  dplyr::arrange(-post_word_freq)\n\n\n\n\n\n\nFirst 10 re-ordered concordances for instances of natural.\n\n\ndocnamefromtoprekeywordpostpatternpost_wordpost_word_freqtext16186184 . NATURAL SELECTION .NaturalSelection : its power comparednaturalSelection4text1666666Circumstances favourable and unfavourable toNaturalSelection , namely , intercrossingnaturalSelection4text1685685action . Extinction caused byNaturalSelection . Divergence of CharacternaturalSelection4text1709709to naturalisation . Action ofNaturalSelection , through Divergence ofnaturalSelection4text14343of Species BY MEANS OFNATURALSELECTION , OR THE PRESERVATIONnaturalSELECTION3text1274274FOR EXISTENCE . 4 .NATURALSELECTION . 5 . LAWSnaturalSELECTION3text1615615relations . CHAPTER 4 .NATURALSELECTION . Natural Selection :naturalSELECTION3text1521521FOR EXISTENCE . Bears onnaturalselection . The term usednaturalselection1\n\n\nWe add more columns according to which we could arrange the concordance following the same schema. For example, we could add another column that represented the frequency of words that immediately preceded the search term and then arrange according to this column."
  },
  {
    "objectID": "kwics.html#ordering-by-subsequent-elements",
    "href": "kwics.html#ordering-by-subsequent-elements",
    "title": "Concordancing with R",
    "section": "Ordering by subsequent elements",
    "text": "Ordering by subsequent elements\nIn this section, we will extract the three words following the keyword (selection) and organize the concordances by the frequencies of the following words. We begin by inspecting the first 6 lines of the concordance of selection.\n\nhead(kwic_natural)\n\nKeyword-in-context with 6 matches.                                                               \n  [text1, 44]         Species BY MEANS OF NATURAL | SELECTION |\n [text1, 275]               EXISTENCE. 4. NATURAL | SELECTION |\n [text1, 411]            and Origin. Principle of | Selection |\n [text1, 421] Effects. Methodical and Unconscious | Selection |\n [text1, 436]        favourable to Man's power of | Selection |\n [text1, 522]         EXISTENCE. Bears on natural | selection |\n                                \n , OR THE PRESERVATION OF       \n . 5. LAWS OF                   \n anciently followed, its Effects\n . Unknown Origin of our        \n . CHAPTER 2. VARIATION         \n . The term used in             \n\n\nNext, we take the concordances and create a clean post column that is all in lower case and that does not contain any punctuation.\n\nkwic_natural %>%\n  # convert to data frame\n  as.data.frame() %>%\n  # create new CleanPost\n  dplyr::mutate(CleanPost = stringr::str_remove_all(post, \"[:punct:]\"),\n                CleanPost = stringr::str_squish(CleanPost),\n                CleanPost = tolower(CleanPost))-> kwic_natural_following\n# inspect\nhead(kwic_natural_following)\n\n  docname from  to                                  pre   keyword\n1   text1   44  44          Species BY MEANS OF NATURAL SELECTION\n2   text1  275 275              EXISTENCE . 4 . NATURAL SELECTION\n3   text1  411 411            and Origin . Principle of Selection\n4   text1  421 421 Effects . Methodical and Unconscious Selection\n5   text1  436 436         favourable to Man's power of Selection\n6   text1  522 522         EXISTENCE . Bears on natural selection\n                              post   pattern                      CleanPost\n1         , OR THE PRESERVATION OF selection         or the preservation of\n2                    . 5 . LAWS OF selection                      5 laws of\n3 anciently followed , its Effects selection anciently followed its effects\n4          . Unknown Origin of our selection          unknown origin of our\n5          . CHAPTER 2 . VARIATION selection            chapter 2 variation\n6               . The term used in selection               the term used in\n\n\nIn a next step, we extract the 1st, 2nd, and 3rd words following the keyword.\n\nkwic_natural_following %>%\n  # extract first element after keyword\n  dplyr::mutate(FirstWord = stringr::str_remove_all(CleanPost, \" .*\")) %>%\n  # extract second element after keyword\n  dplyr::mutate(SecWord = stringr::str_remove(CleanPost, \".*? \"),\n                SecWord = stringr::str_remove_all(SecWord, \" .*\")) %>%\n  # extract third element after keyword\n  dplyr::mutate(ThirdWord = stringr::str_remove(CleanPost, \".*? \"),\n                ThirdWord = stringr::str_remove(ThirdWord, \".*? \"),\n                ThirdWord = stringr::str_remove_all(ThirdWord, \" .*\")) -> kwic_natural_following\n# inspect\nhead(kwic_natural_following)\n\n  docname from  to                                  pre   keyword\n1   text1   44  44          Species BY MEANS OF NATURAL SELECTION\n2   text1  275 275              EXISTENCE . 4 . NATURAL SELECTION\n3   text1  411 411            and Origin . Principle of Selection\n4   text1  421 421 Effects . Methodical and Unconscious Selection\n5   text1  436 436         favourable to Man's power of Selection\n6   text1  522 522         EXISTENCE . Bears on natural selection\n                              post   pattern                      CleanPost\n1         , OR THE PRESERVATION OF selection         or the preservation of\n2                    . 5 . LAWS OF selection                      5 laws of\n3 anciently followed , its Effects selection anciently followed its effects\n4          . Unknown Origin of our selection          unknown origin of our\n5          . CHAPTER 2 . VARIATION selection            chapter 2 variation\n6               . The term used in selection               the term used in\n  FirstWord  SecWord    ThirdWord\n1        or      the preservation\n2         5     laws           of\n3 anciently followed          its\n4   unknown   origin           of\n5   chapter        2    variation\n6       the     term         used\n\n\nNext, we calculate the frequencies of the subsequent words and order in descending order from the 1st to the 3rd word following the keyword.\n\nkwic_natural_following %>%\n  # calculate frequency of following words\n  # 1st word\n  dplyr::group_by(FirstWord) %>%\n  dplyr::mutate(FreqW1 = n()) %>%\n  # 2nd word\n  dplyr::group_by(SecWord) %>%\n  dplyr::mutate(FreqW2 = n()) %>%\n  # 3rd word\n  dplyr::group_by(ThirdWord) %>%\n  dplyr::mutate(FreqW3 = n()) %>%\n  # ungroup\n  dplyr::ungroup() %>%\n  # arrange by following words\n  dplyr::arrange(-FreqW1, -FreqW2, -FreqW3) -> kwic_natural_following\n# inspect results\nhead(kwic_natural_following, 10)\n\n# A tibble: 10 × 14\n   docname  from    to pre     keyword post  pattern CleanPost FirstWord SecWord\n   <chr>   <int> <int> <chr>   <chr>   <chr> <fct>   <chr>     <chr>     <chr>  \n 1 text1    3064  3064 This f… Select… will… select… will be … will      be     \n 2 text1   31421 31421 state … select… will… select… will be … will      be     \n 3 text1   31988 31988 and if… select… will… select… will be … will      be     \n 4 text1   60694 60694 slow p… select… will… select… will in … will      in     \n 5 text1   15600 15600 called… select… will… select… will alw… will      always \n 6 text1   37304 37304 as mig… select… will… select… will alw… will      always \n 7 text1   72213 72213 becaus… select… will… select… will alw… will      always \n 8 text1   39275 39275 new sp… select… will… select… will alw… will      always \n 9 text1   39449 39449 I do b… select… will… select… will alw… will      always \n10 text1   43007 43007 modifi… select… will… select… will alw… will      always \n# … with 4 more variables: ThirdWord <chr>, FreqW1 <int>, FreqW2 <int>,\n#   FreqW3 <int>\n\n\nThe results now show the concordance arranged by the frequency of the words following the keyword."
  },
  {
    "objectID": "kwics.html#concordances-from-transcriptions",
    "href": "kwics.html#concordances-from-transcriptions",
    "title": "Concordancing with R",
    "section": "Concordances from transcriptions",
    "text": "Concordances from transcriptions\nAs many analyses use transcripts as their primary data and because transcripts have features that require additional processing, we will now perform concordancing based on on transcripts. As a first step, we load five example transcripts that represent the first five files from the Irish component of the International Corpus of English.\n\n# define corpus files\nfiles <- paste(\"https://slcladal.github.io/data/ICEIrelandSample/S1A-00\", 1:5, \".txt\", sep = \"\")\n# load corpus files\ntranscripts <- sapply(files, function(x){\n  x <- readLines(x)\n  })\n\n\n\n\n\n\nFirst 10 utterances in the sample transcripts.\n\n\n.<S1A-001 Riding><I><S1A-001$A> <#> Well how did the riding go tonight<S1A-001$B> <#> It was good so it was <#> Just I I couldn't believe that she was going to let me jump <,> that was only the fourth time you know <#> It was great <&> laughter </&><S1A-001$A> <#> What did you call your horse<S1A-001$B> <#> I can't remember <#> Oh Mary 's Town <,> oh<S1A-001$A> <#> And how did Mabel do<S1A-001$B> <#> Did you not see her whenever she was going over the jumps <#> There was one time her horse refused and it refused three times <#> And then <,> she got it round and she just lined it up straight and she just kicked it and she hit it with the whip <,> and over it went the last time you know <#> And Stephanie told her she was very determined and very well-ridden <&> laughter </&> because it had refused the other times you know <#> But Stephanie wouldn't let her give up on it <#> She made her keep coming back and keep coming back <,> until <,> it jumped it you know <#> It was good<S1A-001$A> <#> Yeah I 'm not so sure her jumping 's improving that much <#> She uh <,> seemed to be holding the reins very tight\n\n\nThe first ten lines shown above let us know that, after the header (<S1A-001 Riding>) and the symbol which indicates the start of the transcript (<I>), each utterance is preceded by a sequence which indicates the section, file, and speaker (e.g. <S1A-001$A>). The first utterance is thus uttered by speaker A in file 001 of section S1A. In addition, there are several sequences that provide meta-linguistic information which indicate the beginning of a speech unit (<#>), pauses (<,>), and laughter (<&> laughter </&>).\nTo perform the concordancing, we need to change the format of the transcripts because the kwic function only works on character, corpus, tokens object- in their present form, the transcripts represent a list which contains vectors of strings. To change the format, we collapse the individual utterances into a single character vector for each transcript.\n\ntranscripts_collapsed <- sapply(files, function(x){\n  # read-in text\n  x <- readLines(x)\n  # paste all lines together\n  x <- paste0(x, collapse = \" \")\n  # remove superfluous white spaces\n  x <- str_squish(x)\n})\n\n\n\n\n\n\nFirst 500 characters of the collapsed sample transcripts.\n\n\n.<S1A-001 Riding> <I> <S1A-001$A> <#> Well how did the riding go tonight <S1A-001$B> <#> It was good so it was <#> Just I I couldn't believe that she was going to let me jump <,> that was only the fourth time you know <#> It was great <&> laughter </&> <S1A-001$A> <#> What did you call your horse <S1A-001$B> <#> I can't remember <#> Oh Mary 's Town <,> oh <S1A-001$A> <#> And how did Mabel do <S1A-001$B> <#> Did you not see her whenever she was going over the jumps <#> There was one time her horse<S1A-002 Dinner chat 1> <I> <S1A-002$A> <#> He 's been married for three years and is now <{> <[> getting divorced </[> <S1A-002$B> <#> <[> No no </[> </{> he 's got married last year and he 's getting <{> <[> divorced </[> <S1A-002$A> <#> <[> He 's now </[> </{> getting divorced <S1A-002$C> <#> Just right <S1A-002$D> <#> A wee girl of her age like <S1A-002$E> <#> Well there was a guy <S1A-002$C> <#> How long did she try it for <#> An hour a a year <S1A-002$B> <#> Mhm <{> <[> mhm </[> <S1A-002$E<S1A-003 Dinner chat 2> <I> <S1A-003$A> <#> I <.> wa </.> I want to go to Peru but uh <S1A-003$B> <#> Do you <S1A-003$A> <#> Oh aye <S1A-003$B> <#> I 'd love to go to Peru <S1A-003$A> <#> I want I want to go up the Machu Picchu before it falls off the edge of the mountain <S1A-003$B> <#> Lima 's supposed to be a bit dodgy <S1A-003$A> <#> Mm <S1A-003$B> <#> Bet it would be <S1A-003$B> <#> Mm <S1A-003$A> <#> But I I just I I would like <,> Machu Picchu is collapsing <S1A-003$B> <#> I don't know wh<S1A-004 Nursing home 1> <I> <S1A-004$A> <#> Honest to God <,> I think the young ones <#> Sure they 're flying on Monday in I think it 's Shannon <#> This is from Texas <S1A-004$B> <#> This English girl <S1A-004$A> <#> The youngest one <,> the dentist <,> she 's married to the dentist <#> Herself and her husband <,> three children and she 's six months pregnant <S1A-004$C> <#> Oh God <S1A-004$B> <#> And where are they going <S1A-004$A> <#> Coming to Dublin to the mother <{> <[> or <unclear> 3 sy<S1A-005 Masons> <I> <S1A-005$A> <#> Right shall we risk another beer or shall we try and <,> <{> <[> ride the bikes down there or do something like that </[> <S1A-005$B> <#> <[> Well <,> what about the </[> </{> provisions <#> What time <{> <[> <unclear> 4 sylls </unclear> </[> <S1A-005$C> <#> <[> Is is your </[> </{> man coming here <S1A-005$B> <#> <{> <[> Yeah </[> <S1A-005$A> <#> <[> He said </[> </{> he would meet us here <S1A-005$B> <#> Just the boat 's arriving you know a few minutes ' wa\n\n\nWe can now extract the concordances.\n\nkwic_trans <- quanteda::kwic(\n  # tokenize transcripts\n  quanteda::tokens(transcripts_collapsed), \n  # define search pattern\n  pattern = phrase(\"you know\"))\n\n\n\n\n\n\nFirst 10 concordances for you know in three example transcripts.\n\n\ndocnamefromtoprekeywordpostpatternhttps://slcladal.github.io/data/ICEIrelandSample/S1A-001.txt6263was only the fourth timeyou know< # > It wasyou knowhttps://slcladal.github.io/data/ICEIrelandSample/S1A-001.txt204205it went the last timeyou know< # > And Stephanieyou knowhttps://slcladal.github.io/data/ICEIrelandSample/S1A-001.txt235236had refused the other timesyou know< # > But Stephanieyou knowhttps://slcladal.github.io/data/ICEIrelandSample/S1A-001.txt272273, > it jumped ityou know< # > It wasyou knowhttps://slcladal.github.io/data/ICEIrelandSample/S1A-001.txt602603that one < , >you knowand starting anew fresh <you knowhttps://slcladal.github.io/data/ICEIrelandSample/S1A-001.txt665666{ > < [ >you know< / [ > <you knowhttps://slcladal.github.io/data/ICEIrelandSample/S1A-001.txt736737> We didn't discuss ityou know< S1A-001 $ A >you knowhttps://slcladal.github.io/data/ICEIrelandSample/S1A-001.txt922923on Tuesday < , >you know< # > But Iyou knowhttps://slcladal.github.io/data/ICEIrelandSample/S1A-001.txt1,1261,127that she could take heryou knowthe wee shoulder bag sheyou knowhttps://slcladal.github.io/data/ICEIrelandSample/S1A-001.txt1,2571,258around < , > uhmyou knowtheir timetable and < ,you know\n\n\nThe results show that each non-alphanumeric character is counted as a single word which reduces the context of the keyword substantially. Also, the docname column contains the full path to the data which make it hard to parse the content of the table. To address the first issue, we specify the tokenizer that we will use to not disrupt the annotation too much. In addition, we clean the docname column and extract only the file name. Lastly, we will expand the context window to 10 so that we have a better understanding of the context in which the phrase was used.\n\nkwic_trans <- quanteda::kwic(\n  # tokenize transcripts\n  quanteda::tokens(transcripts_collapsed, what = \"fasterword\"), \n  # define search\n  pattern = phrase(\"you know\"),\n  # extend context\n  window = 10) %>%\n  # clean docnames\n  dplyr::mutate(docname = str_replace_all(docname, \".*/([A-Z][0-9][A-Z]-[0-9]{1,3}).txt\", \"\\\\1\"))\n\n\n\n\n\n\nFirst 10 concordances for you know in three example transcripts.\n\n\ndocnamefromtoprekeywordpostpatternS1A-0014243let me jump <,> that was only the fourth timeyou know<#> It was great <&> laughter </&> <S1A-001$A> <#> Whatyou knowS1A-001140141the whip <,> and over it went the last timeyou know<#> And Stephanie told her she was very determined andyou knowS1A-001164165<&> laughter </&> because it had refused the other timesyou know<#> But Stephanie wouldn't let her give up on ityou knowS1A-001193194and keep coming back <,> until <,> it jumped ityou know<#> It was good <S1A-001$A> <#> Yeah I 'm notyou knowS1A-001402403'd be far better waiting <,> for that one <,>you knowand starting anew fresh <S1A-001$A> <#> Yeah but I meanyou knowS1A-001443444the best goes top of the league <,> <{> <[>you know</[> <S1A-001$A> <#> <[> So </[> </{> it 's likeyou knowS1A-001484485I 'm not sure now <#> We didn't discuss ityou know<S1A-001$A> <#> Well it sounds like more money <S1A-001$B> <#>you know\n\n\nExtending the context can also be used to identify the speaker that has uttered the search pattern that we are interested in. We will do just that as this is a common task in linguistics analyses.\nTo extract speakers, we need to follow these steps:\n\nCreate normal concordances of the pattern that we are interested in.\nGenerate concordances of the pattern that we are interested in with a substantially enlarged context window size.\nExtract the speakers from the enlarged context window size.\nAdd the speakers to the normal concordances using the left-join function from the dplyr package.\n\n\nkwic_normal <- quanteda::kwic(\n  # tokenize transcripts\n  quanteda::tokens(transcripts_collapsed, what = \"fasterword\"), \n  # define search\n  pattern = phrase(\"you know\")) %>%\n  as.data.frame()\nkwic_speaker <- quanteda::kwic(\n    # tokenize transcripts\n  quanteda::tokens(transcripts_collapsed, what = \"fasterword\"), \n  # define search\n  pattern = phrase(\"you know\"), \n  # extend search window\n  window = 500) %>%\n  # convert to data frame\n  as.data.frame() %>%\n  # extract speaker (comes after $ and before >)\n  dplyr::mutate(speaker = stringr::str_replace_all(pre, \".*\\\\$(.*?)>.*\", \"\\\\1\")) %>%\n  # extract speaker\n  dplyr::pull(speaker)\n# add speaker to normal kwic\nkwic_combined <- kwic_normal %>%\n  # add speaker\n  dplyr::mutate(speaker = kwic_speaker) %>%\n  # simplify docname\n  dplyr::mutate(docname = stringr::str_replace_all(docname, \".*/([A-Z][0-9][A-Z]-[0-9]{1,3}).txt\", \"\\\\1\")) %>%\n  # remove superfluous columns\n  dplyr::select(-to, -from, -pattern)\n\n\n\n\n\n\nFirst 10 concordances for you know in three example transcripts with speakers that uttered them..\n\n\ndocnameprekeywordpostspeakerS1A-001was only the fourth timeyou know<#> It was great <&>BS1A-001it went the last timeyou know<#> And Stephanie told herBS1A-001had refused the other timesyou know<#> But Stephanie wouldn't letBS1A-001until <,> it jumped ityou know<#> It was good <S1A-001$A>BS1A-001<,> for that one <,>you knowand starting anew fresh <S1A-001$A>BS1A-001the league <,> <{> <[>you know</[> <S1A-001$A> <#> <[> SoBS1A-001<#> We didn't discuss ityou know<S1A-001$A> <#> Well it soundsBS1A-001her lesson on Tuesday <,>you know<#> But I was keepingBS1A-001that she could take heryou knowthe wee shoulder bag sheBS1A-001show them around <,> uhmyou knowtheir timetable and <,> giveB\n\n\nThe resulting table shows that we have successfully extracted the speakers (identified by the letters in the speaker column) and cleaned the file names (in the docnames column)."
  },
  {
    "objectID": "kwics.html#customizing-concordances",
    "href": "kwics.html#customizing-concordances",
    "title": "Concordancing with R",
    "section": "Customizing concordances",
    "text": "Customizing concordances\nAs R represents a fully-fledged programming environment, we can, of course, also write our own, customized concordance function. The code below shows how you could go about doing so. Note, however, that this function only works if you enter more than a single file.\n\nmykwic <- function(txts, pattern, context) {\n  # activate packages\n  require(stringr)\n  require(plyr)\n  # list files\n  conc <- sapply(txts, function(x) {\n    # determine length of text\n    lngth <- as.vector(unlist(nchar(x)))\n    # determine position of hits\n    idx <- str_locate_all(x, pattern)\n    idx <- idx[[1]]\n    ifelse(nrow(idx) >= 1, idx <- idx, return(\"No hits found\"))\n    # define start position of hit\n    token.start <- idx[,1]\n    # define end position of hit\n    token.end <- idx[,2]\n    # define start position of preceding context\n    pre.start <- ifelse(token.start-context < 1, 1, token.start-context)\n    # define end position of preceding context\n    pre.end <- token.start-1\n    # define start position of subsequent context\n    post.start <- token.end+1\n    # define end position of subsequent context\n    post.end <- ifelse(token.end+context > lngth, lngth, token.end+context)\n    # extract the texts defined by the positions\n    PreceedingContext <- substring(x, pre.start, pre.end)\n    Token <- substring(x, token.start, token.end)\n    SubsequentContext <- substring(x, post.start, post.end)\n    conc <- cbind(PreceedingContext, Token, SubsequentContext)\n    # return concordance\n    return(conc)\n    })\n  concdf <- ldply(conc, data.frame)\n  colnames(concdf)[1]<- \"File\"\n  return(concdf)\n}\n\nWe can now try if this function works by searching for the sequence you know in the transcripts that we have loaded earlier. One difference between the kwic function provided by the quanteda package and the customized concordance function used here is that the kwic function uses the number of words to define the context window, while the mykwic function uses the number of characters or symbols instead (which is why we use a notably higher number to define the context window).\n\nmyconcordances <- mykwic(transcripts_collapsed, \"you know\", 50)\n\n\n\n\n\n\nFirst 10 concordances for you know extracted using the mykwic function.\n\n\nFilePreceedingContextTokenSubsequentContexthttps://slcladal.github.io/data/ICEIrelandSample/S1A-001.txt to let me jump <,> that was only the fourth time you know <#> It was great <&> laughter </&> <S1A-001$A> <#https://slcladal.github.io/data/ICEIrelandSample/S1A-001.txt with the whip <,> and over it went the last time you know <#> And Stephanie told her she was very determinehttps://slcladal.github.io/data/ICEIrelandSample/S1A-001.txtghter </&> because it had refused the other times you know <#> But Stephanie wouldn't let her give up on it https://slcladal.github.io/data/ICEIrelandSample/S1A-001.txtk and keep coming back <,> until <,> it jumped it you know <#> It was good <S1A-001$A> <#> Yeah I 'm not so https://slcladal.github.io/data/ICEIrelandSample/S1A-001.txtshe 'd be far better waiting <,> for that one <,> you know and starting anew fresh <S1A-001$A> <#> Yeah but https://slcladal.github.io/data/ICEIrelandSample/S1A-001.txter 's the best goes top of the league <,> <{> <[> you know </[> <S1A-001$A> <#> <[> So </[> </{> it 's like https://slcladal.github.io/data/ICEIrelandSample/S1A-001.txtot <#> I 'm not sure now <#> We didn't discuss it you know <S1A-001$A> <#> Well it sounds like more money <Shttps://slcladal.github.io/data/ICEIrelandSample/S1A-001.txtn Monday and do without her lesson on Tuesday <,> you know <#> But I was keeping her going cos I says oh I whttps://slcladal.github.io/data/ICEIrelandSample/S1A-001.txte to take it tomorrow <,> that she could take her you know the wee shoulder bag she has <S1A-001$A> <#> Mhm https://slcladal.github.io/data/ICEIrelandSample/S1A-001.txtcker <,> and <,> sort of show them around <,> uhm you know their timetable and <,> give them their timetable\n\n\nAs this concordance function only works for more than one text, we split the text of Darwin’s On the Origin of Species into chapters and assign each section a name.\n\n# read in text\norigin_split <- origin %>%\n  stringr::str_squish() %>%\n  stringr::str_split(\"[CHAPTER]{7,7} [XVI]{1,7}\\\\. \") %>%\n  unlist()\norigin_split <- origin_split[which(nchar(origin_split) > 2000)]\n# add names\nnames(origin_split) <- paste0(\"text\", 1:length(origin_split))\n# inspect data\nnchar(origin_split)\n\n text1  text2  text3  text4  text5  text6  text7  text8  text9 text10 text11 \n 17465  69701  29396  35636  94170  73401  66349  69085  61085  58518  62094 \ntext12 text13 text14 text15 \n 67855  51300  87340  86574 \n\n\nNow that we have named elements, we can search for the pattern natural selection. We also need to clean the concordance as some sections do not contain any instances of the search pattern. To clean the data, we select only the columns File, PreceedingContext, Token, and SubsequentContext and then remove all rows where information is missing.\n\nnatsel_conc <- mykwic(origin_split, \"natural selection\", 50) %>%\n  dplyr::select(File, PreceedingContext, Token, SubsequentContext) %>%\n  na.omit()\n\n\n\n\n\n\nFirst 10 concordances for you know extracted using the mykwic function.\n\n\nFilePreceedingContextTokenSubsequentContexttext1nges. CHAPTER 3. STRUGGLE FOR EXISTENCE. Bears on natural selection. The term used in a wide sense. Geometrical powertext1xternal conditions. Use and disuse, combined with natural selection; organs of flight and of vision. Acclimatisation.text1he immutability of species. How far the theory of natural selection may be extended. Effects of its adoption on the stext2nd reversions of character probably do occur; but natural selection, as will hereafter be explained, will determine htext2ntry than in the other, and thus by a process of “natural selection,” as will hereafter be more fully explained, two text3ly important for us, as they afford materials for natural selection to accumulate, in the same manner as man can accutext3 have not been seized on and rendered definite by natural selection, as hereafter will be explained. Those forms whictext3to one in which it differs more, to the action of natural selection in accumulating (as will hereafter be more fully text4STRUGGLE FOR EXISTENCE. Bears on natural selection. The term used in a wide sense. Geometrical powertext5her useful nor injurious would not be affected by natural selection, and would be left a fluctuating element, as perh\n\n\nYou can go ahead and modify the customized concordance function to suit your needs."
  },
  {
    "objectID": "lex.html#preparation-and-session-set-up",
    "href": "lex.html#preparation-and-session-set-up",
    "title": "Lexicography with R",
    "section": "Preparation and session set up",
    "text": "Preparation and session set up\nThis tutorial is based on R. If you have not installed R or are new to it, you will find an introduction to and more information how to use R here. For this tutorials, we need to install certain packages from an R library so that the scripts shown below are executed without errors. Before turning to the code below, please install the packages by running the code below this paragraph. If you have already installed the packages mentioned below, then you can skip ahead and ignore this section. To install the necessary packages, simply run the following code - it may take some time (between 1 and 5 minutes to install all of the packages so you do not need to worry if it takes some time).\n\n# set options\noptions(stringsAsFactors = F)         # no automatic data transformation\noptions(\"scipen\" = 100, \"digits\" = 4) # suppress math annotation\n# install packages\ninstall.packages(\"dplyr\")\ninstall.packages(\"stringr\")\ninstall.packages(\"udpipe\")\ninstall.packages(\"tidytext\")\ninstall.packages(\"coop\")\ninstall.packages(\"cluster\")\ninstall.packages(\"flextable\")\ninstall.packages(\"textdata\")\n# install klippy for copy-to-clipboard button in code chunks\ninstall.packages(\"remotes\")\nremotes::install_github(\"rlesur/klippy\")\n\nIn a next step, we load the packages.\n\n# load packages\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(udpipe)\nlibrary(tidytext)\nlibrary(coop)\nlibrary(cluster)\nlibrary(flextable)\n# activate klippy for copy-to-clipboard button\nklippy::klippy()\n\n\n\n\nOnce you have installed R and RStudio and once you have initiated the session by executing the code shown above, you are good to go."
  },
  {
    "objectID": "lex.html#correcting-and-extending-dictionaries",
    "href": "lex.html#correcting-and-extending-dictionaries",
    "title": "Lexicography with R",
    "section": "Correcting and Extending Dictionaries",
    "text": "Correcting and Extending Dictionaries\nFortunately, it is very easy in R to correct entries, i.e., changing lemmas or part-of-speech tags, and to extend entries, i.e., adding additional layers of information such as urls or examples.\nWe will begin to extend our dictionary by adding an additional column (called annotation) in which we will add information.\n\n# generate dictionary\ntext_dict_ext <- text_dict %>%\n  # removing an entry\n  dplyr::filter(!(lemma == \"a\" & upos == \"NOUN\")) %>%\n  # editing entries\n  dplyr::mutate(upos = ifelse(lemma == \"aback\" & upos == \"NOUN\", \"PREP\", upos)) %>%\n  # adding comments \n  dplyr::mutate(comment = dplyr::case_when(lemma == \"a\" ~ \"also an before vowels\",\n                                           lemma == \"Aaronson\" ~ \"Name of someone.\", \n                                           T ~ \"\"))\n# inspect\nhead(text_dict_ext, 10)\n\n# A tibble: 10 × 5\n# Groups:   token, lemma [8]\n   token     lemma     upos  frequency comment                \n   <chr>     <chr>     <chr>     <int> <chr>                  \n 1 a         a         DET        2277 \"also an before vowels\"\n 2 A         a         DET         107 \"also an before vowels\"\n 3 Aaronson  Aaronson  PROPN         8 \"Name of someone.\"     \n 4 aback     aback     ADV           1 \"\"                     \n 5 aback     aback     PREP          1 \"\"                     \n 6 abandon   abandon   VERB          2 \"\"                     \n 7 abandon   abandon   ADP           1 \"\"                     \n 8 abandoned abandon   VERB          4 \"\"                     \n 9 abasement abasement NOUN          1 \"\"                     \n10 abashed   abashed   VERB          1 \"\"                     \n\n\nTo make it a bit more interesting but also keep this tutorial simple and straight-forward, we will add information about the polarity and emotionally of the words in our dictionary. We can do this by performing a sentiment analysis on the lemmas using the tidytext package.\nThe tidytext package contains three sentiment dictionaries (nrc, bing, and afinn). For the present purpose, we use the ncrdictionary which represents the Word-Emotion Association Lexicon [4]. The Word-Emotion Association Lexicon which comprises 10,170 terms, and in which lexical elements are assigned scores based on ratings gathered through the crowd-sourced Amazon Mechanical Turk service. For the Word-Emotion Association Lexicon raters were asked whether a given word was associated with one of eight emotions. The resulting associations between terms and emotions are based on 38,726 ratings from 2,216 raters who answered a sequence of questions for each word which were then fed into the emotion association rating [cf. 4]. Each term was rated 5 times. For 85 percent of words, at least 4 raters provided identical ratings. For instance, the word cry or tragedy are more readily associated with SADNESS while words such as happy or beautiful are indicative of JOY and words like fit or burst may indicate ANGER. This means that the sentiment analysis here allows us to investigate the expression of certain core emotions rather than merely classifying statements along the lines of a crude positive-negative distinction.\nTo be able to use the Word-Emotion Association Lexicon we need to add another column to our data frame called word which simply contains the lemmatized word. The reason is that the lexicon expects this column and only works if it finds a word column in the data. The code below shows how to add the emotion and polarity entries to our dictionary.\n\n# generate dictionary\ntext_dict_snt <- text_dict_ext %>%\n  dplyr::mutate(word = lemma) %>%\n  dplyr::left_join(get_sentiments(\"nrc\")) %>%\n  dplyr::group_by(token, lemma, upos, comment) %>%\n  dplyr::summarise(sentiment = paste0(sentiment, collapse = \", \"))\n# inspect\nhead(text_dict_snt, 10) \n\n# A tibble: 10 × 5\n# Groups:   token, lemma, upos [10]\n   token     lemma     upos  comment                 sentiment              \n   <chr>     <chr>     <chr> <chr>                   <chr>                  \n 1 a         a         DET   \"also an before vowels\" NA                     \n 2 A         a         DET   \"also an before vowels\" NA                     \n 3 Aaronson  Aaronson  PROPN \"Name of someone.\"      NA                     \n 4 aback     aback     ADV   \"\"                      NA                     \n 5 aback     aback     PREP  \"\"                      NA                     \n 6 abandon   abandon   ADP   \"\"                      fear, negative, sadness\n 7 abandon   abandon   VERB  \"\"                      fear, negative, sadness\n 8 abandoned abandon   VERB  \"\"                      fear, negative, sadness\n 9 abasement abasement NOUN  \"\"                      NA                     \n10 abashed   abashed   VERB  \"\"                      NA                     \n\n\nThe resulting extended dictionary now contains not only the token, the lemma, and the pos-tag but also the sentiment from the Word-Emotion Association Lexicon."
  },
  {
    "objectID": "lex.html#generating-dictionaries-for-other-languages",
    "href": "lex.html#generating-dictionaries-for-other-languages",
    "title": "Lexicography with R",
    "section": "Generating dictionaries for other languages",
    "text": "Generating dictionaries for other languages\nAs mentioned above, the procedure for generating dictionaries can easily be applied to languages other than English. If you want to follow exactly the procedure described above, then the language set of the TreeTagger is the limiting factors as its R implementation only supports English, German, French, Italian, Spanish, and Dutch. fa part-of-speech tagged text in another language is already available to you, and you do not require the TreeTagger for the part-of-speech tagging, then you can skip the code chunk that is related to the tagging and you can modify the procedure described above to virtually any language.\nWe will now briefly create a German dictionary based on a subsection of the fairy tales collected by the brothers Grimm to show how the above procedure can be applied to a language other than English. In a first step, we load a German text into R.\n\ngrimm <- readLines(\"https://slcladal.github.io/data/GrimmsFairytales.txt\",\n                   encoding = \"latin1\") %>%\n  paste0(collapse = \" \")\n# show the first 500 characters of the text\nsubstr(grimm, start=1, stop=200)\n\n[1] \"Der Froschkönig oder der eiserne Heinrich  Ein Märchen der Brüder Grimm Brüder Grimm  In den alten Zeiten, wo das Wünschen noch geholfen hat, lebte ein König, dessen Töchter waren alle schön; aber die\"\n\n\nNext, we download a udpipe language model. In this case, we download a udpipe language model for German, but you can download udpipe language models for more than 60 languages.\n\n# download language model\nudpipe::udpipe_download_model(language = \"german-hdt\")\n\nIn my case, I have stored this model in a folder called udpipemodels and you can load it (if you have also save the model in a folder called udpipemodels within your Rproj folder as shown below).\n\n# load language model from your computer after you have downloaded it once\nm_ger <- udpipe_load_model(file = here::here(\"udpipemodels\",\n                                             #\"german-hdt-ud-2.5-191206.udpipe\"))\n                                             \"german-gsd-ud-2.5-191206.udpipe\"))\n\nIn a next step, we generating the dictionary based on the brothers’ Grimm fairy tales. We go through the same steps as for the English dictionary and collapse all the steps into a single code block.\n\n# tokenise, tag, dependency parsing\ngrimm_ann <- udpipe::udpipe_annotate(m_ger, x = grimm) %>%\n  # convert into a data frame\n  as.data.frame() %>%\n  # remove non-words\n  dplyr::filter(!stringr::str_detect(token, \"\\\\W\")) %>%\n  # filter out numbers\n  dplyr::filter(!stringr::str_detect(token, \"[0-9]\")) %>%\n  dplyr::group_by(token, lemma, upos) %>%\n  dplyr::summarise(frequency = dplyr::n()) %>%\n  dplyr::arrange(lemma)\n# inspect\nhead(grimm_ann, 10)\n\n# A tibble: 10 × 4\n# Groups:   token, lemma [8]\n   token    lemma    upos  frequency\n   <chr>    <chr>    <chr>     <int>\n 1 A        A        NOUN          1\n 2 ab       ab       ADP          12\n 3 abends   abend    ADV           2\n 4 Abend    Abend    NOUN          3\n 5 abends   abends   ADV           1\n 6 aber     aber     ADJ           1\n 7 aber     aber     ADV          56\n 8 aber     aber     CCONJ        32\n 9 Aber     aber     CCONJ        16\n10 abfallen abfallen VERB          1\n\n\nAs with the English dictionary, we have created a customized German dictionary based of a subsample of the brothers’ Grimm fairy tales holding the word form(token), the part-of-speech tag (tag), the lemmatized word type (lemma), the general word class (wclass), ad the frequency with which a word form occurs as a part-of-speech in the data (frequency)."
  },
  {
    "objectID": "lexsim.html#jaccard-similarity",
    "href": "lexsim.html#jaccard-similarity",
    "title": "Introduction to Lexical Similarity",
    "section": "Jaccard Similarity",
    "text": "Jaccard Similarity\nThe Jaccard similarity is defined as an intersection of two texts divided by the union of that two documents. In other words it can be expressed as the number of common words over the total number of the words in the two texts or documents. The Jaccard similarity of two documents ranges from 0 to 1, where 0 signifies no similarity and 1 signifies complete overlap.The mathematical representation of the Jaccard Similarity is shown below: -\n\\[\\begin{equation}\nJ(A,B) = \\frac{|A \\bigcap B|}{|A \\bigcup B |} = \\frac{|A \\bigcap B|}{|A| + |B| - |A \\bigcap B|}\n\\end{equation}\\]"
  },
  {
    "objectID": "lexsim.html#cosine-similarity",
    "href": "lexsim.html#cosine-similarity",
    "title": "Introduction to Lexical Similarity",
    "section": "Cosine Similarity",
    "text": "Cosine Similarity\nIn case of cosine similarity the two documents are represented in a n-dimensional vector space with each word represented in a vector form. Thus the cosine similarity metric measures the cosine of the angle between two n-dimensional vectors projected in a multi-dimensional space. The cosine similarity ranges from 0 to 1. A value closer to 0 indicates less similarity whereas a score closer to 1 indicates more similarity.The mathematical representation of the Cosine Similarity is shown below: -\n\\[\\begin{equation}\nsimilarity = cos(\\theta) = \\frac{A \\cdot B}{||A|| ||B||} = \\frac{\\sum_{i=1}^{n} A_{i} B_{i}}{\\sqrt{\\sum_{i=1}^{n} A_{i}^{2}} \\sqrt{\\sum_{i=1}^{n} B_{i}^{2}}}\n\\end{equation}\\]"
  },
  {
    "objectID": "lexsim.html#levenshtein-distance",
    "href": "lexsim.html#levenshtein-distance",
    "title": "Introduction to Lexical Similarity",
    "section": "Levenshtein Distance",
    "text": "Levenshtein Distance\nLevenshtein distance comparison is generally carried out between two words. It determines the minimum number of single character edits required to change one word to another. The higher the number of edits more are the texts different from each other.An edit is defined by either an insertion of a character, a deletion of character or a replacement of a character. For two words a and b with lengths i and j the Levenshtein distance is defined as follows: -\n\\[\\begin{equation}\nlev_{a,b}(i,j) =\n\\begin{cases}\n    max(i,j) & \\quad \\text{if min(i,j) = 0,}\\\\\n    min \\begin{cases}\n      lev_{a,b}(i-1,j)+1  \\\\\n      lev_{a,b}(i, j-1)+1  & \\text{otherwise.}\\\\\n      lev_{a,b}(i-1,j-1)+1_{(a_{i} \\neq b_{j})} \\\\\n  \\end{cases}\n  \\end{cases}\n\\end{equation}\\]"
  },
  {
    "objectID": "lexsim.html#preparation-and-session-set-up",
    "href": "lexsim.html#preparation-and-session-set-up",
    "title": "Introduction to Lexical Similarity",
    "section": "Preparation and session set up",
    "text": "Preparation and session set up\nThis tutorial is based on R. If you have not installed R or are new to it, you will find an introduction to and more information how to use R here. For this tutorials, we need to install certain packages from an R library so that the scripts shown below are executed without errors. Before turning to the code below, please install the packages by running the code below this paragraph. If you have already installed the packages mentioned below, then you can skip ahead ignore this section. To install the necessary packages, simply run the following code - it may take some time (between 1 and 5 minutes to install all of the packages so you do not need to worry if it takes some time).\n\n# set options\noptions(stringsAsFactors = F)\n# install libraries\ninstall.packages(\"stringdist\")\ninstall.packages(\"hashr\")\ninstall.packages(\"tidyverse\")\ninstall.packages(\"flextable\")\n# install klippy for copy-to-clipboard button in code chunks\ninstall.packages(\"remotes\")\nremotes::install_github(\"rlesur/klippy\")\n\nNow that we have installed the packages, we activate them as shown below.\n\n# set options\noptions(stringsAsFactors = F)          # no automatic data transformation\noptions(\"scipen\" = 100, \"digits\" = 12) # suppress math annotation\n# activate packages\nlibrary(stringdist)\nlibrary(hashr)\nlibrary(tidyverse)\nlibrary(flextable)\n# activate klippy for copy-to-clipboard button\nklippy::klippy()\n\n\n\n\nOnce you have installed R and RStudio and initiated the session by executing the code shown above, you are good to go."
  },
  {
    "objectID": "lexsim.html#jaccard-similarity-1",
    "href": "lexsim.html#jaccard-similarity-1",
    "title": "Introduction to Lexical Similarity",
    "section": "Jaccard Similarity",
    "text": "Jaccard Similarity\n\n# Using the seq_dist function along with hash function to calculate the Jaccard similarity word-wise\njac_sim_score = seq_dist(hash(strsplit(text1, \"\\\\s+\")), hash(strsplit(text2, \"\\\\s+\")), method = \"jaccard\",q=2)\nprint(paste0(\"The Jaccard similarity for the two texts is \",jac_sim_score))\n\n[1] \"The Jaccard similarity for the two texts is 0.727272727272727\""
  },
  {
    "objectID": "lexsim.html#cosine-similarity-1",
    "href": "lexsim.html#cosine-similarity-1",
    "title": "Introduction to Lexical Similarity",
    "section": "Cosine Similarity",
    "text": "Cosine Similarity\n\n# Using the seq_dist function along with hash function to calculate the Jaccard similarity word-wise\ncos_sim_score = seq_dist(hash(strsplit(text1, \"\\\\s+\")), hash(strsplit(text2, \"\\\\s+\")), method = \"cosine\",q=2)\nprint(paste0(\"The Cosine similarity for the two texts is \",cos_sim_score))\n\n[1] \"The Cosine similarity for the two texts is 0.571428571428572\""
  },
  {
    "objectID": "lexsim.html#levenshtein-distance-1",
    "href": "lexsim.html#levenshtein-distance-1",
    "title": "Introduction to Lexical Similarity",
    "section": "Levenshtein distance",
    "text": "Levenshtein distance\n\n# Insert edit\nins_edit = stringdist(insert_ex[1],insert_ex[2],method = \"lv\")\nprint(paste0(\"The insert edit distance for \",insert_ex[1],\" and \",insert_ex[2],\" is \",ins_edit))\n\n[1] \"The insert edit distance for Marta and Martha is 1\"\n\n# Delete edit\ndel_edit = stringdist(del_ex[1],del_ex[2],method = \"lv\")\nprint(paste0(\"The delete edit distance for \",del_ex[1],\" and \",del_ex[2],\" is \",del_edit))\n\n[1] \"The delete edit distance for Genome and Gnome is 1\"\n\n# Replace edit\nrep_edit = stringdist(rep_ex[1],rep_ex[2],method = \"lv\")\nprint(paste0(\"The replace edit distance for \",rep_ex[1],\" and \",rep_ex[2],\" is \",rep_edit))\n\n[1] \"The replace edit distance for Tim and Tom is 1\""
  },
  {
    "objectID": "license.html",
    "href": "license.html",
    "title": "",
    "section": "",
    "text": "Copyright (C) 2007 Free Software Foundation, Inc. http://fsf.org/ Everyone is permitted to copy and distribute verbatim copies of this license document, but changing it is not allowed."
  },
  {
    "objectID": "license.html#how-to-apply-these-terms-to-your-new-programs",
    "href": "license.html#how-to-apply-these-terms-to-your-new-programs",
    "title": "",
    "section": "How to Apply These Terms to Your New Programs",
    "text": "How to Apply These Terms to Your New Programs\nIf you develop a new program, and you want it to be of the greatest possible use to the public, the best way to achieve this is to make it free software which everyone can redistribute and change under these terms.\nTo do so, attach the following notices to the program. It is safest to attach them to the start of each source file to most effectively state the exclusion of warranty; and each file should have at least the “copyright” line and a pointer to where the full notice is found.\n{one line to give the program's name and a brief idea of what it does.}\nCopyright (C) {year}  {name of author}\n\nThis program is free software: you can redistribute it and/or modify\nit under the terms of the GNU General Public License as published by\nthe Free Software Foundation, either version 3 of the License, or\n(at your option) any later version.\n\nThis program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\nGNU General Public License for more details.\n\nYou should have received a copy of the GNU General Public License\nalong with this program.  If not, see <http://www.gnu.org/licenses/>.\nAlso add information on how to contact you by electronic and paper mail.\nIf the program does terminal interaction, make it output a short notice like this when it starts in an interactive mode:\n{project}  Copyright (C) {year}  {fullname}\nThis program comes with ABSOLUTELY NO WARRANTY; for details type `show w'.\nThis is free software, and you are welcome to redistribute it\nunder certain conditions; type `show c' for details.\nThe hypothetical commands show w' andshow c’ should show the appropriate parts of the General Public License. Of course, your program’s commands might be different; for a GUI interface, you would use an “about box”.\nYou should also get your employer (if you work as a programmer) or school, if any, to sign a “copyright disclaimer” for the program, if necessary. For more information on this, and how to apply and follow the GNU GPL, see http://www.gnu.org/licenses/.\nThe GNU General Public License does not permit incorporating your program into proprietary programs. If your program is a subroutine library, you may consider it more useful to permit linking proprietary applications with the library. If this is what you want to do, use the GNU Lesser General Public License instead of this License. But first, please read http://www.gnu.org/philosophy/why-not-lgpl.html."
  },
  {
    "objectID": "links.html",
    "href": "links.html",
    "title": "LINKS TO RESOURCES",
    "section": "",
    "text": "This page contains links to resources relevant for language technology, text analytics, data management and reproducibility, language data science and natural language processing."
  },
  {
    "objectID": "links.html#antconc-and-other-ant-tools",
    "href": "links.html#antconc-and-other-ant-tools",
    "title": "LINKS TO RESOURCES",
    "section": "AntConc (and other Ant tools)",
    "text": "AntConc (and other Ant tools)\nAntConc is a freeware corpus analysis toolkit for concordancing and text analysis developed by Laurence Anthony. In addition to AntConc, Laurence Anthony’s AntLab contains a conglomeration of extremely useful and very user-friendly software tools, that help and facilitate the analysis of textual data. Laurence has really developed an impressive, very user-friendly selection of tools that assist anyone interested in working with language data."
  },
  {
    "objectID": "links.html#smartool",
    "href": "links.html#smartool",
    "title": "LINKS TO RESOURCES",
    "section": "SMARTool",
    "text": "SMARTool\nSMARTool is a corpus-based language learning and analysis tool for for English-speaking learners of Russian. It is linguist-built and thus informed by modern linguistic theory. SMARTool assists learners with coping with the rich Russian morphology and has user-friendly, corpus-based information and help for learning the most frequent forms of 3,000 basic vocabulary items."
  },
  {
    "objectID": "links.html#applied-language-technology",
    "href": "links.html#applied-language-technology",
    "title": "LINKS TO RESOURCES",
    "section": "Applied Language Technology",
    "text": "Applied Language Technology\nApplied Language Technology is a website hosting learning materials for two courses taught at the University of Helsinki: Working with Text in Python and Natural Language Processing for Linguists. Together, these two courses provide an introduction to applied language technology for audiences who are unfamiliar with language technology and programming. The learning materials assume no previous knowledge of the Python programming language."
  },
  {
    "objectID": "links.html#cultural-analytics-with-python",
    "href": "links.html#cultural-analytics-with-python",
    "title": "LINKS TO RESOURCES",
    "section": "Cultural Analytics with Python",
    "text": "Cultural Analytics with Python\nIntroduction to Cultural Analytics & Python is a website established by Melanie Walsh that hosts an online textbook which offers an introduction to the programming language Python that is specifically designed for people interested in the humanities and social sciences."
  },
  {
    "objectID": "links.html#glam-workbench",
    "href": "links.html#glam-workbench",
    "title": "LINKS TO RESOURCES",
    "section": "GLAM Workbench",
    "text": "GLAM Workbench\nThe GLAM Workbench is a collection of tools, tutorials, examples, and hacks to help you work with data from galleries, libraries, archives, and museums (the GLAM sector). The primary focus is Australia and New Zealand, but new collections are being added all the time.\nThe resources in the GLAM Workbench are created and shared as Jupyter notebooks. Jupyter lets you combine narrative text and live code in an environment that encourages you to learn and explore. Jupyter notebooks run in your browser, and you can get started without installing any software!\nOne really great advantage of the GLAM workbench is that it is interactive: if you click on one of the links that says Run live on Binder, this will open the notebook, ready to use, in a customised computing environment using the Binder service."
  },
  {
    "objectID": "links.html#programming-historian",
    "href": "links.html#programming-historian",
    "title": "LINKS TO RESOURCES",
    "section": "Programming Historian",
    "text": "Programming Historian\nThe Programming Historian is a collaborative blog that contains lessons on various topics associated with computational humanities. The blog was founded in 2008 by William J. Turkel and Alan MacEachern. It focused heavily on the Python programming language and was published open access as a Network in Canadian History & Environment (NiCHE) Digital Infrastructure project. In 2012, Programming Historian expanded its editorial team and launched as an open access peer reviewed scholarly journal of methodology for digital historians."
  },
  {
    "objectID": "links.html#tapor-3",
    "href": "links.html#tapor-3",
    "title": "LINKS TO RESOURCES",
    "section": "TAPoR 3",
    "text": "TAPoR 3\nTAPoR 3, the Text Analysis Portal for Research contains a unique, well-curated list of a wide variety of tools commonly used or widely respected groups of tools used by leading scholars in the various fields of Digital Humanities. TAPoR 3 was developed with support from the Text Mining the Novel Project. The tools mentioned in the list provided by TAPoR 3 represent both tried and trusted tools used by DH scholars, or new advancements that offer exciting new possibilities in their field. Curated lists are excellent places to start exploring TAPoR from, and can help with deciding what to include in your own lists."
  },
  {
    "objectID": "links.html#quick-r",
    "href": "links.html#quick-r",
    "title": "LINKS TO RESOURCES",
    "section": "Quick-R",
    "text": "Quick-R\nQuick-R by DataCamp and maintained by Rob Kabacoff that contains tutorial and R code on R, data management, statistics, and data visualization. The site is extremely helpful and recommendable for everyone that looks for code snippets that can be adapted for your own use."
  },
  {
    "objectID": "links.html#sthda",
    "href": "links.html#sthda",
    "title": "LINKS TO RESOURCES",
    "section": "STHDA",
    "text": "STHDA\nStatistical Tools For High-Throughput Data Analysis (STHDA) is a website containing various tutorials on the implementation of statistical methods in R. It is particularly useful due to the wide range of topics and procedures it covers."
  },
  {
    "objectID": "links.html#text-crunching-centre",
    "href": "links.html#text-crunching-centre",
    "title": "LINKS TO RESOURCES",
    "section": "Text Crunching Centre",
    "text": "Text Crunching Centre\nAt the Text Crunching Centre, a team of experts in Natural Language Processing supports your text technology needs. The TCC is part of the Department of Computational Linguistics at the University of Zurich and it is a service offered to all departments of the University of Zurich as well as to external partners or customers."
  },
  {
    "objectID": "links.html#varieng",
    "href": "links.html#varieng",
    "title": "LINKS TO RESOURCES",
    "section": "VARIENG",
    "text": "VARIENG\nVARIENG stands for the Research Unit for the Study of Variation, Contacts and Change in English. It also stands for innovative thinking and team work in English corpus linguistics and the study of language variation and change. VARIENG members study the English language, its uses and users, both today and in the past."
  },
  {
    "objectID": "links.html#acqva-aurora-lab",
    "href": "links.html#acqva-aurora-lab",
    "title": "LINKS TO RESOURCES",
    "section": "AcqVA Aurora Lab",
    "text": "AcqVA Aurora Lab\nThe AcqVA Aurora Lab, is the lab of the UiT Aurora Center for Language Acquisition, Variation & Attrition at The Arctic University of Norway in Tromsø. Together with the Psycholinguistics of Language Representation (PoLaR) lab, the AcqVA Lab provides methodological support for researchers at the AcqVA Aurora Center."
  },
  {
    "objectID": "links.html#sydney-corpus-lab",
    "href": "links.html#sydney-corpus-lab",
    "title": "LINKS TO RESOURCES",
    "section": "Sydney Corpus Lab",
    "text": "Sydney Corpus Lab\nThe Sydney Corpus Lab aims to promote corpus linguistics in Australia. It’s a virtual, rather than a physical lab, and is an online platform for connecting computer-based linguists across the University of Sydney and beyond. Its mission is to build research capacity in corpus linguistics at the University of Sydney, to connect Australian corpus linguists, and to promote the method in Australia, both in linguistics and in other disciplines. We have strong links with the Sydney Centre for Language Research (Computational Approaches to Language node) and the Sydney Digital Humanities Research Group."
  },
  {
    "objectID": "links.html#antlab",
    "href": "links.html#antlab",
    "title": "LINKS TO RESOURCES",
    "section": "AntLab",
    "text": "AntLab\nLaurence Anthony’s AntLab contains a conglomeration of extremely useful and very user-friendly software tools, such as AntConc that help and facilitate the analysis of textual data."
  },
  {
    "objectID": "links.html#media-research-methods-lab",
    "href": "links.html#media-research-methods-lab",
    "title": "LINKS TO RESOURCES",
    "section": "Media Research Methods Lab",
    "text": "Media Research Methods Lab\nThe Media Research Methods Lab (MRML) at the Leibniz Institute for Media Research │ Hans Bredow Institute (HBI) is designed as a method-oriented lab, which focuses on linking established social science methods (surveys, observations, content analysis, experiments) with new digital methods from the field of computational social science (e.g. automated content analysis, network analysis, log data analysis, experience sampling) across topics and disciplines."
  },
  {
    "objectID": "links.html#national-centre-for-text-mining",
    "href": "links.html#national-centre-for-text-mining",
    "title": "LINKS TO RESOURCES",
    "section": "National Centre for Text Mining",
    "text": "National Centre for Text Mining\nThe National Centre for Text Mining (NaCTeM), located in the UK, is the first publicly-funded text mining centre in the world. We provide text mining services in response to the requirements of the UK academic community. On the NaCTeM website, you can find pointers to sources of information about text mining such as links to text mining services provided by NaCTeM, software tools (both those developed by the NaCTeM team and by other text mining groups), seminars, general events, conferences and workshops, tutorials and demonstrations, and text mining publications."
  },
  {
    "objectID": "links.html#hypotheses",
    "href": "links.html#hypotheses",
    "title": "LINKS TO RESOURCES",
    "section": "Hypotheses",
    "text": "Hypotheses\nHypotheses is a research blog by [Guillaume Desagulier](http://www2.univ-paris8.fr/desagulier/home/. In this blog, entitled Around the word, A corpus linguist’s notebook, Guillaume has recorded reflections and experiments on his practice as a usage-based corpus linguist and code for analyzing language using R."
  },
  {
    "objectID": "links.html#aneesha-bakharia",
    "href": "links.html#aneesha-bakharia",
    "title": "LINKS TO RESOURCES",
    "section": "Aneesha Bakharia",
    "text": "Aneesha Bakharia\nAneesha Bakharia is a blog by Aneesha Bakharia about Data Science, Topic Modeling, Deep Learning, Algorithm Usability and Interpretation, Learning Analytics, and Electronics. Aneesha began her career as an electronics engineer but quickly transitioned to an educational software developer. Aneesha has worked in the higher education and vocational educational sectors in a variety of technical, innovation and project management roles. In her most recent role before commencing at UQ, Aneesha was a project manager for a large OLT Teaching and Learning grant on Learning Analytics at QUT. Aneesha’s primary responsibilities at ITaLI include directing the design, development and implementation of learning analytics initiatives (including the Course Insights teacher facing dashboard) at UQ."
  },
  {
    "objectID": "links.html#linguistics-with-a-corpus",
    "href": "links.html#linguistics-with-a-corpus",
    "title": "LINKS TO RESOURCES",
    "section": "Linguistics with a Corpus",
    "text": "Linguistics with a Corpus\nLinguistics with a Corpus is a companion blog to the Cambridge Element book Doing Linguistics with a Corpus. Methodological Considerations for the Everyday User [1]. The blog is intended to be a forum for methodological discussion in corpus linguistics, and Jesse, Tove, and Doug very much welcome comments and thoughts by visitors and readers!"
  },
  {
    "objectID": "links.html#digital-observatory-blog",
    "href": "links.html#digital-observatory-blog",
    "title": "LINKS TO RESOURCES",
    "section": "Digital Observatory Blog",
    "text": "Digital Observatory Blog\nThe blog of the Digital Observatory at the Queensland University of Technology (QUT) contains updates on resources, meetings, (open) office hours, workshops, and developments at the Digital Observatory which provides services to researchers such as retrieving and processing social media data or offering workshops on data processing, data visualization, and anything related to gathering and working with social media data."
  },
  {
    "objectID": "links.html#booknlp",
    "href": "links.html#booknlp",
    "title": "LINKS TO RESOURCES",
    "section": "BookNLP",
    "text": "BookNLP\nBookNLP is a natural language processing pipeline that scales to books and other long documents (in English), including:\n\nPart-of-speech tagging\n\nDependency parsing\n\nEntity recognition\n\nCharacter name clustering (e.g., “Tom”, “Tom Sawyer”, “Mr. Sawyer”, “Thomas Sawyer” -> TOM_SAWYER) and coreference resolution\n\nQuotation speaker identification\n\nSupersense tagging (e.g., “animal”, “artifact”, “body”, “cognition”, etc.)\n\nEvent tagging\n\nReferential gender inference (TOM_SAWYER -> he/him/his)\n\nBookNLP ships with two models, both with identical architectures but different underlying BERT sizes. The larger and more accurate big model is fit for GPUs and multi-core computers; the faster small model is more appropriate for personal computers."
  },
  {
    "objectID": "links.html#digital-humanities-awards",
    "href": "links.html#digital-humanities-awards",
    "title": "LINKS TO RESOURCES",
    "section": "Digital Humanities Awards",
    "text": "Digital Humanities Awards\nThe Digital Humanities Awards page contains a list of great DH resources for the following categories:\n\nBest Exploration Of DH Failure/Limitations\n\nBest DH Data Visualization\n\nBest Use Of DH For Fun\n\nBest DH Dataset\n\nBest DH Short Publication\n\nBest DH Tool Or Suite Of Tools\n\nBest DH Training Materials\n\nSpecial Category: Best DH Response To COVID-19"
  },
  {
    "objectID": "links.html#text-analysis-in-python-for-social-scientists",
    "href": "links.html#text-analysis-in-python-for-social-scientists",
    "title": "LINKS TO RESOURCES",
    "section": "Text Analysis in Python for Social Scientists",
    "text": "Text Analysis in Python for Social Scientists\nThe book Text Analysis in Python for Social Scientists. Prediction and Classification contains a wealth of information about about a wide variety of sociocultural constructs. Automated prediction methods can infer these quantities (sentiment analysis is probably the most well-known application). However, there is virtually no limit to the kinds of things we can predict from text: power, trust, misogyny, are all signaled in language. These algorithms easily scale to corpus sizes infeasible for manual analysis. Prediction algorithms have become steadily more powerful, especially with the advent of neural network methods. However, applying these techniques usually requires profound programming knowledge and machine learning expertise. As a result, many social scientists do not apply them. This Element provides the working social scientist with an overview of the most common methods for text classification, an intuition of their applicability, and Python code to execute them. It covers both the ethical foundations of such work as well as the emerging potential of neural network methods."
  },
  {
    "objectID": "links.html#data-measurements-tool",
    "href": "links.html#data-measurements-tool",
    "title": "LINKS TO RESOURCES",
    "section": "Data Measurements Tool",
    "text": "Data Measurements Tool\nThis blog entry introduces the Data Measurements Tool. The Data Measurements Tool is a Interactive Tool for Looking at Datasets. The Data Measurements Tool is a open-source Python library and no-code interface called the Data Measurements Tool, using our Dataset and Spaces Hubs paired with the great Streamlit tool. This can be used to help understand, build, curate, and compare datasets.\n\nBack to top\nBack to HOME"
  },
  {
    "objectID": "litsty.html#what-is-computational-literary-stylistics",
    "href": "litsty.html#what-is-computational-literary-stylistics",
    "title": "Computational Literary Stylistics with R",
    "section": "What is computational literary stylistics?",
    "text": "What is computational literary stylistics?\nComputational literary stylistics refers to analyses of the language of literary texts by computational means using linguistic concepts and categories, with the goal of finding patters among the literary texts and explaining how literary meaning/s is/are created by specific language choices. Computational literary stylistics has been linked with distant reading which aims to find patterns in large amounts of literary data that would not be detectable by traditional close reading techniques.\nPreparation and session set up\nThis tutorial is based on R. If you have not installed R or are new to it, you will find an introduction to and more information how to use R here. For this tutorials, we need to install certain packages from an R library so that the scripts shown below are executed without errors. Before turning to the code below, please install the packages by running the code below this paragraph. If you have already installed the packages mentioned below, then you can skip ahead and ignore this section. To install the necessary packages, simply run the following code - it may take some time (between 1 and 5 minutes to install all of the packages so you do not need to worry if it takes some time).\n\n# install packages\ninstall.packages(\"tidytext\")\ninstall.packages(\"janeaustenr\")\ninstall.packages(\"tidyverse\")\ninstall.packages(\"quanteda\")\ninstall.packages(\"forcats\")\ninstall.packages(\"gutenbergr\")\ninstall.packages(\"flextable\")\ninstall.packages(\"quanteda.textstats\")\ninstall.packages(\"quanteda.textplots\")\ninstall.packages(\"lexRankr\")\n# install klippy for copy-to-clipboard button in code chunks\ninstall.packages(\"remotes\")\nremotes::install_github(\"rlesur/klippy\")\n\nWe now activate these packages as shown below.\n\n# set options\noptions(stringsAsFactors = F)          # no automatic data transformation\noptions(\"scipen\" = 100, \"digits\" = 12) # suppress math annotation\nlibrary(tidyverse)\nlibrary(janeaustenr)\nlibrary(tidytext)\nlibrary(forcats)\nlibrary(quanteda)\nlibrary(gutenbergr)\nlibrary(flextable)\n# activate klippy for copy-to-clipboard button\nklippy::klippy()\n\n\n\n\nOnce you have installed R and RStudio and initiated the session by executing the code shown above, you are good to go."
  },
  {
    "objectID": "litsty.html#word-frequency-distributions",
    "href": "litsty.html#word-frequency-distributions",
    "title": "Computational Literary Stylistics with R",
    "section": "Word-Frequency Distributions",
    "text": "Word-Frequency Distributions\nWord-frequency distributions can be used to determine if a text represents natural language (or a simple replacement cipher, for example) or if the text does not represent natural language (or a more complex cipher). In the following, we will check if the language used in texts we have downloaded from Project Gutenberg aligns with distributions that we would expect when dealing with natural language. In a first step, we determine both the term-frequency and the idf.\n\nbook_words <- books %>%\n  tidytext::unnest_tokens(word, text) %>%\n  dplyr::count(book, word, sort = TRUE)  %>% \n  dplyr::group_by(book) %>% \n  dplyr::mutate(total = sum(n))\n\n\n\n\n\n\nFirst 15 rows of the book_words data.\n\n\nbookwordntotalDarwinthe10,301157,002Darwinof7,864157,002Doylethe5,623105,378Darwinand4,443157,002Austenthe4,333122,359Austento4,163122,359Darwinin4,017157,002Twainthe3,79272,200Darwinto3,613157,002Austenof3,612122,359Austenand3,586122,359Twainand3,12572,200Doyleand3,018105,378Doylei3,003105,378Doyleto2,744105,378\n\n\nFrom the above table it is evident that the usual suspects the, and, to and so-forth are leading in terms of their usage frequencies in the novels. Now let us look at the distribution of n/total for each term in each of the novels (which represents the normalized term frequency).\n\nggplot(book_words, aes(n/total, fill = book)) +\n  geom_histogram(show.legend = FALSE) +\n  xlim(NA, 0.005) +\n  facet_wrap(~book, ncol = 2, scales = \"free_y\")\n\n\n\n\nTerm frequency distributions\n\n\n\n\nFrom the plots it is clear that we are dealing with a negative exponential distribution and that many words occur only rarely and that only few words occur frequently. In other words, only few words occur frequently while most words occur rarely. This relationship represents a distribution that is captured by Zipf’s law."
  },
  {
    "objectID": "litsty.html#zipfs-law",
    "href": "litsty.html#zipfs-law",
    "title": "Computational Literary Stylistics with R",
    "section": "Zipf’s Law",
    "text": "Zipf’s Law\nZipf’s Law represents an empirical power law or power function that was established in the 1930s. Zipf’s law is one of the most fundamental laws in linguistics [see 2] and it states that the frequency of a word is inversely proportional to its rank in a text or collection of texts.\nLet\n\nN be the number of elements in a text (or collection of texts);\nk be their rank;\ns be the value of the exponent characterizing the distribution.\n\nZipf’s law then predicts that out of a population of N elements, the normalized frequency of the element of rank k, f(k;s,N), is:\n\\[\\begin{equation}\nf(k;s,N)={\\frac {1/k^{s}}{\\sum \\limits _{n=1}^{N}(1/n^{s})}}\n\\end{equation}\\]\nIn the code chunk below, we check if Zipf’s Law applies to the words that occur in texts that we have downloaded from Project Gutenberg.\n\nfreq_by_rank <- book_words %>% \n  dplyr::group_by(book) %>% \n  dplyr::mutate(rank = row_number(), \n         `term frequency` = n/total) %>%\n  dplyr::ungroup()\n\n\n\n\n\n\nFirst 15 rows of the freq_by_rank data.\n\n\nbookwordntotalrankterm frequencyDarwinthe10,301157,00210.0656106291640Darwinof7,864157,00220.0500885339040Doylethe5,623105,37810.0533602839302Darwinand4,443157,00230.0282990025605Austenthe4,333122,35910.0354121887233Austento4,163122,35920.0340228344462Darwinin4,017157,00240.0255856613292Twainthe3,79272,20010.0525207756233Darwinto3,613157,00250.0230124457013Austenof3,612122,35930.0295196920537Austenand3,586122,35940.0293072025760Twainand3,12572,20020.0432825484765Doyleand3,018105,37820.0286397540284Doylei3,003105,37830.0284974093264Doyleto2,744105,37840.0260395908064\n\n\nTo get a better understanding of Zipf’s law, let us visualize the distribution by plotting on the logged rank of elements on the x-axis and logged frequency of the terms on the y-axis. If Zipf’s law holds, then we should see more or less straight lines that go from top left to bottom right.\n\nfreq_by_rank %>% \n  ggplot(aes(rank, `term frequency`, color = book)) + \n  geom_line(size = 1.1, alpha = 0.8, show.legend = FALSE) + \n  scale_x_log10() +\n  scale_y_log10()\n\n\n\n\nZipf’s law for a sample of literary texts\n\n\n\n\nWe can see that the plot has a negative slope which corroborates the inverse relationship of rank with respect to term frequency which shows that the words in the texts from Project Gutenberg follow Zipf’s law. This would ascertain that we are dealing with natural language and not a made up nonsense language or a complex cipher."
  },
  {
    "objectID": "litsty.html#lexical-diversity",
    "href": "litsty.html#lexical-diversity",
    "title": "Computational Literary Stylistics with R",
    "section": "Lexical Diversity",
    "text": "Lexical Diversity\nLexical diversity is a complexity measure that provides information about the lexicon size of a text, i.e. how many different words occur in a text given the size of the text. The Type-Token-Ratio (TTR) provides information about the number of word tokens (individual instances of a word) divided by the number of different word types (word forms). Let’s briefly elaborate on that and look a bit more closely at the terms types and tokens. The sentence The dog chased the cat contains five tokens but only 4 types because the occurs twice. Now, a text that is 100 words long and consist of 50 distinct words would have a TTR of .5 (50/100) while a text that is 100 words long but consist of 80 distinct words would have a TTR of .8 (80/100). Thus, typically, higher values indicate higher lexical diversity and more complex texts or more advanced learners of a language commonly have higher TTRs compared to simpler texts or less advanced language learners.\nAs such, we can use lexical diversity measures to analyze the complexity of the language in which a text is written which can be used to inspect the advances a language learner makes when acquiring a language: initially, the learner will have high TTR as they do not have a large vocabulary. The TTRs will increase as lexicon of the learner grows.\nIn the following example, we calculate the TTRs for the literary texts we have downloaded from Project Gutenberg. Ina first step, we tokenize the texts, i.e. we split the texts into individual words (tokens).\n\nbooks_texts <- books %>%\n  dplyr::group_by(book) %>%\n  dplyr::summarise(text = paste(text, collapse = \" \"))\ntexts <- books_texts %>%\n  dplyr::pull(text)\nnames(texts) <- books_texts %>%\n  dplyr::pull(book)\ntokens_texts <- texts %>%\n  quanteda::corpus() %>%\n  quanteda::tokens()\n# inspect data\nhead(tokens_texts)\n\nTokens consisting of 6 documents.\nAusten :\n [1] \"THERE\"       \"IS\"          \"AN\"          \"ILLUSTRATED\" \"EDITION\"    \n [6] \"OF\"          \"THIS\"        \"TITLE\"       \"WHICH\"       \"MAY\"        \n[11] \"VIEWED\"      \"AT\"         \n[ ... and 144,002 more ]\n\nDarwin :\n [1] \"Click\"       \"on\"          \"any\"         \"of\"          \"the\"        \n [6] \"filenumbers\" \"below\"       \"to\"          \"quickly\"     \"view\"       \n[11] \"each\"        \"ebook\"      \n[ ... and 177,159 more ]\n\nDoyle :\n [1] \"cover\"      \"The\"        \"Adventures\" \"of\"         \"Sherlock\"  \n [6] \"Holmes\"     \"by\"         \"Arthur\"     \"Conan\"      \"Doyle\"     \n[11] \"Contents\"   \"I\"         \n[ ... and 126,079 more ]\n\nPoe :\n [1] \"The\"      \"Raven\"    \"by\"       \"Edgar\"    \"Allan\"    \"Poe\"     \n [7] \"Once\"     \"upon\"     \"a\"        \"midnight\" \"dreary\"   \",\"       \n[ ... and 1,347 more ]\n\nShakespeare :\n [1] \"THE\"         \"TRAGEDY\"     \"OF\"          \"ROMEO\"       \"AND\"        \n [6] \"JULIET\"      \"by\"          \"William\"     \"Shakespeare\" \"Contents\"   \n[11] \"THE\"         \"PROLOGUE\"   \n[ ... and 32,889 more ]\n\nTwain :\n [1] \"THE\"        \"ADVENTURES\" \"OF\"         \"TOM\"        \"SAWYER\"    \n [6] \"By\"         \"Mark\"       \"Twain\"      \"(\"          \"Samuel\"    \n[11] \"Langhorne\"  \"Clemens\"   \n[ ... and 86,822 more ]\n\n\nNext, we calculate the TTR using the textstat_lexdiv function from the quanteda package and visualize the resulting TTRs for the literary texts that we have downloaded from Project Gutenberg.\n\ndfm(tokens_texts) %>%\n  quanteda.textstats::textstat_lexdiv(measure = \"TTR\") %>%\n  ggplot(aes(x = TTR, y = reorder(document, TTR))) + \n  geom_point() +\n  xlab(\"Type-Token-Ratio (TTR)\") +\n  ylab(\"\")\n\n\n\n\nWe can see that Darwin’s On the Origin of Species has the lowest lexical diversity while Edgar Allan Poe’s The Raven has the highest. This would suggest that the language in The Raven is more complex than the language of On the Origin of Species. However, this is too simplistic and shows that simple Type-Token Ratios are severely affected by text length (as well as orthographic errors) and should only be used to compare texts\n\nthat are comparatively long (at least 200 words)\nthat are approximately of the same length\nthat were error corrected so that orthographic errors do not confound the ratios\n\n\nAverage Sentence Length\nThe average sentence length (ASL) is another measure of textual complexity with more sophisticated language use being associated with longer and more complex sentences. As such, we can use the ASL as an alternative measure of the linguistic complexity of a text or texts.\n\nlibrary(lexRankr)\nbooks_sentences <- books %>%\n  dplyr::group_by(book) %>%\n  dplyr::summarise(text = paste(text, collapse = \" \")) %>%\n  lexRankr::unnest_sentences(sentence, text)\n\n\n\n\n\n\nFirst 15 rows of the books_sentences data.\n\n\nbooksent_idsentenceAusten1THERE IS AN ILLUSTRATED EDITION OF THIS TITLE WHICH MAY VIEWED AT EBOOK [# 42671 ] cover Pride and Prejudice By Jane Austen CONTENTS   Chapter 1   Chapter 2   Chapter 3   Chapter 4   Chapter 5   Chapter 6   Chapter 7   Chapter 8   Chapter 9   Chapter 10   Chapter 11   Chapter 12   Chapter 13   Chapter 14   Chapter 15   Chapter 16   Chapter 17   Chapter 18   Chapter 19   Chapter 20   Chapter 21   Chapter 22   Chapter 23   Chapter 24   Chapter 25   Chapter 26   Chapter 27   Chapter 28   Chapter 29   Chapter 30   Chapter 31   Chapter 32   Chapter 33   Chapter 34   Chapter 35   Chapter 36   Chapter 37   Chapter 38   Chapter 39   Chapter 40   Chapter 41   Chapter 42   Chapter 43   Chapter 44   Chapter 45   Chapter 46   Chapter 47   Chapter 48   Chapter 49   Chapter 50   Chapter 51   Chapter 52   Chapter 53   Chapter 54   Chapter 55   Chapter 56   Chapter 57   Chapter 58   Chapter 59   Chapter 60   Chapter 61 Chapter 1       It is a truth universally acknowledged, that a single man in       possession of a good fortune, must be in want of a wife.Austen2      However little known the feelings or views of such a man may be       on his first entering a neighbourhood, this truth is so well       fixed in the minds of the surrounding families, that he is       considered as the rightful property of some one or other of their       daughters.Austen3      “My dear Mr. Bennet,” said his lady to him one day, “have you       heard that Netherfield Park is let at last?”       Mr. Bennet replied that he had not.Austen4      “But it is,” returned she; “for Mrs.Austen5Long has just been here, and       she told me all about it.”       Mr. Bennet made no answer.Austen6      “Do not you want to know who has taken it?” cried his wife       impatiently.Austen7      “_You_ want to tell me, and I have no objection to hearing it.”       This was invitation enough.Austen8      “Why, my dear, you must know, Mrs.Austen9Long says that Netherfield is       taken by a young man of large fortune from the north of England;       that he came down on Monday in a chaise and four to see the       place, and was so much delighted with it that he agreed with Mr.       Morris immediately; that he is to take possession before       Michaelmas, and some of his servants are to be in the house by       the end of next week.”       “What is his name?”       “Bingley.”       “Is he married or single?”       “Oh!Austen10single, my dear, to be sure!Austen11A single man of large fortune;       four or five thousand a year.Austen12What a fine thing for our girls!”       “How so?Austen13how can it affect them?”       “My dear Mr. Bennet,” replied his wife, “how can you be so       tiresome!Austen14You must know that I am thinking of his marrying one of       them.”       “Is that his design in settling here?”       “Design!Austen15nonsense, how can you talk so!\n\n\nLet’s now visualize the results for potential differences or trends.\n\nbooks_sentences %>%\n  dplyr::mutate(sentlength = stringr::str_count(sentence, '\\\\w+')) %>%\n  ggplot(aes(x = sentlength, y = reorder(book, sentlength, mean), group = book)) +\n  stat_summary(fun = mean, geom = \"point\")   +          \n  stat_summary(fun.data = mean_cl_boot, geom = \"errorbar\", width = 0.2) +\n  xlab(\"Average Sentence Length (ASL)\") +\n  ylab(\"\")\n\n\n\n\nWe can see that that The Raven (which does not contain any punctuation) is (unsurprisingly) the text with the longest ASL while Shakespeare’s play Romeo and Juliet (which contains a lost of dialogues) is deemed the work with the shortest ASL. With the exception of Poe’s The Raven, the ALS results reflect text complexity with Darwin’s On the Origin of Species being more complex or written like than the other texts with Romeo and Juliet being the most similar to spoken dialogue."
  },
  {
    "objectID": "litsty.html#similarity-among-literary-texts",
    "href": "litsty.html#similarity-among-literary-texts",
    "title": "Computational Literary Stylistics with R",
    "section": "Similarity among literary texts",
    "text": "Similarity among literary texts\nWe will now explore how similar the language of literary works is. This approach can, of course, be extended to syntactic features or, for instance, to determine if certain texts belong to a certain literary genre or were written by a certain author. When extending this approach to syntactic features, one would naturally use features like the ALS, TTRs, or the frequency of adjectives as the basis for determining similarity. Regarding authorship, things like bigrams, spacing or punctuation methods would be relevant features.\nTo assess the similarity of literary works (based on the words that occur in the texts), we first create a feature list, i.e. a matrix with word frequencies. In the present case, we remove stop words as well as symbols (it can be useful to retain these but this is depends on the task at hand).\n\nfeature_mat <- books %>%\n  dplyr::group_by(book) %>%\n  dplyr::sample_n(100) %>%\n  dplyr::summarise(text = paste0(text, collapse = \" \")) %>%\n  dplyr::ungroup() %>%\n  quanteda::corpus(text_field = \"text\", docid_field = \"book\") %>%\n  quanteda::dfm(remove_punct = TRUE, remove_symbols = TRUE) %>% \n  quanteda::dfm_remove(pattern = stopwords(\"en\"))\n# inspect data\nfeature_mat[1:6, 1:6]\n\nDocument-feature matrix of: 6 documents, 6 features (75.00% sparse) and 0 docvars.\n             features\ndocs          serviceable nieces shared attendance mrs added\n  Austen                1      1      1          1   2     2\n  Darwin                0      0      0          0   0     0\n  Doyle                 0      0      0          0   1     1\n  Poe                   0      0      0          0   0     0\n  Shakespeare           0      0      0          0   0     1\n  Twain                 0      0      0          0   0     0\n\n\nWe see that the texts are represented as the row names and the terms the column names. The content of the matrix consists of the term frequencies.\nWe can now perform agglomerative hierarchical clustering and visualize the results in a dendrogram to assess the similarity of texts.\n\nbooks_dist <- as.dist(quanteda.textstats::textstat_dist(feature_mat))\nbooks_clust <- hclust(books_dist)\nplot(books_clust)\n\n\n\n\nAccording to the dendrogram, Conan Doyle’s The Adventures of Sherlock Holmes and Shakespeare’s Romeo and Juliet are the most similar texts. Edgar Allen Poe’s The Raven is the most idiosyncratic texts as it is on a branch by its own and is amalgamated with the other texts only as a very last step at the root of the tree."
  },
  {
    "objectID": "litsty.html#networks-of-personas",
    "href": "litsty.html#networks-of-personas",
    "title": "Computational Literary Stylistics with R",
    "section": "Networks of Personas",
    "text": "Networks of Personas\nA final procedure we will perform is a network analysis of the personas in Shakespeare’s Romeo and Juliet. We directly load a co-occurrence matrix which provides information about how often character’s in that play have been in the same scene (as the extraction of this information is a bit cumbersome, I have done that for you and you can simply load the matrix into R).\n\n# load data\nromeo <- read.delim(\"https://slcladal.github.io/data/romeo.txt\", sep = \"\\t\")\n# convert into feature co-occurrence matrix\nromeo_fcm <- as.fcm(as.matrix(romeo))\n# inspect data\nromeo_fcm\n\nFeature co-occurrence matrix of: 23 by 23 features.\n               features\nfeatures        Abraham Benvolio LordCapulet Gregory LadyCapulet LadyMontague\n  Abraham             1        1           1       1           1            1\n  Benvolio            1        7           3       1           2            1\n  LordCapulet         1        3           9       1           7            1\n  Gregory             1        1           1       1           1            1\n  LadyCapulet         1        2           7       1          10            1\n  LadyMontague        1        1           1       1           1            1\n  LordMontague        1        2           2       1           3            1\n  PrinceEscalus       1        2           2       1           3            1\n  Romeo               1        7           5       1           4            1\n  Sampson             1        1           1       1           1            1\n               features\nfeatures        LordMontague PrinceEscalus Romeo Sampson\n  Abraham                  1             1     1       1\n  Benvolio                 2             2     7       1\n  LordCapulet              2             2     5       1\n  Gregory                  1             1     1       1\n  LadyCapulet              3             3     4       1\n  LadyMontague             1             1     1       1\n  LordMontague             3             3     3       1\n  PrinceEscalus            3             3     3       1\n  Romeo                    3             3    14       1\n  Sampson                  1             1     1       1\n[ reached max_feat ... 13 more features, reached max_nfeat ... 13 more features ]\n\n\nAs the quanteda package has a very neat and easy to use function (textplot_network) for generating network graphs, we make use this function and can directly generate the network.\n\nquanteda.textplots::textplot_network(romeo_fcm, min_freq = 0.1, edge_alpha = 0.1, edge_size = 5)\n\n\n\n\nThe thickness of the lines indicates how often characters have co-occurred. We could now generate different network graphs for the personas in different plays to see how these plays and personas differ or we could apply the network analysis to other types of information such as co-occurrences of words.\nWe have reached the end of this tutorial. Please feel free to explore more of our content at https://slcladal.github.io/index.html - for computational literary stylistics, especially the tutorials on part-of-speech tagging and syntactic parsing as well as on lexicography with R provide relevant additional information."
  },
  {
    "objectID": "llr.html#visualizing-collocation-networks",
    "href": "llr.html#visualizing-collocation-networks",
    "title": "Analyzing learner language using R",
    "section": "Visualizing collocation networks",
    "text": "Visualizing collocation networks\nNetwork graphs are a very useful and flexible tool for visualizing relationships between elements such as words, personas, or authors. This section shows how to generate a network graph for collocations of the term transport using the quanteda package.\nIn a first step, we generate a document-feature matrix based on the sentences in the L1 data. A document-feature matrix shows how often elements (here these elements are the words that occur in the L1 data) occur in a selection of documents (here these documents are the sentences in the L1 data).\n\n# create document-feature matrix\nns_dfm <- ns_sen %>% \n  #quanteda::dfm(remove_punct = TRUE) %>%\n    quanteda::dfm(remove = stopwords('english'), remove_punct = TRUE)# %>%\n    #quanteda::dfm_trim(min_termfreq = 10, verbose = FALSE)\n\n\n\n\n\n\nFirst 6 rows and columns of the document-feature matrix.\n\n\ndoc_idtransport01basicdilemafacinguk'stext1110000text2101111text3100000text4000000text5100000text6100000\n\n\nAs we want to generate a network graph of words that collocate with the term organism, we use the calculateCoocStatistics function to determine which words most strongly collocate with our target term (organism).\n\n# load function for co-occurrence calculation\nsource(\"https://slcladal.github.io/rscripts/calculateCoocStatistics.R\")\n# define term\ncoocTerm <- \"transport\"\n# calculate co-occurrence statistics\ncoocs <- calculateCoocStatistics(coocTerm, ns_dfm, measure=\"LOGLIK\")\n# inspect results\ncoocs[1:10]\n\n    public        use    traffic       rail     facing  commuters    cheaper \n113.171974  19.437311  10.508626   9.652830   9.382889   9.382889   9.382889 \n     roads       less      buses \n  9.080648   8.067363   6.702863 \n\n\nWe now reduce the document-feature matrix to contain only the top 20 collocates of transport (plus our target word transport).\n\nredux_dfm <- dfm_select(ns_dfm, \n                        pattern = c(names(coocs)[1:10], \"transport\"))\n\n\n\n\n\n\nFirst 6 rows and columns of the reduced feature co-occurrence matrix.\n\n\ndoc_idtransportfacingrailcommutersusepublictext1100000text2111000text3100100text4000000text5100111text6100101\n\n\nNow, we can transform the document-feature matrix into a feature-co-occurrence matrix as shown below. A feature-co-occurrence matrix shows how often each element in that matrix co-occurs with every other element in that matrix.\n\ntag_fcm <- fcm(redux_dfm)\n\n\n\n\n\n\nFirst 6 rows and columns of the feature co-occurrence matrix.\n\n\ndoc_idtransportfacingrailcommutersusepublictransport341741838facing002000rail005142commuters000012use0000016public000001\n\n\nUsing the feature-co-occurrence matrix, we can generate the network graph which shows the terms that collocate with the target term transport with the edges representing the co-occurrence frequency. To generate this network graph, we use the textplot_network function from the quanteda.textplots package.\n\n# generate network graph\nquanteda.textplots::textplot_network(tag_fcm, \n                                     min_freq = 1, \n                                     edge_alpha = 0.3, \n                                     edge_size = 5,\n                                     edge_color = \"gray80\",\n                                     vertex_labelsize = log(rowSums(tag_fcm)*15))"
  },
  {
    "objectID": "loading.html#preparation-and-session-set-up",
    "href": "loading.html#preparation-and-session-set-up",
    "title": "Loading and saving data in R",
    "section": "Preparation and session set up",
    "text": "Preparation and session set up\nThis tutorial is based on R. If you have not installed R or are new to it, you will find an introduction to and more information how to use R here. For this tutorials, we need to install certain packages from an R library so that the scripts shown below are executed without errors. Before turning to the code below, please install the packages by running the code below this paragraph. If you have already installed the packages mentioned below, then you can skip ahead ignore this section. To install the necessary packages, simply run the following code - it may take some time (between 1 and 5 minutes to install all of the packages so you do not need to worry if it takes some time).\n\n# install packages\ninstall.packages(\"xlsx\")\ninstall.packages(\"tidyverse\")\ninstall.packages(\"flextable\")\ninstall.packages(\"openxlsx\")\ninstall.packages(\"here\")\n# install klippy for copy-to-clipboard button in code chunks\nremotes::install_github(\"rlesur/klippy\")\n\nNow that we have installed the packages, we can activate them as shown below.\n\n# set options\noptions(stringsAsFactors = F)         # no automatic data transformation\noptions(\"scipen\" = 100, \"digits\" = 4) # suppress math annotation\n# load packages\nlibrary(tidyverse)\nlibrary(flextable)\nlibrary(xlsx)\nlibrary(openxlsx)\nlibrary(here)\n# activate klippy for copy-to-clipboard button\nklippy::klippy()\n\n\n\n\nOnce you have installed RStudio and initiated the session by executing the code shown above, you are good to go."
  },
  {
    "objectID": "loading.html#writesave-excel-files",
    "href": "loading.html#writesave-excel-files",
    "title": "Loading and saving data in R",
    "section": "Write/Save Excel files",
    "text": "Write/Save Excel files"
  },
  {
    "objectID": "loading.html#load-txt-with-tabular-data",
    "href": "loading.html#load-txt-with-tabular-data",
    "title": "Loading and saving data in R",
    "section": "Load txt with tabular data",
    "text": "Load txt with tabular data\nIf the data is tabular and stored as a txt-file, there are various functions to read in the data. The most common functions are read.delim and read.table.\n\n# load tab txt 1\ntab1 <- read.delim(\"https://slcladal.github.io/data/mlrdata.txt\", \n                   sep = \"\\t\", header = TRUE)\n# inspect data\nhead(tab1)\n\n\n# load tab txt\ntab2 <- read.table(\"https://slcladal.github.io/data/mlrdata.txt\", \n                   header = TRUE)\n# inspect \nhead(tab2)"
  },
  {
    "objectID": "motion.html#preparation-and-session-set-up",
    "href": "motion.html#preparation-and-session-set-up",
    "title": "Creating Interactive Visualizations in R",
    "section": "Preparation and session set up",
    "text": "Preparation and session set up\nThis tutorial is based on R. If you have not installed R or are new to it, you will find an introduction to and more information how to use R here. For this tutorials, we need to install certain packages from an R library so that the scripts shown below are executed without errors. Before turning to the code below, please install the packages by running the code below this paragraph. If you have already installed the packages mentioned below, then you can skip ahead and ignore this section. To install the necessary packages, simply run the following code - it may take some time (between 1 and 5 minutes to install all of the libraries so you do not need to worry if it takes some time).\n\n# install packages\ninstall.packages(\"googleVis\")\ninstall.packages(\"tidyverse\")\ninstall.packages(\"DT\")\ninstall.packages(\"flextable\")\ninstall.packages(\"ggplot2\")\ninstall.packages(\"gganimate\")\ninstall.packages(\"gapminder\")\ninstall.packages(\"maptools\")\ninstall.packages(\"plotly\")\ninstall.packages(\"leaflet\")\n# install klippy for copy-to-clipboard button in code chunks\ninstall.packages(\"remotes\")\nremotes::install_github(\"rlesur/klippy\")\n\nNow that we have installed the packages, we activate them as shown below.\n\n# set options\noptions(stringsAsFactors = F)          # no automatic data transformation\noptions(\"scipen\" = 100, \"digits\" = 12) # suppress math annotation\n# Warning: the following option adaptation requires re-setting during session outro!\nop <- options(gvis.plot.tag='chart')  # set gViz options\n# activate packages\nlibrary(googleVis)\nlibrary(tidyverse)\nlibrary(DT)\nlibrary(flextable)\nlibrary(ggplot2)\nlibrary(gganimate)\nlibrary(gapminder)\nlibrary(maptools)\nlibrary(plotly)\nlibrary(leaflet)\n# activate klippy for copy-to-clipboard button\nklippy::klippy()\n\n\n\n\nOnce you have installed R and RStudio and also initiated the session by executing the code shown above, you are good to go."
  },
  {
    "objectID": "motion.html#getting-started",
    "href": "motion.html#getting-started",
    "title": "Creating Interactive Visualizations in R",
    "section": "Getting Started",
    "text": "Getting Started\nTo get started with motion charts, we load the googleVis package for the visualizations, the tidyverse package for data processing, and we load a data set called coocdata. The coocdata contains information about how often adjectives were amplified by a degree adverb across time (see below).\n\n# load data\ncoocdata  <- base::readRDS(url(\"https://slcladal.github.io/data/coo.rda\", \"rb\"))\n\n\n\n\n\n\nFirst 15 rows of teh coocdata data.\n\n\nDecadeAmpAdjectiveN.Amp.N.Adj.OBSEXPProbabilityCollStrengthOddsRatiopattrsig1,870prettyamiable7400.00.00001repeln.s.1,870prettyamusing7400.00.00001repeln.s.1,870prettyangry7800.00.00001repeln.s.1,870prettyannoyed7100.00.00001repeln.s.1,870prettybad7700.00.00001repeln.s.1,870prettybeautiful71600.00.00001repeln.s.1,870prettybusy710800.10.02001repeln.s.1,870prettycharming7700.00.00001repeln.s.1,870prettyclever7400.00.00001repeln.s.1,870prettycomfortable7700.00.00001repeln.s.1,870prettycontemptible7200.00.00001repeln.s.1,870prettyconvenient71300.00.00001repeln.s.1,870prettydangerous74400.10.01001repeln.s.1,870prettydark71100.00.00001repeln.s.1,870prettydifferent720200.20.03001repeln.s.\n\n\nThe coocdata is rather complex and requires some processing. First, we rename the columns to render their naming more meaningful. In this context we rename the OBS column Frequency and the Amp column Amplifier. As we are only interested if an adjective was amplified by very, we collapse all amplifiers that are not very in a bin category called other. We then calculate the frequency of the adjective within each time period and also the frequency with which each adjective is amplified by either very or other amplifiers. Then, we calculate the percentage with which each adjective is amplified by very.\n\n# process data\ncoocs <- coocdata %>%\n  dplyr::select(Decade, Amp, Adjective, OBS) %>%\n  dplyr::rename(Frequency = OBS,\n         Amplifier = Amp) %>%\n  dplyr::mutate(Amplifier = ifelse(Amplifier == \"very\", \"very\", \"other\")) %>%\n  dplyr::group_by(Decade, Adjective, Amplifier) %>%\n  dplyr::summarise(Frequency = sum(Frequency)) %>%\n  dplyr::ungroup() %>%\n  tidyr::spread(Amplifier, Frequency) %>%\n  dplyr::group_by(Decade, Adjective) %>%\n  dplyr::mutate(Frequency_Adjective = sum(other + very),\n         Percent_very = round(very/(other+very)*100, 2)) %>%\n  dplyr::mutate(Percent_very = ifelse(is.na(Percent_very), 0, Percent_very),\n         Adjective = factor(Adjective))\n\n\n\n\n\n\nFirst 10 rows of teh coocs data.\n\n\nDecadeAdjectiveotherveryFrequency_AdjectivePercent_very1,870amiable00001,870amusing00001,870angry00001,870annoyed00001,870bad00001,870beautiful00001,870busy10101,870charming00001,870clever00001,870comfortable0000\n\n\nWe now have a data set that we can use to generate interactive visualization."
  },
  {
    "objectID": "motion.html#scatter-plots",
    "href": "motion.html#scatter-plots",
    "title": "Creating Interactive Visualizations in R",
    "section": "Scatter Plots",
    "text": "Scatter Plots\nScatter plots show the relationship between two numeric variables if you have more than one observation per variable level (if the data is not grouped by another variable). This means that you can use scatter plots to display data when you have, e.g. more than one observation for each data in your data set. If you only have a single observation, you could also use a line graph (which we will turn to below).\n\nscdat <- coocs %>%\n  dplyr::group_by(Decade) %>%\n  dplyr::summarise(Precent_very = mean(Percent_very))\n# create scatter plot\nSC <- gvisScatterChart(scdat, \n                       options=list(\n                         title=\"Interactive Scatter Plot\",\n                         legend=\"none\",\n                         pointSize=5))\n\nIf you want to display the visualization in a Notebook environment, you can use the plot function as shown below.\n\nplot(SC)\n\nHowever, if you want to display the visualization on a website, you must use the print function rather than the plot function and specify that you want to print a chart.\nprint(SC, 'chart')"
  },
  {
    "objectID": "motion.html#line-graphs",
    "href": "motion.html#line-graphs",
    "title": "Creating Interactive Visualizations in R",
    "section": "Line Graphs",
    "text": "Line Graphs\nTo create an interactive line chart, we use the gvisLineChart function as shown below.\n\n# create scatter plot\nSC <- gvisLineChart(scdat, \n                    options=list(\n                      title=\"Interactive Scatter Plot\",\n                      legend=\"none\"))\n\nIf you want to display the visualization in a Notebook environment, you can use the plot function. For website, you must use the print function and specify that you want to print a chart.\nprint(SC, 'chart')"
  },
  {
    "objectID": "motion.html#bar-plots",
    "href": "motion.html#bar-plots",
    "title": "Creating Interactive Visualizations in R",
    "section": "Bar Plots",
    "text": "Bar Plots\nTo create an interactive bar chart, we use the gvisBarChart function as shown below.\n\n# create scatter plot\nSC <- gvisBarChart(scdat, \n                       options=list(\n                         title=\"Interactive Bar chart\",\n                         legend=\"right\",\n                         pointSize=10))\n\nNormally, you would use the plot function to display the interactive chart but you must use the print function with the chart argument if you want to display the result on a website.\nprint(SC, 'chart')"
  },
  {
    "objectID": "motion.html#session-outro",
    "href": "motion.html#session-outro",
    "title": "Creating Interactive Visualizations in R",
    "section": "Session Outro",
    "text": "Session Outro\nWhen generating interactive maps, it is important that you reset the default graphics parameters which had to be adapted in the session set-up. Therefore, in a final step, we restore the default graphics options.\n\n## Set options back to original options\noptions(op)"
  },
  {
    "objectID": "net.html#creating-a-matrix",
    "href": "net.html#creating-a-matrix",
    "title": "Network Analysis using R",
    "section": "Creating a matrix",
    "text": "Creating a matrix\nNow that we have loaded the data, we need to split the data into scenes. Scenes during which personas leave or enter will have to be split too so that we arrive at a table that contains the personas that are present during a sub-scene.\n\n# load data\nrom <- read.delim(\"https://slcladal.github.io/data/romeo_tidy.txt\", sep = \"\\t\")\n\n\n\n\n\n\nFirst 15 rows of rom data.\n\n\nactscenepersoncontriboccurrencesACT I_SCENE IBENVOLIO247ACT I_SCENE ICAPULET29ACT I_SCENE IFIRST CITIZEN12ACT I_SCENE ILADY CAPULET110ACT I_SCENE IMONTAGUE63ACT I_SCENE IPRINCE13ACT I_SCENE IROMEO1614ACT I_SCENE ITYBALT23ACT I_SCENE IIBENVOLIO57ACT I_SCENE IICAPULET39ACT I_SCENE IIPARIS25ACT I_SCENE IIROMEO1114ACT I_SCENE IISERVANT83ACT I_SCENE IIIJULIET511ACT I_SCENE IIILADY CAPULET1110\n\n\nWe now transform that table into a co-occurrence matrix.\n\n\n\n\n\n\n\n\nFirst 5 rows and 5 columns of the of romeoco-occurrence matrix.\n\n\nPersonaBALTHASARBENVOLIOCAPULETFIRST CITIZENFIRST SERVANTBALTHASAR00100BENVOLIO00321CAPULET13012FIRST CITIZEN02100FIRST SERVANT01200\n\n\nThe data shows how often a character has appeared with each other character in the play - only Friar Lawrence and Friar John were excluded because they only appear in one scene where they talk to each other."
  },
  {
    "objectID": "net.html#tidy-networks",
    "href": "net.html#tidy-networks",
    "title": "Network Analysis using R",
    "section": "Tidy Networks",
    "text": "Tidy Networks\nA great way to generate network graphs is to combine functions from the igraph, the ggraph, and the tidygraph packages. The advantages are that the syntax of for creating the networks aligns with the tidyverse style of writing R and that the graphs can be modified very easily.\nTo generate network graphs in this way, we define the nodes and we can also add information about the odes that we can use later on (such as frequency information).\n\nva <- romeo %>%\n  dplyr::mutate(Persona = rownames(.),\n                Occurrences = rowSums(.)) %>%\n  dplyr::select(Persona, Occurrences) %>%\n  dplyr::filter(!str_detect(Persona, \"SCENE\"))\n\n\n\n\n\n\nPersonas and their frequencies of occurrence in Shakespeare’s Romeo and Juliet.\n\n\nPersonaOccurrencesBALTHASAR9BENVOLIO34CAPULET46FIRST CITIZEN14FIRST SERVANT12FRIAR LAWRENCE20JULIET36LADY CAPULET45MERCUTIO15MONTAGUE22NURSE38PARIS21PETER9PRINCE22ROMEO54SECOND SERVANT16SERVANT15TYBALT22\n\n\nNow, we define the edges, i.e., the connections between nodes and, again, we can add information in separate variables that we can use later on.\n\ned <- romeo %>%\n  dplyr::mutate(from = rownames(.)) %>%\n  tidyr::gather(to, Frequency, BALTHASAR:TYBALT) %>%\n  dplyr::mutate(Frequency = ifelse(Frequency == 0, NA, Frequency))\n\n\n\n\n\n\nEdges between personasin Shakespeare’s Romeo and Juliet.\n\n\nfromtoFrequencyBALTHASARBALTHASARBENVOLIOBALTHASARCAPULETBALTHASAR1FIRST CITIZENBALTHASARFIRST SERVANTBALTHASARFRIAR LAWRENCEBALTHASAR1JULIETBALTHASAR1LADY CAPULETBALTHASAR1MERCUTIOBALTHASARMONTAGUEBALTHASAR1NURSEBALTHASARPARISBALTHASAR1PETERBALTHASARPRINCEBALTHASAR1ROMEOBALTHASAR2\n\n\nNow that we have generated tables for the edges and the nodes, we can generate a graph object.\n\nig <- igraph::graph_from_data_frame(d=ed, vertices=va, directed = FALSE)\n\nWe will also add labels to the nodes as follows:\n\ntg <- tidygraph::as_tbl_graph(ig) %>% \n  tidygraph::activate(nodes) %>% \n  dplyr::mutate(label=name)\n\nWhen we now plot our network, it looks as shown below.\n\n# set seed\nset.seed(12345)\n# edge size shows frequency of co-occurrence\ntg %>%\n   ggraph(layout = \"fr\") +\n   geom_edge_arc(colour= \"gray50\",\n                 lineend = \"round\",\n                 strength = .1,\n                 alpha = .1) +\n   geom_node_text(aes(label = name), \n                  repel = TRUE, \n                  point.padding = unit(0.2, \"lines\"), \n                  colour=\"gray10\") +\n  theme_graph(background = \"white\") +\n  guides(edge_width = FALSE,\n         edge_alpha = FALSE)\n\n\n\n\nNow, we use the number of occurrences to define vertex size (or node size): the more often a character appears, the bigger it will appear in the graph.\n\nv.size <- V(tg)$Occurrences\n# inspect\nv.size\n\n [1]  9 34 46 14 12 20 36 45 15 22 38 21  9 22 54 16 15 22\n\n\nWhen we include this into our network, it looks as shown below.\n\n# set seed\nset.seed(12345)\n# edge size shows frequency of co-occurrence\ntg %>%\n   ggraph(layout = \"fr\") +\n   geom_edge_arc(colour= \"gray50\",\n                  lineend = \"round\",\n                 strength = .1) +\n   geom_node_point(size=log(v.size)*2) +\n   geom_node_text(aes(label = name), \n                  repel = TRUE, \n                  point.padding = unit(0.2, \"lines\"), \n                  size=sqrt(v.size), \n                  colour=\"gray10\") +\n  scale_edge_width(range = c(0, 2.5)) +\n  scale_edge_alpha(range = c(0, .3)) +\n  theme_graph(background = \"white\") +\n  guides(edge_width = FALSE,\n         edge_alpha = FALSE)\n\n\n\n\nNext, we modify the edges by using frequency information to define weights: the more often two characters appear in the same scene, the bigger the edge.\n\nE(tg)$weight <- E(tg)$Frequency\n# inspect weights\nhead(E(tg)$weight, 10)\n\n [1] NA NA  1 NA NA  1  1  1 NA  1\n\n\nWhen we include this into our network, it looks as shown below.\n\n# set seed\nset.seed(12345)\n# edge size shows frequency of co-occurrence\ntg %>%\n   ggraph(layout = \"fr\") +\n   geom_edge_arc(colour= \"gray50\",\n                  lineend = \"round\",\n                 strength = .1,\n                 aes(edge_width = weight,\n                     alpha = weight)) +\n   geom_node_point(size=log(v.size)*2) +\n   geom_node_text(aes(label = name), \n                  repel = TRUE, \n                  point.padding = unit(0.2, \"lines\"), \n                  size=sqrt(v.size), \n                  colour=\"gray10\") +\n  scale_edge_width(range = c(0, 2.5)) +\n  scale_edge_alpha(range = c(0, .3)) +\n  theme_graph(background = \"white\") +\n  theme(legend.position = \"top\") +\n  guides(edge_width = FALSE,\n         edge_alpha = FALSE)\n\n\n\n\nFinally, we define colors so that characters belonging to the same family have the same color.\n\n# define colors (by family)\nmon <- c(\"ABRAM\", \"BALTHASAR\", \"BENVOLIO\", \"LADY MONTAGUE\", \"MONTAGUE\", \"ROMEO\")\ncap <- c(\"CAPULET\", \"CAPULET’S COUSIN\", \"FIRST SERVANT\", \"GREGORY\", \"JULIET\", \"LADY CAPULET\", \"NURSE\", \"PETER\", \"SAMPSON\", \"TYBALT\")\noth <- c(\"APOTHECARY\", \"CHORUS\", \"FIRST CITIZEN\", \"FIRST MUSICIAN\", \"FIRST WATCH\", \"FRIAR JOHN\" , \"FRIAR LAWRENCE\", \"MERCUTIO\", \"PAGE\", \"PARIS\", \"PRINCE\", \"SECOND MUSICIAN\", \"SECOND SERVANT\", \"SECOND WATCH\", \"SERVANT\", \"THIRD MUSICIAN\")\n# create color vectors\nFamily <- dplyr::case_when(sapply(tg, \"[\")$nodes$name %in% mon ~ \"MONTAGUE\",\n                           sapply(tg, \"[\")$nodes$name %in% cap ~ \"CAPULET\",\n                           TRUE ~ \"Other\")\n# inspect colors\nFamily\n\n [1] \"MONTAGUE\" \"MONTAGUE\" \"CAPULET\"  \"Other\"    \"CAPULET\"  \"Other\"   \n [7] \"CAPULET\"  \"CAPULET\"  \"Other\"    \"MONTAGUE\" \"CAPULET\"  \"Other\"   \n[13] \"CAPULET\"  \"Other\"    \"MONTAGUE\" \"Other\"    \"Other\"    \"CAPULET\" \n\n\nNow, that we have created the different objects and defined their properties, we can finally visualize the finished network.\n\n# set seed\nset.seed(12345)\n# edge size shows frequency of co-occurrence\ntg %>%\n   ggraph(layout = \"fr\") +\n   geom_edge_arc(colour= \"gray50\",\n                  lineend = \"round\",\n                 strength = .1,\n                 aes(edge_width = weight,\n                     alpha = weight)) +\n   geom_node_point(size=log(v.size)*2, \n                   aes(color=Family)) +\n   geom_node_text(aes(label = name), \n                  repel = TRUE, \n                  point.padding = unit(0.2, \"lines\"), \n                  size=sqrt(v.size), \n                  colour=\"gray10\") +\n  scale_edge_width(range = c(0, 2.5)) +\n  scale_edge_alpha(range = c(0, .3)) +\n  theme_graph(background = \"white\") +\n  theme(legend.position = \"top\") +\n  guides(edge_width = FALSE,\n         edge_alpha = FALSE)"
  },
  {
    "objectID": "net.html#quanteda-networks",
    "href": "net.html#quanteda-networks",
    "title": "Network Analysis using R",
    "section": "Quanteda Networks",
    "text": "Quanteda Networks\nThe quanteda package contains many very useful functions for analyzing texts. Among these functions is the textplot_network function which provides a very handy way to display networks. The advantage of the network plots provided by or generated with the quanteda package is that you can create them with very little code. However, this comes at a cost as these visualizations cannot be modified easily (which means that their design is not very flexible compared to other methods for generating network visualizations).\nIn a first step, we transform the text vectors of the romeo data into a document-feature matrix using the dfm function.\n\n# create a document feature matrix\nromeo_dfm <- quanteda::as.dfm(romeo)\n# create feature co-occurrence matrix\nromeo_fcm <- quanteda::fcm(romeo_dfm)\n# inspect data\nhead(romeo_fcm)\n\nFeature co-occurrence matrix of: 6 by 18 features.\n                features\nfeatures         BALTHASAR BENVOLIO CAPULET FIRST CITIZEN FIRST SERVANT\n  BALTHASAR              1       25      31            11             6\n  BENVOLIO               0       39      93            39            27\n  CAPULET                0        0      65            42            39\n  FIRST CITIZEN          0        0       0             6            10\n  FIRST SERVANT          0        0       0             0             3\n  FRIAR LAWRENCE         0        0       0             0             0\n                features\nfeatures         FRIAR LAWRENCE JULIET LADY CAPULET MERCUTIO MONTAGUE\n  BALTHASAR                  20     26           31       11       17\n  BENVOLIO                   53     87           99       42       55\n  CAPULET                    74    131          117       52       65\n  FIRST CITIZEN              18     32           36       24       29\n  FIRST SERVANT              17     40           42       12       15\n  FRIAR LAWRENCE             15     61           72       23       32\n[ reached max_nfeat ... 8 more features ]\n\n\nThis feature-co-occurrence matrix can then serve as the input for the textplot_network function which already generates a nice network graph. The network graph can then be modified or customized easily by defining the arguments of the textplot_network function. To see how and which arguments can be modified, you can use ?textplot_network.\n\nquanteda.textplots::textplot_network(romeo_fcm, \n                                     min_freq = .5, \n                                     edge_alpha = 0.5, \n                                     edge_color = \"purple\",\n                                     vertex_labelsize = log(rowSums(romeo_fcm)),\n                                     edge_size = 2)"
  },
  {
    "objectID": "net.html#igraph-networks",
    "href": "net.html#igraph-networks",
    "title": "Network Analysis using R",
    "section": "iGraph Networks",
    "text": "iGraph Networks\n[2] have written a very recommendable tutorial on co-occurrence analysis and they propose an alternative for generating complex network visualization for co-occurrences. Their approach is to create and customize a graph object based on the iGraph package. To see how to create sophisticated network graphs using the iGraph package, see this tutorial on analyzing collocations or this tutorial."
  },
  {
    "objectID": "news.html",
    "href": "news.html",
    "title": "NEWS",
    "section": "",
    "text": "Keep up to date with the current developments at LADAL!\nBelow you will find information on and links to the latest developments at LADAL such as updates to the LADAL website, upcoming workshops and presentations, planned events, and links to resources.\n \n\n\n\n\nUSER STORIESWe are currently looking for user stories (also known as testimonials) to see and show what people use LADAL resources for. If you have used LADAL resources - be it by simply copying some code, attending a workshop, learning about a method using a tutorial, or in any other way - we would be extremely grateful, if you would send us your user story! To submit your user story, simply write up a paragraph describing how you have used LADAL resources and what you have used them for and send it to ladal@uq.edu.au. We really appreciate any feedback from you about this!\n\n\n\n\n\n\n\nANNOUNCEMENTS\n\n\n2022/09/01: Computational Thinking in the Humanities\n\n\n\n\n\nThe workshop Computational Thinking in the Humanities is a 3-hour online workshop featuring two plenary talks, lightning presentations, as well as a panel discussion. The workshop is co-organized by the Australian Text Analytics Platform (ATAP), FIN-CLARIAH and its UEF representatives, and the Australian Digital Observatory.\n\n\n\n2022/05/09: Episode 2 of the LADAL Webinar Series 2022!\n\n\n\n\n\nThe second talk in the LADAL Webinar Series 2022 is by Cedric Courtois about using archives as source and as study object. Cedric is a senior lecturer in the School of Communication and Arts in the Faculty of Humanities and Social Sciences at the University of Queensland. Before joining UQ, Cedric worked, for example, at the Hans Bredow Institute (HBI) which is now also the Leibniz Institute for Media Research and collaborating partner of LADAL. He is both an audience researcher and a methodologist. His research interests include algorithmic impact in digital culture and data science applications in (digital) media and communication research (including text mining and image processing).\n\n\n2022/03/07: LADAL Webinar Series 2022 - WE ARE BACK!\n\n\n\n\n\nThe LADAL Webinar Series 2022 starts off with a talk by Jack Grieve about cultural and regional constraints on the spread of linguistic innovations. Jack is Professor of Corpus Linguistics at the University of Birmingham and Turing Fellow at the Alan Turing Institute. His research involves analysing large corpora of natural language to understand language variation and change. He is especially interested in grammatical and lexical variation in the English language across time, space and communicative context, as well as developing methods for quantitative linguistic analysis. Jack also conducts research on authorship analysis and sometimes consults on casework as a forensic linguist.\n\n\n2022/01/10: We are growing again!\n\n\n\n\n\nBen Foley has joined LADAL and we are more than thrilled about this! Ben was the project manager of CoEDL’s Transcription Acceleration Project (TAP) and he has specialized on speech recognition and the development of user-friendly speech recognition tools. Also, Ben’s previous experience with Aboriginal and Torres Strait Islander language resource development has resulted in apps and websites galore.\nWelcome Ben!\n\n\n2021/12/02: LADAL officially joins ATAP!\n\n\n\n\n\nLADAL is now officially part of the Australian Text Analytics Platform (ATAP). The aim of the Australian Text Analytics Platform (ATAP) is to provide researchers with a toolset that is more powerful and customisable than those contained in the standard packages, while being accessible to a large number of researchers who do not have strong coding skills. A key outcome of the project will be the development of an integrated notebooks-based platform for processing and mining text data.\nATAP is funded by the Australian Research Data Commons (ARDC) Platforms Program and ATAP is lead by Michael - Martin is part of the steering committee, a Chief investigator (CI), and he is chairing a User Group.\n\n\n2021/09/02: Media Research Methods Lab collab!\n\n\n\n\n\nGregor Wiedemann who is co-directing the Media Research Methods Lab (MRML) at the Leibniz Institute for Media Research │ Hans Bredow Institute (HBI) has agreed to collaborate with LADAL. We are extremely happy about this here at LADAL given that the MRML is designed as a method-oriented lab, which focuses on linking established social science methods (surveys, observations, content analysis, experiments) with new digital methods from the field of computational social science (e.g. automated content analysis, network analysis, log data analysis, experience sampling) across topics and disciplines.\n\n\n\n2021/06/03: LADAL Opening\n\n\nLADAL Opening Webinar Series 2021\n\n\n\n\n\nFINALLY! We are really happy to announce that LADAL will have its OFFICIAL OPENING EVENT!\nThe LADAL Opening event will consist of weekly presentations from eminent figures in linguistics, data science, and computational humanities and will cover a wide range of topics related to LADAL-relevant issues!\nThe first event of the LADAL Opening is a presentation by Stefan Th. Gries on MuPADRF (Multifactorial Prediction and Deviation Analysis Using Regression/Random Forests) on June 3, 2021, 5pm Brisbane time (9am CET). The event will take place on Zoom (the Zoom link will be announced here, on Twitter, and via our collaborators).\nClick HERE for more information about the weekly LADAL Opening presentations\n\n\n\n\n\n2020/12/15: Statistical excellence - Stefan Th. Gries\n\n\nStefan Th. Gries affiliates with LADAL!\n\n\n\n\n\n\nWe are really pound and feel both honored and privileged that Stefan Th. Gries has agreed to contribute to LADAL! Stefan has played an outstanding role in promoting statistical and computational skills in the language sciences. Stefan’s textbooks Statistics for Linguistics with R – A Practical Introduction and Quantitative Corpus Linguistics with R: A Practical Introduction, but also with his research output and bootcamps have had a tremendous influence on many linguists working with empirical data!\n\n\n2020/12/02: Summer Scholar program support for LADAL\n\n\nLADAL get summer scholars\n\nUQ’s summer research program supports LADAL: four junior academics will assist LADAL, acquire new skills, and produce new materials. A very warm welcome to the summer scholars and we hope that they get as much as possible out of the program and provide great new materials!\n\n\n2020/10/09: VARIENG - LADAL becomes more Finlandish\n\n\nCollaboration with VARIENG\n\n\n\n\n\n\nThe VARIENG at the University of Helsinki has agreed to affiliate with LADAL! VARIENG is a perfect affiliate due to the similarity in the outlook and the alignment of aims of both VARIENG and LADAL. In addition, we are super happy to have VARIENG as an affiliate institution because of their extremely high scientific merit!\n\n\n2020/10/07: DDL expertism and Phylolygisms\n\n\nErich Round and Peter Crosthwaite affiliate with LADAL!\n\n\n\n\n\n\nErich Round - director of the Ancient Languages Lab and world-renowned phylogenetics expert as well as recipient of the British Academy Global Professorship - is now officially a contributor to LADAL! His expertise in R and phylogenetics fantastically complement our skill set here and we are more than going crazy for having him on board!\n\n\n\n\n\n\nAlso - and on an equally enthusiastic note - Peter Crosthwaite, the foremost proponent on Data Driven Learning in Australia, has agreed to be a LADAL affiliate! Peter is not only a fantastic second language acquisition scholar but he has probably one of the best overviews of existing software in this domain and is amazingly well versed in finding the right applications to do awesome research!\nWelcome on board!\n\n\n2020/10/05: Big in Japan\n\n\nLaurence Anthony affiliates with LADAL!\n\n\n\n\n\n\nLaurence Anthony - the royal highness of AntConc empire - has agreed to be an affiliate of LADAL! We are so glad to have him on board as Laurence is not only tech savvy as few others in Corpus Linguistics but also because Laurence is overall wholesome and a fantastic promoter of computation in HASS research!\n\n\n2020/10/02: Library Excellence!\n\n\nStephane Guillou has agreed to be a contributor and affiliate of LADAL!\n\nStephane Guillou has agreed to be a contributor and affiliate of LADAL! That is really fantastic not only because Stephane is all-around awesome and a true R wiz but Stephane is also directing the upskilling efforts in R, Python, and Git at the UQ library and thus brings along a fantastic skill-set!\n\n\n2020/09/29: Syndey Corpus Lab collab!\n\n\nCollaboration with the Sydney Corpus Lab!\n\n\n\n\n\n\nMonika Bednarek who is running the Sydney Corpus Lab at the University of Sydney, has agreed to be an affiliate member of LADAL. This is perfect for LADAL given Monika’s expertise and excellent research in Corpus Linguistics as well as the close alignment of the Sydney Corpus Lab with LADAL!\n\n\n2020/09/25: LADAL goes Finland!\n\n\nLADAL goes to Finland!\n\n\n\n\n\n\nThe news that LADAL exists has reached the other side of the globe: Martin was invited by Mikko Laitinen from the University of Eastern Finland to give a guest lecture about his experiences in establishing LADAL in the context of an event about developing support infrastructures for computational social sciences and humanities research.\n\n\n\n\n\n\n\n2020/09/22: Interaction!\n\n\nAiming at interactivity using Binder!\n\nWe have decided to include interactive exercises into our tutorials and we are currently looking into different options how to achieve this. Currently Binder appears to be a viable pathway forward.\n\n\n2020/09/15: They multiply!\n\n\nThe LADAL team is expanding!\n\nWe are delighted to announce that Katy McHugh, Stephen Clark, and Restuadi Restuadi have joined the LADAL team!\nKaty, Stephen, and Restuadi will be involved in the restructuring, professionalizing, and revamping the LADAL webpage. We would like to extend our warmest welcome to them and express our gratitude to the School of Languages and Cultures at UQ for providing the funding for the RA positions.\n\n\n\n\n\n\n\n\n\n\nEVENTS\n\nThe LADAL team organizes workshops and LADAL members present their research or information relevant to LADAL at conferences. Below are links to upcoming events (conferences/workshops/presentations) and presentations containing information about LADAL or research based on LADAL.\n\n\n\nUPCOMING\n\n\nLADAL Opening Event\nThe LADAL Opening event signifies the official kick off for LADAL. Originally this kick-off was planned for June 2020 as a 5-day conference with an invited speaker (Stefan Gries), workshops on data science, and social events. Unfortunately, this kick-off had to be postponed due to COVID19 and will be held as an online event.\nHERE you will find updates and the current state of plans relating to the LADAL opening.\n\n\n\n\nPAST EVENTS\n\n\nBest Practices in Corpus Linguistics – What lessons should we take from the Replication Crisis and how can we guarantee high quality in our research?\n\n\nSpeaker: Martin Schweinberger\n\n\n\n\n\nDate: 20–24 May 2020\nPresentation at ICAME 41 (41th Meeting of the International Computer Archive of Modern and Medieval English). Heidelberg, Germany.\n\nMaterials: slides, video\nAbstract: This paper addresses issues relating to best practices in Data Management and Data Analysis in Corpus Linguistics (CL) and offers guidelines for compiling, storing, handling, and analysing data according to best practices which guarantee transparency and high quality in CL.\nOpen Data and Best Practices in Data Science are increasingly attracting attention as a result of the so-called Replication Crisis (RC) which is an ongoing methodological crisis primarily affecting parts of the social and life sciences that began in the early 2010s (Diener & Biswas-Diener 2019). The RC has contributed to the loss of trust that the Humanities and Social Science have been experienced over the past two decades (Yong 2018). While a discussion about Best Practices in CL has recently begun (Berez-Kroeker et al. 2018) more attention has to be placed on the causes of the RC and the lessons that can be learnt from it.\nCL is somewhat disjunct from current developments in Data Science due to a lack of communication and unawareness of existing resources. This talk aims to raise awareness in CL about existing resources and problematic practices that are still common in CL, and it proposes solutions that are easily implemented and can guarantee transparency, replicability, and high quality of research outputs in CL.\nThe solutions that this talk focuses on encompass\n\nbeing aware and following the FAIR principles (Findable, Accessible, Interoperable, and Reusable) in data management;\nthe recognition of corpora as research outputs which allows corpora to be uniquely indexed (DOIs) and thereby enabling corpus compilers to profit from making corpora accessible as these can be cited like other publications which increases citation scores and visibility;\nthe use of Git to share code and data which is an easy way to share resources free of charge by utilizing existing research infrastructure;\nthe use of R Notebooks to document analyses and making them available to the community and reviewers to enable full replicability and reproducibility;\nmaking use of documentation and policy protocols in departments, schools and institutes to ease onboarding procedures and prevent data loss and corruption.\n\nThe talk thus offers relevant information for authors as well as editors and publishers to enable replication, avoid “bad” research practices, and increase the quality of research.\nReferences\nBerez-Kroeker, A. L., L. Gawne, S. S. Kung, B. F. Kelly, T. Heston, G. Holton, P. Pulsifer, D. I. Beaver, S. Chelliah, S. Dubinsky, et al. (2018). Reproducible research in linguistics: A position statement on data citation and attribution in our field. Linguistics 56(1), 1–18.\nDiener, Edward and Biswas-Diener, Robert (2019). The Replication Crisis in Psychology. NOBA Project.\nYong, Ed (2018). Psychology’s Replication Crisis Is Running Out of Excuses. Another big project has found that only half of studies can be repeated. And this time, the usual explanations fall flat. The Atlantic.\n\n\n\n\nImplementing school-based support infrastructure for digital humanities research at UQ - The Language Technology and Data Analysis Laboratory (LADAL)\n\n\n\n\n\n\n\nDate: 30 October 2019\nSpeaker: Michael Haugh & Martin Schweinberger\nPresentation at the Australian Research Data Commons (ARDC): The Australian eResearch Skilled Workforce Summit. Sydney, Australia, 29-30/7/2019.\n\nMaterials: slides\nAbstract: This presentation introduces the Language Technology and Data Analysis Laboratory (LADAL), and discusses the implications of our experiences to date in establishing it for broader efforts to develop researcher capacity in the digital humanities.\nThe LADAL is school-based support infrastructure for digital humanities researchers. It aims to assist staff and postgraduate students within the UQ School of Languages and Cultures to learn how to use data analytics, digital research tools, and other forms of technology to enhance their existing research programs, as well as offer pathways to new research possibilities. It complements the more generic resources and training in digital humanities methods offered by libraries (e.g. the Digital Scholars Hub at UQ) with the more specialised training/support in particular digital research methods and technologies that are required by researchers working on specific languages and cultures.\nThe LADAL consists of a specialist computing lab for language-based computational and experimental work (the Computational and Experimental Workshop) and an online virtual lab. With respect to web-based materials, the LADAL website (https://slcladal.github.io/index.html) offers self-guided study materials and hands-on tutorials on topics relating to digital tools, computational methods for data extraction and processing, data visualization, statistical analyses of language data, and provides links to further resources and short descriptions of digital tools relevant for digital HASS research.\nIn addition, the LADAL offers face-to-face consultations and specialized workshops. UQ researchers are encouraged to contact LADAL staff for advice and guidance on matters relating to digital research tools, data visualization, various statistical procedures, and text analytics.\nStaff feedback during face-to-face consultations and workshop attendance confirms there is substantial demand for the kind of digital humanities infrastructure offered by LADAL. It also suggests that support and training for researchers in the digital humanities should be conceptualized on a continuum from more generic through to more localized support.\n\n\n\n\n\nUsing R for Corpus Linguistics – an Introduction and Discussion Note on Sustainability and Replicability in Corpus Linguistics\n\n\n\n\n\n\n\nDate: 2 April 2019\nSpeaker: Martin Schweinberger\nPresentation at the Center of Excellence for the Dynamics of Language (CoEDL) Corpus Workshop. Melbourne, Australia, 2–3/4/2019.\n\nMaterials: slides\n\n\n\n\n\n\nWORKSHOPS\n\nBelow are links to additional resources, workshops, and presentations.\n\nWorkshop materials\n\n\n\n\n\n\n\nGetting started with R for (absolute) beginners: This workshop focused on why you should use R, what you can do with R, and how you can use it for your data analysis.\nStatistics – Analyzing Survey and Questionnaire Data: This workshop introduced basic visualizations and statistical tests for analyzing survey and questionnaire data.\nHappy Computer, Happy Me!: This workshop shows ways to keep your computer happy and your data clean by providing simple tips and tricks for computer maintenance that keep your computer running at optimum speed and reliability.\n\n\nBack to top\nBack to HOME"
  },
  {
    "objectID": "opening.html",
    "href": "opening.html",
    "title": "LADAL Opening Webinar Series 2021",
    "section": "",
    "text": "The LADAL Opening Webinar Series 2021 consists of 25 webinars or online presentations from a wide range of voices with backgrounds in linguistics, data science, or computational humanities and it covers a variety of topics related to digital handling of language data! All recordings of the webinar series are available on the LADAL YouTube channel.\nAll events were announced on Twitter (@slcladal), via the UQ School of Languages and Cultures, and via our collaborators) - so please follow us if you like to catch up with the activities at LADAL. The LADAL Opening Webinar Series 2021 kicked off with a presentation by Stefan Th. Gries on MuPADRF (Multifactorial Prediction and Deviation Analysis Using Regression/Random Forests) on June 3, 2021, 5pm Brisbane time. See below for the full list of presentations that are part of the LADAL Opening Webinar Series 2021."
  },
  {
    "objectID": "opening.html#mupadrf-s.-th.-gries",
    "href": "opening.html#mupadrf-s.-th.-gries",
    "title": "LADAL Opening Webinar Series 2021",
    "section": "MuPADRF (S. Th. Gries)",
    "text": "MuPADRF (S. Th. Gries)\n\n\nMuPADRF (Multifactorial Prediction and Deviation Analysis Using Regression/Random Forests)\n\n\n\n\n\nThis talk was recorded June 3, 2021, as part of the LADAL Opening Webinar Series 2021.\nRecording link: https://www.youtube.com/watch?v=cLocET9CC-E&t=253s\nAbstract\nIn this talk, Stefan gave a brief and relatively practical introduction to an approach called MuPDAR(F) (for Multifactorial Prediction and Deviation Analysis using Regressions/Random Forests) that he developed (see [1], [2] for the first applications). The main part of the talk involved using a version of the data in [2] to exemplify how this protocol works and how it can be done in R. Second, Stefan discussed a few recent extensions proposed in [3] and [4], which have to do with\n\nhow to deal with situations with more than two linguistic choices,\nhow predictions are made, and\nhow deviations are quantified.\n\nFinally, he briefly comment on exploring individual variation among the target speakers (based on [5]).\n\nAbout Stefan\nStefan Th. Gries is full professor at the University of California, Santa Barbara (UCSB), as well as Honorary Liebig-Professor and Chair of English Linguistics at the Justus-Liebig-Universität Giessen. Stefan has held several prestigious visiting professorships at top universities and, methodologically, he is a quantitative corpus linguist at the intersection of corpus linguistics, cognitive linguistics, and computational linguistics. Stefan has applied a variety of different statistical methods to investigate a wide range of linguistic topics and much of his work involves the open-source software R. Stefan has produced more than 200 publications (articles, chapters, books, and edited volumes), he is an active member of various editorial boards as well as academic societies."
  },
  {
    "objectID": "opening.html#ladal-atap-m.-schweinberger-m.-haugh",
    "href": "opening.html#ladal-atap-m.-schweinberger-m.-haugh",
    "title": "LADAL Opening Webinar Series 2021",
    "section": "LADAL & ATAP (M. Schweinberger & M. Haugh)",
    "text": "LADAL & ATAP (M. Schweinberger & M. Haugh)\n\n\nThe Australian Text Analytics Platform (ATAP) and the Language Technology and Data Analysis Laboratory (LADAL) - building computational humanities infrastructures: experiences, problems, and potentials\n\n\n\n\n\nThis talk was recorded June 10, 2021, as part of the LADAL Opening Webinar Series 2021.\nRecording on YouTube: https://www.youtube.com/watch?v=qGIGCubyJs0\nAbstract\nThis talk introduces the Language Technology and Data Analysis Laboratory (LADAL) which is a computational humanities resource infrastructure maintained by the School of Languages and Cultures at the University of Queensland. The talk will also provide information about its relations to the Australian Text Analytics Platform (ATAP) which represents an effort to promote text analytics in Australia and to make resources for using text analytics available to a wider community of researchers.\n\nAbout Martin\n\n\n\n\n\nMartin Schweinberger is a language data scientist with a PhD in English linguistics who has specialized in corpus linguistics and quantitative, computational analyses of language data. Martin is a Lecturer in Applied Linguistics at the University of Queensland, Australia where he has been establishing the Language Technology and Data Analysis Laboratory (LADAL) and he holds an additional part-time Associate Professorship in the AcqVA-Aurora Center at the Arctic University of Norway in Tromsø.\n\nAbout Michael\n\n\n\n\n\nMichael Haugh is Professor of Linguistics and a Fellow of the Australian Academy of the Humanities. His research interests lie primarily in the field of pragmatics, the science of language-in-use. He works with recordings and transcriptions of naturally occurring spoken interactions, as well as data from digitally-mediated forms of communication across a number of languages. An area of emerging importance in his view is the role that language corpora can play in the humanities and social sciences more broadly. He has been involved in the establishment of the Australian National Corpus and the Language Technology and Data Analytics Lab, and is currently leading the establishment of a national language data commons."
  },
  {
    "objectID": "opening.html#data-collection-in-the-field-f.-meakins",
    "href": "opening.html#data-collection-in-the-field-f.-meakins",
    "title": "LADAL Opening Webinar Series 2021",
    "section": "Data Collection in the Field (F. Meakins)",
    "text": "Data Collection in the Field (F. Meakins)\n\n\nField-based methods for collecting quantitative data\n\n\n\n\n\nThis talk was recorded June 18, 2021, as part of the LADAL Opening Webinar Series 2021.\nRecording on YouTube: https://youtu.be/BGa0gkkkWsc\nAbstract\nShana Poplack has set benchmarks for the development of corpora since the early 1980s. Poplack (2015, p. 921) maintains that the “gold standard remains the (standard sociolinguistic-style) … corpus”. The aim of producing corpora using these principles is to avoid the ‘cherry picking’ approach which dominates much of the theoretical literature. Poplack and her team have created the Ottawa-Hull Corpus which consists of 3.5 million words of informal speech data. This corpus is enormous and beyond the capabilities of a single linguist in a small language community. This talk offers suggestions for corpus development in the field that follow Poplack’s principles, but also shows where compromises can be made. I discuss the method developed during the Gurindji Kriol project called ‘peer elicitation’. It supplements Poplack’s gold standard of naturally occurring speech with semi-formal elicitation to ensure sufficient data for quantitative analyses.\n\nAbout Felicity\n\n\n\n\n\nFelicity Meakins is an ARC Future Fellow in Linguistics at the University of Queensland and a CI in the ARC Centre of Excellence for the Dynamics of Language. She is a field linguist who specialises in the documentation of Australian Indigenous languages in the Victoria River District of the Northern Territory and the effect of English on Indigenous languages. She has worked as a community linguist as well as an academic over the past 20 years, facilitating language revitalisation programs, consulting on Native Title claims and conducting research into Indigenous languages. She has compiled a number of dictionaries and grammars of traditional Indigenous languages and has written numerous papers on language change in Australia."
  },
  {
    "objectID": "opening.html#the-uzh-text-crunching-center-g.-schneider",
    "href": "opening.html#the-uzh-text-crunching-center-g.-schneider",
    "title": "LADAL Opening Webinar Series 2021",
    "section": "The UZH Text Crunching Center (G. Schneider)",
    "text": "The UZH Text Crunching Center (G. Schneider)\n\n\nText Crunching Center (TCC): Data-driven methods for linguists, social science and digital humanities\n\n\n\n\n\nThis talk was recorded June 24, 2021, as part of the LADAL Opening Webinar Series 2021.\nRecording on YouTube: https://youtu.be/2_l3ViTzBOM\nAbstract\nThis talk introduces the Text Crunching Centre (TCC) which is a Computational Linguistics and Digital Humanities service hosted at the University of Zurich, and a collaboration partner of LADAL. We present a selection of our case studies using text analytics, from cognitive linguistics, social, political and historical studies. We show how stylistics, document classification, topic modelling, conceptual maps, distributional semantics and eye-tracking can offer new perspectives. Our case studies include language and age, learner language, the history of medicine, democratisation, religion, and attitudes to migration. We conclude with an outlook to the future of text analytics.\n\n\n\n\n\n\nAbout Gerold\nGerold Schneider is a Senior Lecturer, researcher and computing scientist at the department of Computational Linguistics at the University of Zurich, Switzerland. His doctoral degree is on large-scale dependency parsing, his habilitation on using computational models for corpus linguistics. His research interests include corpus linguistics, statistical approaches, Digital Humanities, text mining and language modeling. He has published over 100 articles on these topics. He has published a book on statistics for linguists [6], and a book on digital humanities is under way. His Google scholar page can be accessed here."
  },
  {
    "objectID": "opening.html#corpus-based-media-linguistics-m.-bednarek",
    "href": "opening.html#corpus-based-media-linguistics-m.-bednarek",
    "title": "LADAL Opening Webinar Series 2021",
    "section": "Corpus-Based Media Linguistics (M. Bednarek)",
    "text": "Corpus-Based Media Linguistics (M. Bednarek)\n\n\nCorpus-based media linguistics: A case study of linguistic diversity in Australian television\n\n\n\n\n\nThis talk was recorded July 1, 2021, as part of the LADAL Opening Webinar Series 2021.\nRecording on YouTube: https://youtu.be/g54yLpYefbI\nAbstract\nIn this golden age of Indigenous television [7], it is important to analyze Indigenous-authored drama series, so that we can move beyond ‘a deficit perspective’ [8] in relation to mediated linguistic diversity. This talk presents a corpus linguistic case study of Australian Aboriginal English (AAE) lexis as present in three such Indigenous-authored television series: Redfern Now, Cleverman, and Mystery Road. For television viewers, mediated AAE can be an important source of information, especially if they do not regularly interact with Aboriginal and/or Torres Strait Islander people. This would be the case for many Australians, and even more so for international viewers of Australian television series. All three analyzed series were exported overseas and thus have both Australian and international audiences. Using lexical profiling analysis [AntWordProfiler; [9]] in combination with qualitative concordance analysis, the talk identifies and compares the use of AAE lexis across the three series. Analysis of frequency and distribution will pinpoint words that appear to be particularly significant lexical resources in mediated AAE. The talk will be framed through the notion of diversity, as conceptualised in relation to television series.\n\nAbout Monika\n\n\n\n\n\nMonika Bednarek is Professor of Linguistics at the University of Sydney and Director of the Sydney Corpus Lab. Her research uses corpus linguistic methodologies across a variety of fields, including media linguistics, discourse analysis and sociolinguistics. She has a particular interest in the linguistic expression of emotion and opinion, with a focus on English. Monika is the author or co-author of six books and two short volumes as well as numerous journal articles and book chapters. She has co-edited several edited volumes and special issues of journals, most recently Corpus approaches to telecinematic language (International Journal of Corpus Linguistics 26/1, 2021) and Corpus linguistics and Education in Australia (Australian Review of Applied Linguistics 43/2, 2020). She is on the steering committee of the Asia Pacific Corpus Linguistics Association and tweets @corpusling."
  },
  {
    "objectID": "opening.html#bayesian-vs-frequentist-n.-levshina",
    "href": "opening.html#bayesian-vs-frequentist-n.-levshina",
    "title": "LADAL Opening Webinar Series 2021",
    "section": "Bayesian vs Frequentist (N. Levshina)",
    "text": "Bayesian vs Frequentist (N. Levshina)\n\n\nRecycle, relax, repeat: Advantages of Bayesian inference in comparison with frequentist methods\n\n\n\n\n\nThis talk was recorded July 8, 2021, as part of the LADAL Opening Webinar Series 2021.\nRecording on YouTube: https://youtu.be/vpsJj7Nkgw4\nResources\nThe slides for Natalia’s talk are available here. An Open Science Foundation (OSF) repository containing the R code and data used for the case study in this talk are available here. Here is a link to the GitHub repo of Shravan Vasishth (Professor for Psycholinguisics at the University of Potsdam) with additional resources on Bayesian statistics.\nAbstract\nBayesian inference is becoming increasingly popular in linguistic research. In this talk I will compare frequentist (maximum likelihood) and Bayesian approaches to generalized linear mixed-effects regression, which is de facto the standard method for testing linguistic hypotheses about linguistic variation. The main advantages of Bayesian inference include an opportunity to test the research hypothesis directly, instead of trying to reject the null hypothesis. One can also use information from previous research as priors for subsequent models, which helps to overcome the recent crisis of reproducibility. This also enables one to use smaller samples. It helps to solve such problems as overfitting, data separation and convergence issues, which often arise when one fits generalized mixed-effect models with complex structure. These advantages will be illustrated by a multifactorial case study of help + (to-)infinitive in US magazines, as in the example These simple tips will help you (to) survive the Zombie apocalypse.\n\nAbout Natalia\n\n\n\n\n\nNatalia Levshina is a linguist working at the Max Planck Institute for Psycholinguistics in Nijmegen. Her main research interests are cognitive and functional linguistics, pragmatics, typology, corpora and data science. She obtained her PhD at KU Leuven in 2011 and got her habilitation qualification at Leipzig University in 2019 with a thesis Towards a theory of communicative efficiency. In addition to papers on causatives, differential case marking, politeness, word order variation and other linguistic topics, Natalia is the author of a best-selling statistical manual How to Do Linguistics with R [10]."
  },
  {
    "objectID": "opening.html#online-data-collection-m.-vos",
    "href": "opening.html#online-data-collection-m.-vos",
    "title": "LADAL Opening Webinar Series 2021",
    "section": "Online Data Collection (M. Vos)",
    "text": "Online Data Collection (M. Vos)\n\n\nGathering data and creating online experiments with jsPsych and JATOS\n\n\n\n\n\nThis talk was recorded July 15, 2021, as part of the LADAL Opening Webinar Series 2021.\nRecording on YouTube: https://youtu.be/7-9WRYpXEtE\nResources: The materials, including scripts, files, etc., can be accessed via and downloaded from this GitHub repository.\nAbstract\nWeb-based studies are an increasingly popular and attractive alternative to lab- and field-based studies, enabling remote data collection for a rapidly growing variety of (psycho-)linguistic methods ranging from acceptability judgment tasks and self-paced reading to interactive group designs and the Visual World paradigm. Most platforms for building and hosting online studies are proprietary and subscription-based (e.g., Gorilla, Pavlovia, and FindingFive), but there also exist various free, open-source tools for writing and managing studies on your own server. This talk gives a practical introduction to building and hosting studies using jsPsych [11] and JATOS [12], by demonstrating a speedrun of the entire process: from an empty file in a code editor, to distributing URLs to participants.\n\n\n\n\n\n\nAbout Myrte\nMyrte Vos (she/they) is a doctoral research fellow at the Arctic University of Norway (Tromsø). They’re supposed to be studying incremental processing of aspect and modality in English, but got sidetracked by figuring out how to do that using webcam-based eye tracking."
  },
  {
    "objectID": "opening.html#reproducible-research-a.-miotto-j.-toohey",
    "href": "opening.html#reproducible-research-a.-miotto-j.-toohey",
    "title": "LADAL Opening Webinar Series 2021",
    "section": "Reproducible Research (A. Miotto & J. Toohey)",
    "text": "Reproducible Research (A. Miotto & J. Toohey)\n\n\nGoing down the Reproducible Research pathway: You have to begin somewhere, right?\n\n\n\n\n\nThis talk was recorded July 22, 2021, as part of the LADAL Opening Webinar Series 2021.\nRecording on YouTube: https://youtu.be/bANTr9RvnGg\nResources: The materials that are mentioned in this presentation can be accessed via the following url: https://www.qcif.edu.au/training/training-courses/\nAbstract\nThe idea that you can duplicate an experiment and get the same conclusion is the basis for all scientific discoveries. Reproducible research is data analysis that starts with the raw data and offers a transparent workflow to arrive at the same results and conclusions. However not all studies are replicable due to lack of information on the process. Therefore, reproducibility in research is extremely important.\nResearchers genuinely want to make their research more reproducible, but sometimes don’t know where to start and often don’t have the available time to investigate or establish methods on how reproducible research can speed up every day work. We aim for the philosophy “Be better than you were yesterday”. Reproducibility is a process, and we highlight there is no expectation to go from beginner to expert in a single workshop. Instead, we offer some steps you can take towards the reproducibility path following our Steps to Reproducible Research self paced program.\n\n\n\n\n\n\nAbout Amanda\nAmanda Miotto is an eResearch Analyst for Griffith University and QCIF. She started off in the field of Bioinformatics and learnt to appreciate the beauty of science before discovering the joys of coding. She is also heavily involved in Software Carpentry, Hacky Hours and ResBaz, and has developed on platforms around HPC and scientific portals.\n\n\n\n\n\n\nAbout Julie\nJulie Toohey is a Library Research Data Management Specialist at Griffith University Library. Julie has an extensive career in academic libraries and is passionate about research data management practices. Previously, Julie co-facilitated the Australian National Data Services 23 Things (research data) Health and Medical Data Community series and is currently a member of the QULOC Research Support Working Party. Julie works closely with Griffith eResearch Services delivering education awareness programs around managing research data, reproducible research and working with sensitive data. Julie has co-authored several research data related publications with Griffith researchers and eResearch Services partners.\nhttps://orcid.org/0000-0002-4249-8180"
  },
  {
    "objectID": "opening.html#neurolinguistics-of-bilingualism-deluca-voits-rothman",
    "href": "opening.html#neurolinguistics-of-bilingualism-deluca-voits-rothman",
    "title": "LADAL Opening Webinar Series 2021",
    "section": "Neurolinguistics of Bilingualism (DeLuca, Voits & Rothman)",
    "text": "Neurolinguistics of Bilingualism (DeLuca, Voits & Rothman)\n\n\nNeurocognitive effects of bilingual experience\n\n\n\n\n\nThis talk was recorded July 28, 2021, as part of the LADAL Opening Webinar Series 2021.\nRecording on YouTube: https://youtu.be/wVBkUSPQFUw\nAbstract\nMuch research over the past two decades shows that bilingualism affects brain structure, function, and potentially domain-general cognition [see e.g., 13, 14]. The specificity of these effects, however, has become the subject of significant debate in recent years, in large part due to variability of findings across studies [see 15 for review]. In this talk, we will introduce our research programs within the Psycholinguistics of Language Representation (PoLaR) lab that addresses the juxtaposition of data and argumentation. Our work is guided by the principle that although bilingual effects are existent, they are conditional. In other words, bilingualism per se is not a sufficient condition for relevant effects on neurocognition. We will review our work that is generally designed to test the hypothesis that specific experience-based factors (EBFs) variably affect neural activity and plasticity in brain regions and pathways implicated in language- and executive control across the lifespan. We present results from a series of MRI studies showing a specificity of neural adaptations to different EBFs [16–18] in younger adults. We will also present data from older adults, showing similar EBF effects in healthy cognitive ageing [19] and with mild cognitive impairment [20]. EBFs related to duration of bilingual language use correlate to neurocognitive adaptations suggesting increased efficiency in language control, whereas those related to extent of additional language use correlate with adaptations suggesting increased control demands. Considered together, these data suggest that the brain strives to be maximally effective and efficient in language processing and control, which in turn affects domain-general cognitive processes proportionally to degree of engagement with bilingual experiences. The work in older populations leads to the conclusion that degree of engagement with bilingualism is a catalyst for cognitive/brain reserve and thus has some real-world benefits in aging.\n\n\n\n\n\n\nAbout Vince\nVincent DeLuca is Associate Professor in the Neurocognition of Bilingualism and co-director of the Psycholinguistics of Language Representation (PoLaR) lab in the AcqVa Aurora Centre at UiT-The Arctic University of Norway. His research is focused on how different aspects of bilingual language experience variably impact brain structure, function, and several cognitive processes. His work focuses on how these neural and cognitive adaptations dynamically shift over time and with changes to patterns of language use.\n\n\n\n\n\nAbout Toms\nToms Voits is a Postdoctoral Researcher at UiT the Arctic University of Norway. He is affiliated with the Psycholinguistics of Language Representation (PoLaR) lab and the AcqVA Aurora Center in the Department of Language and Culture at UiT. His work is primarily focused on investigating the effects of bilingualism on neurocognition, with a particular interest in examining bilingualism as a contributing factor to cognitive and brain reserves in the later years of life.\n\n\n\n\n\nAbout Jason\nJason Rothman is Professor of Linguistics at UiT the Arctic University of Norway and Senior Researcher in Cognitive Science at Universidad Nebrija (Spain). He is deputy director of the AcqVA Aurora Center at UiT, where he co-leads the center’s theme/concentration on the Neurocognition of Bilingualism. Professor Rothman also co-directs the center’s Psycholinguistics of Language Representation (PoLaR) lab. A linguist by training, he has worked extensively on language acquisition, linguistic processing and language-associated links to domain general neurocognition across the lifespan of varioustypes of bi-/multilingual populations. He is founding editor of the journal Linguistic Approaches to Bilingualism and serves as executive editor of the book series Studies in Bilingualism."
  },
  {
    "objectID": "opening.html#introducing-network-analysis-s.-musgrave",
    "href": "opening.html#introducing-network-analysis-s.-musgrave",
    "title": "LADAL Opening Webinar Series 2021",
    "section": "Introducing Network Analysis (S. Musgrave)",
    "text": "Introducing Network Analysis (S. Musgrave)\n\n\nA gentle introduction to networks\n\n\n\n\n\nThis talk was recorded Aug. 2, 2021, as part of the LADAL Opening Webinar Series 2021.\nRecording on YouTube: https://youtu.be/QW0lVaHcNX4\nAbstract\nLinguists have adopted several methods from data science, but network analysis has been used rather less than others even though it is a useful tool. This presentation will introduce the basics of using network analysis, discussing the types of problems for which the method is useful, the kinds of data which are amenable to analysis, and the graphical outputs which can be achieved. These points will be illustrated with several examples from different areas of linguistic research, as well as with an example with data concerning a social network.\n\n\n\n\n\n\nAbout Simon\nSimon Musgrave was a lecturer in the Linguistics program at Monash University until the end of 2020. His research covers various areas in linguistics and sociolinguistics, linked by the themes of the use of computational tools in linguistic research and the relationship between Linguistics and Digital Humanities, an interest continued in his current work in the Linguistic Data Commons of Australia project."
  },
  {
    "objectID": "opening.html#speech-recognition-with-elpis-j.-wiles-b.-foley",
    "href": "opening.html#speech-recognition-with-elpis-j.-wiles-b.-foley",
    "title": "LADAL Opening Webinar Series 2021",
    "section": "Speech Recognition with Elpis (J. Wiles & B. Foley)",
    "text": "Speech Recognition with Elpis (J. Wiles & B. Foley)\n\n\n(Semi-)Automated speech recognition using Elpis\n\n\n\n\n\nThis talk was recorded Aug. 12, 2021, as part of the LADAL Opening Webinar Series 2021.\nRecording on YouTube: https://youtu.be/9YjZH4lBco0\nResources: See here for a guide on how to use Elpis.\nAbstract\nSpeech recognition (ASR) technologies can be useful to accelerate transcription of language recordings, and to provide new insights into corpora. ASR tools are available now for hundreds of languages from commercial providers and can be trained for languages that are not commercially supported. However, there are significant hurdles to using ASR tools, from the preparation of data through to the training of the systems. This presentation introduces the motivations and process of co-designing Elpis, a speech recognition system built to be usable by ordinary working linguits. We discuss some examples of using Elpis with a range of low-resource language corpora, and how the co-design process can be used to benefit other language technologies.\n\n\n\n\n\n\nAbout Ben\nBen Foley is the project manager of CoEDL’s Transcription Acceleration Project (TAP). TAP brings cutting-edge language technology within reach of people working with some of the world’s oldest languages. A major focus of TAP is the development of user-friendly speech recognition tools. Ben’s previous experience with Aboriginal and Torres Strait Islander language resource development has resulted in apps and websites galore. Highlights include the Iltyem-iltyem sign language database and website, and the Gambay First Languages Map, showing the hundreds of languages in Australia.\n\n\n\n\n\nAbout Janet\nJanet Wiles is a Professor in Human Centred Computing at the University of Queensland and leads the Future Technologies Thread of the ARC Centre of Excellence for the Dynamics of Language (CoEDL). She has 30 years’ experience in cross-disciplinary research and teaching, including artificial intelligence, language technologies and social robotics, leading teams that span engineering, humanities, social sciences and neuroscience.\n\nBen and Janet currently teach a cross disciplinary course Voyages in Language Technologies”* that introduces computing students to the diversity of the world’s languages, and state-of-the-art tools for deep learning and other analysis techniques for working with language data."
  },
  {
    "objectID": "opening.html#societal-big-data-m.-laitinen",
    "href": "opening.html#societal-big-data-m.-laitinen",
    "title": "LADAL Opening Webinar Series 2021",
    "section": "Societal Big Data (M. Laitinen)",
    "text": "Societal Big Data (M. Laitinen)\n\n\nAdding social information to societal big data?\n\n\n\n\n\nThis talk was recorded Aug. 19, 2021, as part of the LADAL Opening Webinar Series 2021.\nRecording on YouTube: https://youtu.be/kfb7xLC2nBs\nAbstract\nSocietal big data today provides a large source of language data in naturalistic settings. Such data have substantially enlarged pools of evidence in various fields in social sciences and the humanities. In linguistics, one direct impact has been the emergence of computational sociolinguistics, a field that intersects sociolinguistics with computational techniques.\nHowever, social big data have one major limitation, at least when it comes to computational sociolinguistics. This limitation, quite surprisingly, concerns the lack of social background information. Researchers have no direct access to background information (author’s gender, social layer, education, occupation, etc.), and it is difficult to combine evidence from social media with rich social information for the simple reason that such information is not available for proprietary reasons. If some social information is available, it is often self-reported and therefore prone to inaccuracies. Or, it is ethically unsustainable to link big language data with socio-cultural information (cf. the Cambridge Analytica scandal).\nThis talk introduces a method that is being developed in my SOCOS research group. The method builds on social network theory and utilizes freely available interaction data. These data can be quantified easily through a set of algorithms and used as proxies for social parameters. One downside is that it requires some degree of technical competence to extract these data, which can easily be accomplished through interdisciplinary partnerships in digital humanities environments.\n\n\n\n\n\n\nAbout Mikko\nMikko Laitinen is Professor of English Language at the University of Eastern Finland. He obtained PhD in 2007 and has been a member of the Academy of Finland Center of Excellence in Research Unit for Variation, Contacts and Change (VARIENG) since 2000. He previously worked as Professor of English at Linnaeus University, where he is one of the two founding members of the Center of Data Intensive Sciences and Applications (DISA), a multidisciplinary big data research consortium that consists of scholars in the humanities, computer scientists, mathematicians, and social scientists. His research focuses on the role social networks in language variation and change, computational sociolinguistics, and digital humanities."
  },
  {
    "objectID": "opening.html#tackling-social-media-data-s.-hames",
    "href": "opening.html#tackling-social-media-data-s.-hames",
    "title": "LADAL Opening Webinar Series 2021",
    "section": "Tackling Social Media Data (S. Hames)",
    "text": "Tackling Social Media Data (S. Hames)\n\n\nWorking with Social Media Data\n\n\n\n\n\nThis talk was recorded Aug. 26, 2021, as part of the LADAL Opening Webinar Series 2021.\nRecording on YouTube: https://youtu.be/QVwWPfxE93U\nAbstract\nSocial media platforms, built on the web and the internet, are now just part of life for most of us. The pervasive communication and connection the web and social media enables is a potentially rich source of data for research - but there are pitfalls for the unprepared. This talk provides a survey of the web as a source of data for research projects, including considerations of privacy, ethics and governance; the technical approaches to data collection; and an overview of approaches to analysing such data.\n\n\n\n\n\n\nAbout Sam\nSam is a developer/data scientist at Queensland University of Technology’s Digital Observatory (yes, the slash is important!). He has a PhD under examination in machine learning for medical image analysis and a commercial background in software development for text analytics algorithms and products. At the Digital Observatory, Sam helps researchers deal with the challenges of collecting, modelling and analysing digital data to address their research questions. He is particularly interested in both the web as a medium, and in the development of computationally assisted methods for bridging the qualitative and quantitative divide."
  },
  {
    "objectID": "opening.html#data-driven-learning-p.-crosthwaite",
    "href": "opening.html#data-driven-learning-p.-crosthwaite",
    "title": "LADAL Opening Webinar Series 2021",
    "section": "Data-Driven Learning (P. Crosthwaite)",
    "text": "Data-Driven Learning (P. Crosthwaite)\n\n\nData-driven learning for younger learners: Boosting schoolgirls’ knowledge of passive voice constructions for STEM education\n\n\n\n\n\nThis talk was recorded Sep. 2, 2021, as part of the LADAL Opening Webinar Series 2021.\nRecording on YouTube: https://youtu.be/JlHchgpzO3o\nAbstract\nThis paper explores how corpus technology and DDL pedagogy can support secondary schoolgirls’ reporting of an observed science experiment through a written research report, focusing particularly on how corpora were used to develop receptive and productive knowledge of passive voice constructions. A pre-test of the grammaticality of passive constructions was conducted, alongside a diagnostic pre-instruction written report requiring the retelling of an observed science experiment were collected from 60 Year 9-10 girls at a high school in Australia. During a full 10-week term, students were given guided individual homework tasks and short in-class pair/group DDL activities focusing on passive voice constructions, using freely available online corpus applications such as SketchEngine. Following this treatment, a post-test was conducted while an additional written research report was collected. Questionnaire and interview data was also collected to determine the perceptions of younger female learners and their teachers regarding their engagement with corpora and DDL for improving knowledge and use of passive constructions over time. The data suggest that while the DDL treatment did not result in increased receptive knowledge of the grammaticality of passive voice constructions, their productive use of the passive was significantly improved. Moreover, when students were given the opportunity to use corpora to check their intuitions in the post-test, they produced accurate responses over 75% of the time. Qualitative stakeholder perceptions of improved disciplinary linguistic knowledge, increased data management skills, and positive engagement with “science” were also found in the survey/interview data, although a number of challenges at the technical and conceptual levels for DDL still remain.\n\n\n\n\n\n\nAbout Peter\nPeter Crosthwaite is Senior Lecturer in Applied Linguistics in the School of Languages and Cultures at the University of Queensland. Before he joined UQ, Peter was assistant professor at the Centre for Applied English Studies (CAES) at the University of Hong Kong. His areas of research and supervisory expertise include corpus linguistics and the use of corpora for language learning (known as data-driven learning), as well as English for General and Specific Academic Purposes. Peter is the author of the monograph Learning the language of Dentistry: Disciplinary corpora in the teaching of English for specific academic purposes which is part of Benjamins’ Studies in Corpus Linguistics series (with Lisa Cheung, published 2019), as well as the edited volumes Data Driven Learning for the Next Generation: Corpora and DDL for Pre-tertiary Learners (published 2019) and Referring in a second language: Reference to person in a multilingual world (with Jonathon Ryan, published 2020 with Routledge). Peter is also currently serving as the corpus linguistics section editor for Open Linguistics - an open access linguistics journal from De Gruyter, and he is on the editorial board of Applied Corpus Linguistics, a new journal covering the direct applications of corpora to teaching and learning."
  },
  {
    "objectID": "opening.html#text-classification-hate-speech-g.-wiedemann",
    "href": "opening.html#text-classification-hate-speech-g.-wiedemann",
    "title": "LADAL Opening Webinar Series 2021",
    "section": "Text Classification & Hate Speech (G. Wiedemann)",
    "text": "Text Classification & Hate Speech (G. Wiedemann)\n\n\nText classification for automatic detection of hate speech, counter speech, and protest events\n\n\n\n\n\nThis talk was recorded Sep. 13, 2021, as part of the LADAL Opening Webinar Series 2021.\nRecording on YouTube: https://youtu.be/2oLJNYl_Ipw\nAdditional Resources\nText Mining Tutorials: https://tm4ss.github.io\nR package for active learning experiments: https://github.com/tm4ss/tmca.classify\nExperiment code for reproducing the experiments from the Journal Article Proportional Classification Revisited: https://github.com/tm4ss/tmca.classify-experiments\nAbstract\nSocial sciences have opened up to text mining, i.e., a set of methods to automatically identify semantic structures in large document collections. However, the methods have often been limited to a statistical analysis of textual data, strongly limiting the scope of possible research questions. The more complex concepts central to the social sciences such as arguments, frames, narratives and claims still are mainly studied using manual content analyses in which the knowledge needed to apply a category (i.e. to “code”) is verbally described in a codebook and implicit in the coder’s own background knowledge. Supervised machine learning provides an approach to scale-up this coding process to large datasets. Recent advantages in neural network-based natural language processing allow for pretraining language models that can transfer semantic knowledge from unsupervised text collections to specific automatic coding problems. With deep learning models such as BERT automatic coding of context-sensitive semantics with substantially lowered efforts in training data generation comes within reach to content analysis. The talk will introduce to the applied usage of these technologies along with two interdisciplinary research projects studying hate speech and counter speech in German Facebook postings, and information extraction for the analysis of the coverage of protest events in local news media.\n\n\n\n\n\n\nAbout Gregor\nGregor Wiedemann is working as Senior Researcher Computational Social Science at the Leibniz Institute for Media Research │ Hans Bredow Institute (HBI). Since September 2020, he heads the Media Research Methods Lab (MRML). His current work focuses on the development of methods and applications of natural language processing and text mining for empirical social and media research. Gregor Wiedemann studied political science and computer science in Leipzig and Miami, USA. In 2016 he received his doctorate from the Department of Computer Science at the University of Leipzig for his thesis on automation of discourse and content analysis using text mining and machine learning methods. Afterwards he worked as a postdoc in the NLP group of Computer Science Department at the University of Hamburg. Among other things, the resulting works are concerned with unsupervised information extraction to support investigative research in unknown document collections (see newsleak.io) and with the detection of hate and counter-speech in social media."
  },
  {
    "objectID": "opening.html#varieng-nevalainen-hiltunen-liimatta",
    "href": "opening.html#varieng-nevalainen-hiltunen-liimatta",
    "title": "LADAL Opening Webinar Series 2021",
    "section": "VARIENG (Nevalainen, Hiltunen & Liimatta)",
    "text": "VARIENG (Nevalainen, Hiltunen & Liimatta)\n\n\nIntroducing the VARIENG research unit in Helsinki \n\n\n\n\n\nThis talk was recorded Sep. 21, 2021, as part of the LADAL Opening Webinar Series 2021.\nRecording on YouTube: https://youtu.be/rDuYGQkAtQg\nAdditional Resources\nVARIENG website: https://www2.helsinki.fi/en/researchgroups/varieng\nCorpus Resource Database (CoRD): https://varieng.helsinki.fi/CoRD/\nStudies in Variation, Contacts and Change in English: https://varieng.helsinki.fi/series/\nAbstract\nOur presentation falls into three parts. It begins with Terttu’s brief introduction to the Research Unit for Variation, Contacts and Change in English (VARIENG), its past, present and future, and its current open-access resources. In addition, the talk presents two short cases studies touching on the methodology of corpus linguistics from different perspectives. The first one (by Turo) discusses how the availability of massive text archives may hold great promise for corpus linguistic work, but they may also present considerable methodological challenges for users [see e.g. 21]. In focus here are some specific problems related to the diachronic British Library Newspapers database, and how those problems might be addressed in the context of register analysis and the study of linguistic variation. The second case study (by Aatu) looks into the role of text length in register-internal variation by analyzing a big data sample of social media comments. Different registers seem to exhibit different kinds of internal functional variation by text length (as shown by the illustration).\n\n\n\n\n\n\nAbout Terttu\nTerttu Nevalainen is the Director of the VARIENG Research Unit. Her research interests include historical sociolinguistics, variation studies, corpus linguistics and digital humanities. She is one of the designers and compilers of the Helsinki Corpus of English Texts and of the Corpus of Early English Correspondence.\n\n\n\n\n\n\nAbout Turo\nTuro Hiltunen works as senior lecturer in English at the Department of Languages, University of Helsinki, Finland. His research interests include corpus linguistics and register analysis, and the grammatical and phraseological variation of scientific English past and present, compilation of specialised corpora, and democratisation in the context of parliamentary discourse.\n\n\n\n\n\n\nAbout Aatu\nAatu Liimatta is researching register variation on social media for his PhD. His interests include register and functional variation, computational methods, and big linguistic data."
  },
  {
    "objectID": "opening.html#antconc-4.0-l.-anthony",
    "href": "opening.html#antconc-4.0-l.-anthony",
    "title": "LADAL Opening Webinar Series 2021",
    "section": "AntConc 4.0 (L. Anthony)",
    "text": "AntConc 4.0 (L. Anthony)\n\n\nAn Introduction to AntConc 4: Developing a general-purpose corpus toolkit for a broad user base\n\n\n\n\n\nThis talk was recorded Sep. 27, 2021, as part of the LADAL Opening Webinar Series 2021.\nRecording on YouTube: https://youtu.be/F_eHRWxCZnw\nAdditional Resources\nLaurence Anthony’s website (software subpage)\nAbstract\nAntConc is a widely used desktop corpus tool that has been downloaded over 2.5 million times since its first release in the early 2000s. Today, it is used by researchers, teachers, and learners in over 140 countries and its tutorial videos have been viewed over 500,000 times. AntConc has a relatively easy-to-use design and works especially well with small corpora of under a few million words. However, it begins to struggle when processing larger corpora and offers relatively few statistics for advanced analysis. In this talk, I will introduce a new version of AntConc (version 4.0) that has been built from the ground up to addresses these limitations. The new version includes features that allow the software to be easily extended, and it produces results in a way that allows for smooth data interoperability with other tools and scripts. I will also discuss various issues that need to be considered when developing software for a broad user base, which should be of interest to both users and developers of software tools.\n\nAbout Laurence\n\n\n\n\n\nLaurence Anthony is Professor of Applied Linguistics at the Faculty of Science and Engineering, Waseda University, Japan. He has a BSc degree (Mathematical Physics) from the University of Manchester, UK, and MA (TESL/TEFL) and PhD (Applied Linguistics) degrees from the University of Birmingham, UK. He is the current Director of the Center for English Language Education in Science and Engineering (CELESE), which runs discipline-specific language courses for the 10,000 students of the faculty. His main research interests are in corpus linguistics, educational technology, and English for Specific Purposes (ESP) program design and teaching methodologies. He received the National Prize of the Japan Association for English Corpus Studies (JAECS) in 2012 for his work in corpus software tools design."
  },
  {
    "objectID": "opening.html#distributional-semantics-g.-desagulier",
    "href": "opening.html#distributional-semantics-g.-desagulier",
    "title": "LADAL Opening Webinar Series 2021",
    "section": "Distributional Semantics (G. Desagulier)",
    "text": "Distributional Semantics (G. Desagulier)\n\n\nDoing diachronic linguistics with distributional semantic models in R\n\n\n\n\n\nThis talk was recorded Sep. 30, 2021, as part of the LADAL Opening Webinar Series 2021.\nRecording on YouTube: https://youtu.be/Cof8lWdlRqI\nResources:\nLink to the Notebook\nLink to Guillaume’s blog Hypotheses\nLink to the latest issue of CogniTextes\nAbstract\nComputational linguistics offers promising tools for tracking language change in diachronic corpora. These tools exploit distributional semantic models, both old and new. DSMs tend to perform well at the level of lexical semantics but are more difficult to fine-tune when it comes to capturing grammatical meaning.\nI present ways in which the above can be improved. I start from well-trodden methodological paths implemented in diachronic construction grammar: changes in the collocational patterns of a linguistic unit reflect changes in meaning/function; distributional word representations can be supplemented with frequency-based methods. I move on to show that when meaning is apprehended with predictive models (e.g. word2vec), one can trace semantic shifts with greater explanatory power than with count models. Although this idea may sound outdated from the perspective of NLP, it actually goes great ways from the viewpoint of theory-informed corpus linguistics.\nI illustrate the above with several case studies, one of which involves complex locative prepositions in the Corpus of Historical American English. I conclude my talk by defending the idea that NLP, with its focus on computational efficiency, and corpus-linguistics, with its focus on tools that maximize data inspection, have much to gain from getting closer.\n\n\n\n\n\n\nAbout Guillaume\nGuillaume Desagulier is Associate Professor of Linguistics at Paris 8 University, France, and a researcher at the MoDyCo laboratory of the University of Paris Nanterre. He is the recipient of a 5-year honorary position at the Institut Universitaire de France, a division of the French Ministry of Higher Education that distinguishes university professors for their research excellence.\nGuillaume’s research interests are at the crossroads of cognitive linguistics and corpus linguistics. More specifically, he uses corpus-linguistics and statistical techniques to test usage-based hypotheses. He has published on modality, evidentiality, and intensification from a construction-grammar perspective.\nIn 2017 Guillaume published a reference textbook on corpus linguistics with R: Corpus Linguistics and Statistics with R (New York: Springer). The same year, he opened a research blog: Around the word, A corpus linguist’s notebook, where he has since recorded reflections and experiments on his practice as a usage-based corpus linguist (https://corpling.hypotheses.org/)."
  },
  {
    "objectID": "opening.html#usage-based-language-learning-l.-janda",
    "href": "opening.html#usage-based-language-learning-l.-janda",
    "title": "LADAL Opening Webinar Series 2021",
    "section": "Usage-Based Language Learning (L. Janda)",
    "text": "Usage-Based Language Learning (L. Janda)\n\n\nStrategic targeting of rich inflectional morphology for linguistic analysis and L2 acquisition\n\n\n\n\n\nThis talk was recorded Oct. 7, 2021, as part of the LADAL Opening Webinar Series 2021.\nRecording on YouTube: https://youtu.be/FP8x1Euf7SI\nResources:\nSMARTool: SMARTool is a language-learning software tool assisting English-speaking learners of Russian.\nTROLLING is a repository of data, code, and other related materials used in linguistic research. The repository is open access, which means that all information is available to everyone. All postings are accompanied by searchable metadata that identify the researchers, the languages and linguistic phenomena involved, the statistical methods applied, and scholarly publications based on the data (where relevant).\nAbstract\nMany languages have rich inflectional morphology signaling grammatical categories such as case, number, tense, etc. Rich morphology presents a challenge for L2 learners because even a basic vocabulary of a few thousand words can entail mastery of over 100,000 word forms. However, only a handful of the potential forms of a given word occur frequently, while the remainder are rare. Access to digital corpora makes it possible to determine which forms of any given word are of highest frequency, as well as what grammatical and collocational contexts motivate those few frequent forms, facilitating strategically focused language learning tools. Corpus analysis of the frequency distributions of inflectional forms provide linguists with added insights into the function of languages. The results achieved primarily by using correspondence analysis of Russian material are potentially portable to any language with rich inflectional morphology.\n\n\n\n\n\n\nAbout Laura\nIn the Cold War era, Laura Janda combined study of Slavic linguistics at Princeton and UCLA with US-government-funded adventures as an exchange student behind the Iron Curtain in countries that have since changed their names: USSR, Czechoslovakia, and Yugoslavia. After over two decades at the University of Rochester and UNC-Chapel Hill, she moved to the University of Tromsø in 2008. Laura Janda was an early adopter of Cognitive Linguistics in the 1980s and has explored quantitative methods since 2007. Her research focuses primarily on the morphology of Slavic languages, with various admixtures (North Saami, conlangs, political discourse)."
  },
  {
    "objectID": "opening.html#analyzing-historical-publications-t.-säily",
    "href": "opening.html#analyzing-historical-publications-t.-säily",
    "title": "LADAL Opening Webinar Series 2021",
    "section": "Analyzing Historical Publications (T. Säily)",
    "text": "Analyzing Historical Publications (T. Säily)\n\n\nLanguage variation and change in eighteenth-century publications and publishing networks\n\n\n\n\n\nThis talk was recorded Oct. 15, 2021, as part of the LADAL Opening Webinar Series 2021.\nRecording on YouTube: https://youtu.be/B3qar4mYE4k\nResources:\nHelsinki Computational History Group\nResearch Unit for Variation, Contacts and Change in English\nProject website: https://blogs.helsinki.fi/c18-publishing/\nAbstract\nThis talk introduces our new project, Rise of Commercial Society and Eighteenth-Century Publishing, by discussing its premises and a pilot study. Combining historical sociolinguistics with intellectual history, book history and data science, the project studies the eighteenth-century print media and publishing networks that enabled the rise of commercial society and its conceptualization in the eighteenth-century Anglophone world. The chief focus is on Scottish, transatlantic and French influences on British print media.\nA key innovation in our project is that we connect the study of eighteenth-century publishing networks with that of language variation and change to gain results that are of interest to both linguists and historians. To do this, we link enriched bibliographic metadata with full-text sources for historical-sociolinguistic analysis. Our main interest is in charting the use and spread of new vocabulary in the networks to better understand how the discourse develops over time. Instead of focusing on well-known authors, we will identify influencers in a data-driven way; these can also be printers or publishers. By combining network analysis with text mining and social metadata on the actors, we are able to conduct large-scale analyses of how linguistic and stylistic changes spread across social groups in eighteenth-century public discourse.\n\n\n\n\n\n\nAbout Tanja\nTanja Säily is a tenure-track assistant professor in English language at the University of Helsinki. Her research interests include corpus linguistics, digital humanities, historical sociolinguistics, and linguistic productivity. She is also interested in the social embedding of language variation and change in general, including gendered styles in the history of English and extralinguistic factors influencing language change. Her overarching aim is to develop new ways of understanding language variation and change, often in collaboration with experts from other fields. Her current project combines historical sociolinguistics, intellectual history, book history and data science to analyse eighteenth-century publications and publishing networks."
  },
  {
    "objectID": "opening.html#git-and-github-s.-guillou",
    "href": "opening.html#git-and-github-s.-guillou",
    "title": "LADAL Opening Webinar Series 2021",
    "section": "Git and GitHub (S. Guillou)",
    "text": "Git and GitHub (S. Guillou)\n\n\nGit and GitHub/Gitlab for versioning and collaborating\n\n\n\n\n\nThis talk was recorded Oct. 18, 2021, as part of the LADAL Opening Webinar Series 2021.\nRecording on YouTube: https://youtu.be/MbRNbJUY0aE\nResources: GitHub repo for this webinar\nAbstract\nGit is a tool for versioning of and collaborating on any text-based file. Widely used in software development, it is now adopted in many different settings, from document versioning to data analysis management. It is also at the centre of major platforms like GitHub and GitLab, used by millions to share and collaborate on code and documents.\nIn this workshop, you will learn about:\n\nThe main commands used in a git workflow\nHow to publish your work online\nHow to collaborate on a GitHub repository\n\nIf you would like to follow along, please do the following before attending:\n\nInstall Git on your computer (here are OS-specific instructions)\nCreate a GitHub account (or GitLab if you want an alternative)\n\n\n\n\n\n\n\nAbout Stéphane\nStéphane Guillou has worked for the last 10 years at the University of Queensland (UQ). After completing a master’s degree in plants science and ecology in France, he worked in research around the topic of sustainable agriculture. In 2018, a drastic move to a Technology Trainer position at the Library allowed him to share data analysis best practice skills, and promote Open Source tools for research. He is motivated by the principles of Open Science and the opportunities an increasingly collaborative research ecosystem offers."
  },
  {
    "objectID": "opening.html#replicability-robustness-j.-flanagan",
    "href": "opening.html#replicability-robustness-j.-flanagan",
    "title": "LADAL Opening Webinar Series 2021",
    "section": "Replicability & Robustness (J. Flanagan)",
    "text": "Replicability & Robustness (J. Flanagan)\n\n\nReproducibility, Replicability, and Robustness\n\n\n\n\n\nThis talk was recorded Oct. 28, 2021, as part of the LADAL Opening Webinar Series 2021.\nRecording on YouTube: https://youtu.be/1zaKr2B2YBU\nResources: Reproducible research: Strategies, tools, and workflows\nAbstract\nScientific progress has long rested on the often unstated assumptions that research is reproducible (i.e., independent analysts can recreate the results claimed by the original authors by using the original data and analysis techniques), replicable (the results claimed by analysts extends beyond the original data to some wider population or phenomenon), and robust (the findings reported by analysts are not ovely sensitive to assumptions in their model). Recently, however, there have been growing concerns about the extent to which current research practices can meet these assumptions. In this talk, I’ll present a high-level discussion of these issues, defining the key terms, demonstrating how and why questions have been raised about why work may not be as reproducible, replicable, or robust as we may wish, and offer some tentative suggestions for and examples of improving the reproducibility, replicability, and robustness of linguistic research in the future.\n\n\n\n\n\n\nAbout Joe\nJoseph Flanagan is a University Lecturer in Languages/English Studies at the University of Helsinki where he is also supervisor for the doctoral program in Philosophy, Arts, and Society. Joe’s research interests primarily center on issues related to English phonetics and phonology, reproducible research, and the digital humanities. Teaching-wise, he is especially interested in exploring how digital technology can enhance student learning.\nJoe has presented, given talks, and co-organized several workshops on reproducibility and replicability in (corpus) linguistics at international conferences such as the 6th Meeting of the International Society for the Linguistics of English (ISLE6) and ICAME42."
  },
  {
    "objectID": "opening.html#linguistic-phylogenetics-j.-macklin-cordes-e.-round",
    "href": "opening.html#linguistic-phylogenetics-j.-macklin-cordes-e.-round",
    "title": "LADAL Opening Webinar Series 2021",
    "section": "Linguistic Phylogenetics (J. Macklin-Cordes & E. Round)",
    "text": "Linguistic Phylogenetics (J. Macklin-Cordes & E. Round)\n\n\nPhylogenetic comparative methods: What all the fuss is about, and how to use them in everyday research\n\n\n\n\n\nThis talk was recorded Nov. 4, 2021, as part of the LADAL Opening Webinar Series 2021.\nRecording on YouTube: https://youtu.be/yJeQ6AGDLK0\nResources: LADAL tutorial on Phylogentics for language data\nAbstract\nIn this talk, we attempt two tasks. Firstly, we dispel some of the mystery around phylogenetic comparative methods and highlight their fundamental relationship to matters of enduring concern in linguistic typology. Secondly, we aim to show how linguists can carry out essential tasks using phylogenetic methods, easily.\nWe begin by laying out the historical commonalities between typology and comparative biology, and the breakthrough insight that makes phylogenetic comparative methods distinct. This is followed by an overview of some fundamental phylogenetic concepts and tools. Finally, we illustrate these with a typological case study from the Pama-Nyungan languages. Our talk is accompanied by online interactive materials, demonstrating how phylogenetic comparative methods can be incorporated into everyday typological workflows.\n\n\n\n\n\n\nAbout Jayden\nJayden Macklin-Cordes is a postdoctoral researcher in linguistics at the CNRS Dynamics of Language Lab, Lumière University Lyon 2. He recently completed his PhD at The University of Queensland as a member of the Ancient Language Lab. Jayden specialises in historical linguistics and typology, particularly using phylogenetic and quantitative methods.\n\n\n\n\n\n\nAbout Erich\nErich Round is British Academy Global Professor of linguistics at the Surrey Morphology Group, University of Surrey, UK, and director of the Ancient Language Lab at the University of Queensland. His research focuses on linguistic evolution and typology, especially in the domains of morphology and phonology. Erich also specialises in the Tangkic languages of Queensland and the phonologies of Australian Indigenous languages."
  },
  {
    "objectID": "opening.html#analyzing-emigrant-letters-c.-p.-amador-moreno",
    "href": "opening.html#analyzing-emigrant-letters-c.-p.-amador-moreno",
    "title": "LADAL Opening Webinar Series 2021",
    "section": "Analyzing Emigrant Letters (C. P. Amador-Moreno)",
    "text": "Analyzing Emigrant Letters (C. P. Amador-Moreno)\n\n\nTracking Irish English through a corpus of emigrants’ letters\n\n\n\n\n\nThis talk was recorded Nov. 11, 2021, as part of the LADAL Opening Webinar Series 2021.\nRecording on YouTube: https://youtu.be/YBC_y12bcF8\nAbstract\nIn recent years, linguists have focused on reconstructing earlier regional and social varieties of English through the quantitative analysis of emigrant letters and other written documents. As demonstrated in these studies this type of material can provide important, quantifiable data on certain features proper to a particular variety, thus allowing for the reconstruction of some of the features proper to that variety prior to the collection of spoken data.\nThe aim of this talk is to present the Corpus of Irish English Correspondence (CORIECOR), a corpus of personal letters written between 1750-1940. The corpus contains some 4700 texts (approx. 3 million words), of which 4100 (2.5m words) are correspondence maintained between Irish emigrants and their relatives, friends and contacts. The letters were sent mainly between Ireland and other countries such as the United States, Canada, Great Britain, New Zealand, Argentina and Australia, and therefore provide an empirical base for studies of historical change in Irish English and its contribution to other major overseas varieties. In order to explore the use of a corpus like this for the study of Irish English, the talk will show some recent findings.\n\n\n\n\n\n\nAbout Carolina\nCarolina P. Amador-Moreno is Professor of English Linguistics at the University of Bergen. She has held different teaching positions at the University of Extremadura (Department of English), the University of Limerick (Department of Languages and Cultural Studies), and University College Dublin (English Department). Her research interests centre on the English spoken in Ireland and include historical linguistics, stylistics, discourse analysis, corpus linguistics, sociolinguistics, and pragmatics. Her publications include articles and chapters dealing with these topics. She is the author, among others, of Orality in written texts: Using historical corpora to investigate Irish English (1700-1900), Routledge (2019); An Introduction to Irish English, Equinox (2010); the co-edited volumes Irish Identities: Sociolinguistic Perspectives (Mouton de Gruyter, 2020); Voice and Discourse in the Irish Context (Palgrave-Macmillan, 2017); Pragmatic Markers in Irish English (John Benjamins, 2015). She’s an associate member of CALS (Centre for Applied Language Studies), IVACS (Inter-Variational Applied Corpus Linguistics network), both at the University of Limerick, and LINGLAP (the Research Institute for Linguistics and Applied Languages), at the University of Extremadura, which she was Director of until August 2020."
  },
  {
    "objectID": "opening.html#aarnet-cloudstor-s.-king",
    "href": "opening.html#aarnet-cloudstor-s.-king",
    "title": "LADAL Opening Webinar Series 2021",
    "section": "AARNet & CloudStor (S. King)",
    "text": "AARNet & CloudStor (S. King)\n\n\nAARNet, CloudStor and SWAN: easy storing and sharing of research data and resources\n\n\n\n\n\nThis talk was recorded Nov. 18, 2021, as part of the LADAL Opening Webinar Series 2021.\nRecording on YouTube: https://youtu.be/YfiV4QWzS6I\nResources: https://jupyterbook.org/intro.html\nAbstract\nThis webinar introduces AARNet, Australia’s Academic Research Network, and CloudStor, a research-specific sync, store and share platform. Discover more about the CloudStor interface and its associated tools and services for managing active research data. Learn how to organise, maintain, store and analyse active data, and understand safe and secure ways of sharing and storing data. This session will also introduce SWAN, the Service for Web-based Analysis, and Jupyter Notebooks, a digital tool that has exploded in popularity in recent years for those working with data. This will be an introductory session for those who are brand new, have little or no knowledge of coding and computational methods in research, but would like to know more about how to get started.\n\n\n\n\n\n\nAbout Sara\nSara King is the Training and Engagement Lead at Australia’s academic and research network provider, AARNet. She has extensive experience in engagement and training, with expertise in research data and technologies in the Humanities and Social Science (HASS) research areas. Prior to eResearch she worked for almost a decade at the National Archives of Australia and a few years in a public library. She has a PhD in Migration Studies and is a little bit obsessed with the idea of knitting as a form of coding.\n\n\nBack to top\nBack to HOME"
  },
  {
    "objectID": "pdf2txt.html#extract-text-from-one-pdf",
    "href": "pdf2txt.html#extract-text-from-one-pdf",
    "title": "Converting PDFs to txt files with R",
    "section": "Extract text from one pdf",
    "text": "Extract text from one pdf\nThe pdf we will convert is a pdf of the Wikipedia article about corpus linguistics. The first part of that pdf is shown below.\n\n\n\n\n\nGiven that the pdf contains tables, urls, reference, etc., the text that we will extract from the pdf will be rather messy - cleaning the content of the text would be another matter (it would be data processing rather than extraction) and we will thus only focus on the conversion process here and not focus on the data cleaning and processing aspect.\nWe begin the extraction by defining a path to the pdf. Once we have defined a path, i.e. where R is supposed to look for that file, we continue by extracting the text from the pdf.\n\n# you can use an url or a path that leads to a pdf document\npdf_path <- \"https://slcladal.github.io/data/PDFs/pdf0.pdf\"\n# extract text\ntxt_output <- pdftools::pdf_text(pdf_path) %>%\n  paste0(collapse = \" \") %>%\n  paste0(collapse = \" \") %>%\n  stringr::str_squish() \n\n\n\n\n\n\nFirst 1000 characters of the extracted text from a pdf of the wikipedia article on corpus linguistics.\n\n\n.Corpus linguistics - Wikipedia https://en.wikipedia.org/wiki/Corpus_linguistics Corpus linguistics Corpus linguistics is the study of language as expressed in corpora (samples) of \"real world\" text. Corpus linguistics proposes that reliable language analysis is more feasible with corpora collected in the field in its natural context (\"realia\"), and with minimal experimental-interference. The field of corpus linguistics features divergent views about the value of corpus annotation. These views range from John McHardy Sinclair, who advocates minimal annotation so texts speak for themselves,[1] to the Survey of English Usage team (University College, London), who advocate annotation as allowing greater linguistic understanding through rigorous recording.[2] The text-corpus method is a digestive approach that derives a set of abstract rules that govern a natural language from texts in that language, and explores how that language relates to other languages. Originally derived manually, cor"
  },
  {
    "objectID": "pdf2txt.html#extracting-text-from-many-pdfs",
    "href": "pdf2txt.html#extracting-text-from-many-pdfs",
    "title": "Converting PDFs to txt files with R",
    "section": "Extracting text from many pdfs",
    "text": "Extracting text from many pdfs\nTo convert many pdf-files, we write a function that preforms the conversion for many documents.\n\nconvertpdf2txt <- function(dirpath){\n  files <- list.files(dirpath, full.names = T)\n  x <- sapply(files, function(x){\n  x <- pdftools::pdf_text(x) %>%\n  paste0(collapse = \" \") %>%\n  stringr::str_squish()\n  return(x)\n    })\n}\n\nWe can now apply the function to the folder in which we have stored the pdf-files we want to convert. In the present case, I have stored 4 pdf-files of Wikipedia articles in a folder called PDFs which is located in my data folder as described in the section above which detailed how to set up the Rproject folder on your computer). The output is a vector with the texts of the pdf-files.\n\n# apply function\ntxts <- convertpdf2txt(here::here(\"data\", \"PDFs/\"))\n\n\n\n\n\n\nFirst 1000 characters of the extracted texts from pdfs of selected wikipedia articles.\n\n\n.Corpus linguistics - Wikipedia https://en.wikipedia.org/wiki/Corpus_linguistics Corpus linguistics Corpus linguistics is the study of language as expressed in corpora (samples) of \"real world\" text. Corpus linguistics proposes that reliable language analysis is more feasible with corpora collected in the field in its natural context (\"realia\"), and with minimal experimental-interference. The field of corpus linguistics features divergent views about the value of corpus annotation. These views range from John McHardy Sinclair, who advocates minimal annotation so texts speak for themselves,[1] to the Survey of English Usage team (University College, London), who advocate annotation as allowing greater linguistic understanding through rigorous recording.[2] The text-corpus method is a digestive approach that derives a set of abstract rules that govern a natural language from texts in that language, and explores how that language relates to other languages. Originally derived manually, corLanguage - Wikipedia https://en.wikipedia.org/wiki/Language Language A language is a structured system of communication. Language, in a broader sense, is the method of communication that involves the use of – particularly human – languages.[1][2][3] The scientific study of language is called linguistics. Questions concerning the philosophy of language, such as whether words can represent experience, have been debated at least since Gorgias and Plato in ancient Greece. Thinkers such as Rousseau have argued that language originated from emotions while others like Kant have held that it originated from rational and logical thought. 20th-century philosophers such as Wittgenstein argued that philosophy is really the study of language. Major figures in linguistics include Ferdinand de Saussure and Noam Chomsky. Estimates of the number of human languages in the world vary between 5,000 and 7,000. However, any precise estimate depends on the arbitrary distinction (dichotomy) between languages Natural language processing - Wikipedia https://en.wikipedia.org/wiki/Natural_language_processing Natural language processing Natural language processing (NLP) is a subfield of linguistics, computer science, information engineering, and artificial intelligence concerned with the interactions between computers and human (natural) languages, in particular how to program computers to process and analyze large amounts of natural language data. Challenges in natural language processing frequently involve speech recognition, natural language understanding, and natural language generation. Contents History Rule-based vs. statistical NLP Major evaluations and tasks Syntax Semantics An automated online assistant Discourse providing customer service on a Speech web page, an example of an Dialogue application where natural Cognition language processing is a major component.[1] See also References Further reading History The history of natural language processing (NLP) generally started in the 195Computational linguistics - Wikipedia https://en.wikipedia.org/wiki/Computational_linguistics Computational linguistics Computational linguistics is an interdisciplinary field concerned with the statistical or rule-based modeling of natural language from a computational perspective, as well as the study of appropriate computational approaches to linguistic questions. Traditionally, computational linguistics was performed by computer scientists who had specialized in the application of computers to the processing of a natural language. Today, computational linguists often work as members of interdisciplinary teams, which can include regular linguists, experts in the target language, and computer scientists. In general, computational linguistics draws upon the involvement of linguists, computer scientists, experts in artificial intelligence, mathematicians, logicians, philosophers, cognitive scientists, cognitive psychologists, psycholinguists, anthropologists and neuroscientists, among \n\n\nThe table above shows the first 1000 characters of the texts extracted from 4 pdf-files of Wikipedia articles associated with language technology (corpus linguistics, linguistics, natural language processing, and computational linguistics)."
  },
  {
    "objectID": "pdf2txt.html#saving-the-texts",
    "href": "pdf2txt.html#saving-the-texts",
    "title": "Converting PDFs to txt files with R",
    "section": "Saving the texts",
    "text": "Saving the texts\nTo save the texts in txt-files on your disc, you can simply replace the predefined location (the data folder of your Rproject located by the string here::here(\"data\") with the folder where you want to store the txt-files and then execute the code below. Also, we will name the texts (or the txt-files if you like) as pdftext plus their index number.\n\n# add names to txt files\nnames(txts) <- paste0(here::here(\"data\",\"pdftext\"), 1:length(txts), sep = \"\")\n# save result to disc\nlapply(seq_along(txts), function(i)writeLines(text = unlist(txts[i]),\n    con = paste(names(txts)[i],\".txt\", sep = \"\")))\n\nIf you check the data folder in your Rproject folder, you should find 4 files called pdftext1, pdftext2, pdftext3, pdftext4."
  },
  {
    "objectID": "pdf2txt.html#spell-correction",
    "href": "pdf2txt.html#spell-correction",
    "title": "Converting PDFs to txt files with R",
    "section": "Spell correction",
    "text": "Spell correction\nIn a first step, we write a function that loops over each text and checks which words occur in a English language dictionary (which we do not specify as it is the default). This spell checking makes use of the the hunspell package (see here for more information). Hunspell is based on MySpell and is backward-compatible with MySpell and aspell dictionaries. This means that we can import and/or make use of many different language dictionaries and it is quite likely that the dictionaries for other languages may already available on your system!\n\n# create token list\ntokens_ocr <- sapply(ocrs, function(x){\n  x <- hunspell::hunspell_parse(x)\n})\n\n\n\n\n\n\nFirst 1000 characters of the spell-checked wikipedia article on corpus linguistics.\n\n\n.c(\"Corpus\", \"linguistics\", \"Wikipedia\", \"https\", \"en\", \"wikipedia\", \"org\", \"wiki\", \"Corpus\", \"linguistics\", \"WIKIPEDIA\", \"e\", \"e\", \"e\", \"Corpus\", \"linguistics\", \"Corpus\", \"linguistics\", \"is\", \"the\", \"study\", \"of\", \"language\", \"as\", \"expressed\", \"in\", \"corpora\", \"samples\", \"of\", \"real\", \"world\", \"text\", \"Corpus\", \"linguistics\", \"proposes\", \"that\", \"reliable\", \"language\", \"analysis\", \"is\", \"more\", \"feasible\", \"with\", \"corpora\", \"collected\", \"in\", \"the\", \"field\", \"in\", \"its\", \"natural\", \"context\", \"realia\", \"and\", \"with\", \"minimal\", \"experimental\", \"interference\", \"The\", \"field\", \"of\", \"corpus\", \"linguistics\", \"features\", \"divergent\", \"views\", \"about\", \"the\", \"value\", \"of\", \"corpus\", \"annotation\", \"These\", \"views\", \"range\", \"from\", \"John\", \"McHardy\", \"Sinclair\", \"who\", \"advocates\", \"minimal\", \"annotation\", \"so\", \"texts\", \"speak\", \"for\", \"themselves\", \"to\", \"the\", \"Survey\", \"of\", \"English\", \"Usage\", \"team\", \"University\", \"College\", \"London\", \"who\", \"advocate\", \"annotation\", \"as\", \"allowinc(\"Language\", \"Wikipedia\", \"https\", \"en\", \"wikipedia\", \"org\", \"wiki\", \"Language\", \"WIKIPEDIA\", \"Language\", \"A\", \"language\", \"is\", \"a\", \"structured\", \"system\", \"of\", \"communication\", \"Language\", \"in\", \"a\", \"broader\", \"sense\", \"is\", \"the\", \"method\", \"of\", \"communication\", \"that\", \"involves\", \"the\", \"use\", \"of\", \"particularly\", \"human\", \"languages\", \"J\", \"II\", \"The\", \"scientific\", \"study\", \"of\", \"language\", \"is\", \"called\", \"linguistics\", \"Questions\", \"concerning\", \"the\", \"philosophy\", \"of\", \"language\", \"such\", \"as\", \"whether\", \"words\", \"can\", \"represent\", \"experience\", \"have\", \"been\", \"o\", \"debated\", \"at\", \"least\", \"since\", \"Gorgias\", \"and\", \"Plato\", \"in\", \"ancient\", \"Greece\", \"Thinkers\", \"such\", \"as\", \"Rousseau\", \"have\", \"argued\", \"that\", \"language\", \"originated\", \"from\", \"emotions\", \"while\", \"others\", \"like\", \"Kant\", \"have\", \"held\", \"that\", \"it\", \"originated\", \"from\", \"rational\", \"and\", \"logical\", \"thought\", \"th\", \"century\", \"philosophers\", \"such\", \"as\", \"Wittgenstein\", \"argued\", \"thatc(\"Natural\", \"language\", \"processing\", \"Wikipedia\", \"https\", \"en\", \"wikipedia\", \"org\", \"wiki\", \"Natural\", \"language\", \"processing\", \"WIKIPEDIA\", \"e\", \"Natural\", \"language\", \"processing\", \"Natural\", \"language\", \"processing\", \"NLP\", \"is\", \"a\", \"subfield\", \"of\", \"linguistics\", \"computer\", \"science\", \"information\", \"engineering\", \"and\", \"artificial\", \"intelligence\", \"concerned\", \"with\", \"the\", \"interactions\", \"ass\", \"between\", \"computers\", \"and\", \"human\", \"natural\", \"languages\", \"in\", \"particular\", \"how\", \"to\", \"program\", \"computers\", \"to\", \"process\", \"and\", \"analyze\", \"large\", \"amounts\", \"of\", \"natural\", \"language\", \"data\", \"ee\", \"ce\", \"ee\", \"oo\", \"Challenges\", \"in\", \"natural\", \"language\", \"processing\", \"frequently\", \"involve\", \"speech\", \"recognition\", \"natural\", \"language\", \"understanding\", \"and\", \"natural\", \"language\", \"generation\", \"non\", \"Contents\", \"am\", \"History\", \"i\", \"cseunec\", \"Rule\", \"based\", \"vs\", \"statistical\", \"NLP\", \"jim\", \"Hi\", \"I'm\", \"your\", \"automated\", \"online\", \"ser\", c(\"Computational\", \"linguistics\", \"Wikipedia\", \"https\", \"en\", \"wikipedia\", \"org\", \"wiki\", \"Computational\", \"linguistics\", \"WIKIPEDIA\", \"e\", \"e\", \"e\", \"e\", \"Computational\", \"linguistics\", \"Computational\", \"linguistics\", \"is\", \"an\", \"interdisciplinary\", \"field\", \"concerned\", \"with\", \"the\", \"statistical\", \"or\", \"rule\", \"based\", \"modeling\", \"of\", \"natural\", \"language\", \"from\", \"a\", \"computational\", \"perspective\", \"as\", \"well\", \"as\", \"the\", \"study\", \"of\", \"appropriate\", \"computational\", \"approaches\", \"to\", \"linguistic\", \"questions\", \"Traditionally\", \"computational\", \"linguistics\", \"was\", \"performed\", \"by\", \"computer\", \"scientists\", \"who\", \"had\", \"specialized\", \"in\", \"the\", \"application\", \"of\", \"computers\", \"to\", \"the\", \"processing\", \"of\", \"a\", \"natural\", \"language\", \"Today\", \"computational\", \"linguists\", \"often\", \"work\", \"as\", \"members\", \"of\", \"interdisciplinary\", \"teams\", \"which\", \"can\", \"include\", \"regular\", \"linguists\", \"experts\", \"in\", \"the\", \"target\", \"language\", \"and\", \"computer\", \"s\n\n\nIn a next step, we can correct errors resulting from the OCR process, correct the errors and paste th texts back together (which is all done by the code chunk below).\n\n# clean\nclean_ocrtext <- sapply(tokens_ocr, function(x){\n  correct <- hunspell::hunspell_check(x)\n  x <- ifelse(correct == F, \n              x[hunspell::hunspell_check(x)],\n              x)\n  x <- paste0(x, collapse = \" \")\n})\n\n\n\n\n\n\nFirst 1000 characters of the processed text from a pdf of the wikipedia article on corpus linguistics.\n\n\n.Corpus linguistics Wikipedia en en wiki org wiki Corpus linguistics WIKIPEDIA e e e Corpus linguistics Corpus linguistics is the study of language as expressed in corpora samples of real world text Corpus linguistics proposes that reliable language analysis is more feasible with corpora collected in the field in its natural context minimal and with minimal experimental interference The field of corpus linguistics features divergent views about the value of corpus annotation These views range from John minimal Sinclair who advocates minimal annotation so texts speak for themselves to the Survey of English Usage team University College London who advocate annotation as allowing greater linguistic understanding through rigorous recording The text corpus method is a digestive approach that derives a set of abstract rules that govern a natural language from texts in that language and explores how that language relates to other languages Originally derived manually corpora now are automaticaLanguage Wikipedia en en wiki org wiki Language WIKIPEDIA Language A language is a structured system of communication Language in a broader sense is the method of communication that involves the use of particularly human languages J II The scientific study of language is called linguistics Questions concerning the philosophy of language such as whether words can represent experience have been o debated at least since in and Plato in ancient Greece Thinkers such as Rousseau have argued that language originated from emotions while others like Kant have held that it originated from rational and logical thought as century philosophers such as Wittgenstein argued that philosophy is really the study of language Major include An figures in linguistics include Ferdinand of Saussure and of Chomsky Lie Estimates of the number of human languages in the world vary between and However any precise estimate depends on the arbitrary distinction Natural Y is dichotomy between languages and dialect NatuNatural language processing Wikipedia en en wiki org wiki Natural language processing WIKIPEDIA e Natural language processing Natural language processing of is a science of linguistics computer science information engineering and artificial intelligence concerned with the interactions ass between computers and human natural languages in particular how to program computers to process and analyze large amounts of natural language data processing frequently involve speech Challenges in natural language processing frequently involve speech recognition natural language understanding and natural language generation non Contents am History i online Rule based vs statistical How may Hi I'm your automated online online assistant Major evaluations and tasks service How may page an f of Syntax Dialogue Semantics where An automated online assistant Discourse a providing customer service also Speech web page an example of an Dialogue application where natural Cognition language processing is a majoComputational linguistics Wikipedia en en wiki org wiki Computational linguistics WIKIPEDIA e e e e Computational linguistics Computational linguistics is an interdisciplinary field concerned with the statistical or rule based modeling of natural language from a computational perspective as well as the study of appropriate computational approaches to linguistic questions Traditionally computational linguistics was performed by computer scientists who had specialized in the application of computers to the processing of a natural language Today computational linguists often work as members of interdisciplinary teams which can include regular linguists experts in the target language and computer scientists In general computational linguistics draws upon the involvement of linguists computer scientists experts in artificial intelligence mathematicians logicians philosophers cognitive scientists cognitive psychologists among anthropologists and linguistics among others Computational linguis\n\n\nWe have reached the end of this tutorial and we hope that the tutoral helps you in performing OCR on your own pdfs."
  },
  {
    "objectID": "people.html",
    "href": "people.html",
    "title": "OUR PEOPLE",
    "section": "",
    "text": "The LADAL is a collaborative effort that is sponsored by the School of Languages and Cultures at the University of Queensland. If you are interested in becoming an affiliate member or even a contributor contact the LADAL team via email (ladal@uq.edu.au).\n\n\n\n\n\nUSER STORIESWe are currently looking for user stories (also known as testimonials) to see and show what people use LADAL resources for. If you have used LADAL resources - be it by simply copying some code, attending a workshop, learning about a method using a tutorial, or in any other way - we would be extremely grateful, if you would send us your user story! To submit your user story, simply write up a paragraph describing how you have used LADAL resources and what you have used them for and send it to ladal@uq.edu.au. We really appreciate any feedback from you about this!\n\n\n\n\n\n\n\nDIRECTORS\nMartin Schweinberger\n\n\n\n\n\nMartin is Lecturer in Applied Linguistics at the School of Languages and Cultures at the University of Queensland, Australia as well as Associate Professor and Lab Director at the AcqVA-Aurora Center at Arctic University of Norway in Tromsø .\nMartin has initiated and established LADAL and is its leading proponent. His role at LADAL encompasses creating content, supervising team members, and directing the activities at LADAL. He is a language data scientist with a PhD in English linguistics specialized in corpus linguistics, computational analyses and visualization of linguistic data.\n\nMichael Haugh\n\n\n\n\n\nMichael is Full Professor of Linguistics and Applied Linguistics at School of Languages and Cultures, a Fellow of the Australian Academy of the Humanities, and project lead of the Australian Text Analytics Platform (ATAP) as well as the Language Data Commons of Australia (LDaCA)\nMichael is supervising and managing the activities of LADAL and overseeing its promotion. He is a long-standing proponent of Digital Humanities in Australia and globally with a special focus on data management and the accessibility and usability of language data\n\n\n\nCONTRIBUTORS | MEMBERS\nContributors are actively engaged in the LADAL and assist in the development of LADAL infrastructure or resources.\nBen Foley\n\n\n\n\n\nBen Foley was the project manager of CoEDL’s Transcription Acceleration Project (TAP).\nBen has specialized on speech recognition and the development of user-friendly speech recognition tools.\nBen’s previous experience with Aboriginal and Torres Strait Islander language resource development has resulted in apps and websites galore. Highlights include the Iltyem-iltyem sign language database and website, and the Gambay First Languages Map, showing the hundreds of languages in Australia.\nStefan Th. Gries\n\n\n\n\n\nStefan is full professor of linguistics in the Department of Linguistics at the University of California, Santa Barbara (UCSB), Honorary Liebig-Professor and Chair of English Linguistics at the Justus-Liebig-Universität Giessen, Germany.\nHis research is mainly situated at the intersection of quantitative corpus linguistics, cognitive linguistics, computational linguistics, and L1/Ln acquisition. Stefan’s research also uses experimental methods.\nStefan is a major proponent of using the open source software R in language data science.\nAndreas Niekler\n\n\n\n\n\nAndreas Niekler is a research associate in Computer Science at the University of Leipzig and he develops computer-based methods in the field of semantic properties in language and language-based AI.\nHe develops computer-based algorithmic methods for computational social science, including the Postdemocracy and Neoliberalism project and the interactive analysis platform Leipzig Corpus Miner (iLCM).\nThe focus is on machine learning methods and data management. This includes processing of unstructured data for knowledge and document management.\nGregor Wiedemann\n\n\n\n\n\nGregor is a Senior Researcher in Computational Social Science at the Leibniz Institute for Media Research │ Hans Bredow Institute (HBI).\nTogether with Sascha Hölig he heads the Media Research Methods Lab (MRML) where he focuses on the development of methods and applications of natural language processing and text mining for empirical social and media research.\nAfter studying in Leipzig and Miami, USA, Gregor worked at the Computer Science departments of the universities of Leipzig and Hamburg.\n\nErich Round\n\n\n\n\n\nErich is a British Academy Global Professor in linguistics at the University of Surrey, UK.\nHis research is in phonology and morphology, especially of Australian Indigenous languages, and the modeling of language evolution and diversification.\nHe is an active creator and analyzer of large scale cross-linguistic data sets, for investigating the nature and origins of linguistic diversity.\n Stephane Guillou\n\n\n\n\n\nStephane has worked for the last 10 years at the University of Queensland (UQ) and he is the leading proponent of computational up-skilling at the UQ library\nAfter completing a master’s degree in plants science and ecology in France, he worked in research but moved to a Technology Trainer position at the UQ Library in 2018 that allowed him to share data analysis best practice skills, and promote Open Source tools for research.\nHe has extensive experience in several programming environments and teaching courses on computation at various levels of competence\nJoeseph Flanagan\n\n\n\n\n\n\nJoe is a Senior Lecturer and computational linguist at the University of Helsinki\nHis research interests primarily center on issues related to English phonetics and phonology, reproducible research, and the digital humanities.\nTeaching-wise, he is especially interested in exploring how digital technology can enhance student learning. (Joe’s GitHub repo)\n\n\n\n\nAFFILIATE MEMBERS\nAffiliate members support the LADAL and are informed about events, workshops, and training opportunities at LADAL.\nGerold Schneider (University of Zurich, Switzerland)\nMonika Bednarek (University of Sydney, Australia)\nLaurence Anthony (Waseda University, Japan)\nPeter Crosthwaite (The University of Queensland, Australia)\nSimon Musgave (Monash University, Australia)\n\n\n\nCOLLABORATIONS\nCollaborating institutions and organizations support LADAL and share information or resources with LADAL.\n\n\n\n\n\n\nThe School of Languages and Cultures at the University of Queensland\n\n\n\n\n\n\n\nThe Text Crunching Center at the University of Zurich (UZH)\n\n\n\n\n\n\n\nThe Sydney Corpus Lab at The University of Sydney\n\n\n\n\n\n\n\nVARIENG at the University of Helsinki\n\n\n\n\n\n\n\nThe AcqVA Aurora Lab in the UiT Aurora Center for Language Acquisition, Variation & Attrition at The Arctic University of Norway in Tromsø. (AcqVA Aurora Lab’s GitHub repo)\n\n\n\n\n\n\n\nThe Media Research Methods Lab (MRML) at the Leibniz Institute for Media Research │ Hans Bredow Institute (HBI). .\n\n\n\n\nFORMER MEMBERS\nFormer members were engaged with LADAL but have taken up new positions, changed affiliations, or moved institutions resulting in parting trajectories.\nKatherine Dallaston\nRestuadi Restuadi\nKaty McHugh\nAlex Trueman\nDattatreya Majumdar\nStephen Kennedy-Clark\nLiam Crowhurst\n\nBack to top\nBack to HOME"
  },
  {
    "objectID": "phylo.html#an-intuitive-introduction-to-the-challenge-of-genealogy",
    "href": "phylo.html#an-intuitive-introduction-to-the-challenge-of-genealogy",
    "title": "Practical phylogenetic methods for linguistic typology",
    "section": "An intuitive introduction to the challenge of genealogy",
    "text": "An intuitive introduction to the challenge of genealogy\nTo get an intuitive notion of the challenge that is presented by the genealogical relatedness of languages, consider the small language family pictured in the figure below. It contains four languages, which are either SOV or SVO. The question is: What proportion of this family is SOV? Should you just count the languages, in which case the answer is literally 75%? This answer may seem not quite right, because now one half of the family is counting three times as much as the other half, just because it had the fortune of containing more languages. Put another way, the figure of 75% is strongly influenced by contingencies of history.\n\n\n\n\n\nIn a LADAL seminar on 4 November, 2021 [1] and in a related journal article [2], Jayden Macklin-Cordes and I review some of history of thinking in linguists’ attempts to account for historical relatedness of exactly this kind when comparing across languages. We also explain how phylogenetic comparative methods present the most principled response to it developed so far. We recommend reviewing either of these sources, since they cover the scientific content which corresponds to the practical, technical content in this tutorial. Here, we will not review the scientific content at any length.\nThis document provides a guide to the practical use of phylogenetic comparative methods for linguistic typology, specifically, the calculation of genealogically-sensitive proportions and averages. These are methods which provide an answer to the question: “When characterising the frequencies of traits among the world’s languages, how can we take genealogy into account?” Here we cover two methods for calculating genealogically-sensitive proportions and averages: the ‘ACL’ method [3] and the ‘BM’ method [4].\nBecause a key part of this kind of analysis is the preparation of a phylogenetic tree, an important component of the tutorial will be about how such trees can be prepared.\nThe tutorial is divided into four main sections. Section Trees in R is an introduction to how trees are represented in R. Section Genealogically-sensitive averages and proportions discusses the calculation of genealogically-sensitive proportions and averages. Section Using and adapting trees from glottolog.com explains how typologists can prepare phylogenetic trees by adapting resources freely available from glottolog.com [5]. And Section Putting it together: A worked example provides a worked example used in a typological investigation of sonority sequencing by [6].\nMuch of the discussion below is an introduction to the functionality of two R packages, glottoTrees [7] and phyloWeights [8], which have been specifically written for these tasks. The text of this tutorial is taken largely from the Supplementary Materials section S1 of [2]."
  },
  {
    "objectID": "phylo.html#glottolog-genealogical",
    "href": "phylo.html#glottolog-genealogical",
    "title": "Practical phylogenetic methods for linguistic typology",
    "section": "3.1 Glottolog’s genealogical data",
    "text": "3.1 Glottolog’s genealogical data\nGlottolog provides metadata about the world’s language varieties, their division into language families and the hierarchical subgrouping of languages inside those families. Naturally, there are many points of contention in linguistics about what the world’s stock of languages and dialects actually is, how it groups into families, and how the families themselves are subgrouped. Glottolog provides one set of answers, and structures them in a way which provides typologists with a basis for carrying out changes to suit their own hypotheses. In later sections we will see how this can be done. In this section we describe glottolog’s own global linguistic metadata.\nAt time of writing, the current version of glottolog is v4.4. The glottoTrees package contains a copy of the v4.4 metadata covering language names, language identification codes, family names, geographical groupings, and family trees. The original metadata files that contain this information are currently available at https://glottolog.org/meta/downloads, where a file named tree_glottolog_newick.txt2 contains glottolog’s trees, and languages_and_dialects_geo.csv provides geographical metadata.\nLanguage metadata can be accessed using the glottoTrees function get_glottolog_languages(). This function returns a dataframe of close to twenty-six thousand rows. To view it in full, we suggest saving it to a CSV file and opening it in spreadsheet software such as Excel:\n\nlanguage_metadata <- get_glottolog_languages()\nwrite.csv(language_metadata, \"language_metadata.csv\")\n\nHere are the first ten rows:\n\nlanguage_metadata <- get_glottolog_languages()\nhead(language_metadata, n = 10)\n\n   glottocode isocodes       name name_in_tree position tree\n1    3adt1234          3Ad-Tekles   3Ad-Tekles      tip  391\n2    aala1237              Aalawa       Aalawa      tip   94\n3    aant1238          Aantantara   Aantantara      tip   90\n4    aari1238     <NA>       <NA>   Aari-Gayil     node   22\n5    aari1239      aiw       Aari         Aari      tip   22\n6    aari1240      aay     Aariya       Aariya     <NA>   NA\n7    aasa1238      aas      Aasax        Aasax      tip  391\n8    aasd1234            Aasdring     Aasdring      tip  269\n9    aata1238           Aatasaara    Aatasaara      tip   90\n10   abaa1238              Rngaba       Rngaba      tip  345\n               tree_name\n1           Afro-Asiatic\n2           Austronesian\n3  NuclearTransNewGuinea\n4            SouthOmotic\n5            SouthOmotic\n6                   <NA>\n7           Afro-Asiatic\n8          Indo-European\n9  NuclearTransNewGuinea\n10          Sino-Tibetan\n\n\nListed here are glottolog’s languages, dialects, subgroups and families. These entities are identified by a name, an ISO-639-3 code if available (format: three letters) and a glottolog-specific glottocode (format: four letters followed by four digits3). Also described is the entity’s relationship to a glottolog tree: the representation of its name in the tree (which may differ slightly from the name used elsewhere by glottolog4), its position (as tip or node), and the tree’s number and name.\nBy default, the metadata functions in glottoTrees, such as get_glottolog_languages(), will return information about the most recent version of glottolog which the package contains. To access older versions, supply the version number via the glottolog_version argument:5\n\nlanguage_metadata_v4.3 <- get_glottolog_languages(glottolog_version = \"4.3\")\nhead(language_metadata_v4.3, n = 10)\n\n   glottocode isocodes       name name_in_tree position tree\n1    3adt1234          3Ad-Tekles   3Ad-Tekles      tip  186\n2    aala1237              Aalawa       Aalawa      tip  205\n3    aant1238          Aantantara   Aantantara      tip  145\n4    aari1238     <NA>       <NA>   Aari-Gayil     node   85\n5    aari1239      aiw       Aari         Aari      tip   85\n6    aari1240      aay     Aariya       Aariya     <NA>   NA\n7    aasa1238      aas      Aasax        Aasax      tip  186\n8    aasd1234            Aasdring     Aasdring      tip  179\n9    aata1238           Aatasaara    Aatasaara      tip  145\n10   abaa1238              Rngaba       Rngaba      tip  329\n               tree_name\n1           Afro-Asiatic\n2           Austronesian\n3  NuclearTransNewGuinea\n4            SouthOmotic\n5            SouthOmotic\n6                   <NA>\n7           Afro-Asiatic\n8          Indo-European\n9  NuclearTransNewGuinea\n10          Sino-Tibetan\n\n\nBriefer metadata about glottolog’s language families can be accessed using the glottoTrees function get_glottolog_families(). This returns a dataframe of 420 rows, so to view it in full, we also suggest saving it to a CSV file and opening it in spreadsheet software. Here are the first ten rows:\n\nfamily_metadata <- get_glottolog_families()\nhead(family_metadata, n = 10)\n\n   tree           tree_name n_tips n_nodes main_macroarea\n1     1                 Yam     33      18      Papunesia\n2     2     Mongolic-Khitan     66      25        Eurasia\n3     3 Kol{PapuaNewGuinea}      2       1      Papunesia\n4     4       Namla-Tofanma      2       1      Papunesia\n5     5          Tanahmerah      1       1      Papunesia\n6     6         Jarawa-Onge      2       1        Eurasia\n7     7        Ta-Ne-Omotic     29      15         Africa\n8     8              Pomoan     10       7  North America\n9     9         WesternDaly     14       7      Australia\n10   10           Yangmanic      3       1      Australia\n\n\nGlottolog v4.4 divides the world’s languages into 420 families, including 138 isolates, and it provides a tree for each. Together, the 420 trees contain 8,209 internal nodes and 17,008 tips, many of which represent varieties that would typically be considered dialects. Geographically, glottolog assigns each language variety to one of six macroareas: Africa, Australia, Eurasia, Papunesia, South America or North America. The glottoTrees metadata includes a column main_macroarea. This is the one macroarea which contains more of the family’s language varieties than any other. We will see how this information can be useful in Section @ref(combining-trees).\nGlottolog’s 420 family trees are stored in a multiPhylo object named glottolog_trees_v4.4. For example, here is glottolog’s representation of the Great Andamanese family, which is tree 340 within the object glottolog_trees_v4.4. For readability, we plot this tree horizontally:\n\ntree_GA <- glottolog_trees_v4.4[[340]]\nplot(tree_GA, x.lim = c(-0.3, 14))\n\n\n\n\n\n\n\n\nJust above, we obtained the tree for Great Andamanese by referring to its tree number (340) in the glottolog_trees_v4.4 object. The package glottoTrees also provides a function get_glottolog_trees() which enables trees to be obtained using the glottolog name for their families, for instance:\n\ntree_GA <- get_glottolog_trees(\"GreatAndamanese\")\nplot(tree_GA, x.lim = c(-0.3, 14))\n\n\n\n\n\n\n\n\nIf you know the name of one or more families and would like to now the number of their trees, use which_tree():\n\nwhich_tree(\"GreatAndamanese\")\n\nGreatAndamanese \n            340 \n\nwhich_tree(c(\"Turkic\", \"Tupian\", \"Tuu\"))\n\nTurkic Tupian    Tuu \n   217     32     76 \n\n\nBoth get_glottolog_trees() and which_tree() allow the usage of a glottolog_version argument, to refer to older versions of glottolog. For instance, here are the tree numbers of the same families in version 4.1:\n\nwhich_tree(\"GreatAndamanese\", glottolog_version = \"4.1\")\n\nGreatAndamanese \n              6 \n\nwhich_tree(c(\"Turkic\", \"Tupian\", \"Tuu\"), glottolog_version = \"4.1\")\n\nTurkic Tupian    Tuu \n    66    297     80 \n\n\nIn glottolog’s trees, the tip labels are rather long, consisting of a name followed by a glottocode in angled brackets, an ISO code in angled brackets (if one exists) and possibly the string “-l-”. Node labels (not shown in the tree above) have the same structure. The glottoTrees function abridge_labels() will shorten labels to just the glottocode, for example:\n\ntree_GA_abr <- abridge_labels(tree_GA)\nplot_glotto(tree_GA_abr)\n\n\n\n\n\n\n\n\nThe function abridge_labels() will issue a warning if there are tip or node labels in which it is unable to identify a glottocode. We will see an example of this shortly below.\nIn glottolog’s trees, the branches are all of equal length. We will discuss how to assign more realistic branch lengths in Section How to add branch lengths."
  },
  {
    "objectID": "phylo.html#combining-trees",
    "href": "phylo.html#combining-trees",
    "title": "Practical phylogenetic methods for linguistic typology",
    "section": "3.2 How to combine trees",
    "text": "3.2 How to combine trees\nAs discussed [2], the comparison of languages across language families unavoidably carries a commitment to a genealogical hypothesis, even if that hypotheses is, tacitly, that all families are equally (un)related. Given that making such hypotheses is unavoidable, it will be most beneficial for progress in the field to make them explicit. To enable typologists to explore genealogical hypotheses and to make those hypotheses explicit, glottoTrees provides tools for combining multiple glottolog trees into one.\nTo begin with a small example, here we combine five glottolog families to represent the hypothesised Arnhem group in northern Australia [10]. First we create a multiPhylo object containing the five glottolog language families (Gunwinyguan, Mangarrayi-Maran, Maningrida, and the isolates Kungarakany and Gaagudju):\n\narnhem_family_names <- \n  c(\"Gunwinyguan\", \"Mangarrayi-Maran\", \"Maningrida\", \"Kungarakany\", \"Gaagudju\")\nmultiPhylo_arnhem <- get_glottolog_trees(arnhem_family_names)\n\nThe glottoTrees function assemble_rake() enables the trees in a multiPhylo object to be assembled into a single tree with a rake structure at its root. Here we apply assemble_rake() to our multiPhylo object and assign the resulting, single tree to the object tree_arnhem.\n\ntree_arnhem <- assemble_rake(multiPhylo_arnhem)\n\nFor plotting, it will be convenient to shorten the labels in the tree to just the glottocodes, using abridge_labels(). However, the root node in our newly created tree tree_arnhem has no label. Accordingly, the function abridge_labels() will issue a warning, that it encountered one node without a glottocode. This is not an error message, and abridge_labels() still shortens all labels to glottocodes where it can; it is just flagging the fact that it was not able to do so in all cases.\n\ntree_arnhem_abr <- abridge_labels(tree_arnhem)\n\nWarning in abridge_labels(tree_arnhem): Labels without glottocodes were detected\nand left unchanged for: 0 tip(s); 1 node(s):\n\n\nPlotting the resulting tree enables us to inspect our newly created Arnhem tree. Note how all five families are joined to the root in a rake-like structure, without any additional subgrouping.\n\nplot_glotto(tree_arnhem_abr, nodelabels = FALSE)\n\n\n\n\n\n\n\n\nIt is possible to give a combined tree more structure, by using assemble_rake() iteratively. For instance, suppose we wished to hypothesise that Gunwinyguan, Mangarrayi-Maran and Maningrida form their own subgroup. First, we create a multiPhylo object containing those three trees, and combine it into a single rake tree, which we call tree_A:\n\nmultiPhylo_A <- get_glottolog_trees(c(\"Gunwinyguan\", \"Mangarrayi-Maran\", \"Maningrida\"))\ntree_A <- assemble_rake(multiPhylo_A)\n\nThen we create the final tree by combining tree_A with the two isolate family trees:\n\nmultiPhylo_arnhem2 <- c(tree_A, get_glottolog_trees(c(\"Kungarakany\", \"Gaagudju\")))\ntree_arnhem2 <- assemble_rake(multiPhylo_arnhem2)\ntree_arnhem2_abr <- abridge_labels(tree_arnhem2)\n\nWarning in abridge_labels(tree_arnhem2): Labels without glottocodes were\ndetected and left unchanged for: 0 tip(s); 2 node(s): ,\n\nplot_glotto(tree_arnhem2_abr, nodelabels = FALSE)\n\n\n\n\n\n\n\n\nTypological studies often examine languages from very many families. To group all 420 families into a single ‘supertree’, glottoTrees provides the function assemble_supertree(). By default, the function returns a supertree that divides first into glottolog’s six macroareas, with an internal node for each, and directly below these macroarea nodes appear all of the glottolog families, grouped by their main_macroarea mentioned in Section Glottolog’s genealogical data above. This tree is enormous, so we do not plot it here. It is obtained like this:\n\nmy_supertree <- assemble_supertree()\n\nThe highest-level, macroarea groupings can also be controlled through the function’s argument macro_groups. For instance, to group all of the world’s families directly into a 420-pronged rake structure, set macro_groups = NULL:\n\nmy_supertree <- assemble_supertree(macro_groups = NULL)\n\nIt is also possible to group macroareas together, for example, to combine North and South America into a single group. Grouping of macroareas is achieved by setting the macro_groups argument to a list whose items are the desired groups of macroareas. Each group will then appear as one of the highest-level nodes of the tree, and all of its families below it. For instance, to keep all of glottolog’s macroareas separate, but to combine North and South America into a single group, the following code would be used. First we define a list, which we’ve called my_list, within which any groupings containing more than one macroarea are represented as a vector, using the c() function:\n\nmy_list <- list(\"Africa\", \"Australia\", \"Eurasia\", \"Papunesia\",\n                c(\"South America\", \"North America\"))\n\nWe then use that list as the macro_groups argument of assemble_supertree():\n\nmy_supertree <- assemble_supertree(macro_groups = my_list)\n\nTaking a second example, to create a supertree containing only the families whose main_macroarea is either Africa or Eurasia, and to place Africa and Eurasia under separate, highest-level nodes, we would use:\n\nmy_list <- list(\"Africa\", \"Eurasia\")\nmy_supertree <- assemble_supertree(macro_groups = my_list)"
  },
  {
    "objectID": "phylo.html#filter-tips",
    "href": "phylo.html#filter-tips",
    "title": "Practical phylogenetic methods for linguistic typology",
    "section": "3.3 How to modify trees",
    "text": "3.3 How to modify trees\nThere are several reasons why typologists may wish to use a tree that departs from the glottolog trees. Most commonly, a typological study will cover a set of languages that differs from the set of tips in any single glottolog tree, either through the exclusion of some of the lects that glottolog represents as tips or through the distinction of additional lects. A third case that can arise is when glottolog places one or more dialects at the tree’s tips and more a general, language node above them. The typologist may have data that applies to the language (an internal node) rather than the dialects (the tips), yet the calculation of genealogically-sensitive averages and proportions requires one’s typological variables to be related to the tips of trees, not to internal nodes. In these cases and many others, the typologist may wish to alter the glottolog tree to suit the purposes of the research. The glottoTrees package supplies a set of functions to aid in performing each of these tree manipulations. In this section we introduce them and illustrate their use.\nIn the following examples, we will make use glottolog’s representation of the Great Andamanese family, whose labels we shorten to just the glottocodes using abridge_labels():\n\ntree_GA <- get_glottolog_trees(\"GreatAndamanese\")\ntree_GA_abr <- abridge_labels(tree_GA)\nplot_glotto(tree_GA_abr)\n\n\n\n\n\n\n\n\n\n3.3.1 How to remove tips\nFirstly, we illustrate the removal of tips from a tree. There are two functions in glottoTrees for doing this. The function remove_tip() works by specifying which tips are to be removed, while the function keep_tip() works by specifying which tips are to be retained. First we will remove three of the original ten tips in the Great Andamanese tree. We do this by setting the label argument of remove_tip() to a vector containing the labels of the tips to be removed. Within the vector, the labels can appear in any order.\n\ntree_GAa <- remove_tip(tree_GA_abr, label = c(\"akab1249\", \"akak1251\", \"apuc1241\"))\nplot_glotto(tree_GAa)\n\n\n\n\n\n\n\n\nIn this next example, we remove the tips akab1249 and akar1243. These tips are the only tips that sit below the internal node sout2683. This is significant, because it triggers a convention in tree manipulation, that if all tips below a node are removed, then the node is removed also. We see that here:\n\ntree_GAb <- remove_tip(tree_GA_abr, label = c(\"akab1249\", \"akar1243\"))\nplot_glotto(tree_GAb)\n\n\n\n\n\n\n\n\nWe now illustrate the usage of the glottoTrees function keep_tip(). Here we use it to retain six of the original ten tips in the Great Andamanese. We do this by setting the label argument to a vector containing the labels of the six desired tips.\n\ntree_GAc <- keep_tip(tree_GA_abr, label = c(\"akar1243\", \"akak1251\", \"akac1240\",\n                                            \"akak1252\", \"apuc1241\", \"okoj1239\"))\nplot_glotto(tree_GAc)\n\n\n\n\n\n\n\n\nAs before, if our use of keep_tip() results in a node having all of the tips below it removed, then the node will also be removed automatically. This is illustrated here, where the node boca1235 is removed automatically because neither of the tips below it are kept:\n\ntree_GAd <- keep_tip(tree_GA_abr, label = c(\"akar1243\", \"akak1251\", \"akak1252\",\n                                            \"apuc1241\", \"okoj1239\"))\nplot_glotto(tree_GAd)\n\n\n\n\n\n\n\n\n\n\n3.3.2 How to remove tips and convert nodes to tips\nAs mentioned earlier, many of the tips in glottolog’s trees correspond to dialects, with languages represented as nodes above the dialectal tips. One usage case we foresee is that a typologist will wish to study a set of language varieties, some of which correspond to glottolog’s tips and some of which correspond to nodes. The glottoTrees function keep_as_tips() takes an argument label which can contain both tip labels and node labels. Any tips will be kept, and any nodes will be converted into tips, with all of the structure below them being removed. Be mindful when using keep_as_tips() that it is not possible to both convert a node into a tip and also retain the structure below it, such as tips that it dominates. Here we keep the same tips as in the tree above, while also converting the node boca1235 into a tip:6\n\ntree_GAe <- keep_as_tip(tree_GA_abr, label = c(\"akar1243\", \"akak1251\", \"akak1252\",\n                                               \"apuc1241\", \"okoj1239\", \"boca1235\"))\nplot_glotto(tree_GAe)\n\n\n\n\n\n\n\n\nOne workflow that we envision for keep_as_tip() is that the typologist has prepared a CSV file, one of whose columns is named tip and contains the glottocodes of all the language varieties in the study. This CSV file can be loaded in R and assigned to a dataframe, and then its tip column can be passed to keep_as_tip() as the value of the labels argument, like this:\n\nmy_dataframe <- read.csv(\"my_data_file.csv\", stringsAsFactors = FALSE)\nmy_new_tree <- keep_as_tip(my_old_tree, label = my_dataframe$tip)\n\nTo just convert one or more nodes into tips, use convert_to_tip(), as we do here to convert the nodes okol1242 and sout2683 to tips:\n\ntree_GAf <- convert_to_tip(tree_GA_abr, label = c(\"okol1242\", \"sout2683\"))\nplot_glotto(tree_GAf)\n\n\n\n\n\n\n\n\n\n\n3.3.3 How to remove internal nodes\nSometimes, the removal of tips will cause one or more of the remaining tips to sit below a node which dominates only it. This reflects that fact that remove_tip(), keep_tip() and keep_as_tip() all preserve the original depth of any tips that remain in the tree (you may like to confirm this by reviewing the plots above). Depending on the researcher’s needs, this outcome may or may not be desirable. If it is undesirable, then non-branching, internal nodes can be removed using the glottoTrees function collapse_node(). For instance, here we remove two of the non-branching nodes from the tree tree_GAc above, by naming them in the label argument of collapse_node(). In the resulting tree, these nodes have been removed, thus reducing the depth of the tips below them:\n\ntree_GAg <- collapse_node(tree_GAc, label = c(\"boca1235\", \"okol1242\"))\nplot_glotto(tree_GAg)\n\n\n\n\n\n\n\n\nWhen deciding whether to collapse nodes, in can be handy to know which nodes in a tree that have only one child below them. The function nonbranching_nodes() will return a vector of all such nodes, for example:\n\nnonbranching_nodes(tree_GAc)\n\n[1] \"okol1242\" \"boca1235\" \"jeru1239\" \"sout2683\"\n\nnonbranching_nodes(tree_GAg)\n\n[1] \"jeru1239\" \"sout2683\"\n\n\nThe function collapse_node() can also be used to alter a subgrouping hypothesis, and specifically, to remove a layer of subgrouping, converting a nested structure ((A,B),C) into a flat structure (A,B,C). For instance, here we remove the okol1242 node of the original glottolog Great Andamanese tree, converting its two daughter languages into sisters of okoj1239:\n\ntree_GAh <- collapse_node(tree_GA_abr, label= \"okol1242\")\nplot_glotto(tree_GAh)\n\n\n\n\n\n\n\n\n\n\n3.3.4 How to add tips\nThe function add_tip() allows tips to be added to a tree. The label argument specifies the name of the new tip, while parent_label specifies the label of the node below which the new tip should appear. Here we add a tip xxxx1234 below the node sout2683:\n\ntree_GAi <- add_tip(tree_GA_abr, label = \"xxxx1234\", parent_label = \"sout2683\")\nplot_glotto(tree_GAi)\n\n\n\n\n\n\n\n\n\n\n3.3.5 How to clone tips\nNext we illustrate the cloning of tips. Cloning tips may be useful when glottolog provides only one glottocode, and thus only one tree tip, corresponding to multiple lects in the typologist’s study. To clone a tip, use the function clone_tip() and in the label argument, provide a vector of the tips to be cloned. Here we clone tips akar1243 and akak1252:\n\ntree_GAj <- clone_tip(tree_GA_abr, label = c(\"akar1243\", \"akak1252\"))\nplot_glotto(tree_GAj)\n\n\n\n\n\n\n\n\nBy default, clones are added to the tree as sisters directly beneath the parent node of the original tip. An alternative is to create a new subgroup for each set of sister clones, using the subgroup argument and setting it to subgroup = TRUE. Each newly created subgroup node is given a label that matches the cloned tips it dominates:\n\ntree_GAk <- clone_tip(tree_GA_abr, label = c(\"akar1243\", \"akak1252\"), subgroup = TRUE)\nplot_glotto(tree_GAk)\n\n\n\n\n\n\n\n\nIt is also possible to make more than one clone using the n argument. Here we create three new clones of akab1248 and place them in a subgroup:\n\ntree_GAl <- clone_tip(tree_GA_abr, label = \"akab1248\", n = 3, subgroup = TRUE)\nplot_glotto(tree_GAl)\n\n\n\n\n\n\n\n\nOne of the consequences of cloning tips is that the in the resulting tree, not all tips will have distinct names. The function apply_duplicate_suffixes() will add a suffix to any tips with duplicate labels, to make them unique.7 The suffix will consist of a hyphen followed by a number. Here we add suffixes to the tree tree_GAj:\n\ntree_GAm <- apply_duplicate_suffixes(tree_GAj)\nplot_glotto(tree_GAm)\n\n\n\n\n\n\n\n\n\n\n3.3.6 How to move a tip\nUsing the function move_tip(), a tip can be moved to a new position, beneath a new parent node (one of the nodes already in the tree) which is specified with the parent_label argument:\n\ntree_GAn <- move_tip(tree_GA_abr, label = \"apuc1241\", parent_label = \"jeru1239\")\nplot_glotto(tree_GAn)\n\n\n\n\n\n\n\n\n\n\n3.3.7 How to move a node and its descendants\nIn a similar fashion, the function move_node() is used to move an internal node, along with all of the structure below it, to a position beneath a new parent node:\n\ntree_GAo <- move_node(tree_GA_abr, label = \"jeru1239\", parent_label = \"okol1242\")\nplot_glotto(tree_GAo)\n\n\n\n\n\n\n\n\n\n\n3.3.8 Summary: a general-purpose toolkit for curating trees’ topology\nThe functions remove_tip(), keep_tip(), keep_as_tip(), convert_to_tip(), collapse_node(), add_tip(), clone_tip(), move_tip() and move_node() provide a general-purpose toolkit for modifying a single glottolog tree, or a combined tree, or supertree, to make its set of tips, and the subgrouping of those tips, conform to the set of lects that a typologist is analysing in a typological study."
  },
  {
    "objectID": "phylo.html#add-lengths",
    "href": "phylo.html#add-lengths",
    "title": "Practical phylogenetic methods for linguistic typology",
    "section": "3.4 How to add branch lengths",
    "text": "3.4 How to add branch lengths\nBranch lengths in a tree convey information, and most phylogenetic comparative methods, including genealogically-sensitive averages and proportions, are sensitive to the information represented by the branch lengths. (To be specific, genealogically-sensitive averages and proportions are sensitive to the relative lengths of the branches, so multiplying all of the branch lengths in a tree by some constant amount would not affect the results.)\nGlottolog’s trees contain informative subgrouping structure, but the branch lengths are all equal. Even without knowing what the true branch lengths are for a linguistic tree, we do know that a situation in which all are equal is highly unlikely. A good approximation to the most-likely8 distribution of branch lengths in a phylogenetic tree, under a variety of assumptions, is exponential [11], i.e., very long branches are rare, and very short ones are frequent. This notion is implemented in the glottoTrees package by the function rescale_branches_exp(), which sets the deepest branches to length 1/2, then next layer to length 1/4, then the next to 1/8 and so on. This will produce a more plausible set of branch lengths, even in the absence of firm knowledge of exact lengths, and on these grounds we advocate its use if additional information about branch lengths is not available.\nHere is an example of the result of applying exponential branch lengths to glottolog’s Great Andamanese tree:\n\ntree_GAp <- rescale_branches_exp(tree_GA_abr)\nplot_glotto(tree_GAp)\n\n\n\n\n\n\n\n\nHere is an example of the result of applying them to glottolog’s Eskimo-Aleut tree, of 30 tips:\n\ntree_EA <- get_glottolog_trees(\"Eskimo-Aleut\")\ntree_EA_abr <- abridge_labels(tree_EA)\ntree_EAa <- rescale_branches_exp(tree_EA_abr)\nplot_glotto(tree_EAa, nodelabels = FALSE)\n\n\n\n\n\n\n\n\nAn additional option is to stretch the terminal branches so that all tips are equidistant from the root, creating what is known as an ultrametric tree. This is done using the function ultrametricize().\n\ntree_EAb <- ultrametricize(tree_EAa)\nplot_glotto(tree_EAb, nodelabels = FALSE)\n\n\n\n\n\n\n\n\nAn additional function, rescale_deepest_branches(), can be used to adjust just the deepest layer of branches. This may be useful where multiple family trees have been joined together, and there is a desire to manipulate the implied closeness or distance between the first-order branches. For example, here we take the hypothesised Arnhem group from Section @ref(combining-trees). First we assign exponential branch lengths with set_branch_lengths_exp(), which sets the deepest branch length to 1/2. Then we triple the distance of the deepest level of relationships by changing the first branch length to 1.5 using rescale_deepest_branches(), before ultrametricising the tree:\n\ntree_arnhem_a <- rescale_branches_exp(tree_arnhem_abr)\ntree_arnhem_b <- rescale_deepest_branches(tree_arnhem_a, 1.5)\ntree_arnhem_c <- ultrametricize(tree_arnhem_b)\nplot_glotto(tree_arnhem_c, nodelabels = FALSE)"
  },
  {
    "objectID": "phylo.html#exporting",
    "href": "phylo.html#exporting",
    "title": "Practical phylogenetic methods for linguistic typology",
    "section": "3.5 Exporting trees for use with other software",
    "text": "3.5 Exporting trees for use with other software\nIn R, trees can be saved to file in Newick format using the function write.tree() in the ape package. Files like this can be opened by other software such as FigTree9, which can be used to interactively generate tree plots that may be useful for publication and dissemination. For instance, here we write the tree tree_arnhem_c to a file whose filename ends in the standard file extension, .tree:\n\nwrite.tree(tree_arnhem_c, \"my_arnhem_tree.tree\")\n\nOften it will be desirable to reproduce a tree with labels that are more reader-friendly than glottocodes. glottoTrees provides the function relabel_with_names(), which will replace full glottolog labels, or labels consisting of just a glottocode, with glottolog’s corresponding language, dialect, subgroup or family name. Here we relabel the Arnhem tree by the languages’ names. As was the case with abridge_labels(), warnings are given by relabel_with_names() if a tree contains any nodes that cannot be relabeled in this way; these are not errors, just alerts.\n\ntree_arnhem_c_namelabels <- relabel_with_names(tree_arnhem_c)\n\nWarning in relabel_with_names(tree_arnhem_c): Labels without glottocodes were\ndetected and left unchanged for: 0 tip(s); 1 node(s):\n\nplot_glotto(tree_arnhem_c_namelabels, nodelabels = FALSE)"
  },
  {
    "objectID": "phylo.html#preparing-a-tree",
    "href": "phylo.html#preparing-a-tree",
    "title": "Practical phylogenetic methods for linguistic typology",
    "section": "4.1 Preparing a tree",
    "text": "4.1 Preparing a tree\nThe tree for Yin’s study was constructed from a glottolog supertree, using glottolog version 4.2. Yin’s supertree made use of glottolog’s macroareas. Since the language sample covered relatively few families in the Americas, a single group was used for South America and North America. Additionally, the only African language available in the sample was Arabic, so Africa and Eurasia were grouped together:\n\nyin_macro <- list(c(\"South America\", \"North America\"), c(\"Africa\", \"Eurasia\"),\n                  \"Papunesia\", \"Australia\")\nsupertree <- assemble_supertree(macro_groups = yin_macro, glottolog_version = \"4.2\")\nsupertree_abr <- abridge_labels(supertree)\n\nWarning in abridge_labels(supertree): Labels without glottocodes were detected\nand left unchanged for: 0 tip(s); 5 node(s): World, SouthAmerica-NorthAmerica,\nAfrica-Eurasia, Papunesia, Australia\n\n\nFive tips were cloned, in cases where Yin had data for two varieties corresponding to just one tip in the glottolog supertree:\n\nsupertree_a <- clone_tip(supertree_abr, subgroup = TRUE,\n                         label = c(\"ayab1239\", \"basu1242\", \"biri1256\",\n                                   \"ikar1243\", \"peri1265\"))\nsupertree_b <- apply_duplicate_suffixes(supertree_a)\n\nEight tips were added, in cases where for sister lects (A,B), glottolog placed A as a node above B. In such cases, in new tip A was placed below the existing glottolog node A:\n\nsupertree_c <- supertree_b\nnodes_to_add_as_tips <- c(\"alor1249\", \"gami1243\", \"guri1247\", \"mand1415\", \n                          \"sins1241\", \"wang1291\", \"warl1254\", \"yand1253\")\n# Loop through these nodes, and use add_tip() to add the new tip:\nfor (node_i in nodes_to_add_as_tips) {\n  supertree_c <- add_tip(supertree_c, label = node_i, parent_label = node_i)\n}\n\nFrom this supertree, the 496 languages in Yin’s dataset were kept. The internal node mada1298 was collapsed, as were all non-branching internal nodes:\n\nsupertree_d <- keep_as_tip(supertree_c, label = yin_2020_data$tip)\nsupertree_e <- collapse_node(supertree_d, label = \"mada1298\")\nsupertree_f <- collapse_node(supertree_e, label = nonbranching_nodes(supertree_e))\n\nFinally, branch lengths were assigned. Branches were first assigned exponential lengths. Then, in order to diminish the importance of the macro groups, the branches above them were shortened to a length of 1/40. The effect of this decision is that the implied distance between families in different macro groups is only marginally greater than between families within a single macro group.\n\nsupertree_g <- rescale_branches_exp(supertree_f) \nyin_2020_tree <- rescale_deepest_branches(supertree_g, 1/40)\n\nThe resulting tree appears as in Figure @ref(fig:yin-tree), which is plotted with the following code:\n\nfull_names <- yin_2020_data$name[match(yin_2020_tree$tip.label, yin_2020_data$tip)]\nname_tree <- yin_2020_tree\nname_tree$tip.label <- full_names\nplot(ladderize(name_tree, right = FALSE), type = \"fan\", \n     cex = 0.3, label.offset = 0.002, edge.width = 0.5)\n\n\n\n\nSupertree of 496 languages used in Yin (2020)."
  },
  {
    "objectID": "phylo.html#preparing-the-dataframe-of-typological-data",
    "href": "phylo.html#preparing-the-dataframe-of-typological-data",
    "title": "Practical phylogenetic methods for linguistic typology",
    "section": "4.2 Preparing the dataframe of typological data",
    "text": "4.2 Preparing the dataframe of typological data\nIn order to calculate phylogenetic weights and genealogically-sensitive proportions, in addition to the tree (or a set of trees) we require a dataframe with (a) one column tip, whose contents match the tip labels in the trees, and (b) other columns containing numerical data to be averaged. The dataframe yin_2020_data has a column tip and two columns of numerical data has_onset_violation and has_coda_violation, and thus it meets the requirements we need. It also contains a column, names, of non-numeric data. Columns of non-numeric data (other than tip) are ignored by phylo_average(), so we do not need to remove them."
  },
  {
    "objectID": "phylo.html#calculating-genealogically-sensitive-proportions",
    "href": "phylo.html#calculating-genealogically-sensitive-proportions",
    "title": "Practical phylogenetic methods for linguistic typology",
    "section": "4.3 Calculating genealogically-sensitive proportions",
    "text": "4.3 Calculating genealogically-sensitive proportions\nThe results are calculated using phylo_average(), setting its phy argument to the tree we have constructed, yin_2020_tree, and its data argument to the dataframe we have prepared, yin_2020_data. A warning is issued alerting us that the dataframe contains a non-numeric column that gets ignored:\n\nyin_2020_results <- phylo_average(phy = yin_2020_tree, data = yin_2020_data)\n\nWarning in phylo_average(phy = yin_2020_tree, data = yin_2020_data): `data`\ncontains non-numeric columns other than `tip`, which have been ignored: name.\n\n\nResults are in the format described in Section @ref(averages). In this case, we are using only one tree, so the results are brief. The genealogically-sensitive proportions according to the ACL and BM methods are the following.\n\nyin_2020_results$ACL_averages\n\n   tree has_onset_violation has_coda_violation\n1 tree1           0.3711102           0.409806\n\nyin_2020_results$BM_averages\n\n   tree has_onset_violation has_coda_violation\n1 tree1           0.4014756          0.3847729\n\n\nThe first ten rows of phylogenetic weights according to the ACL and BM methods are:\n\nhead(yin_2020_results$ACL_weights, n = 10)\n\n               name      tip        tree1\n1            Abkhaz abkh1244 0.0069858330\n2              Abui abui1241 0.0012637162\n3           Achagua acha1250 0.0002456679\n4             Adang adan1251 0.0002197767\n5     Adnyamathanha adny1235 0.0000637717\n6            Adyghe adyg1241 0.0069858330\n7      Hokkaidoainu ainu1240 0.0174645825\n8             Alawa alaw1244 0.0019652742\n9  Standardalbanian alba1267 0.0023699724\n10            Aleut aleu1260 0.0085266857\n\nhead(yin_2020_results$BM_weights, n = 10)\n\n               name      tip        tree1\n1            Abkhaz abkh1244 0.0049475265\n2              Abui abui1241 0.0021424363\n3           Achagua acha1250 0.0009109899\n4             Adang adan1251 0.0009391784\n5     Adnyamathanha adny1235 0.0008929209\n6            Adyghe adyg1241 0.0049475265\n7      Hokkaidoainu ainu1240 0.0064304759\n8             Alawa alaw1244 0.0027423594\n9  Standardalbanian alba1267 0.0040995307\n10            Aleut aleu1260 0.0055527679\n\n\nAs a point of comparison, the raw proportions, which are equal to the means of the columns has_onset_violation and has_coda_violation, are these:\n\nmean(yin_2020_data$has_onset_violation)\n\n[1] 0.3649194\n\nmean(yin_2020_data$has_coda_violation)\n\n[1] 0.3145161"
  },
  {
    "objectID": "postag.html",
    "href": "postag.html",
    "title": "Part-of-Speech Tagging and Dependency Parsing with R",
    "section": "",
    "text": "Introduction\n\n\n\n\n\nThis tutorial introduces part-of-speech tagging and syntactic parsing using R. This tutorial is aimed at beginners and intermediate users of R with the aim of showcasing how to annotate textual data with part-of-speech (pos) tags and how to syntactically parse textual data using R. The aim is not to provide a fully-fledged analysis but rather to show and exemplify selected useful methods associated with pos-tagging and syntactic parsing. Another highly recommendable tutorial on part-of-speech tagging in R with UDPipe is available here and another tutorial on pos-tagging and syntactic parsing by Andreas Niekler and Gregor Wiedemann can be found here [see 1].\n\n\n\nThe entire R Notebook for the tutorial can be downloaded here. If you want to render the R Notebook on your machine, i.e. knitting the document to html or a pdf, you need to make sure that you have R and RStudio installed and you also need to download the bibliography file and store it in the same folder where you store the Rmd file.  Here is a link to an interactive and simplified version of this tutorial on Google Colab. The interactive tutorial is based on a Jupyter notebook of this tutorial. This interactive Jupyter notebook allows you to execute code yourself and - if you copy the Jupyter notebook - you can also change and edit the notebook, e.g. you can change code and upload your own data.\n\n\n\n\n\n\n\n\nPart-Of-Speech Tagging\nMany analyses of language data require that we distinguish different parts of speech. In order to determine the word class of a certain word, we use a procedure which is called part-of-speech tagging (commonly referred to as pos-, pos-, or PoS-tagging). pos-tagging is a common procedure when working with natural language data. Despite being used quite frequently, it is a rather complex issue that requires the application of statistical methods that are quite advanced. In the following, we will explore different options for pos-tagging and syntactic parsing.\nParts-of-speech, or word categories, refer to the grammatical nature or category of a lexical item, e.g. in the sentence Jane likes the girl each lexical item can be classified according to whether it belongs to the group of determiners, verbs, nouns, etc. pos-tagging refers to a (computation) process in which information is added to existing text. This process is also called annotation. Annotation can be very different depending on the task at hand. The most common type of annotation when it comes to language data is part-of-speech tagging where the word class is determined for each word in a text and the word class is then added to the word as a tag. However, there are many different ways to tag or annotate texts.\nPos–tagging assigns part-of-speech tags to character strings (these represent mostly words, of course, but also encompass punctuation marks and other elements). This means that pos–tagging is one specific type of annotation, i.e. adding information to data (either by directly adding information to the data itself or by storing information in e.g. a list which is linked to the data). It is important to note that annotation encompasses various types of information such as pauses, overlap, etc. pos–tagging is just one of these many ways in which corpus data can be enriched. Sentiment Analysis, for instance, also annotates texts or words with respect to its or their emotional value or polarity.\nAnnotation is required in many machine-learning contexts because annotated texts are commonly used as training sets on which machine learning or deep learning models are trained that then predict, for unknown words or texts, what values they would most likely be assigned if the annotation were done manually. Also, it should be mentioned that by many online services offer pos-tagging (e.g. here or here.\nWhen pos–tagged, the example sentence could look like the example below.\n\nJane/NNP likes/VBZ the/DT girl/NN\n\nIn the example above, NNP stands for proper noun (singular), VBZ stands for 3rd person singular present tense verb, DT for determiner, and NN for noun(singular or mass). The pos-tags used by the openNLPpackage are the Penn English Treebank pos-tags. A more elaborate description of the tags can be found here which is summarised below:\n\n\n\n\n\nOverview of Penn English Treebank part-of-speech tags.\n\n\nTagDescriptionExamplesCCCoordinating conjunctionand, or, butCDCardinal numberone, two, threeDTDeterminera, theEXExistential thereThere/EX was a party in progressFWForeign wordpersona/FW non/FW grata/FWINPreposition or subordinating conuh, well, yesJJAdjectivegood, bad, uglyJJRAdjective, comparativebetter, nicerJJSAdjective, superlativebest, nicestLSList item markera., b., 1., 2.MDModalcan, would, willNNNoun, singular or masstree, chairNNSNoun, pluraltrees, chairsNNPProper noun, singularJohn, Paul, CIANNPSProper noun, pluralJohns, Pauls, CIAsPDTPredeterminerall/PDT this marble, many/PDT a soulPOSPossessive endingJohn/NNP 's/POS, the parentss/NNP '/POS distressPRPPersonal pronounI, you, hePRP$Possessive pronounmine, yoursRBAdverbevry, enough, notRBRAdverb, comparativelaterRBSAdverb, superlativelatestRPParticleRPSYMSymbolCO2TOtotoUHInterjectionuhm, uhVBVerb, base formgo, walkVBDVerb, past tensewalked, sawVBGVerb, gerund or present participwalking, seeingVBNVerb, past participlewalked, thoughtVBPVerb, non-3rd person singular prwalk, thinkVBZVerb, 3rd person singular presenwalks, thinksWDTWh-determinerwhich, thatWPWh-pronounwhat, who, whom (wh-pronoun)WP$Possessive wh-pronounwhose, who (wh-words)WRBWh-adverbhow, where, why (wh-adverb)\n\n\nAssigning these pos-tags to words appears to be rather straight forward. However, pos-tagging is quite complex and there are various ways by which a computer can be trained to assign pos-tags. For example, one could use orthographic or morphological information to devise rules such as. . .\n\nIf a word ends in ment, assign the pos-tag NN (for common noun)\nIf a word does not occur at the beginning of a sentence but is capitalized, assign the pos-tag NNP (for proper noun)\n\nUsing such rules has the disadvantage that pos-tags can only be assigned to a relatively small number of words as most words will be ambiguous – think of the similarity of the English plural (-(e)s) and the English 3rd person, present tense indicative morpheme (-(e)s), for instance, which are orthographically identical.Another option would be to use a dictionary in which each word is as-signed a certain pos-tag and a program could assign the pos-tag if the word occurs in a given text. This procedure has the disadvantage that most words belong to more than one word class and pos-tagging would thus have to rely on additional information.The problem of words that belong to more than one word class can partly be remedied by including contextual information such as. .\n\nIf the previous word is a determiner and the following word is a common noun, assign the pos-tag JJ (for a common adjective)\n\nThis procedure works quite well but there are still better options.The best way to pos-tag a text is to create a manually annotated training set which resembles the language variety at hand. Based on the frequency of the association between a given word and the pos-tags it is assigned in the training data, it is possible to tag a word with the pos-tag that is most often assigned to the given word in the training data.All of the above methods can and should be optimized by combining them and additionally including pos–n–grams, i.e. determining a pos-tag of an unknown word based on which sequence of pos-tags is most similar to the sequence at hand and also most common in the training data.This introduction is extremely superficial and only intends to scratch some of the basic procedures that pos-tagging relies on. The interested reader is referred to introductions on machine learning and pos-tagging such as e.g.https://class.coursera.org/nlp/lecture/149.\nThere are several different R packages that assist with pos-tagging texts [see 2]. In this tutorial, we will use the udpipe [3] and the openNLP packages [4]. Each of these has advantages and shortcomings and it is advantageous to try which result best matches one’s needs. That said, the udpipe package is really great as it is easy to use, covers a wide range of languages, is very flexible, and very accurate.\nPreparation and session set up\nThis tutorial is based on R. If you have not installed R or are new to it, you will find an introduction to and more information how to use R here. For this tutorials, we need to install certain packages from an R library so that the scripts shown below are executed without errors. Before turning to the code below, please install the packages by running the code below this paragraph. If you have already installed the packages mentioned below, then you can skip ahead ignore this section. To install the necessary packages, simply run the following code - it may take some time (between 1 and 5 minutes to install all of the libraries so you do not need to worry if it takes some time).\n\n# install packages\ninstall.packages(\"dplyr\") \ninstall.packages(\"stringr\")\ninstall.packages(\"udpipe\")\ninstall.packages(\"flextable\")\n# install klippy for copy-to-clipboard button in code chunks\ninstall.packages(\"remotes\")\nremotes::install_github(\"rlesur/klippy\")\n\nNow that we have installed the packages, we activate them as shown below.\n\n# load packages\nlibrary(dplyr)\nlibrary(stringr) \nlibrary(udpipe) \nlibrary(flextable)\n# activate klippy for copy-to-clipboard button\nklippy::klippy()\n\n\n\n\nOnce you have installed R and RStudio and initiated the session by executing the code shown above, you are good to go.\n\n\nPOS-Tagging with UDPipe\nUDPipe was developed at the Charles University in Prague and the udpipe R package [3] is an extremely interesting and really fantastic package as it provides a very easy and handy way for language-agnostic tokenization, pos-tagging, lemmatization and dependency parsing of raw text in R. It is particularly handy because it addresses and remedies major shortcomings that previous methods for pos-tagging had, namely\n\nit offers a wide range of language models (64 languages at this point)\nit does not rely on external software (like, e.g., TreeTagger, that had to be installed separately and could be challenging when using different operating systems)\nit is really easy to implement as one only need to install and load the udpipe package and download and activate the language model one is interested in\nit allows to train and tune one’s own models rather easily\n\nThe available pre-trained language models in UDPipe are:\n\n\n\n\n\nLanguages and langauge modesl available via udpipe.\n\n\nLanguagesModelsAfrikaansafrikaans-afriboomsAncient Greekancient_greek-perseus, ancient_greek-proielArabicarabic-padtArmenianarmenian-armtdpBasquebasque-bdtBelarusianbelarusian-hsebulgarian-btbbulgarian-btbBuryatburyat-bdtCatalancatalan-ancoraChinesechinese-gsd, chinese-gsdsimp, classical_chinese-kyotoCopticcoptic-scriptoriumCroatiancroatian-setCzechczech-cac, czech-cltt, czech-fictree, czech-pdtDanishdanish-ddtDutchdutch-alpino, dutch-lassysmallEnglishenglish-ewt, english-gum, english-lines, english-partutEstonianestonian-edt, estonian-ewtFinnishfinnish-ftb, finnish-tdtFrenchfrench-gsd, french-partut, french-sequoia, french-spokenGaliciangalician-ctg, galician-treegalGermangerman-gsd, german-hdtGothicgothic-proielGreekgreek-gdtHebrewhebrew-htbHindihindi-hdtbHungarianhungarian-szegedIndonesianindonesian-gsdIrish Gaelicirish-idtItalianitalian-isdt, italian-partut, italian-postwita, italian-twittiro, italian-vitJapanesejapanese-gsdKazakhkazakh-ktbKoreankorean-gsd, korean-kaistKurmanjikurmanji-mgLatinlatin-ittb, latin-perseus, latin-proielLatvianlatvian-lvtbLithuanianlithuanian-alksnis, lithuanian-hseMaltesemaltese-mudtMarathimarathi-ufalNorth Saminorth_sami-giellaNorwegiannorwegian-bokmaal, norwegian-nynorsk, norwegian-nynorskliaOld Church Slavonicold_church_slavonic-proielOld Frenchold_french-srcmfOld Russianold_russian-torotPersianpersian-serajiPolishpolish-lfg, polish-pdb, polish-szPortugeseportuguese-bosque, portuguese-br, portuguese-gsdRomanianromanian-nonstandard, romanian-rrtRussianrussian-gsd, russian-syntagrus, russian-taigaSanskritsanskrit-ufalScottish Gaelicscottish_gaelic-arcosgSerbianserbian-setSlovakslovak-snkSlovenianslovenian-ssj, slovenian-sstSpanishspanish-ancora, spanish-gsdSwedishswedish-lines, swedish-talbankenTamiltamil-ttbTelugutelugu-mtgTurkishturkish-imstUkrainianukrainian-iuUpper Sorbiaupper_sorbian-ufalUrduurdu-udtbUyghuruyghur-udtVietnamesevietnamese-vtbWolofwolof-wtb\n\n\n\n\n\nThe udpipe R package also allows you to easily train your own models, based on data in CONLL-U format, so that you can use these for your own commercial or non-commercial purposes. This is described in the other vignette of this package which you can view by the command  vignette(\"udpipe-train\", package = \"udpipe\")\n\n\n\n\n\n\nTo download any of these models, we can use the udpipe_download_model function. For example, to download the english-ewt model, we would use the call: m_eng   <- udpipe::udpipe_download_model(language = \"english-ewt\").\nWe start by loading a text\n\n# load text\ntext <- readLines(\"https://slcladal.github.io/data/testcorpus/linguistics06.txt\", skipNul = T) %>%\n str_squish() %>%\n  .[1]\n# inspect\ntext\n\n[1] \"Linguistics also deals with the social, cultural, historical and political factors that influence language, through which linguistic and language-based context is often determined. Research on language through the sub-branches of historical and evolutionary linguistics also focus on how languages change and grow, particularly over an extended period of time.\"\n\n\nNow that we have a text that we can work with, we will download a pre-trained language model.\n\n# download language model\nm_eng   <- udpipe::udpipe_download_model(language = \"english-ewt\")\n\nIf you have downloaded a model once, you can also load the model directly from the place where you stored it on your computer. In my case, I have stored the model in a folder called udpipemodels\n\n# load language model from your computer after you have downloaded it once\nm_eng <- udpipe_load_model(file = here::here(\"udpipemodels\", \"english-ewt-ud-2.5-191206.udpipe\"))\n\nWe can now use the model to annotate out text.\n\n# tokenise, tag, dependency parsing\ntext_anndf <- udpipe::udpipe_annotate(m_eng, x = text) %>%\n  as.data.frame() %>%\n  dplyr::select(-sentence)\n# inspect\nhead(text_anndf, 10)\n\n   doc_id paragraph_id sentence_id token_id       token      lemma  upos xpos\n1    doc1            1           1        1 Linguistics Linguistic  NOUN  NNS\n2    doc1            1           1        2        also       also   ADV   RB\n3    doc1            1           1        3       deals       deal  NOUN  NNS\n4    doc1            1           1        4        with       with   ADP   IN\n5    doc1            1           1        5         the        the   DET   DT\n6    doc1            1           1        6      social     social   ADJ   JJ\n7    doc1            1           1        7           ,          , PUNCT    ,\n8    doc1            1           1        8    cultural   cultural   ADJ   JJ\n9    doc1            1           1        9           ,          , PUNCT    ,\n10   doc1            1           1       10  historical historical   ADJ   JJ\n                       feats head_token_id  dep_rel deps          misc\n1                Number=Plur             3 compound <NA>          <NA>\n2                       <NA>             3   advmod <NA>          <NA>\n3                Number=Plur             0     root <NA>          <NA>\n4                       <NA>            13     case <NA>          <NA>\n5  Definite=Def|PronType=Art            13      det <NA>          <NA>\n6                 Degree=Pos            13     amod <NA> SpaceAfter=No\n7                       <NA>             8    punct <NA>          <NA>\n8                 Degree=Pos             6     conj <NA> SpaceAfter=No\n9                       <NA>            10    punct <NA>          <NA>\n10                Degree=Pos             6     conj <NA>          <NA>\n\n\nIt can be useful to extract only the words and their pos-tags and convert them back into a text format (rather than a tabular format).\n\ntagged_text <- paste(text_anndf$token, \"/\", text_anndf$xpos, collapse = \" \", sep = \"\")\n# inspect tagged text\ntagged_text\n\n[1] \"Linguistics/NNS also/RB deals/NNS with/IN the/DT social/JJ ,/, cultural/JJ ,/, historical/JJ and/CC political/JJ factors/NNS that/WDT influence/VBP language/NN ,/, through/IN which/WDT linguistic/NN and/CC language/NN -/HYPH based/VBN context/NN is/VBZ often/RB determined/JJ ./. Research/VB on/IN language/NN through/IN the/DT sub-branches/NNS of/IN historical/JJ and/CC evolutionary/JJ linguistics/NNS also/RB focus/RB on/IN how/WRB languages/NNS change/VBP and/CC grow/VBP ,/, particularly/RB over/IN an/DT extended/JJ period/NN of/IN time/NN ./.\"\n\n\n\n\nPOS-Tagging non-English texts\nWe can apply the same method for annotating, e.g. adding pos-tags, to other languages. For this, we could train our own model, or, we can use one of the many pre-trained language models that udpipe provides.\nLet us explore how to do this by using example texts from different languages, here from German and Spanish (but we could also annotate texts from any of the wide variety of languages for which UDPipe provides pre-trained models.\nWe begin by loading a German and a Dutch text.\n\n# load texts\ngertext <- readLines(\"https://slcladal.github.io/data/german.txt\") \nduttext <- readLines(\"https://slcladal.github.io/data/dutch.txt\") \n# inspect texts\ngertext; duttext\n\n[1] \"Sprachwissenschaft untersucht in verschiedenen Herangehensweisen die menschliche Sprache.\"\n\n\n[1] \"Taalkunde, ook wel taalwetenschap of linguïstiek, is de wetenschappelijke studie van de natuurlijke talen.\"\n\n\nNext, we install the pre-trained language models.\n\n# download language model\nm_ger   <- udpipe::udpipe_download_model(language = \"german-gsd\")\nm_dut   <- udpipe::udpipe_download_model(language = \"dutch-alpino\")\n\nOr we load them from our machine (if we have downloaded and saved them before).\n\n# load language model from your computer after you have downloaded it once\nm_ger   <- udpipe::udpipe_load_model(file = here::here(\"udpipemodels\", \"german-gsd-ud-2.5-191206.udpipe\"))\nm_dut   <- udpipe::udpipe_load_model(file = here::here(\"udpipemodels\", \"dutch-alpino-ud-2.5-191206.udpipe\"))\n\nNow, pos-tag the German text.\n\n# tokenise, tag, dependency parsing of german text\nger_pos <- udpipe::udpipe_annotate(m_ger, x = gertext) %>%\n  as.data.frame() %>%\n  dplyr::summarise(postxt = paste(token, \"/\", xpos, collapse = \" \", sep = \"\")) %>%\n  dplyr::pull(unique(postxt))\n# inspect\nger_pos\n\n[1] \"Sprachwissenschaft/NN untersucht/VVFIN in/APPR verschiedenen/ADJA Herangehensweisen/NN die/ART menschliche/NN Sprache/NN ./$.\"\n\n\nAnd finally, we also pos-tag the Dutch text.\n\n# tokenise, tag, dependency parsing of german text\nnl_pos <- udpipe::udpipe_annotate(m_dut, x = duttext) %>%\n   as.data.frame() %>%\n  dplyr::summarise(postxt = paste(token, \"/\", xpos, collapse = \" \", sep = \"\")) %>%\n  dplyr::pull(unique(postxt))\n# inspect\nnl_pos\n\n[1] \"Taalkunde/N|soort|ev|basis|zijd|stan ,/LET ook/BW wel/BW taalwetenschap/N|soort|ev|basis|zijd|stan of/VG|neven linguïstiek/N|soort|ev|basis|zijd|stan ,/LET is/WW|pv|tgw|ev de/LID|bep|stan|rest wetenschappelijke/ADJ|prenom|basis|met-e|stan studie/N|soort|ev|basis|zijd|stan van/VZ|init de/LID|bep|stan|rest natuurlijke/ADJ|prenom|basis|met-e|stan talen/N|soort|mv|basis ./LET\"\n\n\n\n\nDependency Parsing Using UDPipe\nIn addition to pos-tagging, we can also generate plots showing the syntactic dependencies of the different constituents of a sentence. For this, we generate an object that contains a sentence (in this case, the sentence Linguistics is the scientific study of language), and we then plot (or visualize) the dependencies using the textplot_dependencyparser function.\n\n# parse text\nsent <- udpipe::udpipe_annotate(m_eng, x = \"Linguistics is the scientific study of language\") %>%\n  as.data.frame()\n# inspect\nhead(sent)\n\n  doc_id paragraph_id sentence_id\n1   doc1            1           1\n2   doc1            1           1\n3   doc1            1           1\n4   doc1            1           1\n5   doc1            1           1\n6   doc1            1           1\n                                         sentence token_id       token\n1 Linguistics is the scientific study of language        1 Linguistics\n2 Linguistics is the scientific study of language        2          is\n3 Linguistics is the scientific study of language        3         the\n4 Linguistics is the scientific study of language        4  scientific\n5 Linguistics is the scientific study of language        5       study\n6 Linguistics is the scientific study of language        6          of\n       lemma upos xpos                                                 feats\n1 Linguistic NOUN  NNS                                           Number=Plur\n2         be  AUX  VBZ Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin\n3        the  DET   DT                             Definite=Def|PronType=Art\n4 scientific  ADJ   JJ                                            Degree=Pos\n5      study NOUN   NN                                           Number=Sing\n6         of  ADP   IN                                                  <NA>\n  head_token_id dep_rel deps misc\n1             5   nsubj <NA> <NA>\n2             5     cop <NA> <NA>\n3             5     det <NA> <NA>\n4             5    amod <NA> <NA>\n5             0    root <NA> <NA>\n6             7    case <NA> <NA>\n\n\nWe now generate the plot.\n\n# generate dependency plot\ndplot <- textplot::textplot_dependencyparser(sent, size = 3) \n# show plot\ndplot\n\n\n\n\nThat’s it for this tutorial. We hope that you have enjoyed this tutorial and learned how to annotate texts using language models and perform pos-tagging and dependency parsing of English texts as well as texts in other languages.\n\n\nCitation & Session Info\nSchweinberger, Martin. 2022. Part-of-Speech Tagging and Dependency Parsing with R. Brisbane: The University of Queensland. url: https://slcladal.github.io/postag.html (Version 2022.08.31).\n@manual{schweinberger2022postag,\n  author = {Schweinberger, Martin},\n  title = {Part-of-Speech Tagging and Dependency Parsing with R},\n  note = {https://slcladal.github.io/postag.html},\n  year = {2022},\n  organization = \"The University of Queensland, School of Languages and Cultures},\n  address = {Brisbane},\n  edition = {2022.08.31}\n}\n\nsessionInfo()\n\nR version 4.2.1 (2022-06-23)\nPlatform: x86_64-pc-linux-gnu (64-bit)\nRunning under: Ubuntu 22.04.1 LTS\n\nMatrix products: default\nBLAS:   /usr/lib/x86_64-linux-gnu/blas/libblas.so.3.10.0\nLAPACK: /usr/lib/x86_64-linux-gnu/lapack/liblapack.so.3.10.0\n\nlocale:\n [1] LC_CTYPE=en_AU.UTF-8       LC_NUMERIC=C              \n [3] LC_TIME=en_AU.UTF-8        LC_COLLATE=en_AU.UTF-8    \n [5] LC_MONETARY=en_AU.UTF-8    LC_MESSAGES=en_AU.UTF-8   \n [7] LC_PAPER=en_AU.UTF-8       LC_NAME=C                 \n [9] LC_ADDRESS=C               LC_TELEPHONE=C            \n[11] LC_MEASUREMENT=en_AU.UTF-8 LC_IDENTIFICATION=C       \n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] udpipe_0.8.9    stringr_1.4.0   dplyr_1.0.9     flextable_0.7.3\n\nloaded via a namespace (and not attached):\n [1] ggrepel_0.9.1      Rcpp_1.0.8.3       here_1.0.1         lattice_0.20-45   \n [5] tidyr_1.2.0        assertthat_0.2.1   rprojroot_2.0.3    digest_0.6.29     \n [9] utf8_1.2.2         ggforce_0.3.3      R6_2.5.1           evaluate_0.15     \n[13] ggplot2_3.3.6      pillar_1.7.0       gdtools_0.2.4      rlang_1.0.4       \n[17] uuid_1.1-0         rstudioapi_0.13    data.table_1.14.2  Matrix_1.4-1      \n[21] textplot_0.2.2     klippy_0.0.0.9500  rmarkdown_2.14     labeling_0.4.2    \n[25] htmlwidgets_1.5.4  igraph_1.3.2       polyclip_1.10-0    munsell_0.5.0     \n[29] compiler_4.2.1     xfun_0.31          pkgconfig_2.0.3    systemfonts_1.0.4 \n[33] base64enc_0.1-3    htmltools_0.5.2    tidyselect_1.1.2   gridExtra_2.3     \n[37] tibble_3.1.7       graphlayouts_0.8.0 viridisLite_0.4.0  fansi_1.0.3       \n[41] crayon_1.5.1       MASS_7.3-58.1      grid_4.2.1         jsonlite_1.8.0    \n[45] gtable_0.3.0       lifecycle_1.0.1    DBI_1.1.3          magrittr_2.0.3    \n[49] scales_1.2.0       zip_2.2.0          cli_3.3.0          stringi_1.7.8     \n[53] farver_2.1.1       viridis_0.6.2      xml2_1.3.3         ellipsis_0.3.2    \n[57] generics_0.1.3     vctrs_0.4.1        tools_4.2.1        glue_1.6.2        \n[61] officer_0.4.3      tweenr_1.0.2       purrr_0.3.4        ggraph_2.0.5      \n[65] fastmap_1.1.0      yaml_2.3.5         colorspace_2.0-3   tidygraph_1.2.1   \n[69] knitr_1.39        \n\n\n\nBack to top\nBack to HOME\n\n\n\n\n\n\n\n\n\nReferences\n\n1. Wiedemann, G., Niekler, A.: Hands-on: A five day text mining course for humanists and social scientists in R. In: Proceedings of the workshop on teaching NLP for digital humanities (Teach4DH), berlin, germany, september 12, 2017. pp. 57–65 (2017).\n\n\n2. Kumar, A., Paul, A.: Mastering text mining with r. Packt Publishing Ltd (2016).\n\n\n3. Wijffels, J.: Udpipe: Tokenization, parts of speech tagging, lemmatization and dependency parsing with the ’UDPipe’ ’NLP’ toolkit. (2021).\n\n\n4. Hornik, K.: openNLP: Apache OpenNLP tools interface. (2019)."
  },
  {
    "objectID": "pwr.html#what-determines-if-you-find-an-effect",
    "href": "pwr.html#what-determines-if-you-find-an-effect",
    "title": "Power Analysis in R",
    "section": "What determines if you find an effect?",
    "text": "What determines if you find an effect?\nTo explore this issue, let us have a look at some distributions of samples with varying features sampled from either one or two distributions.\nLet us start with the distribution of two samples (N = 30) sampled from the same population.\n\n\n\n\n\n\n\n\nThe means of the samples are very similar and a t-test confirms that we can assume that the samples are drawn from the same population (as the p-value is greater than .05).\nLet us now draw another two samples (N = 30) but from different populations where the effect of group is weak (the population difference is small).\n\n\n\n\n\nLet us briefly check if the effect size, Cohen’s d of 0.2, is correct (for this, we increase the sample size dramatically to get very accurate estimates). If the above effect size is correct (Cohen’s d = 0.2), then the reported effect size should be 0.2 (or -0.2). This is so because Cohen’s d represents distances between group means measured in standard deviations (in our example, the standard deviation is 10 and the difference between the means is 2, i.e., 20 percent of a standard deviation; which is equal to a Cohen’s d value of 0.2). Let’s check the effect size now (we will only do this for this distribution but you can easily check for yourself that the effect sizes provided in the plot headers below are correct by adapting the code chunk below and using the numbers provided in the plot headers).\n\n# generate two vectors with numbers using the means and sd from above \n# means = 101, 99, sd = 10\nnum1 <- rnorm(1000000, mean = 101, sd = 10)\nnum2 <- rnorm(1000000, mean = 99, sd = 10)\n# perform t-test\ntt <- t.test(num1, num2, alternative = \"two.sided\")\n# extract effect size\neffectsize::effectsize(tt)\n\nCohen's d | 95% CI\n------------------\n0.20      |       \n\n- Estimated using un-pooled SD.\n\n\nBased on the t-test results in the distribution plot, we assume that the data represents two samples from the same population (which is false) because the p-value is higher than .05. This means that the sample size is insufficient or does not enough power to show that they are actually from two different populations given the variability in the data nd the size of the effect (which is weak).\nWhat happens if we increase the effect size to medium? This means that we again draw two samples from two different populations but the difference between the populations is a bit larger (group has a medium effect, Cohen’s d = .5)\n\n\n\n\n\nNow, lets have a look at at the distribution of two different groups (group has a strong effect, Cohen’s d = .8)\n\n\n\n\n\nNow, lets have a look at at the distribution of two different groups (group has a very strong effect, Cohen’s d = 1.2)\n\n\n\n\n\nIf variability and sample size remain constant, larger effects are easier to detect than smaller effects!"
  },
  {
    "objectID": "pwr.html#sample-size",
    "href": "pwr.html#sample-size",
    "title": "Power Analysis in R",
    "section": "Sample size",
    "text": "Sample size\nAnd let’s now look at sample size.\n\n\n\n\n\nLet us now increase the sample size to N = 50.\n\n\n\n\n\nIf variability and effect size remain constant, effects are easier to detect with increasing sample size!"
  },
  {
    "objectID": "pwr.html#variability",
    "href": "pwr.html#variability",
    "title": "Power Analysis in R",
    "section": "Variability",
    "text": "Variability\nAnd let’s now look at variability.\n\n\n\n\n\nLet’s decrease the variability to sd = 5.\n\n\n\n\n\nIf the sample and effect size remain constant, effects are easier to detect with decreasing variability!\nIn summary, there are three main factors that determine if a model finds an effect. The accuracy (i.e., the probability of finding an effect):\n\nthe size of the effect (bigger effects are easier to detect)\nthe variability of the effect (less variability makes it easier to detect an effect), and\nthe sample size (the bigger the sample size, the easier it is to detect an effect);\n\nnumber of subjects/participants\nnumber of items/questions\nnumber of observations per item within subjects/participants\n\n\nNow, if a) we dealing with a very big effect, then we need only few participants and few items to accurately find this effect.\nOr b) if we dealing with an effect that has low variability (it is observable for all subjects with the same strength), then we need only few participants and few items to accurately find this effect.\nBefore we conduct a study, we should figure out, what sample we need to detect a small/medium effect with medium variability so that our model is sufficient to detect this kind of effect. In order to do this, we would generate a data set that mirrors the kind of data that we expect to get (with the properties that we expect to get). We can then fit a model to this data and check if a model would be able to detect the expected effect. However, because a single model does not tell us that much (it could simply be luck that it happened to find the effect), we run many different models on variations of the data and see how many of them find the effect. As a general rule of thumb, we want a data set that allows a model to find a medium sized effect with at least an accuracy of 80 percent [7].\nIn the following, we will go through how to determine what sample size we need for an example analysis."
  },
  {
    "objectID": "pwr.html#preparation-and-session-set-up",
    "href": "pwr.html#preparation-and-session-set-up",
    "title": "Power Analysis in R",
    "section": "Preparation and session set up",
    "text": "Preparation and session set up\nThis tutorial is based on R. If you have not installed R or are new to it, you will find an introduction to and more information how to use R here. For this tutorials, we need to install certain packages into the R library on your computer so that the scripts shown below are executed without errors. Before turning to the code below, please install the packages by running the code below this paragraph. If you have already installed the packages mentioned below, then you can skip ahead and ignore this section. To install the necessary packages, simply run the following code - it may take some time (between 1 and 5 minutes to install all of the libraries so you do not need to worry if it takes some time).\n\n# install libraries\ninstall.packages(c(\"tidyverse\", \"pwr\", \"lme4\", \"sjPlot\", \"simr\", \"effectsize\"))\ninstall.packages(c(\"DT\", \"knitr\", \"flextable\"))\ninstall.packages(\"DescTools\")\ninstall.packages(\"pacman\")\n# install klippy for copy-to-clipboard button in code chunks\ninstall.packages(\"remotes\")\nremotes::install_github(\"rlesur/klippy\")\n\nNow that we have installed the packages, we can activate them as shown below.\n\n# set options\noptions(stringsAsFactors = F)         # no automatic conversion of factors\noptions(\"scipen\" = 100, \"digits\" = 4) # suppress math annotation\noptions(max.print=1000)               # print max 1000 results\n# load packages\nlibrary(tidyverse)\nlibrary(pwr)\nlibrary(lme4)\nlibrary(sjPlot)\nlibrary(simr)\nlibrary(effectsize)\nlibrary(DT)\nlibrary(knitr)\nlibrary(flextable)\npacman::p_load_gh(\"trinker/entity\")\n# activate klippy for copy-to-clipboard button\nklippy::klippy()\n\n\n\n\nOnce you have installed R and RStudio and initiated the session by executing the code shown above, you are good to go."
  },
  {
    "objectID": "pwr.html#one-way-anova",
    "href": "pwr.html#one-way-anova",
    "title": "Power Analysis in R",
    "section": "One-way ANOVA",
    "text": "One-way ANOVA\nLet check how to calculate the necessary sample size for each group for a one-way ANOVA that compares 5 groups (k) and that has a power of 0.80 (80 percent), when the effect size is moderate (f = 0.25) and the significance level is 0.05 (5 percent)..\n\n# load package\nlibrary(pwr)\n# calculate minimal sample size\npwr.anova.test(k=5,            # 5 groups are compared\n               f=.25,          # moderate effect size\n               sig.level=.05,  # alpha/sig. level = .05\n               power=.8)       # confint./power = .8\n\n\n     Balanced one-way analysis of variance power calculation \n\n              k = 5\n              n = 39.15\n              f = 0.25\n      sig.level = 0.05\n          power = 0.8\n\nNOTE: n is number in each group\n\n\nIn this case, the minimum number of participants in each group would be 40.\nLet’s check how we could calculate the power if we had already collected data (with 30 participants in each group) and we want to report the power of our analysis (and let us assume that the effect size was medium).\n\n# calculate minimal sample size\npwr.anova.test(k=5,            # 5 groups are compared\n               f=.25,          # moderate effect size\n               sig.level=.05,  # alpha/sig. level = .05\n               n=30)           # n of participants\n\n\n     Balanced one-way analysis of variance power calculation \n\n              k = 5\n              n = 30\n              f = 0.25\n      sig.level = 0.05\n          power = 0.6676\n\nNOTE: n is number in each group\n\n\nIn this case our analysis would only have a power or 66.8 percent. This means that we would only detect a medium sized effect in 66.7 percent of cases (which is considered insufficient)."
  },
  {
    "objectID": "pwr.html#power-analysis-for-glms",
    "href": "pwr.html#power-analysis-for-glms",
    "title": "Power Analysis in R",
    "section": "Power Analysis for GLMs",
    "text": "Power Analysis for GLMs\nWhen determining the power of a generalized linear model, we need to provide\n\nthe degrees of freedom for numerator (´u´)\nthe degrees of freedom for denominator (v)\nthe effect size (the estimate of the intercept and the slope/estimates of the predictors)\nthe level of significance (i.e., the type I error probability)\n\nThe values of the parameters in the example below are adapted from the fixed-effects regression example that was used to analyze different teaching styles (see here).\nThe effect size used here is \\(f^2^\\) that has be categorized as follows [see 8]: small \\(≥\\) 0.02, medium \\(≥\\) 0.15, and large \\(≥\\) 0.35. So in order to determine if the data is sufficient to find a weak effect when comparing 2 groups with 30 participants in both groups (df_numerator_: 2-1; df_denominator_: (30-1) + (30-1)) and a significance level at \\(\\alpha\\) = .05, we can use the following code.\n\n# general linear model\npwrglm <- pwr.f2.test(u = 1,\n                      v = 58,\n                      f2 = .02,\n                      sig.level = 0.05)\n# inspect results\npwrglm\n\n\n     Multiple regression power calculation \n\n              u = 1\n              v = 58\n             f2 = 0.02\n      sig.level = 0.05\n          power = 0.1899\n\n\nThe results show that the regression analyses used to evaluate the effectiveness of different teaching styles only had a power of 0.1899."
  },
  {
    "objectID": "pwr.html#power-analysis-for-t-tests",
    "href": "pwr.html#power-analysis-for-t-tests",
    "title": "Power Analysis in R",
    "section": "Power Analysis for t-tests",
    "text": "Power Analysis for t-tests\nFor t-tests (both paired and 2-sample t-tests), the effect size measure is Cohen’s \\(d\\) that has be categorized as follows [see 8]: small \\(≥\\) 0.2, medium \\(≥\\) 0.5, and large \\(≥\\) 0.8.\nPaired t-test\nSo in order to determine if the data is sufficient to find a weak effect when comparing the pre- and post-test results of a group with 30 participants, evaluating an undirected hypothesis (thus the two-tailed approach), and a significance level at \\(\\alpha\\) = .05, we can use the following code.\n\n# paired t-test\npwrpt <- pwr.t.test(d=0.2,\n                    n=30,\n                    sig.level=0.05,\n                    type=\"paired\",\n                    alternative=\"two.sided\")\n# inspect\npwrpt\n\n\n     Paired t test power calculation \n\n              n = 30\n              d = 0.2\n      sig.level = 0.05\n          power = 0.1852\n    alternative = two.sided\n\nNOTE: n is number of *pairs*\n\n\nGiven the data, a weak effect in this design can only be detected with a certainty of 0.1852 percent. This means that we would need to substantively increase the sample size to detect a small effect with this design.\nTwo-sample (independent) t-test\nThe power in a similar scenario but with two different groups (with 25 and 35 subjects) can be determined as follows (in this case we test a directed hypothesis that checks if the intervention leads to an increase in the outcome - hence the greater in the alternative argument):\n\n# independent t-test\npwr2t <- pwr.t2n.test(d=0.2,\n                      n1=35,\n                      n2 = 25,\n                      sig.level=0.05,\n                      alternative=\"greater\")\n# inspect\npwr2t\n\n\n     t test power calculation \n\n             n1 = 35\n             n2 = 25\n              d = 0.2\n      sig.level = 0.05\n          power = 0.1867\n    alternative = greater\n\n\nGiven the data, a weak effect in this design can only be detected with a certainty of 0.1867 percent. This means that we would need to substantively increase the sample size to detect a small effect with this design."
  },
  {
    "objectID": "pwr.html#power-analysis-for-chi2-tests",
    "href": "pwr.html#power-analysis-for-chi2-tests",
    "title": "Power Analysis in R",
    "section": "Power Analysis for \\(\\chi\\)2-tests",
    "text": "Power Analysis for \\(\\chi\\)2-tests\nLet us now check the power of a \\(\\chi^2\\)^-test. For \\(\\chi^2\\)^-test, the effect size measure used in the power analysis is \\(w\\) that has be categorized as follows [see 8]: small \\(≥\\) 0.1, medium \\(≥\\) 0.3, and large \\(≥\\) 0.5. Also, keep in mind that for \\(\\chi^2\\)^-tests, at least 80 percent of cells need to have values \\(≥\\) 5 and none of the cells should have expected values smaller than 1 [see 9].\n\n# x2-test\npwrx2 <- pwr.chisq.test(w=0.2,\n                        N = 25, # total number of observations\n                        df = 1,\n                        sig.level=0.05)\n# inspect\npwrx2\n\n\n     Chi squared power calculation \n\n              w = 0.2\n              N = 25\n             df = 1\n      sig.level = 0.05\n          power = 0.1701\n\nNOTE: N is the number of observations\n\n\nGiven the data, a weak effect in this design can only be detected with a certainty of 0.1701 percent. This means that we would need to substantively increase the sample size to detect a small effect with this design.\n\n\n\n\nEXERCISE TIME!\n\n\n\n\n\n\n`\n\nFor the tests above (anova, glm, paired and independent t-test, and the \\(\\chi^2\\)-test), how many participants would you need to have a power of 80 percent?\n\nHere are the commands we used to help you:\n\nANOVA: pwr.anova.test(k=5, f=.25, sig.level=.05, n=30)\nGLM: pwr.f2.test(u = 1, v = 58, f2 = .02, sig.level = 0.05)\npaired t-test: pwr.t.test(d=0.2, n=30, sig.level=0.05, type=\"paired\", alternative=\"two.sided\")\nindependent t-test: pwr.t2n.test(d=0.2,n1=35, n2 = 25, sig.level=0.05, alternative=\"greater\")\n\\(\\chi^2\\)-test: pwr.chisq.test(w=0.2, N = 25,  df = 1, sig.level=0.05) \n\n\nAnswer\n\n\npwr.anova.test(k=5, f=.25, sig.level=.05, p=.8)\n\n\n     Balanced one-way analysis of variance power calculation \n\n              k = 5\n              n = 39.15\n              f = 0.25\n      sig.level = 0.05\n          power = 0.8\n\nNOTE: n is number in each group\n\npwr.f2.test(u = 1, f2 = .02, sig.level = 0.05, p = .8)\n\n\n     Multiple regression power calculation \n\n              u = 1\n              v = 392.4\n             f2 = 0.02\n      sig.level = 0.05\n          power = 0.8\n\npwr.t.test(d=0.2, p = .8, sig.level=0.05, type=\"paired\", alternative=\"two.sided\")\n\n\n     Paired t test power calculation \n\n              n = 198.2\n              d = 0.2\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number of *pairs*\n\npwr.t2n.test(d=0.2, n1=310, n2 = 310, sig.level=0.05, alternative=\"greater\") # by checking N values\n\n\n     t test power calculation \n\n             n1 = 310\n             n2 = 310\n              d = 0.2\n      sig.level = 0.05\n          power = 0.8002\n    alternative = greater\n\npwr.chisq.test(w=0.2, p = .8,  df = 1, sig.level=0.05)\n\n\n     Chi squared power calculation \n\n              w = 0.2\n              N = 196.2\n             df = 1\n      sig.level = 0.05\n          power = 0.8\n\nNOTE: N is the number of observations\n\n\n\n\n\n`"
  },
  {
    "objectID": "pwr.html#excursion-language-is-never-ever-random",
    "href": "pwr.html#excursion-language-is-never-ever-random",
    "title": "Power Analysis in R",
    "section": "Excursion: Language is never ever random",
    "text": "Excursion: Language is never ever random\nIn 2005, Adam [10] made a point that Language is never, ever, ever, random. Here is part of the abstract:\n\nLanguage users never choose words randomly, and language is essentially non-random. Statistical hypothesis testing uses a null hypothesis, which posits randomness. Hence, when we look at linguistic phenomena in corpora, the null hypothesis will never be true. Moreover, where there is enough data, we shall (almost) always be able to establish that it is not true. In corpus studies, we frequently do have enough data, so the fact that a relation between two phenomena is demonstrably non-random, does not support the inference that it is not arbitrary. We present experimental evidence of how arbitrary associations between word frequencies and corpora are systematically non-random.\n\nThis is a problem because if we are using ever bigger corpora, even the tiniest of difference will become significant. have a look at the following example.\n\n# first let us generate some data\nfreqs1 <- matrix(c(10, 28, 30, 92), byrow = T, ncol = 2)\n# inspect data\nfreqs1\n\n     [,1] [,2]\n[1,]   10   28\n[2,]   30   92\n\n\nNow, we perform a simple \\(\\chi^2\\)-test and extract the effect size.\n\n# x2-test\nx21 <- chisq.test(freqs1)\n# inspect results\nx21\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  freqs1\nX-squared = 0, df = 1, p-value = 1\n\n# effect size\neffectsize::effectsize(x21)\n\nCramer's V |       95% CI\n-------------------------\n0.02       | [0.00, 1.00]\n\n- One-sided CIs: upper bound fixed at [1.00].\n\n\nthe output shows that the difference is not significant and that the effect size is extremely(!) small.\nNow, let us simply increase the sample size by a factor of 1000 and also perform a \\(\\chi^2\\)-test on this extended data set and extract the effect size.\n\n# \nfreqs2 <- matrix(c(10000, 28000, 30000, 92000), byrow = T, ncol = 2)\n# first let us generate some data\nx22 <- chisq.test(freqs2)\n# inspect results\nx22\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  freqs2\nX-squared = 46, df = 1, p-value = 0.00000000001\n\n# effect size\neffectsize::effectsize(x22)\n\nCramer's V |       95% CI\n-------------------------\n0.02       | [0.01, 1.00]\n\n- One-sided CIs: upper bound fixed at [1.00].\n\n\nThe output shows that the difference is not significant but that the effect size has remain the same. For this reason, in a response to Kilgariff, Stefan [11] suggested to always also report effect size in addition to significance so that the reader has an understanding of whether a significant effect is meaningful.\nThis is relevant here because we have focused on the power for finding small effects as these can be considered the smallest meaningful effects. However, even tiny effects can be meaningful under certain circumstances - as such, focusing on small effects is only a rule of thumb, should be taken with a pinch of salt, and should be re-evaluated in the context of the study at hand."
  },
  {
    "objectID": "pwr.html#using-piloted-data-and-a-lmer",
    "href": "pwr.html#using-piloted-data-and-a-lmer",
    "title": "Power Analysis in R",
    "section": "Using piloted data and a lmer",
    "text": "Using piloted data and a lmer\nFor this analysis, we load an existing data set resulting from a piloted study that only contains the predictors (not the response variable).\n\n# load data\nregdat  <- base::readRDS(url(\"https://slcladal.github.io/data/regdat.rda\", \"rb\"))\n# inspect\nhead(regdat, 10);str(regdat)\n\n       ID   Sentence     Group WordOrder SentenceType\n1  Part01 Sentence01 L1English        V2  NoAdverbial\n2  Part01 Sentence02 L1English        V3  NoAdverbial\n3  Part01 Sentence03 L1English        V3  NoAdverbial\n4  Part01 Sentence04 L1English        V2  NoAdverbial\n5  Part01 Sentence05 L1English        V2  NoAdverbial\n6  Part01 Sentence06 L1English        V3  NoAdverbial\n7  Part01 Sentence07 L1English        V2  NoAdverbial\n8  Part01 Sentence08 L1English        V3  NoAdverbial\n9  Part01 Sentence09 L1English        V3  NoAdverbial\n10 Part01 Sentence10 L1English        V2  NoAdverbial\n\n\n'data.frame':   480 obs. of  5 variables:\n $ ID          : Factor w/ 20 levels \"Part01\",\"Part02\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ Sentence    : Factor w/ 24 levels \"Sentence01\",\"Sentence02\",..: 1 2 3 4 5 6 7 8 9 10 ...\n $ Group       : Factor w/ 2 levels \"L1English\",\"L2English\": 1 1 1 1 1 1 1 1 1 1 ...\n $ WordOrder   : Factor w/ 2 levels \"V2\",\"V3\": 1 2 2 1 1 2 1 2 2 1 ...\n $ SentenceType: Factor w/ 2 levels \"NoAdverbial\",..: 1 1 1 1 1 1 1 1 1 1 ...\n\n\nWe inspect the data and check how many levels we have for each predictor and if the levels are distributed correctly (so that we do not have incomplete information).\n\ntable(regdat$Group, regdat$WordOrder); \n\n           \n             V2  V3\n  L1English 120 120\n  L2English 120 120\n\ntable(regdat$Group, regdat$SentenceType); \n\n           \n            NoAdverbial SentenceAdverbial\n  L1English         120               120\n  L2English         120               120\n\ntable(regdat$ID)\n\n\nPart01 Part02 Part03 Part04 Part05 Part06 Part07 Part08 Part09 Part10 Part11 \n    24     24     24     24     24     24     24     24     24     24     24 \nPart12 Part13 Part14 Part15 Part16 Part17 Part18 Part19 Part20 \n    24     24     24     24     24     24     24     24     24 \n\ntable(regdat$Sentence)\n\n\nSentence01 Sentence02 Sentence03 Sentence04 Sentence05 Sentence06 Sentence07 \n        20         20         20         20         20         20         20 \nSentence08 Sentence09 Sentence10 Sentence11 Sentence12 Sentence13 Sentence14 \n        20         20         20         20         20         20         20 \nSentence15 Sentence16 Sentence17 Sentence18 Sentence19 Sentence20 Sentence21 \n        20         20         20         20         20         20         20 \nSentence22 Sentence23 Sentence24 \n        20         20         20 \n\n\nWe could also add a response variable (but we will do this later when we deal with post-hoc power analyses).\n\nGenerating the model\nWe now generate model that has per-defined parameters. We begin by specifying the parameters by setting effect sizes of the fixed effects and the intercept, the variability accounted fro by the random effects and the residuals.\n\n# Intercept + slopes for fixed effects \n# (Intercept + Group, SentenceType, WordOrder, and an interaction between Group * SentenceType)\nfixed <- c(.52, .52, .52, .52, .52)\n# Random intercepts for Sentence and ID\nrand <- list(0.5, 0.1)\n# res. variance\nres <- 2\n\nWe now generate the model and fit it to our data.\n\nm1 <- makeLmer(y ~ (1|Sentence) + (1|ID) + Group * SentenceType + WordOrder, \n               fixef=fixed, \n               VarCorr=rand, \n               sigma=res, \n               data=regdat)\n\n# inspect\nm1\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: y ~ (1 | Sentence) + (1 | ID) + Group * SentenceType + WordOrder\n   Data: regdat\nREML criterion at convergence: 2100\nRandom effects:\n Groups   Name        Std.Dev.\n Sentence (Intercept) 0.707   \n ID       (Intercept) 0.316   \n Residual             2.000   \nNumber of obs: 480, groups:  Sentence, 24; ID, 20\nFixed Effects:\n                                 (Intercept)  \n                                        0.52  \n                              GroupL2English  \n                                        0.52  \n               SentenceTypeSentenceAdverbial  \n                                        0.52  \n                                 WordOrderV3  \n                                        0.52  \nGroupL2English:SentenceTypeSentenceAdverbial  \n                                        0.52  \n\n\nInspect summary table\n\nsjPlot::tab_model(m1)\n\n\n\n\n \ny\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n0.52\n-0.14 – 1.18\n0.124\n\n\nGroup [L2English]\n0.52\n-0.06 – 1.10\n0.078\n\n\nSentenceType[SentenceAdverbial]\n0.52\n-0.24 – 1.28\n0.180\n\n\nWordOrder [V3]\n0.52\n-0.15 – 1.19\n0.129\n\n\nGroup [L2English] *SentenceType[SentenceAdverbial]\n0.52\n-0.20 – 1.24\n0.155\n\n\nRandom Effects\n\n\n\nσ2\n4.00\n\n\n\nτ00 Sentence\n0.50\n\n\nτ00 ID\n0.10\n\n\nICC\n0.13\n\n\nN Sentence\n24\n\n\nN ID\n20\n\nObservations\n480\n\n\nMarginal R2 / Conditional R2\n0.078 / 0.198\n\n\n\n\n\n\nThe summary table shows that the effects are correct but none of them are reported as being significant!\n\n\nPower analysis\nLet us now check if the data is sufficient to detect the main effect of WordOrder. In the test argument we use fcompare which allows us to compare a model with that effect (our m1 model) to a model without that effect. Fortunately, we only need to specify the fixed effects structure.\n\nsim_wo <- simr::powerSim(m1, \n                        nsim=20,\n                        \n                        test = fcompare(y ~ Group * SentenceType)) \n\nSimulating: |                                                                  |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |===                                                               |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |======                                                            |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=========                                                         |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=============                                                     |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |================                                                  |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |===================                                               |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=======================                                           |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |==========================                                        |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=============================                                     |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=================================                                 |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |====================================                              |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=======================================                           |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |==========================================                        |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |==============================================                    |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=================================================                 |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |====================================================              |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |========================================================          |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |===========================================================       |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |==============================================================    |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |==================================================================|\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\n# inspect results\nsim_wo\n\nPower for model comparison, (95% confidence interval):\n      35.00% (15.39, 59.22)\n\nTest: Likelihood ratio\n      Comparison to y ~ Group * SentenceType + [re]\n\nBased on 20 simulations, (0 warnings, 0 errors)\nalpha = 0.05, nrow = 480\n\nTime elapsed: 0 h 0 m 1 s\n\n\nThe data is not sufficient and would only detect a weak effect of WordOrder with 7 percent accuracy!\nBefore we continue to see how to test the power of data to find interactions, let us briefly think about why I chose 0.52 when we specified the effect sizes.\n\n\nWhy did I set the estimates to 0.52?\nLet us now inspect the effect sizes and see why I used 0.52 as an effect size in the model. We need to determine the odds ratios of the fixed effects and then convert them into Cohen’s d values for which we have associations between traditional denominations (small, medium, and large) and effect size values.\n\n# extract fixed effect estimates\nestimatesfixedeffects <- fixef(m1)\n# convert estimates into odds ratios\nexp(estimatesfixedeffects)\n\n                                 (Intercept) \n                                       1.682 \n                              GroupL2English \n                                       1.682 \n               SentenceTypeSentenceAdverbial \n                                       1.682 \n                                 WordOrderV3 \n                                       1.682 \nGroupL2English:SentenceTypeSentenceAdverbial \n                                       1.682 \n\n\nWe will now check the effect size which can be interpreted according to [12, see also 8, 13].\n\nsmall effect (Cohen’s d 0.2, OR = 1.68)\nmedium effect (Cohen’s d 0.5, OR = 3.47)\nstrong effect (Cohen’s d 0.8, OR = 6.71)\n\n\n\nPower analysis for interactions\nLet us now check if the data set has enough power to detect a weak effect for the interaction between Group:SentenceType.\n\nsim_gst <- simr::powerSim(m1, \n                          nsim=20, \n                          # compare model with interaction to model without interaction\n                          test = fcompare(y ~ WordOrder + Group + SentenceType))\n\nSimulating: |                                                                  |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |===                                                               |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |======                                                            |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=========                                                         |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=============                                                     |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |================                                                  |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |===================                                               |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=======================                                           |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |==========================                                        |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=============================                                     |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=================================                                 |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |====================================                              |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=======================================                           |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |==========================================                        |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |==============================================                    |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=================================================                 |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |====================================================              |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |========================================================          |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |===========================================================       |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |==============================================================    |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |==================================================================|\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\n# inspect\nsim_gst\n\nPower for model comparison, (95% confidence interval):\n      25.00% ( 8.66, 49.10)\n\nTest: Likelihood ratio\n      Comparison to y ~ WordOrder + Group + SentenceType + [re]\n\nBased on 20 simulations, (0 warnings, 0 errors)\nalpha = 0.05, nrow = 480\n\nTime elapsed: 0 h 0 m 1 s\n\n\nThe data is not sufficient to detect a weak effect of Group:SentenceType with 5 percent accuracy!\n\n\n\n\nNOTEOh no… What can we do?"
  },
  {
    "objectID": "pwr.html#extending-data",
    "href": "pwr.html#extending-data",
    "title": "Power Analysis in R",
    "section": "Extending Data",
    "text": "Extending Data\nWe will now extend the data to see what sample size is needed to get to the 80 percent accuracy threshold. We begin by increasing the number of sentences from 10 to 30 to see if this would lead to a sufficient sample size. After increasing the number of sentences, we will extend the data to see how many participants we would need.\n\nAdding sentences\n\nm1_as <- simr::extend(m1,\n                      along=\"Sentence\", \n                      n=120)\n# inspect model\nm1_as\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: y ~ (1 | Sentence) + (1 | ID) + Group * SentenceType + WordOrder\n   Data: regdat\nREML criterion at convergence: 2100\nRandom effects:\n Groups   Name        Std.Dev.\n Sentence (Intercept) 0.707   \n ID       (Intercept) 0.316   \n Residual             2.000   \nNumber of obs: 480, groups:  Sentence, 24; ID, 20\nFixed Effects:\n                                 (Intercept)  \n                                        0.52  \n                              GroupL2English  \n                                        0.52  \n               SentenceTypeSentenceAdverbial  \n                                        0.52  \n                                 WordOrderV3  \n                                        0.52  \nGroupL2English:SentenceTypeSentenceAdverbial  \n                                        0.52  \n\n\nCheck power when using 120 sentences\n\nsim_m1_as_gst <- powerSim(m1_as, \n                   nsim=20, \n                   test = fcompare(y ~ WordOrder + Group + SentenceType))\n\nSimulating: |                                                                  |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |===                                                               |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |======                                                            |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=========                                                         |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=============                                                     |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |================                                                  |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |===================                                               |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=======================                                           |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |==========================                                        |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=============================                                     |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=================================                                 |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |====================================                              |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=======================================                           |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |==========================================                        |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |==============================================                    |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=================================================                 |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |====================================================              |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |========================================================          |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |===========================================================       |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |==============================================================    |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |==================================================================|\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\n# inspect\nsim_m1_as_gst\n\nPower for model comparison, (95% confidence interval):\n      90.00% (68.30, 98.77)\n\nTest: Likelihood ratio\n      Comparison to y ~ WordOrder + Group + SentenceType + [re]\n\nBased on 20 simulations, (0 warnings, 0 errors)\nalpha = 0.05, nrow = 2400\n\nTime elapsed: 0 h 0 m 3 s\n\n\nThe data with 120 sentences is sufficient and would detect a weak effect of Group:SentenceType with 18 percent accuracy!\nLet us now plot a power curve to see where we cross the 80 percent threshold.\n\npcurve_m1_as_gst <- simr::powerCurve(m1_as, \n                               test=fcompare(y ~ WordOrder + Group + SentenceType), \n                               along=\"Sentence\",\n                               nsim = 20, \n                               breaks=seq(20, 120, 20))\n# show plot\nplot(pcurve_m1_as_gst)\n\n\n\n\nUsing more items (or in this case sentences) is rather easy but it can make experiments longer which may lead to participants becoming tired or annoyed. An alternative would be to use more participants. Let us thus check how we can determine how many subjects we would need to reach a power of at least 80 percent.\n\n\nChecking participants\nWhat about increasing the number of participants?\nWe increase the number of participants to 120.\n\nm1_ap <- simr::extend(m1,\n                      along=\"ID\", \n                      n=120)\n# inspect model\nm1_ap\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: y ~ (1 | Sentence) + (1 | ID) + Group * SentenceType + WordOrder\n   Data: regdat\nREML criterion at convergence: 2100\nRandom effects:\n Groups   Name        Std.Dev.\n Sentence (Intercept) 0.707   \n ID       (Intercept) 0.316   \n Residual             2.000   \nNumber of obs: 480, groups:  Sentence, 24; ID, 20\nFixed Effects:\n                                 (Intercept)  \n                                        0.52  \n                              GroupL2English  \n                                        0.52  \n               SentenceTypeSentenceAdverbial  \n                                        0.52  \n                                 WordOrderV3  \n                                        0.52  \nGroupL2English:SentenceTypeSentenceAdverbial  \n                                        0.52  \n\n\nCheck power when using 120 participants\n\nsim_m1_ap_gst <- powerSim(m1_ap, \n                   nsim=20, \n                   test = fcompare(y ~ WordOrder + Group + SentenceType))\n\nSimulating: |                                                                  |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |===                                                               |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |======                                                            |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=========                                                         |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=============                                                     |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |================                                                  |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |===================                                               |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=======================                                           |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |==========================                                        |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=============================                                     |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=================================                                 |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |====================================                              |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=======================================                           |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |==========================================                        |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |==============================================                    |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=================================================                 |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |====================================================              |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |========================================================          |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |===========================================================       |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |==============================================================    |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |==================================================================|\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\n# inspect\nsim_m1_ap_gst\n\nPower for model comparison, (95% confidence interval):\n      80.00% (56.34, 94.27)\n\nTest: Likelihood ratio\n      Comparison to y ~ WordOrder + Group + SentenceType + [re]\n\nBased on 20 simulations, (0 warnings, 0 errors)\nalpha = 0.05, nrow = 2880\n\nTime elapsed: 0 h 0 m 4 s\n\n\nThe data with 120 participants is sufficient and would detect a weak effect of Group * SentenceType with 16 percent accuracy\n\npcurve_m1_ap_gst <- simr::powerCurve(m1_ap, \n                               test=fcompare(y ~ Group + SentenceType + WordOrder),\n                               along = \"ID\", \n                               nsim=20)\n\nCalculating power at 10 sample sizes along ID\n\n\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\n\n# show plot\nplot(pcurve_m1_ap_gst)"
  },
  {
    "objectID": "pwr.html#generating-data-and-a-glmer",
    "href": "pwr.html#generating-data-and-a-glmer",
    "title": "Power Analysis in R",
    "section": "Generating data and a glmer",
    "text": "Generating data and a glmer\nIn order to perform a power analysis, we will start by loading the tidyverse package to process the data and by generating a data that we will use to determine the power of a regression model.\nThis simulated data set has\n\n200 data points\n2 Conditions (Control, Test)\n10 Subjects\n10 Items\n\n\n# generate data\nsimdat <- data.frame(\n  sub <- rep(paste0(\"Sub\", 1:10), each = 20),\n  cond <- rep(c(\n    rep(\"Control\", 10),\n    rep(\"Test\", 10))\n    , 10),\n  itm <- as.character(rep(1:10, 20))\n) %>%\n  dplyr::rename(Subject = 1,\n                Condition = 2,\n                Item = 3) %>%\n  dplyr::mutate_if(is.character, factor)\n# inspect\nhead(simdat, 15)\n\n   Subject Condition Item\n1     Sub1   Control    1\n2     Sub1   Control    2\n3     Sub1   Control    3\n4     Sub1   Control    4\n5     Sub1   Control    5\n6     Sub1   Control    6\n7     Sub1   Control    7\n8     Sub1   Control    8\n9     Sub1   Control    9\n10    Sub1   Control   10\n11    Sub1      Test    1\n12    Sub1      Test    2\n13    Sub1      Test    3\n14    Sub1      Test    4\n15    Sub1      Test    5\n\n\n\n\n\n\nEXERCISE TIME!\n\n\n\n\n\n\n`\n\nCan you create a data set with 5 Subjects, 5 Items (each) for 4 Conditions? \n\n\n\nAnswer\n\n::: {.cell} ::: {.cell-output .cell-output-stdout} Subjects Items  Condition   1  Subject1 Item1 Condition1   2  Subject1 Item2 Condition1   3  Subject1 Item3 Condition1   4  Subject1 Item4 Condition1   5  Subject1 Item5 Condition1   6  Subject1 Item1 Condition2   7  Subject1 Item2 Condition2   8  Subject1 Item3 Condition2   9  Subject1 Item4 Condition2   10 Subject1 Item5 Condition2   11 Subject1 Item1 Condition3   12 Subject1 Item2 Condition3   13 Subject1 Item3 Condition3   14 Subject1 Item4 Condition3   15 Subject1 Item5 Condition3   16 Subject1 Item1 Condition4   17 Subject1 Item2 Condition4   18 Subject1 Item3 Condition4   19 Subject1 Item4 Condition4   20 Subject1 Item5 Condition4 ::: :::\n\n\n`\n\n\nGenerating the model\nAs before with the lmer, we specify the model parameters - but when generating glmers, we only need to specify the effects for the fixed effects and the intercept and define the variability in the random effects (we do not need to specify the residuals).\n\n# Intercept + slopes for fixed effects \n# (Group, SentenceType, WordOrder, and an interaction between Group * SentenceType)\nfixed <- c(.52, .52)\n# Random intercepts for Sentence and ID\nrand <- list(0.5, 0.1)\n\nWe now generate the model and fit it to the data.\n\nm2 <- simr::makeGlmer(y ~ (1|Subject) + (1|Item) + Condition, \n                      family = \"binomial\", \n                      fixef=fixed, \n                      VarCorr=rand, \n                      data=simdat)\n# inspect\nsjPlot::tab_model(m2)\n\n\n\n\n \ny\n\n\nPredictors\nOdds Ratios\nCI\np\n\n\n(Intercept)\n1.68\n0.89 – 3.17\n0.108\n\n\nCondition [Test]\n1.68\n0.94 – 3.02\n0.081\n\n\nRandom Effects\n\n\n\nσ2\n3.29\n\n\n\nτ00 Subject\n0.50\n\n\nτ00 Item\n0.10\n\n\nICC\n0.15\n\n\nN Subject\n10\n\n\nN Item\n10\n\nObservations\n200\n\n\nMarginal R2 / Conditional R2\n0.017 / 0.169\n\n\n\n\n\n\nNext, we extract power. In this case, we use fixed in the test argument which allows us to test a specific predictor.\n\n# set seed for replicability\nset.seed(12345)\n# perform power analysis for present model\nrsim_m2_c <- powerSim(m2, fixed(\"ConditionTest\", \"z\"), \n                      nsim=20)\n\nSimulating: |                                                                  |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |===                                                               |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |======                                                            |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=========                                                         |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=============                                                     |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |================                                                  |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |===================                                               |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=======================                                           |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |==========================                                        |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=============================                                     |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=================================                                 |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |====================================                              |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=======================================                           |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |==========================================                        |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |==============================================                    |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=================================================                 |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |====================================================              |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |========================================================          |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |===========================================================       |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |==============================================================    |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |==================================================================|\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\n# inspect\nrsim_m2_c\n\nPower for predictor 'ConditionTest', (95% confidence interval):\n      35.00% (15.39, 59.22)\n\nTest: z-test\n      Effect size for ConditionTest is 0.52\n\nBased on 20 simulations, (0 warnings, 0 errors)\nalpha = 0.05, nrow = 200\n\nTime elapsed: 0 h 0 m 2 s\n\n\nThe data is sufficient and would detect a weak effect of ConditionTest with only 7 percent accuracy"
  },
  {
    "objectID": "pwr.html#extending-the-data",
    "href": "pwr.html#extending-the-data",
    "title": "Power Analysis in R",
    "section": "Extending the data",
    "text": "Extending the data\nLike before, we can now extend the data to see how many participants or items we would need to reach the 80 percent confidence threshold.\n\nAdding Items\nWe start of by increasing the number of items from 10 to 40. This means that our new data/model has the following characteristics.\n\n2 Conditions\n10 Subjects\n40 Items (from 10)\n\nKeep in mind though that when extending the data/model in this way, each combination occurs only once!\n\nm2_ai <- simr::extend(m2,\n                      along=\"Item\", \n                      n=40)\n\nNext, we plot the power curve.\n\npcurve_m2_ai_c <- powerCurve(m2_ai, \n                             fixed(\"ConditionTest\", \"z\"), \n                             along = \"Item\",\n                             nsim = 20,\n                             breaks=seq(10, 40, 5))\n\nCalculating power at 7 sample sizes along Item\n\n\nSimulating: |                                                                  |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |===                                                               |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |======                                                            |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=========                                                         |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=============                                                     |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |================                                                  |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |===================                                               |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=======================                                           |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |==========================                                        |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=============================                                     |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=================================                                 |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |====================================                              |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=======================================                           |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |==========================================                        |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |==============================================                    |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=================================================                 |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |====================================================              |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |========================================================          |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |===========================================================       |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |==============================================================    |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |==================================================================|\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/7) \b\b\b\b\b\b(1/7) Simulating: |                                                            |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/7) Simulating: |===                                                         |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/7) Simulating: |======                                                      |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/7) Simulating: |=========                                                   |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/7) Simulating: |============                                                |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/7) Simulating: |===============                                             |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/7) Simulating: |==================                                          |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/7) Simulating: |=====================                                       |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/7) Simulating: |========================                                    |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/7) Simulating: |===========================                                 |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/7) Simulating: |==============================                              |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/7) Simulating: |=================================                           |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/7) Simulating: |====================================                        |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/7) Simulating: |=======================================                     |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/7) Simulating: |==========================================                  |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/7) Simulating: |=============================================               |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/7) Simulating: |================================================            |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/7) Simulating: |===================================================         |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/7) Simulating: |======================================================      |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/7) Simulating: |=========================================================   |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/7) Simulating: |============================================================|\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/7) \b\b\b\b\b\b(2/7) \b\b\b\b\b\b(2/7) Simulating: |                                                            |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/7) Simulating: |===                                                         |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/7) Simulating: |======                                                      |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/7) Simulating: |=========                                                   |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/7) Simulating: |============                                                |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/7) Simulating: |===============                                             |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/7) Simulating: |==================                                          |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/7) Simulating: |=====================                                       |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/7) Simulating: |========================                                    |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/7) Simulating: |===========================                                 |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/7) Simulating: |==============================                              |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/7) Simulating: |=================================                           |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/7) Simulating: |====================================                        |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/7) Simulating: |=======================================                     |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/7) Simulating: |==========================================                  |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/7) Simulating: |=============================================               |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/7) Simulating: |================================================            |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/7) Simulating: |===================================================         |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/7) Simulating: |======================================================      |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/7) Simulating: |=========================================================   |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/7) Simulating: |============================================================|\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/7) \b\b\b\b\b\b(3/7) \b\b\b\b\b\b(3/7) Simulating: |                                                            |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/7) Simulating: |===                                                         |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/7) Simulating: |======                                                      |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/7) Simulating: |=========                                                   |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/7) Simulating: |============                                                |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/7) Simulating: |===============                                             |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/7) Simulating: |==================                                          |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/7) Simulating: |=====================                                       |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/7) Simulating: |========================                                    |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/7) Simulating: |===========================                                 |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/7) Simulating: |==============================                              |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/7) Simulating: |=================================                           |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/7) Simulating: |====================================                        |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/7) Simulating: |=======================================                     |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/7) Simulating: |==========================================                  |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/7) Simulating: |=============================================               |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/7) Simulating: |================================================            |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/7) Simulating: |===================================================         |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/7) Simulating: |======================================================      |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/7) Simulating: |=========================================================   |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/7) Simulating: |============================================================|\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/7) \b\b\b\b\b\b(4/7) \b\b\b\b\b\b(4/7) Simulating: |                                                            |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/7) Simulating: |===                                                         |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/7) Simulating: |======                                                      |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/7) Simulating: |=========                                                   |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/7) Simulating: |============                                                |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/7) Simulating: |===============                                             |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/7) Simulating: |==================                                          |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/7) Simulating: |=====================                                       |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/7) Simulating: |========================                                    |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/7) Simulating: |===========================                                 |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/7) Simulating: |==============================                              |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/7) Simulating: |=================================                           |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/7) Simulating: |====================================                        |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/7) Simulating: |=======================================                     |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/7) Simulating: |==========================================                  |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/7) Simulating: |=============================================               |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/7) Simulating: |================================================            |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/7) Simulating: |===================================================         |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/7) Simulating: |======================================================      |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/7) Simulating: |=========================================================   |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/7) Simulating: |============================================================|\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/7) \b\b\b\b\b\b(5/7) \b\b\b\b\b\b(5/7) Simulating: |                                                            |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/7) Simulating: |===                                                         |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/7) Simulating: |======                                                      |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/7) Simulating: |=========                                                   |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/7) Simulating: |============                                                |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/7) Simulating: |===============                                             |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/7) Simulating: |==================                                          |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/7) Simulating: |=====================                                       |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/7) Simulating: |========================                                    |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/7) Simulating: |===========================                                 |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/7) Simulating: |==============================                              |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/7) Simulating: |=================================                           |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/7) Simulating: |====================================                        |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/7) Simulating: |=======================================                     |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/7) Simulating: |==========================================                  |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/7) Simulating: |=============================================               |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/7) Simulating: |================================================            |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/7) Simulating: |===================================================         |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/7) Simulating: |======================================================      |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/7) Simulating: |=========================================================   |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/7) Simulating: |============================================================|\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/7) \b\b\b\b\b\b(6/7) \b\b\b\b\b\b(6/7) Simulating: |                                                            |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/7) Simulating: |===                                                         |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/7) Simulating: |======                                                      |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/7) Simulating: |=========                                                   |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/7) Simulating: |============                                                |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/7) Simulating: |===============                                             |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/7) Simulating: |==================                                          |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/7) Simulating: |=====================                                       |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/7) Simulating: |========================                                    |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/7) Simulating: |===========================                                 |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/7) Simulating: |==============================                              |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/7) Simulating: |=================================                           |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/7) Simulating: |====================================                        |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/7) Simulating: |=======================================                     |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/7) Simulating: |==========================================                  |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/7) Simulating: |=============================================               |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/7) Simulating: |================================================            |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/7) Simulating: |===================================================         |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/7) Simulating: |======================================================      |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/7) Simulating: |=========================================================   |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/7) Simulating: |============================================================|\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/7) \b\b\b\b\b\b(7/7) \b\b\b\b\b\b(7/7) Simulating: |                                                            |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(7/7) Simulating: |===                                                         |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(7/7) Simulating: |======                                                      |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(7/7) Simulating: |=========                                                   |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(7/7) Simulating: |============                                                |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(7/7) Simulating: |===============                                             |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(7/7) Simulating: |==================                                          |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(7/7) Simulating: |=====================                                       |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(7/7) Simulating: |========================                                    |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(7/7) Simulating: |===========================                                 |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(7/7) Simulating: |==============================                              |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(7/7) Simulating: |=================================                           |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(7/7) Simulating: |====================================                        |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(7/7) Simulating: |=======================================                     |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(7/7) Simulating: |==========================================                  |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(7/7) Simulating: |=============================================               |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(7/7) Simulating: |================================================            |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(7/7) Simulating: |===================================================         |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(7/7) Simulating: |======================================================      |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(7/7) Simulating: |=========================================================   |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(7/7) Simulating: |============================================================|\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(7/7) \b\b\b\b\b\b\n\n# show plot\nplot(pcurve_m2_ai_c)\n\n\n\n\nThe power curve shows that we breach the 80 percent threshold with about 35 items.\n\n\nAdding subjects\nAn alternative to adding items is, of course, to use more subjects or participants. We thus continue by increasing the number of participants from 10 to 40. This means that our new data/model has the following characteristics.\n\n2 Conditions\n10 Items\n40 Subjects (from 10)\n\nAgain, keep in mind though that when extending the data/model in this way, each combination occurs only once!\n\nm2_as <- simr::extend(m2,\n                      along=\"Subject\", \n                      n=40)\n# inspect model\nsjPlot::tab_model(m2_as)\n\n\n\n\n \ny\n\n\nPredictors\nOdds Ratios\nCI\np\n\n\n(Intercept)\n1.68\n0.89 – 3.17\n0.108\n\n\nCondition [Test]\n1.68\n0.94 – 3.02\n0.081\n\n\nRandom Effects\n\n\n\nσ2\n3.29\n\n\n\nτ00 Subject\n0.50\n\n\nτ00 Item\n0.10\n\n\nICC\n0.15\n\n\nN Subject\n10\n\n\nN Item\n10\n\nObservations\n200\n\n\nMarginal R2 / Conditional R2\n0.017 / 0.169\n\n\n\n\n\n\nAs before, we plot power curve.\n\npcurve_m2_as_c <- powerCurve(m2_as, \n                             fixed(\"ConditionTest\", \"z\"), \n                             along = \"Subject\",\n                             nsim = 20,\n                             breaks=seq(10, 40, 5))\n\nCalculating power at 7 sample sizes along Subject\n\n\nSimulating: |                                                                  |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |===                                                               |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |======                                                            |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=========                                                         |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=============                                                     |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |================                                                  |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |===================                                               |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=======================                                           |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |==========================                                        |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=============================                                     |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=================================                                 |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |====================================                              |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=======================================                           |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |==========================================                        |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |==============================================                    |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=================================================                 |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |====================================================              |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |========================================================          |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |===========================================================       |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |==============================================================    |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |==================================================================|\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/7) \b\b\b\b\b\b(1/7) Simulating: |                                                            |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/7) Simulating: |===                                                         |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/7) Simulating: |======                                                      |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/7) Simulating: |=========                                                   |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/7) Simulating: |============                                                |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/7) Simulating: |===============                                             |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/7) Simulating: |==================                                          |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/7) Simulating: |=====================                                       |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/7) Simulating: |========================                                    |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/7) Simulating: |===========================                                 |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/7) Simulating: |==============================                              |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/7) Simulating: |=================================                           |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/7) Simulating: |====================================                        |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/7) Simulating: |=======================================                     |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/7) Simulating: |==========================================                  |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/7) Simulating: |=============================================               |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/7) Simulating: |================================================            |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/7) Simulating: |===================================================         |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/7) Simulating: |======================================================      |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/7) Simulating: |=========================================================   |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/7) Simulating: |============================================================|\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/7) \b\b\b\b\b\b(2/7) \b\b\b\b\b\b(2/7) Simulating: |                                                            |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/7) Simulating: |===                                                         |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/7) Simulating: |======                                                      |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/7) Simulating: |=========                                                   |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/7) Simulating: |============                                                |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/7) Simulating: |===============                                             |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/7) Simulating: |==================                                          |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/7) Simulating: |=====================                                       |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/7) Simulating: |========================                                    |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/7) Simulating: |===========================                                 |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/7) Simulating: |==============================                              |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/7) Simulating: |=================================                           |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/7) Simulating: |====================================                        |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/7) Simulating: |=======================================                     |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/7) Simulating: |==========================================                  |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/7) Simulating: |=============================================               |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/7) Simulating: |================================================            |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/7) Simulating: |===================================================         |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/7) Simulating: |======================================================      |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/7) Simulating: |=========================================================   |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/7) Simulating: |============================================================|\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/7) \b\b\b\b\b\b(3/7) \b\b\b\b\b\b(3/7) Simulating: |                                                            |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/7) Simulating: |===                                                         |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/7) Simulating: |======                                                      |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/7) Simulating: |=========                                                   |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/7) Simulating: |============                                                |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/7) Simulating: |===============                                             |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/7) Simulating: |==================                                          |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/7) Simulating: |=====================                                       |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/7) Simulating: |========================                                    |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/7) Simulating: |===========================                                 |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/7) Simulating: |==============================                              |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/7) Simulating: |=================================                           |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/7) Simulating: |====================================                        |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/7) Simulating: |=======================================                     |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/7) Simulating: |==========================================                  |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/7) Simulating: |=============================================               |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/7) Simulating: |================================================            |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/7) Simulating: |===================================================         |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/7) Simulating: |======================================================      |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/7) Simulating: |=========================================================   |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/7) Simulating: |============================================================|\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/7) \b\b\b\b\b\b(4/7) \b\b\b\b\b\b(4/7) Simulating: |                                                            |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/7) Simulating: |===                                                         |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/7) Simulating: |======                                                      |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/7) Simulating: |=========                                                   |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/7) Simulating: |============                                                |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/7) Simulating: |===============                                             |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/7) Simulating: |==================                                          |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/7) Simulating: |=====================                                       |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/7) Simulating: |========================                                    |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/7) Simulating: |===========================                                 |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/7) Simulating: |==============================                              |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/7) Simulating: |=================================                           |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/7) Simulating: |====================================                        |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/7) Simulating: |=======================================                     |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/7) Simulating: |==========================================                  |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/7) Simulating: |=============================================               |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/7) Simulating: |================================================            |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/7) Simulating: |===================================================         |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/7) Simulating: |======================================================      |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/7) Simulating: |=========================================================   |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/7) Simulating: |============================================================|\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/7) \b\b\b\b\b\b(5/7) \b\b\b\b\b\b(5/7) Simulating: |                                                            |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/7) Simulating: |===                                                         |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/7) Simulating: |======                                                      |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/7) Simulating: |=========                                                   |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/7) Simulating: |============                                                |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/7) Simulating: |===============                                             |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/7) Simulating: |==================                                          |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/7) Simulating: |=====================                                       |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/7) Simulating: |========================                                    |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/7) Simulating: |===========================                                 |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/7) Simulating: |==============================                              |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/7) Simulating: |=================================                           |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/7) Simulating: |====================================                        |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/7) Simulating: |=======================================                     |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/7) Simulating: |==========================================                  |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/7) Simulating: |=============================================               |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/7) Simulating: |================================================            |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/7) Simulating: |===================================================         |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/7) Simulating: |======================================================      |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/7) Simulating: |=========================================================   |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/7) Simulating: |============================================================|\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/7) \b\b\b\b\b\b(6/7) \b\b\b\b\b\b(6/7) Simulating: |                                                            |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/7) Simulating: |===                                                         |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/7) Simulating: |======                                                      |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/7) Simulating: |=========                                                   |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/7) Simulating: |============                                                |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/7) Simulating: |===============                                             |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/7) Simulating: |==================                                          |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/7) Simulating: |=====================                                       |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/7) Simulating: |========================                                    |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/7) Simulating: |===========================                                 |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/7) Simulating: |==============================                              |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/7) Simulating: |=================================                           |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/7) Simulating: |====================================                        |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/7) Simulating: |=======================================                     |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/7) Simulating: |==========================================                  |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/7) Simulating: |=============================================               |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/7) Simulating: |================================================            |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/7) Simulating: |===================================================         |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/7) Simulating: |======================================================      |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/7) Simulating: |=========================================================   |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/7) Simulating: |============================================================|\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/7) \b\b\b\b\b\b(7/7) \b\b\b\b\b\b(7/7) Simulating: |                                                            |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(7/7) Simulating: |===                                                         |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(7/7) Simulating: |======                                                      |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(7/7) Simulating: |=========                                                   |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(7/7) Simulating: |============                                                |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(7/7) Simulating: |===============                                             |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(7/7) Simulating: |==================                                          |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(7/7) Simulating: |=====================                                       |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(7/7) Simulating: |========================                                    |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(7/7) Simulating: |===========================                                 |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(7/7) Simulating: |==============================                              |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(7/7) Simulating: |=================================                           |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(7/7) Simulating: |====================================                        |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(7/7) Simulating: |=======================================                     |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(7/7) Simulating: |==========================================                  |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(7/7) Simulating: |=============================================               |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(7/7) Simulating: |================================================            |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(7/7) Simulating: |===================================================         |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(7/7) Simulating: |======================================================      |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(7/7) Simulating: |=========================================================   |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(7/7) Simulating: |============================================================|\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(7/7) \b\b\b\b\b\b\n\n# show plot\nplot(pcurve_m2_as_c)\n\n\n\n\nHow often does each combination occur?\nOnly once!\nSo, what if we increase the number of combinations (this is particularly important when using a *repeated measures** design)? Below, we increase the number of configuration from 1 to 10 so that each item is shown 10 times to the same participant. This means that our new data/model has the following characteristics.\n\n2 Conditions\n10 Items\n10 Subjects\n\nNow each combination of item and subject occurs 10 times!\n\nm2_asi_c <- extend(m2, \n                   within=\"Subject+Item\", \n                   n=10)\n# perform power calculation\npcurve_m2_asi_c <- powerCurve(m2_asi_c, \n                              fixed(\"ConditionTest\", \"z\"), \n                              within=\"Subject+Item\",\n                              nsim = 20,\n                              breaks=seq(2, 10, 2))\n\nCalculating power at 5 sample sizes within Subject+Item\n\n\nSimulating: |                                                                  |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |===                                                               |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |======                                                            |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=========                                                         |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=============                                                     |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |================                                                  |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |===================                                               |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=======================                                           |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |==========================                                        |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=============================                                     |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=================================                                 |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |====================================                              |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=======================================                           |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |==========================================                        |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |==============================================                    |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=================================================                 |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |====================================================              |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |========================================================          |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |===========================================================       |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |==============================================================    |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |==================================================================|\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/5) \b\b\b\b\b\b(1/5) Simulating: |                                                            |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/5) Simulating: |===                                                         |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/5) Simulating: |======                                                      |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/5) Simulating: |=========                                                   |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/5) Simulating: |============                                                |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/5) Simulating: |===============                                             |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/5) Simulating: |==================                                          |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/5) Simulating: |=====================                                       |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/5) Simulating: |========================                                    |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/5) Simulating: |===========================                                 |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/5) Simulating: |==============================                              |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/5) Simulating: |=================================                           |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/5) Simulating: |====================================                        |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/5) Simulating: |=======================================                     |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/5) Simulating: |==========================================                  |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/5) Simulating: |=============================================               |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/5) Simulating: |================================================            |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/5) Simulating: |===================================================         |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/5) Simulating: |======================================================      |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/5) Simulating: |=========================================================   |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/5) Simulating: |============================================================|\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/5) \b\b\b\b\b\b(2/5) \b\b\b\b\b\b(2/5) Simulating: |                                                            |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/5) Simulating: |===                                                         |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/5) Simulating: |======                                                      |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/5) Simulating: |=========                                                   |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/5) Simulating: |============                                                |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/5) Simulating: |===============                                             |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/5) Simulating: |==================                                          |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/5) Simulating: |=====================                                       |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/5) Simulating: |========================                                    |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/5) Simulating: |===========================                                 |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/5) Simulating: |==============================                              |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/5) Simulating: |=================================                           |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/5) Simulating: |====================================                        |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/5) Simulating: |=======================================                     |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/5) Simulating: |==========================================                  |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/5) Simulating: |=============================================               |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/5) Simulating: |================================================            |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/5) Simulating: |===================================================         |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/5) Simulating: |======================================================      |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/5) Simulating: |=========================================================   |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/5) Simulating: |============================================================|\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/5) \b\b\b\b\b\b(3/5) \b\b\b\b\b\b(3/5) Simulating: |                                                            |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/5) Simulating: |===                                                         |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/5) Simulating: |======                                                      |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/5) Simulating: |=========                                                   |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/5) Simulating: |============                                                |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/5) Simulating: |===============                                             |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/5) Simulating: |==================                                          |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/5) Simulating: |=====================                                       |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/5) Simulating: |========================                                    |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/5) Simulating: |===========================                                 |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/5) Simulating: |==============================                              |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/5) Simulating: |=================================                           |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/5) Simulating: |====================================                        |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/5) Simulating: |=======================================                     |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/5) Simulating: |==========================================                  |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/5) Simulating: |=============================================               |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/5) Simulating: |================================================            |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/5) Simulating: |===================================================         |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/5) Simulating: |======================================================      |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/5) Simulating: |=========================================================   |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/5) Simulating: |============================================================|\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/5) \b\b\b\b\b\b(4/5) \b\b\b\b\b\b(4/5) Simulating: |                                                            |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/5) Simulating: |===                                                         |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/5) Simulating: |======                                                      |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/5) Simulating: |=========                                                   |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/5) Simulating: |============                                                |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/5) Simulating: |===============                                             |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/5) Simulating: |==================                                          |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/5) Simulating: |=====================                                       |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/5) Simulating: |========================                                    |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/5) Simulating: |===========================                                 |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/5) Simulating: |==============================                              |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/5) Simulating: |=================================                           |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/5) Simulating: |====================================                        |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/5) Simulating: |=======================================                     |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/5) Simulating: |==========================================                  |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/5) Simulating: |=============================================               |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/5) Simulating: |================================================            |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/5) Simulating: |===================================================         |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/5) Simulating: |======================================================      |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/5) Simulating: |=========================================================   |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/5) Simulating: |============================================================|\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/5) \b\b\b\b\b\b(5/5) \b\b\b\b\b\b(5/5) Simulating: |                                                            |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/5) Simulating: |===                                                         |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/5) Simulating: |======                                                      |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/5) Simulating: |=========                                                   |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/5) Simulating: |============                                                |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/5) Simulating: |===============                                             |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/5) Simulating: |==================                                          |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/5) Simulating: |=====================                                       |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/5) Simulating: |========================                                    |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/5) Simulating: |===========================                                 |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/5) Simulating: |==============================                              |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/5) Simulating: |=================================                           |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/5) Simulating: |====================================                        |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/5) Simulating: |=======================================                     |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/5) Simulating: |==========================================                  |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/5) Simulating: |=============================================               |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/5) Simulating: |================================================            |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/5) Simulating: |===================================================         |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/5) Simulating: |======================================================      |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/5) Simulating: |=========================================================   |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/5) Simulating: |============================================================|\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/5) \b\b\b\b\b\b\n\n# show plot\nplot(pcurve_m2_asi_c)\n\n\n\n\nIf we did this, then even 5 subjects may be enough to reach the 80 percent threshold.\n\n\nAdding subjects and items\nWe can also add subjects and items simultaneously to address questions like How many subjects would I need if I had 30 items?. Hence, we increase both subjects and items from 10 to 30. This means that our new data/model has the following characteristics.\n\n2 Conditions\n30 Items (from 10)\n30 Subjects (from 10)\n\nIn this case, we return to each combination only occurring once.\n\nm2_as <- simr::extend(m2,\n                      along=\"Subject\", \n                      n=30)\nm2_asi <- simr::extend(m2_as,\n                      along=\"Item\", \n                      n=30)\n# inspect model\nsjPlot::tab_model(m2_asi)\n\n\n\n\n \ny\n\n\nPredictors\nOdds Ratios\nCI\np\n\n\n(Intercept)\n1.68\n0.89 – 3.17\n0.108\n\n\nCondition [Test]\n1.68\n0.94 – 3.02\n0.081\n\n\nRandom Effects\n\n\n\nσ2\n3.29\n\n\n\nτ00 Subject\n0.50\n\n\nτ00 Item\n0.10\n\n\nICC\n0.15\n\n\nN Subject\n10\n\n\nN Item\n10\n\nObservations\n200\n\n\nMarginal R2 / Conditional R2\n0.017 / 0.169\n\n\n\n\n\n\nWe can now plot power curve to answer the question How many subjects do you need if you have 30 Items?.\n\npcurve_m2_asi_c <- powerCurve(m2_asi, \n                             fixed(\"ConditionTest\", \"z\"), \n                             along = \"Subject\", \n                             breaks = seq(5, 30, 5), \n                             nsim = 20)\n\nCalculating power at 6 sample sizes along Subject\n\n\nSimulating: |                                                                  |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |===                                                               |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |======                                                            |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=========                                                         |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=============                                                     |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |================                                                  |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |===================                                               |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=======================                                           |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |==========================                                        |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=============================                                     |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=================================                                 |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |====================================                              |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=======================================                           |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |==========================================                        |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |==============================================                    |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=================================================                 |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |====================================================              |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |========================================================          |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |===========================================================       |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |==============================================================    |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |==================================================================|\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/6) \b\b\b\b\b\b(1/6) Simulating: |                                                            |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/6) Simulating: |===                                                         |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/6) Simulating: |======                                                      |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/6) Simulating: |=========                                                   |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/6) Simulating: |============                                                |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/6) Simulating: |===============                                             |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/6) Simulating: |==================                                          |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/6) Simulating: |=====================                                       |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/6) Simulating: |========================                                    |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/6) Simulating: |===========================                                 |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/6) Simulating: |==============================                              |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/6) Simulating: |=================================                           |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/6) Simulating: |====================================                        |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/6) Simulating: |=======================================                     |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/6) Simulating: |==========================================                  |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/6) Simulating: |=============================================               |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/6) Simulating: |================================================            |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/6) Simulating: |===================================================         |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/6) Simulating: |======================================================      |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/6) Simulating: |=========================================================   |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/6) Simulating: |============================================================|\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/6) \b\b\b\b\b\b(2/6) \b\b\b\b\b\b(2/6) Simulating: |                                                            |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/6) Simulating: |===                                                         |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/6) Simulating: |======                                                      |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/6) Simulating: |=========                                                   |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/6) Simulating: |============                                                |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/6) Simulating: |===============                                             |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/6) Simulating: |==================                                          |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/6) Simulating: |=====================                                       |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/6) Simulating: |========================                                    |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/6) Simulating: |===========================                                 |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/6) Simulating: |==============================                              |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/6) Simulating: |=================================                           |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/6) Simulating: |====================================                        |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/6) Simulating: |=======================================                     |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/6) Simulating: |==========================================                  |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/6) Simulating: |=============================================               |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/6) Simulating: |================================================            |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/6) Simulating: |===================================================         |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/6) Simulating: |======================================================      |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/6) Simulating: |=========================================================   |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/6) Simulating: |============================================================|\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/6) \b\b\b\b\b\b(3/6) \b\b\b\b\b\b(3/6) Simulating: |                                                            |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/6) Simulating: |===                                                         |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/6) Simulating: |======                                                      |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/6) Simulating: |=========                                                   |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/6) Simulating: |============                                                |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/6) Simulating: |===============                                             |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/6) Simulating: |==================                                          |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/6) Simulating: |=====================                                       |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/6) Simulating: |========================                                    |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/6) Simulating: |===========================                                 |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/6) Simulating: |==============================                              |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/6) Simulating: |=================================                           |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/6) Simulating: |====================================                        |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/6) Simulating: |=======================================                     |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/6) Simulating: |==========================================                  |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/6) Simulating: |=============================================               |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/6) Simulating: |================================================            |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/6) Simulating: |===================================================         |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/6) Simulating: |======================================================      |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/6) Simulating: |=========================================================   |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/6) Simulating: |============================================================|\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/6) \b\b\b\b\b\b(4/6) \b\b\b\b\b\b(4/6) Simulating: |                                                            |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/6) Simulating: |===                                                         |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/6) Simulating: |======                                                      |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/6) Simulating: |=========                                                   |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/6) Simulating: |============                                                |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/6) Simulating: |===============                                             |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/6) Simulating: |==================                                          |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/6) Simulating: |=====================                                       |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/6) Simulating: |========================                                    |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/6) Simulating: |===========================                                 |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/6) Simulating: |==============================                              |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/6) Simulating: |=================================                           |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/6) Simulating: |====================================                        |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/6) Simulating: |=======================================                     |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/6) Simulating: |==========================================                  |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/6) Simulating: |=============================================               |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/6) Simulating: |================================================            |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/6) Simulating: |===================================================         |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/6) Simulating: |======================================================      |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/6) Simulating: |=========================================================   |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/6) Simulating: |============================================================|\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/6) \b\b\b\b\b\b(5/6) \b\b\b\b\b\b(5/6) Simulating: |                                                            |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/6) Simulating: |===                                                         |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/6) Simulating: |======                                                      |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/6) Simulating: |=========                                                   |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/6) Simulating: |============                                                |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/6) Simulating: |===============                                             |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/6) Simulating: |==================                                          |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/6) Simulating: |=====================                                       |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/6) Simulating: |========================                                    |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/6) Simulating: |===========================                                 |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/6) Simulating: |==============================                              |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/6) Simulating: |=================================                           |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/6) Simulating: |====================================                        |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/6) Simulating: |=======================================                     |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/6) Simulating: |==========================================                  |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/6) Simulating: |=============================================               |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/6) Simulating: |================================================            |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/6) Simulating: |===================================================         |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/6) Simulating: |======================================================      |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/6) Simulating: |=========================================================   |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/6) Simulating: |============================================================|\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/6) \b\b\b\b\b\b(6/6) \b\b\b\b\b\b(6/6) Simulating: |                                                            |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/6) Simulating: |===                                                         |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/6) Simulating: |======                                                      |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/6) Simulating: |=========                                                   |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/6) Simulating: |============                                                |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/6) Simulating: |===============                                             |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/6) Simulating: |==================                                          |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/6) Simulating: |=====================                                       |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/6) Simulating: |========================                                    |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/6) Simulating: |===========================                                 |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/6) Simulating: |==============================                              |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/6) Simulating: |=================================                           |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/6) Simulating: |====================================                        |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/6) Simulating: |=======================================                     |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/6) Simulating: |==========================================                  |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/6) Simulating: |=============================================               |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/6) Simulating: |================================================            |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/6) Simulating: |===================================================         |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/6) Simulating: |======================================================      |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/6) Simulating: |=========================================================   |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/6) Simulating: |============================================================|\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/6) \b\b\b\b\b\b\n\n# show plot\nplot(pcurve_m2_asi_c)\n\n\n\n\nWe can see that with 30 items, we would need only about 15 subjects to reach the 80 percent threshold.\nWe can also check the results in tabular form as shown below.\nThe results are shown below.\n\n# print results\nprint(pcurve_m2_asi_c)\n\nPower for predictor 'ConditionTest', (95% confidence interval),\nby number of levels in Subject:\n      5: 65.00% (40.78, 84.61) - 300 rows\n     10: 95.00% (75.13, 99.87) - 600 rows\n     15: 100.0% (83.16, 100.0) - 900 rows\n     20: 100.0% (83.16, 100.0) - 1200 rows\n     25: 100.0% (83.16, 100.0) - 1500 rows\n     30: 100.0% (83.16, 100.0) - 1800 rows\n\nTime elapsed: 0 h 0 m 18 s"
  },
  {
    "objectID": "pwr.html#post-hoc-analyses",
    "href": "pwr.html#post-hoc-analyses",
    "title": "Power Analysis in R",
    "section": "Post-Hoc Analyses",
    "text": "Post-Hoc Analyses\n\n\n\n\nNOTEPower analysis have also been used post-hoc to test if the sample size of studies was sufficient to detect meaningful effects. However, such post-hoc power calculations where the target effect size comes from the data, give misleading results [13, 14] and should thus be treated with extreme care!\n\n\n\n\n\n\nWe begin by adding a response variable to our data. In this case, we vary the response variable ti a higher likelihood of obtaining gazes in the area of interests (AOI) in the test condition.\n\nsimdat2 <- simdat %>%\n  dplyr::arrange(Condition) %>%\n  dplyr::mutate(\n  dep <- c(sample(c(\"yes\", \"no\"), 100, replace = T, prob = c(.5, .5)),\n           sample(c(\"yes\", \"no\"), 100, replace = T, prob = c(.7, .3)))\n  ) %>%\n  dplyr::mutate_if(is.character, factor) %>%\n  dplyr::rename(AOI = 4)\n# inspect\nhead(simdat2, 20)\n\n   Subject Condition Item AOI\n1     Sub1   Control    1  no\n2     Sub1   Control    2 yes\n3     Sub1   Control    3  no\n4     Sub1   Control    4 yes\n5     Sub1   Control    5  no\n6     Sub1   Control    6 yes\n7     Sub1   Control    7  no\n8     Sub1   Control    8 yes\n9     Sub1   Control    9  no\n10    Sub1   Control   10 yes\n11    Sub2   Control    1  no\n12    Sub2   Control    2  no\n13    Sub2   Control    3 yes\n14    Sub2   Control    4  no\n15    Sub2   Control    5  no\n16    Sub2   Control    6 yes\n17    Sub2   Control    7  no\n18    Sub2   Control    8 yes\n19    Sub2   Control    9 yes\n20    Sub2   Control   10 yes\n\n\nNow that we have generated some data, we will fit a model to it and perform a power analysis on the observed effects.\nWe will fit a first model to the data. Thus, in a first step, we load the lme4 package to create a model, set a seed (to save the results and so that the results can be replicated), and then create an initial mixed-effects model.\n\n# set seed for replicability\nset.seed(12345)\n# fit model\nm3 <- glmer(AOI ~ (1|Subject) +(1|Item) + Condition, \n            family=\"binomial\", \n            data=simdat2)\n# inspect results\nsummary(m3)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: AOI ~ (1 | Subject) + (1 | Item) + Condition\n   Data: simdat2\n\n     AIC      BIC   logLik deviance df.resid \n   259.3    272.5   -125.6    251.3      196 \n\nScaled residuals: \n   Min     1Q Median     3Q    Max \n-1.687 -1.151  0.593  0.869  0.869 \n\nRandom effects:\n Groups  Name        Variance Std.Dev.\n Subject (Intercept) 0        0       \n Item    (Intercept) 0        0       \nNumber of obs: 200, groups:  Subject, 10; Item, 10\n\nFixed effects:\n              Estimate Std. Error z value Pr(>|z|)  \n(Intercept)      0.282      0.202    1.40    0.163  \nConditionTest    0.764      0.305    2.51    0.012 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr)\nConditinTst -0.663\noptimizer (Nelder_Mead) convergence code: 0 (OK)\nboundary (singular) fit: see help('isSingular')\n\n\nWe now check the effect sizes of the predictors in the model. We can do this by displaying the results of the model using the tab_model function from the sjPlot package.\n\n# tabulate results\nsjPlot::tab_model(m3)\n\n\n\n\n \nAOI\n\n\nPredictors\nOdds Ratios\nCI\np\n\n\n(Intercept)\n1.33\n0.89 – 1.97\n0.163\n\n\nCondition [Test]\n2.15\n1.18 – 3.90\n0.012\n\n\nRandom Effects\n\n\n\nσ2\n3.29\n\n\n\nτ00 Subject\n0.00\n\n\nτ00 Item\n0.00\n\n\nN Subject\n10\n\n\nN Item\n10\n\nObservations\n200\n\n\nMarginal R2 / Conditional R2\n0.043 / NA\n\n\n\n\n\n\nNow, we perform power analysis on an observed effect. This analysis tells us how likely the model is to find an observed effect given the data.\n\n\n\n\nNOTEWe use a very low number of simulations (10) and we use the default z-test which is suboptimal for small samples [15]. In a proper study, you should increase the number of simulations (to at least 1000) and you should use a bootstrapping rather than a z-test [cf. 16].\n\n\n\n\n\n\nWhat is the probability of finding the observed effect given the data?\n\n# set seed for replicability\nset.seed(12345)\n# perform power analysis for present model\nm3_pwr <- powerSim(m3, \n                  fixed(\"ConditionTest\", \"z\"), \n                  nsim=20)\n# inspect results\nm3_pwr\n\nThe results of the power analysis show that, given the data at hand, the model would have detected the effect of Conidition:Test with a probability of m3_pwr$x percent. However, and as stated above, the results of such post-hoc power calculations (where the target effect size comes from the data) give misleading results [14] and should thus be treated with extreme care!\n\nFixing effects\nWe will set the effects that we obtained based on our “observed” data to check if, given the size of the data, we have enough power to detect a small effect of Condition. In a first step, we check the observed effects.\n\nestimatesfixedeffects <- fixef(m3)\nexp(estimatesfixedeffects)\n\n  (Intercept) ConditionTest \n        1.326         2.147 \n\n\nWe can see that the effect of Condition is rather small which makes it very hard to detect an effect. We will now change the size of the effect of ConditionTest to represent a truly small effect, i.e. on the brink of being noise but being just strong enough to be considered small. In other words, we will set the effect so that its odds ratio is exactly 1.68.\n\n# set seed for replicability\nset.seed(12345)\n# perform power analysis for small effect\nfixef(m3)[\"ConditionTest\"] <- 0.519\nestimatesfixedeffects <- fixef(m3)\nexp(estimatesfixedeffects)\n\n  (Intercept) ConditionTest \n        1.326         1.680 \n\n\nWhat is the probability of finding a weak effect given the data?\nWe now perform the power analysis.\n\n# set seed for replicability\nset.seed(12345)\n# perform power analysis for present model\nm3_pwr_se <- powerSim(m3, \n                  fixed(\"ConditionTest\", \"z\"), \n                  nsim=20)\n# inspect results\nm3_pwr_se\n\nThe data is not sufficient and would detect a weak effect of Condition with only 8 percent accuracy\n\n\nPower Analysis of Extended Data\nWe will now extend the data to see what sample size is needed to get to the 80 percent accuracy threshold. We begin by increasing the number of items from 10 to 30 to see if this would lead to a sufficient sample size.\n\n# increase sample size\nm3_ai <- extend(m3, \n             along=\"Item\", \n             n=30)\n# perform power simulation\nm3_ai_pwr <- powerSim(m3_ai, \n                  fixed(\"ConditionTest\", \"z\"), \n                  nsim=20)\n# show results\nm3_ai_pwr\n\nThe data with 30 Items is sufficient and would detect a weak effect of Condition with 18 percent accuracy\nAt what number of sentences are the data sufficient to detect an effect?\n\npcurve_m3_asi_c <- powerCurve(m3_ai, \n                             fixed(\"ConditionTest\", \"z\"), \n                             along = \"Item\", \n                             breaks = seq(5, 30, 5), \n                             nsim = 20)\n\nSimulating: |                                                                  |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |===                                                               |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |======                                                            |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=========                                                         |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=============                                                     |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |================                                                  |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |===================                                               |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=======================                                           |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |==========================                                        |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=============================                                     |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=================================                                 |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |====================================                              |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=======================================                           |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |==========================================                        |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |==============================================                    |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=================================================                 |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |====================================================              |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |========================================================          |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |===========================================================       |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |==============================================================    |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |==================================================================|\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/6) \b\b\b\b\b\b(1/6) Simulating: |                                                            |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/6) Simulating: |===                                                         |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/6) Simulating: |======                                                      |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/6) Simulating: |=========                                                   |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/6) Simulating: |============                                                |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/6) Simulating: |===============                                             |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/6) Simulating: |==================                                          |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/6) Simulating: |=====================                                       |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/6) Simulating: |========================                                    |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/6) Simulating: |===========================                                 |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/6) Simulating: |==============================                              |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/6) Simulating: |=================================                           |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/6) Simulating: |====================================                        |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/6) Simulating: |=======================================                     |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/6) Simulating: |==========================================                  |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/6) Simulating: |=============================================               |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/6) Simulating: |================================================            |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/6) Simulating: |===================================================         |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/6) Simulating: |======================================================      |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/6) Simulating: |=========================================================   |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/6) Simulating: |============================================================|\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/6) \b\b\b\b\b\b(2/6) \b\b\b\b\b\b(2/6) Simulating: |                                                            |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/6) Simulating: |===                                                         |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/6) Simulating: |======                                                      |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/6) Simulating: |=========                                                   |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/6) Simulating: |============                                                |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/6) Simulating: |===============                                             |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/6) Simulating: |==================                                          |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/6) Simulating: |=====================                                       |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/6) Simulating: |========================                                    |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/6) Simulating: |===========================                                 |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/6) Simulating: |==============================                              |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/6) Simulating: |=================================                           |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/6) Simulating: |====================================                        |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/6) Simulating: |=======================================                     |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/6) Simulating: |==========================================                  |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/6) Simulating: |=============================================               |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/6) Simulating: |================================================            |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/6) Simulating: |===================================================         |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/6) Simulating: |======================================================      |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/6) Simulating: |=========================================================   |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/6) Simulating: |============================================================|\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/6) \b\b\b\b\b\b(3/6) \b\b\b\b\b\b(3/6) Simulating: |                                                            |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/6) Simulating: |===                                                         |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/6) Simulating: |======                                                      |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/6) Simulating: |=========                                                   |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/6) Simulating: |============                                                |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/6) Simulating: |===============                                             |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/6) Simulating: |==================                                          |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/6) Simulating: |=====================                                       |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/6) Simulating: |========================                                    |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/6) Simulating: |===========================                                 |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/6) Simulating: |==============================                              |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/6) Simulating: |=================================                           |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/6) Simulating: |====================================                        |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/6) Simulating: |=======================================                     |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/6) Simulating: |==========================================                  |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/6) Simulating: |=============================================               |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/6) Simulating: |================================================            |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/6) Simulating: |===================================================         |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/6) Simulating: |======================================================      |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/6) Simulating: |=========================================================   |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/6) Simulating: |============================================================|\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/6) \b\b\b\b\b\b(4/6) \b\b\b\b\b\b(4/6) Simulating: |                                                            |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/6) Simulating: |===                                                         |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/6) Simulating: |======                                                      |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/6) Simulating: |=========                                                   |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/6) Simulating: |============                                                |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/6) Simulating: |===============                                             |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/6) Simulating: |==================                                          |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/6) Simulating: |=====================                                       |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/6) Simulating: |========================                                    |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/6) Simulating: |===========================                                 |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/6) Simulating: |==============================                              |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/6) Simulating: |=================================                           |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/6) Simulating: |====================================                        |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/6) Simulating: |=======================================                     |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/6) Simulating: |==========================================                  |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/6) Simulating: |=============================================               |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/6) Simulating: |================================================            |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/6) Simulating: |===================================================         |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/6) Simulating: |======================================================      |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/6) Simulating: |=========================================================   |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/6) Simulating: |============================================================|\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/6) \b\b\b\b\b\b(5/6) \b\b\b\b\b\b(5/6) Simulating: |                                                            |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/6) Simulating: |===                                                         |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/6) Simulating: |======                                                      |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/6) Simulating: |=========                                                   |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/6) Simulating: |============                                                |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/6) Simulating: |===============                                             |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/6) Simulating: |==================                                          |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/6) Simulating: |=====================                                       |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/6) Simulating: |========================                                    |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/6) Simulating: |===========================                                 |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/6) Simulating: |==============================                              |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/6) Simulating: |=================================                           |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/6) Simulating: |====================================                        |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/6) Simulating: |=======================================                     |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/6) Simulating: |==========================================                  |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/6) Simulating: |=============================================               |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/6) Simulating: |================================================            |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/6) Simulating: |===================================================         |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/6) Simulating: |======================================================      |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/6) Simulating: |=========================================================   |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/6) Simulating: |============================================================|\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/6) \b\b\b\b\b\b(6/6) \b\b\b\b\b\b(6/6) Simulating: |                                                            |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/6) Simulating: |===                                                         |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/6) Simulating: |======                                                      |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/6) Simulating: |=========                                                   |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/6) Simulating: |============                                                |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/6) Simulating: |===============                                             |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/6) Simulating: |==================                                          |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/6) Simulating: |=====================                                       |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/6) Simulating: |========================                                    |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/6) Simulating: |===========================                                 |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/6) Simulating: |==============================                              |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/6) Simulating: |=================================                           |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/6) Simulating: |====================================                        |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/6) Simulating: |=======================================                     |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/6) Simulating: |==========================================                  |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/6) Simulating: |=============================================               |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/6) Simulating: |================================================            |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/6) Simulating: |===================================================         |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/6) Simulating: |======================================================      |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/6) Simulating: |=========================================================   |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/6) Simulating: |============================================================|\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/6) \b\b\b\b\b\b\n\n# show plot\nplot(pcurve_m3_asi_c)\n\n\n\n\nWe reach the 80 percent threshold with about 25 subjects."
  },
  {
    "objectID": "regex.html",
    "href": "regex.html",
    "title": "Regular Expressions in R",
    "section": "",
    "text": "1 Introduction\nThis tutorial introduces regular expressions and how they can be used when working with language data. The entire R Notebook for the sections below can be downloaded here.\n\n\n\n\n\nThis tutorial is aimed at beginners and intermediate users of R with the aim of showcasing how to use regular expression (or wild cards) in R. The aim is not to provide a fully-fledged analysis but rather to show and exemplify selected useful functions and methods associated with regular expressions.\n\n\n\nThe entire R Notebook for the tutorial can be downloaded here. If you want to render the R Notebook on your machine, i.e. knitting the document to html or a pdf, you need to make sure that you have R and RStudio installed and you also need to download the bibliography file and store it in the same folder where you store the Rmd file.  Here is a link to an interactive version of this tutorial on Google Colab. The interactive tutorial is based on a Jupyter notebook of this tutorial. This interactive Jupyter notebook allows you to execute code yourself and - if you copy the Jupyter notebook - you can also change and edit the notebook, e.g. you can change code and upload your own data.\n\n\n\n\n\nHow can you search texts for complex patterns or combinations of patterns? This question will answered in this tutorial and at the end you will be able to perform very complex searches yourself. The key concept of this tutorial is that of a regular expression. A regular expression (in short also called regex or regexp) is a special sequence of characters (or string) for describing a search pattern. You can think of regular expressions as very powerful combinations of wildcards or as wildcards on steroids.\nIf you would like to get deeper into regular expressions, I can recommend [1] and, in particular, chapter 17 of [2] for further study (although the latter uses base R rather than tidyverse functions, but this does not affect the utility of the discussion of regular expressions in any major or meaningful manner). Also, here is a so-called cheatsheet about regular expressions written by Ian Kopacka and provided by RStudio. Nick Thieberger has also recorded a very nice Introduction to Regular Expressions for humanities scholars to YouTube.\nPreparation and session set up\nThis tutorial is based on R. If you have not installed R or are new to it, you will find an introduction to and more information how to use R here. For this tutorials, we need to install certain packages from an R library so that the scripts shown below are executed without errors. Before turning to the code below, please install the packages by running the code below this paragraph. If you have already installed the packages mentioned below, then you can skip ahead and ignore this section. To install the necessary packages, simply run the following code - it may take some time (between 1 and 5 minutes to install all of the libraries so you do not need to worry if it takes some time).\n\n# set options\noptions(stringsAsFactors = F)         # no automatic data transformation\noptions(\"scipen\" = 100, \"digits\" = 4) # suppress math annotation\n# install packages\ninstall.packages(\"tidyverse\")\ninstall.packages(\"flextable\")\ninstall.packages(\"htmlwidgets\")\n# install klippy for copy-to-clipboard button in code chunks\ninstall.packages(\"remotes\")\nremotes::install_github(\"rlesur/klippy\")\n\nIn a next step, we load the packages.\n\nlibrary(tidyverse)\nlibrary(flextable)\n# activate klippy for copy-to-clipboard button\nklippy::klippy()\n\n\n\n\nOnce you have installed RStudio and have initiated the session by executing the code shown above, you are good to go.\n\n\n2 Getting started with Regular Expressions\nTo put regular expressions into practice, we need some text that we will perform out searches on. In this tutorial, we will use texts from wikipedia about grammar.\n\n# read in first text\ntext1 <- readLines(\"https://slcladal.github.io/data/testcorpus/linguistics02.txt\")\net <-  paste(text1, sep = \" \", collapse = \" \")\n# inspect example text\net\n\n[1] \"Grammar is a system of rules which governs the production and use of utterances in a given language. These rules apply to sound as well as meaning, and include componential subsets of rules, such as those pertaining to phonology (the organisation of phonetic sound systems), morphology (the formation and composition of words), and syntax (the formation and composition of phrases and sentences). Many modern theories that deal with the principles of grammar are based on Noam Chomsky's framework of generative linguistics.\"\n\n\nIn addition, we will split the example text into words to have another resource we can use to understand regular expressions\n\n# split example text\nset <- str_split(et, \" \") %>%\n  unlist()\n# inspect\nhead(set)\n\n[1] \"Grammar\" \"is\"      \"a\"       \"system\"  \"of\"      \"rules\"  \n\n\nBefore we delve into using regular expressions, we will have a look at the regular expressions that can be used in R and also check what they stand for.\nThere are three basic types of regular expressions:\n\nregular expressions that stand for individual symbols and determine frequencies\nregular expressions that stand for classes of symbols\nregular expressions that stand for structural properties\n\nThe regular expressions below show the first type of regular expressions, i.e. regular expressions that stand for individual symbols and determine frequencies.\n\n\n\n\n\nRegular expressions that stand for individual symbols and determine frequencies.\n\n\nRegEx Symbol/SequenceExplanationExample?The preceding item is optional and will be matched at most oncewalk[a-z]? = walk, walks*The preceding item will be matched zero or more timeswalk[a-z]* = walk, walks, walked, walking+The preceding item will be matched one or more timeswalk[a-z]+ = walks, walked, walking{n}The preceding item is matched exactly n timeswalk[a-z]{2} = walked{n,}The preceding item is matched n or more timeswalk[a-z]{2,} = walked, walking{n,m}The preceding item is matched at least n times, but not more than m timeswalk[a-z]{2,3} = walked, walking\n\n\nThe regular expressions below show the second type of regular expressions, i.e. regular expressions that stand for classes of symbols.\n\n\n\n\n\nRegular expressions that stand for classes of symbols.\n\n\nRegEx Symbol/SequenceExplanation[ab]lower case a and b[a-z]all lower case characters from a to z[AB]upper case a and b[A-Z]all upper case characters from A to Z[12]digits 1 and 2[0-9]digits: 0 1 2 3 4 5 6 7 8 9[:digit:]digits: 0 1 2 3 4 5 6 7 8 9[:lower:]lower case characters: a–z[:upper:]upper case characters: A–Z[:alpha:]alphabetic characters: a–z and A–Z[:alnum:]digits and alphabetic characters[:punct:]punctuation characters: . , ; etc.[:graph:]graphical characters: [:alnum:] and [:punct:][:blank:]blank characters: Space and tab[:space:]space characters: Space, tab, newline, and other space characters\n\n\nThe regular expressions that denote classes of symbols are enclosed in [] and :. The last type of regular expressions, i.e. regular expressions that stand for structural properties are shown below.\n\n\n\n\n\nRegular expressions that stand for structural properties.\n\n\nRegEx Symbol/SequenceExplanation\\\\wWord characters: [[:alnum:]_]\\\\WNo word characters: [^[:alnum:]_]\\\\sSpace characters: [[:blank:]]\\\\SNo space characters: [^[:blank:]]\\\\dDigits: [[:digit:]]\\\\DNo digits: [^[:digit:]]\\\\bWord edge\\\\BNo word edge<Word beginning>Word end^Beginning of a string$End of a string\n\n\n\n\n3 Practice\nIn this section, we will explore how to use regular expressions. At the end, we will go through some exercises to help you understand how you can best utilize regular expressions.\nShow all words in the split example text that contain a or n.\n\nset[str_detect(set, \"[an]\")]\n\n [1] \"Grammar\"      \"a\"            \"governs\"      \"production\"   \"and\"         \n [6] \"utterances\"   \"in\"           \"a\"            \"given\"        \"language.\"   \n[11] \"apply\"        \"sound\"        \"as\"           \"as\"           \"meaning,\"    \n[16] \"and\"          \"include\"      \"componential\" \"as\"           \"pertaining\"  \n[21] \"phonology\"    \"organisation\" \"phonetic\"     \"sound\"        \"formation\"   \n[26] \"and\"          \"composition\"  \"and\"          \"syntax\"       \"formation\"   \n[31] \"and\"          \"composition\"  \"phrases\"      \"and\"          \"sentences).\" \n[36] \"Many\"         \"modern\"       \"that\"         \"deal\"         \"principles\"  \n[41] \"grammar\"      \"are\"          \"based\"        \"on\"           \"Noam\"        \n[46] \"framework\"    \"generative\"   \"linguistics.\"\n\n\nShow all words in the split example text that begin with a lower case a.\n\nset[str_detect(set, \"^a\")]\n\n [1] \"a\"     \"and\"   \"a\"     \"apply\" \"as\"    \"as\"    \"and\"   \"as\"    \"and\"  \n[10] \"and\"   \"and\"   \"and\"   \"are\"  \n\n\nShow all words in the split example text that end in a lower case s.\n\nset[str_detect(set, \"s$\")]\n\n [1] \"is\"         \"rules\"      \"governs\"    \"utterances\" \"rules\"     \n [6] \"as\"         \"as\"         \"subsets\"    \"as\"         \"phrases\"   \n[11] \"theories\"   \"principles\" \"Chomsky's\" \n\n\nShow all words in the split example text in which there is an e, then any other character, and than another n.\n\nset[str_detect(set, \"e.n\")]\n\n[1] \"governs\"  \"meaning,\" \"modern\"  \n\n\nShow all words in the split example text in which there is an e, then two other characters, and than another n.\n\nset[str_detect(set, \"e.{2,2}n\")]\n\n[1] \"utterances\"\n\n\nShow all words that consist of exactly three alphabetical characters in the split example text.\n\nset[str_detect(set, \"^[:alpha:]{3,3}$\")]\n\n [1] \"the\" \"and\" \"use\" \"and\" \"and\" \"and\" \"and\" \"and\" \"the\" \"are\"\n\n\nShow all words that consist of six or more alphabetical characters in the split example text.\n\nset[str_detect(set, \"^[:alpha:]{6,}$\")]\n\n [1] \"Grammar\"      \"system\"       \"governs\"      \"production\"   \"utterances\"  \n [6] \"include\"      \"componential\" \"subsets\"      \"pertaining\"   \"phonology\"   \n[11] \"organisation\" \"phonetic\"     \"morphology\"   \"formation\"    \"composition\" \n[16] \"syntax\"       \"formation\"    \"composition\"  \"phrases\"      \"modern\"      \n[21] \"theories\"     \"principles\"   \"grammar\"      \"framework\"    \"generative\"  \n\n\nReplace all lower case as with upper case Es in the example text.\n\nstr_replace_all(et, \"a\", \"E\")\n\n[1] \"GrEmmEr is E system of rules which governs the production End use of utterEnces in E given lEnguEge. These rules Epply to sound Es well Es meEning, End include componentiEl subsets of rules, such Es those pertEining to phonology (the orgEnisEtion of phonetic sound systems), morphology (the formEtion End composition of words), End syntEx (the formEtion End composition of phrEses End sentences). MEny modern theories thEt deEl with the principles of grEmmEr Ere bEsed on NoEm Chomsky's frEmework of generEtive linguistics.\"\n\n\nRemove all non-alphabetical characters in the split example text.\n\nstr_remove_all(set, \"\\\\W\")\n\n [1] \"Grammar\"      \"is\"           \"a\"            \"system\"       \"of\"          \n [6] \"rules\"        \"which\"        \"governs\"      \"the\"          \"production\"  \n[11] \"and\"          \"use\"          \"of\"           \"utterances\"   \"in\"          \n[16] \"a\"            \"given\"        \"language\"     \"These\"        \"rules\"       \n[21] \"apply\"        \"to\"           \"sound\"        \"as\"           \"well\"        \n[26] \"as\"           \"meaning\"      \"and\"          \"include\"      \"componential\"\n[31] \"subsets\"      \"of\"           \"rules\"        \"such\"         \"as\"          \n[36] \"those\"        \"pertaining\"   \"to\"           \"phonology\"    \"the\"         \n[41] \"organisation\" \"of\"           \"phonetic\"     \"sound\"        \"systems\"     \n[46] \"morphology\"   \"the\"          \"formation\"    \"and\"          \"composition\" \n[51] \"of\"           \"words\"        \"and\"          \"syntax\"       \"the\"         \n[56] \"formation\"    \"and\"          \"composition\"  \"of\"           \"phrases\"     \n[61] \"and\"          \"sentences\"    \"Many\"         \"modern\"       \"theories\"    \n[66] \"that\"         \"deal\"         \"with\"         \"the\"          \"principles\"  \n[71] \"of\"           \"grammar\"      \"are\"          \"based\"        \"on\"          \n[76] \"Noam\"         \"Chomskys\"     \"framework\"    \"of\"           \"generative\"  \n[81] \"linguistics\" \n\n\nRemove all white spaces in the example text.\n\nstr_remove_all(et, \" \")\n\n[1] \"Grammarisasystemofruleswhichgovernstheproductionanduseofutterancesinagivenlanguage.Theserulesapplytosoundaswellasmeaning,andincludecomponentialsubsetsofrules,suchasthosepertainingtophonology(theorganisationofphoneticsoundsystems),morphology(theformationandcompositionofwords),andsyntax(theformationandcompositionofphrasesandsentences).ManymoderntheoriesthatdealwiththeprinciplesofgrammararebasedonNoamChomsky'sframeworkofgenerativelinguistics.\"\n\n\nHighlighting patterns\nWe use the str_view and str_view_all functions to show the occurrences of regular expressions in the example text.\nTo begin with, we match an exactly defined pattern (ang).\n\nstr_view_all(et, \"ang\")\n\n\n\n\n\nNow, we include . which stands for any symbol (except a new line symbol).\n\nstr_view_all(et, \".n.\")\n\n\n\n\n\n\n\n\n\nEXERCISE TIME!\n\n\n\n\n\n\n`\n\nWhat regular expression can you use to extract all forms of walk from a text?\n\n\n\nAnswer\n\n[Ww][Aa][Ll][Kk].*\n\nMore exercises will follow - bear with us ;)\n\n`\n\n\n\nCitation & Session Info\nSchweinberger, Martin. 2022. Regular Expressions in R. Brisbane: The University of Queensland. url: https://slcladal.github.io/regex.html (Version 2022.08.31).\n@manual{schweinberger2022regex,\n  author = {Schweinberger, Martin},\n  title = {Regular Expressions in R},\n  note = {https://slcladal.github.io/regex.html},\n  year = {2022},\n  organization = {The University of Queensland, School of Languages and Cultures},\n  address = {Brisbane},\n  edition = {2022.08.31}\n}\n\nsessionInfo()\n\nR version 4.2.1 (2022-06-23)\nPlatform: x86_64-pc-linux-gnu (64-bit)\nRunning under: Ubuntu 22.04.1 LTS\n\nMatrix products: default\nBLAS:   /usr/lib/x86_64-linux-gnu/blas/libblas.so.3.10.0\nLAPACK: /usr/lib/x86_64-linux-gnu/lapack/liblapack.so.3.10.0\n\nlocale:\n [1] LC_CTYPE=en_AU.UTF-8       LC_NUMERIC=C              \n [3] LC_TIME=en_AU.UTF-8        LC_COLLATE=en_AU.UTF-8    \n [5] LC_MONETARY=en_AU.UTF-8    LC_MESSAGES=en_AU.UTF-8   \n [7] LC_PAPER=en_AU.UTF-8       LC_NAME=C                 \n [9] LC_ADDRESS=C               LC_TELEPHONE=C            \n[11] LC_MEASUREMENT=en_AU.UTF-8 LC_IDENTIFICATION=C       \n\nattached base packages:\n[1] stats     graphics  grDevices datasets  utils     methods   base     \n\nother attached packages:\n [1] flextable_0.7.3 forcats_0.5.1   stringr_1.4.0   dplyr_1.0.9    \n [5] purrr_0.3.4     readr_2.1.2     tidyr_1.2.0     tibble_3.1.7   \n [9] ggplot2_3.3.6   tidyverse_1.3.2\n\nloaded via a namespace (and not attached):\n [1] Rcpp_1.0.8.3        lubridate_1.8.0     assertthat_0.2.1   \n [4] digest_0.6.29       utf8_1.2.2          R6_2.5.1           \n [7] cellranger_1.1.0    backports_1.4.1     reprex_2.0.1       \n[10] evaluate_0.15       httr_1.4.3          pillar_1.7.0       \n[13] gdtools_0.2.4       rlang_1.0.4         uuid_1.1-0         \n[16] googlesheets4_1.0.0 readxl_1.4.0        data.table_1.14.2  \n[19] klippy_0.0.0.9500   rmarkdown_2.14      googledrive_2.0.0  \n[22] htmlwidgets_1.5.4   munsell_0.5.0       broom_1.0.0        \n[25] compiler_4.2.1      modelr_0.1.8        xfun_0.31          \n[28] systemfonts_1.0.4   pkgconfig_2.0.3     base64enc_0.1-3    \n[31] htmltools_0.5.2     tidyselect_1.1.2    fansi_1.0.3        \n[34] crayon_1.5.1        tzdb_0.3.0          dbplyr_2.2.1       \n[37] withr_2.5.0         grid_4.2.1          jsonlite_1.8.0     \n[40] gtable_0.3.0        lifecycle_1.0.1     DBI_1.1.3          \n[43] magrittr_2.0.3      scales_1.2.0        zip_2.2.0          \n[46] cli_3.3.0           stringi_1.7.8       renv_0.15.4        \n[49] fs_1.5.2            xml2_1.3.3          ellipsis_0.3.2     \n[52] generics_0.1.3      vctrs_0.4.1         tools_4.2.1        \n[55] glue_1.6.2          officer_0.4.3       hms_1.1.1          \n[58] fastmap_1.1.0       yaml_2.3.5          colorspace_2.0-3   \n[61] gargle_1.2.0        rvest_1.0.2         knitr_1.39         \n[64] haven_2.5.0        \n\n\n\nBack to top\nBack to HOME\n\n\n\n\n\n\n\n\n\nReferences\n\n1. Friedl, J.E.: Mastering regular expressions. \"O’Reilly Media\", Sebastopol, CA (2006).\n\n\n2. Peng, R.D.: R programming for data science. Leanpub (2020)."
  },
  {
    "objectID": "regression.html#simple-linear-regression",
    "href": "regression.html#simple-linear-regression",
    "title": "Fixed- and Mixed-Effects Regression Models in R",
    "section": "Simple Linear Regression",
    "text": "Simple Linear Regression\nThis section focuses on a very widely used statistical method which is called regression. Regressions are used when we try to understand how independent variables correlate with a dependent or outcome variable. So, if you want to investigate how a certain factor affects an outcome, then a regression is the way to go. We will have a look at two simple examples to understand what the concepts underlying a regression mean and how a regression works. The R code, that we will use, is adapted from many highly recommendable introductions which also focus on regression (among other types of analyses), for example, [7], [12], [8], [12] or [9]. [10] is also very good but probably not the first book one should read about statistics but it is highly recommendable for advanced learners.\nAlthough the basic logic underlying regressions is identical to the conceptual underpinnings of analysis of variance (ANOVA), a related method, sociolinguists have traditionally favored regression analysis in their studies while ANOVAs have been the method of choice in psycholinguistics. The preference for either method is grounded in historical happenstances and the culture of these subdisciplines rather than in methodological reasoning. However, ANOVA are more restricted in that they can only take numeric dependent variables and they have stricter model assumptions that are violated more readily. In addition, a minor difference between regressions and ANOVA lies in the fact that regressions are based on the \\(t\\)-distribution while ANOVAs use the F-distribution (however, the F-value is simply the value of t squared or t2). Both t- and F-values report on the ratio between explained and unexplained variance.\nThe idea behind regression analysis is expressed formally in the equation below where\\(f_{(x)}\\) is the y-value we want to predict, \\(\\alpha\\) is the intercept (the point where the regression line crosses the y-axis at x = 0), \\(\\beta\\) is the coefficient (the slope of the regression line).\n\\[\\begin{equation}\nf_{(x)} = \\alpha + \\beta_{i}x + \\epsilon\n\\end{equation}\\]\nIn other words, to estimate how much some weights who is 180cm tall, we would multiply the coefficient (slope of the line) with 180 (x) and add the value of the intercept (point where line crosses the y-axis at x = 0).\nHowever, the idea behind regressions can best be described graphically: imagine a cloud of points (like the points in the scatterplot in the upper left panel below). Regressions aim to find that line which has the minimal summed distance between points and the line (like the line in the lower panels). Technically speaking, the aim of a regression is to find the line with the minimal deviance (or the line with the minimal sum of residuals). Residuals are the distance between the line and the points (the red lines) and it is also called variance.\nThus, regression lines are those lines where the sum of the red lines should be minimal. The slope of the regression line is called coefficient and the point where the regression line crosses the y-axis at x = 0 is called the intercept.\n\n\n\n\n\nA word about standard errors (SE) is in order here because most commonly used statistics programs will provide SE values when reporting regression models. The SE is a measure that tells us how much the coefficients were to vary if the same regression were applied to many samples from the same population. A relatively small SE value therefore indicates that the coefficients will remain very stable if the same regression model is fitted to many different samples with identical parameters. In contrast, a large SE tells you that the model is volatile and not very stable or reliable as the coefficients vary substantially if the model is applied to many samples.\nMathematically, the SE is the standard deviation (SD) divided by the square root of the sample size (N) (see below).The SD is the square root of the deviance (that is, the SD is the square root of the sum of the mean \\(\\bar{x}\\) minus each data point (xi) squared divided by the sample size (N) minus 1).\n\\[\\begin{equation}\nStandard Error (SE) = \\frac{\\sum (\\bar{x}-x_{i})^2/N-1}{\\sqrt{N}} = \\frac{SD}{\\sqrt{N}}\n\\end{equation}\\]\n\nExample 1: Preposition Use across Real-Time\nWe will now turn to our first example. In this example, we will investigate whether the frequency of prepositions has changed from Middle English to Late Modern English. The reasoning behind this example is that Old English was highly synthetic compared with Present-Day English which comparatively analytic. In other words, while Old English speakers used case to indicate syntactic relations, speakers of Present-Day English use word order and prepositions to indicate syntactic relationships. This means that the loss of case had to be compensated by different strategies and maybe these strategies continued to develop and increase in frequency even after the change from synthetic to analytic had been mostly accomplished. And this prolonged change in compensatory strategies is what this example will focus on.\nThe analysis is based on data extracted from the Penn Corpora of Historical English (see http://www.ling.upenn.edu/hist-corpora/), that consists of 603 texts written between 1125 and 1900. In preparation of this example, all elements that were part-of-speech tagged as prepositions were extracted from the PennCorpora.\nThen, the relative frequencies (per 1,000 words) of prepositions per text were calculated. This frequency of prepositions per 1,000 words represents our dependent variable. In a next step, the date when each letter had been written was extracted. The resulting two vectors were combined into a table which thus contained for each text, when it was written (independent variable) and its relative frequency of prepositions (dependent or outcome variable).\nA regression analysis will follow the steps described below:\n\nExtraction and processing of the data\nData visualization\nApplying the regression analysis to the data\nDiagnosing the regression model and checking whether or not basic model assumptions have been violated.\n\nIn a first step, we load functions that we may need (which in this case is a function that we will use to summarize the results of the analysis).\n\n# load functions\nsource(\"https://slcladal.github.io/rscripts/slrsummary.r\")\n\nAfter preparing our session, we can now load and inspect the data to get a first impression of its properties.\n\n# load data\nslrdata  <- base::readRDS(url(\"https://slcladal.github.io/data/sld.rda\", \"rb\"))\n\n\n\n\n\n\nFirst 15 rows of slrdata.\n\n\nDateGenreTextPrepositionsRegion1,736Sciencealbin166.01North1,711Educationanon139.86North1,808PrivateLetterausten130.78North1,878Educationbain151.29North1,743Educationbarclay145.72North1,908Educationbenson120.77North1,906Diarybenson119.17North1,897Philosophyboethja132.96North1,785Philosophyboethri130.49North1,776Diaryboswell135.94North1,905Travelbradley154.20North1,711Educationbrightland149.14North1,762Sermonburton159.71North1,726Sermonbutler157.49North1,835PrivateLettercarlyle124.16North\n\n\nInspecting the data is very important because it can happen that a data set may not load completely or that variables which should be numeric have been converted to character variables. If unchecked, then such issues could go unnoticed and cause trouble.\nWe will now plot the data to get a better understanding of what the data looks like.\n\np1 <- ggplot(slrdata, aes(Date, Prepositions)) +\n  geom_point() +\n  theme_bw() +\n  labs(x = \"Year\") +\n  labs(y = \"Prepositions per 1,000 words\") +\n  geom_smooth()\np2 <- ggplot(slrdata, aes(Date, Prepositions)) +\n  geom_point() +\n  theme_bw() +\n  labs(x = \"Year\") +\n  labs(y = \"Prepositions per 1,000 words\") +\n  geom_smooth(method = \"lm\") # with linear model smoothing!\n# display plots\nggpubr::ggarrange(p1, p2, ncol = 2, nrow = 1)\n\n\n\n\nBefore beginning with the regression analysis, we will center the year. We center the values of year by subtracting each value from the mean of year. This can be useful when dealing with numeric variables because if we did not center year, we would get estimated values for year 0 (a year when English did not even exist yet). If a variable is centered, the regression provides estimates of the model refer to the mean of that numeric variable. In other words, centering can be very helpful, especially with respect to the interpretation of the results that regression models report.\n\n# center date\nslrdata$Date <- slrdata$Date - mean(slrdata$Date) \n\nWe will now begin the regression analysis by generating a first regression model and inspect its results.\n\n# create initial model\nm1.lm <- lm(Prepositions ~ Date, data = slrdata)\n# inspect results\nsummary(m1.lm)\n\n\nCall:\nlm(formula = Prepositions ~ Date, data = slrdata)\n\nResiduals:\n        Min          1Q      Median          3Q         Max \n-69.1012471 -13.8549421   0.5779091  13.3208913  62.8580401 \n\nCoefficients:\n                   Estimate      Std. Error   t value             Pr(>|t|)    \n(Intercept) 132.19009310987   0.83863748040 157.62483 < 0.0000000000000002 ***\nDate          0.01732180307   0.00726746646   2.38347             0.017498 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 19.4339648 on 535 degrees of freedom\nMultiple R-squared:  0.010507008,   Adjusted R-squared:  0.00865748837 \nF-statistic: 5.68093894 on 1 and 535 DF,  p-value: 0.017498081\n\n\nThe summary output starts by repeating the regression equation. Then, the model provides the distribution of the residuals. The residuals should be distributed normally with the absolute values of the Min and Max as well as the 1Q (first quartile) and 3Q (third quartile) being similar or ideally identical. In our case, the values are very similar which suggests that the residuals are distributed evenly and follow a normal distribution. The next part of the report is the coefficients table. The estimate for the intercept is the value of y at x = 0. The estimate for Date represents the slope of the regression line and tells us that with each year, the predicted frequency of prepositions increase by .01732 prepositions. The t-value is the Estimate divided by the standard error (Std. Error). Based on the t-value, the p-value can be calculated manually as shown below.\n\n# use pt function (which uses t-values and the degrees of freedom)\n2*pt(-2.383, nrow(slrdata)-1)\n\n[1] 0.0175196401501\n\n\nThe R2-values tell us how much variance is explained by our model. The baseline value represents a model that uses merely the mean. 0.0105 means that our model explains only 1.05 percent of the variance (0.010 x 100) - which is a tiny amount. The problem of the multiple R2 is that it will increase even if we add variables that explain almost no variance. Hence, multiple R2 encourages the inclusion of junk variables.\n\\[\\begin{equation}\nR^2 = R^2_{multiple} = 1 - \\frac{\\sum (y_i - \\hat{y_i})^2}{\\sum (y_i - \\bar y)^2}\n\\end{equation}\\]\nThe adjusted R2-value takes the number of predictors into account and, thus, the adjusted R2 will always be lower than the multiple R2. This is so because the adjusted R2 penalizes models for having predictors. The equation for the adjusted R2 below shows that the amount of variance that is explained by all the variables in the model (the top part of the fraction) must outweigh the inclusion of the number of variables (k) (lower part of the fraction). Thus, the adjusted R2 will decrease when variables are added that explain little or even no variance while it will increase if variables are added that explain a lot of variance.\n\\[\\begin{equation}\nR^2_{adjusted} = 1 - (\\frac{(1 - R^2)(n - 1)}{n - k - 1})\n\\end{equation}\\]\nIf there is a big difference between the two R2-values, then the model contains (many) predictors that do not explain much variance which is not good. The F-statistic and the associated p-value tell us that the model, despite explaining almost no variance, is still significantly better than an intercept-only base-line model (or using the overall mean to predict the frequency of prepositions per text).\nWe can test this and also see where the F-values comes from by comparing the\n\n# create intercept-only base-line model\nm0.lm <- lm(Prepositions ~ 1, data = slrdata)\n# compare the base-line and the more saturated model\nanova(m1.lm, m0.lm, test = \"F\")\n\nAnalysis of Variance Table\n\nModel 1: Prepositions ~ Date\nModel 2: Prepositions ~ 1\n  Res.Df         RSS Df   Sum of Sq       F   Pr(>F)  \n1    535 202058.2576                                  \n2    536 204203.8289 -1 -2145.57126 5.68094 0.017498 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe F- and p-values are exactly those reported by the summary which shows where the F-values comes from and what it means; namely it denote the difference between the base-line and the more saturated model.\nThe degrees of freedom associated with the residual standard error are the number of cases in the model minus the number of predictors (including the intercept). The residual standard error is square root of the sum of the squared residuals of the model divided by the degrees of freedom. Have a look at he following to clear this up:\n\n# DF = N - number of predictors (including intercept)\nDegreesOfFreedom <- nrow(slrdata)-length(coef(m1.lm))\n# sum of the squared residuals\nSumSquaredResiduals <- sum(resid(m1.lm)^2)\n# Residual Standard Error\nsqrt(SumSquaredResiduals/DegreesOfFreedom); DegreesOfFreedom\n\n[1] 19.4339647585\n\n\n[1] 535\n\n\nWe will now check if mathematical assumptions have been violated (homogeneity of variance) or whether the data contains outliers. We check this using diagnostic plots.\n\n# generate data\ndf2 <- data.frame(id = 1:length(resid(m1.lm)),\n                 residuals = resid(m1.lm),\n                 standard = rstandard(m1.lm),\n                 studend = rstudent(m1.lm))\n# generate plots\np1 <- ggplot(df2, aes(x = id, y = residuals)) + \n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +\n  geom_point() +\n  labs(y = \"Residuals\", x = \"Index\")\np2 <- ggplot(df2, aes(x = id, y = standard)) + \n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +\n  geom_point() +\n  labs(y = \"Standardized Residuals\", x = \"Index\")\np3 <- ggplot(df2, aes(x = id, y = studend)) + \n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +\n  geom_point() +\n  labs(y = \"Studentized Residuals\", x = \"Index\")\n# display plots\nggpubr::ggarrange(p1, p2, p3, ncol = 3, nrow = 1)\n\n\n\n\nThe left graph shows the residuals of the model (i.e., the differences between the observed and the values predicted by the regression model). The problem with this plot is that the residuals are not standardized and so they cannot be compared to the residuals of other models. To remedy this deficiency, residuals are normalized by dividing the residuals by their standard deviation. Then, the normalized residuals can be plotted against the observed values (center panel). In this way, not only are standardized residuals obtained, but the values of the residuals are transformed into z-values, and one can use the z-distribution to find problematic data points. There are three rules of thumb regarding finding problematic data points through standardized residuals [6]:\n\nPoints with values higher than 3.29 should be removed from the data.\nIf more than 1% of the data points have values higher than 2.58, then the error rate of our model is too high.\nIf more than 5% of the data points have values greater than 1.96, then the error rate of our model is too high.\n\nThe right panel shows the * studentized residuals* (adjusted predicted values: each data point is divided by the standard error of the residuals). In this way, it is possible to use Student’s t-distribution to diagnose our model.\nAdjusted predicted values are residuals of a special kind: the model is calculated without a data point and then used to predict this data point. The difference between the observed data point and its predicted value is then called the adjusted predicted value. In summary, studentized residuals are very useful because they allow us to identify influential data points.\nThe plots show that there are two potentially problematic data points (the top-most and bottom-most point). These two points are clearly different from the other data points and may therefore be outliers. We will test later if these points need to be removed.\nWe will now generate more diagnostic plots.\n\n# generate plots\nautoplot(m1.lm) + \n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) \n\n\n\n\nThe diagnostic plots are very positive and we will go through why this is so for each panel. The graph in the upper left panel is useful for finding outliers or for determining the correlation between residuals and predicted values: when a trend becomes visible in the line or points (e.g., a rising trend or a zigzag line), then this would indicate that the model would be problematic (in such cases, it can help to remove data points that are too influential (outliers)).\nThe graphic in the upper right panel indicates whether the residuals are normally distributed (which is desirable) or whether the residuals do not follow a normal distribution. If the points lie on the line, the residuals follow a normal distribution. For example, if the points are not on the line at the top and bottom, it shows that the model does not predict small and large values well and that it therefore does not have a good fit.\nThe graphic in the lower left panel provides information about homoscedasticity. Homoscedasticity means that the variance of the residuals remains constant and does not correlate with any independent variable. In unproblematic cases, the graphic shows a flat line. If there is a trend in the line, we are dealing with heteroscedasticity, that is, a correlation between independent variables and the residuals, which is very problematic for regressions.\nThe graph in the lower right panel shows problematic influential data points that disproportionately affect the regression (this would be problematic). If such influential data points are present, they should be either weighted (one could generate a robust rather than a simple linear regression) or they must be removed. The graph displays Cook’s distance, which shows how the regression changes when a model without this data point is calculated. The cook distance thus shows the influence a data point has on the regression as a whole. Data points that have a Cook’s distance value greater than 1 are problematic [6].\nThe so-called leverage is also a measure that indicates how strongly a data point affects the accuracy of the regression. Leverage values range between 0 (no influence) and 1 (strong influence: suboptimal!). To test whether a specific data point has a high leverage value, we calculate a cut-off point that indicates whether the leverage is too strong or still acceptable. The following two formulas are used for this:\n\\[\\begin{equation}\nLeverage = \\frac{3(k + 1)}{n} |  \\frac{2(k + 1)}{n}\n\\end{equation}\\]\nWe will look more closely at leverage in the context of multiple linear regression and will therefore end the current analysis by summarizing the results of the regression analysis in a table.\n\n# create summary table\nslrsummary(m1.lm)  \n\n\n\n\n\n\nResults of a simple linear regression analysis.\n\n\nParametersEstimatePearson's rStd. Errort valuePr(>|t|)P-value sig.(Intercept)132.190.84157.620p < .001***Date0.020.10.012.380.0175p < .05*Model statisticsValueNumber of cases in model537Residual standard error on 535 DF19.43Multiple R-squared0.0105Adjusted R-squared0.0087F-statistic (1, 535)5.68Model p-value0.0175\n\n\nAn alternative but less informative summary table of the results of a regression analysis can be generated using the tab_model function from the sjPlot package [13] (as is shown below).\n\n# generate summary table\nsjPlot::tab_model(m1.lm) \n\n\n\n\n \nPrepositions\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n132.19\n130.54 – 133.84\n<0.001\n\n\nDate\n0.02\n0.00 – 0.03\n0.017\n\n\nObservations\n537\n\n\nR2 / R2 adjusted\n0.011 / 0.009\n\n\n\n\n\n\n\nTypically, the results of regression analyses are presented in such tables as they include all important measures of model quality and significance, as well as the magnitude of the effects.\nIn addition, the results of simple linear regressions should be summarized in writing.\nWe can use the reports package [14] to summarize the analysis.\n\nreport::report(m1.lm)\n\nWe fitted a linear model (estimated using OLS) to predict Prepositions with Date (formula: Prepositions ~ Date). The model explains a statistically significant and very weak proportion of variance (R2 = 0.01, F(1, 535) = 5.68, p = 0.017, adj. R2 = 8.66e-03). The model's intercept, corresponding to Date = 0, is at 132.19 (95% CI [130.54, 133.84], t(535) = 157.62, p < .001). Within this model:\n\n  - The effect of Date is statistically significant and positive (beta = 0.02, 95% CI [3.05e-03, 0.03], t(535) = 2.38, p = 0.017; Std. beta = 0.10, 95% CI [0.02, 0.19])\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using the Wald approximation.\n\n\nWe can use this output to write up a final report:\nA simple linear regression has been fitted to the data. A visual assessment of the model diagnostic graphics did not indicate any problematic or disproportionately influential data points (outliers) and performed significantly better compared to an intercept-only base line model but only explained .87 percent of the variance (adjusted R2: .0087, F-statistic (1, 535): 5,68, p-value: 0.0175*). The final minimal adequate linear regression model is based on 537 data points and confirms a significant and positive correlation between the year in which the text was written and the relative frequency of prepositions (coefficient estimate: .02 (standardized : 0.10, 95% CI [0.02, 0.19]), SE: 0.01, t-value535: 2.38, p-value: .0175*). Standardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using the Wald approximation.\n\n\nExample 2: Teaching Styles\nIn the previous example, we dealt with two numeric variables, while the following example deals with a categorical independent variable and a numeric dependent variable. The ability for regressions to handle very different types of variables makes regressions a widely used and robust method of analysis.\nIn this example, we are dealing with two groups of students that have been randomly assigned to be exposed to different teaching methods. Both groups undergo a language learning test after the lesson with a maximum score of 20 points.\nThe question that we will try to answer is whether the students in group A have performed significantly better than those in group B which would indicate that the teaching method to which group A was exposed works better than the teaching method to which group B was exposed.\nLet’s move on to implementing the regression in R. In a first step, we load the data set and inspect its structure.\n\n# load data\nslrdata2  <- base::readRDS(url(\"https://slcladal.github.io/data/sgd.rda\", \"rb\"))\n\n\n\n\n\n\nFirst 15 rows of the slrdata2 data.\n\n\nGroupScoreA15A12A11A18A15A15A9A19A14A13A11A12A18A15A16\n\n\nNow, we graphically display the data. In this case, a boxplot represents a good way to visualize the data.\n\n# extract means\nslrdata2 %>%\n  dplyr::group_by(Group) %>%\n  dplyr::mutate(Mean = round(mean(Score), 1), SD = round(sd(Score), 1)) %>%\n  ggplot(aes(Group, Score)) + \n  geom_boxplot(fill=c(\"orange\", \"darkgray\")) +\n  geom_text(aes(label = paste(\"M = \", Mean, sep = \"\"), y = 1)) +\n  geom_text(aes(label = paste(\"SD = \", SD, sep = \"\"), y = 0)) +\n  theme_bw(base_size = 15) +\n  labs(x = \"Group\") +                      \n  labs(y = \"Test score (Points)\", cex = .75) +   \n  coord_cartesian(ylim = c(0, 20)) +  \n  guides(fill = FALSE)                \n\n\n\n\nThe data indicate that group A did significantly better than group B. We will test this impression by generating the regression model and creating the model and extracting the model summary.\n\n# generate regression model\nm2.lm <- lm(Score ~ Group, data = slrdata2) \n# inspect results\nsummary(m2.lm)                             \n\n\nCall:\nlm(formula = Score ~ Group, data = slrdata2)\n\nResiduals:\n        Min          1Q      Median          3Q         Max \n-6.76666667 -1.93333333  0.15000000  2.06666667  6.23333333 \n\nCoefficients:\n                Estimate   Std. Error  t value               Pr(>|t|)    \n(Intercept) 14.933333333  0.534571121 27.93517 < 0.000000000000000222 ***\nGroupB      -3.166666667  0.755997730 -4.18873            0.000096692 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.92796662 on 58 degrees of freedom\nMultiple R-squared:  0.232249929,   Adjusted R-squared:  0.219012859 \nF-statistic:  17.545418 on 1 and 58 DF,  p-value: 0.0000966923559\n\n\nThe model summary reports that Group A performed significantly better compared with Group B. This is shown by the fact that the p-value (the value in the column with the header (Pr(>|t|)) is smaller than .001 as indicated by the three * after the p-values). Also, the negative Estimate for Group B indicates that Group B has lower scores than Group A. We will now generate the diagnostic graphics.3\n\npar(mfrow = c(1, 3))        # plot window: 1 plot/row, 3 plots/column\nplot(resid(m2.lm))     # generate diagnostic plot\nplot(rstandard(m2.lm)) # generate diagnostic plot\nplot(rstudent(m2.lm)); par(mfrow = c(1, 1))  # restore normal plot window\n\n\n\n\nThe graphics do not indicate outliers or other issues, so we can continue with more diagnostic graphics.\n\npar(mfrow = c(2, 2)) # generate a plot window with 2x2 panels\nplot(m2.lm); par(mfrow = c(1, 1)) # restore normal plot window\n\n\n\n\nThese graphics also show no problems. In this case, the data can be summarized in the next step.\n\n# tabulate results\nslrsummary(m2.lm)\n\n\n\n\n\n\nResults of the regression analysis.\n\n\nParametersEstimatePearson's rStd. Errort valuePr(>|t|)P-value sig.(Intercept)14.930.5327.940p < .001***GroupB-3.170.480.76-4.190.0001p < .001***Model statisticsValueNumber of cases in model60Residual standard error on 58 DF2.93Multiple R-squared0.2322Adjusted R-squared0.219F-statistic (1, 58)17.55Model p-value0.0001\n\n\nWe can use the reports package [14] to summarize the analysis.\n\nreport::report(m2.lm)\n\nWe fitted a linear model (estimated using OLS) to predict Score with Group (formula: Score ~ Group). The model explains a statistically significant and moderate proportion of variance (R2 = 0.23, F(1, 58) = 17.55, p < .001, adj. R2 = 0.22). The model's intercept, corresponding to Group = A, is at 14.93 (95% CI [13.86, 16.00], t(58) = 27.94, p < .001). Within this model:\n\n  - The effect of Group [B] is statistically significant and negative (beta = -3.17, 95% CI [-4.68, -1.65], t(58) = -4.19, p < .001; Std. beta = -0.96, 95% CI [-1.41, -0.50])\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using the Wald approximation.\n\n\nWe can use this output to write up a final report:\nA simple linear regression was fitted to the data. A visual assessment of the model diagnostics did not indicate any problematic or disproportionately influential data points (outliers). The final linear regression model is based on 60 data points, performed significantly better than an intercept-only base line model (F (1, 58): 17.55, p-value <. 001\\(***\\)), and reported that the model explained 21.9 percent of variance which confirmed a good model fit. According to this final model, group A scored significantly better on the language learning test than group B (coefficient: -3.17, 95% CI [-4.68, -1.65], Std. : -0.96, 95% CI [-1.41, -0.50], SE: 0.48, t-value58: -4.19, p-value <. 001\\(***\\)). Standardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using the Wald approximation."
  },
  {
    "objectID": "regression.html#multiple-linear-regression",
    "href": "regression.html#multiple-linear-regression",
    "title": "Fixed- and Mixed-Effects Regression Models in R",
    "section": "Multiple Linear Regression",
    "text": "Multiple Linear Regression\nIn contrast to simple linear regression, which estimates the effect of a single predictor, multiple linear regression estimates the effect of various predictor (see the equation below). A multiple linear regression can thus test the effects of various predictors simultaneously.\n\\[\\begin{equation}\n\nf_{(x)} = \\alpha + \\beta_{1}x_{i} + \\beta_{2}x_{i+1} + \\dots + \\beta_{n}x_{i+n} + \\epsilon\n\n\\end{equation}\\]\nThere exists a wealth of literature focusing on multiple linear regressions and the concepts it is based on. For instance, there are [2], [3], [4], [5], [6], [7], [8], [12] and [9] to name just a few. Introductions to regression modeling in R are [10], [11], [7], or [8].\nThe model diagnostics we are dealing with here are partly identical to the diagnostic methods discussed in the section on simple linear regression. Because of this overlap, diagnostics will only be described in more detail if they have not been described in the section on simple linear regression.\n\n\n\n\nEXCURSION\n\n\n\n\n\n\n`\n\n\nA note on sample size and power\n\nA brief note on minimum necessary sample or data set size appears necessary here. Although there appears to be a general assumption that 25 data points per group are sufficient, this is not necessarily correct (it is merely a general rule of thumb that is actually often incorrect). Such rules of thumb are inadequate because the required sample size depends on the number of variables in a given model, the size of the effect and the variance of the effect - in other words, the minimum necessary sample size relates to statistical power (see here for a tutorial on power). If a model contains many variables, then this requires a larger sample size than a model which only uses very few predictors.\nAlso, to detect an effect with a very minor effect size, one needs a substantially larger sample compared to cases where the effect is very strong. In fact, when dealing with small effects, model require a minimum of 600 cases to reliably detect these effects. Finally, effects that are very robust and do not vary much require a much smaller sample size compared with effects that are spurious and vary substantially. Since the sample size depends on the effect size and variance as well as the number of variables, there is no final one-size-fits-all answer to what the best sample size is.\nAnother, slightly better but still incorrect, rule of thumb is that the more data, the better. This is not correct because models based on too many cases are prone for overfitting and thus report correlations as being significant that are not. However, given that there are procedures that can correct for overfitting, larger data sets are still preferable to data sets that are simply too small to warrant reliable results. In conclusion, it remains true that the sample size depends on the effect under investigation.\n\n\n`\n\nDespite there being no ultimate rule of thumb, [6], based on [15], provide data-driven suggestions for the minimal size of data required for regression models that aim to find medium sized effects (k = number of predictors; categorical variables with more than two levels should be transformed into dummy variables):\n\nIf one is merely interested in the overall model fit (something I have not encountered), then the sample size should be at least 50 + k (k = number of predictors in model).\nIf one is only interested in the effect of specific variables, then the sample size should be at least 104 + k (k = number of predictors in model).\nIf one is only interested in both model fit and the effect of specific variables, then the sample size should be at least the higher value of 50 + k or 104 + k (k = number of predictors in model).\n\nYou will see in the R code below that there is already a function that tests whether the sample size is sufficient.\n\nExample: Gifts and Availability\nThe example we will go through here is taken from [6]. In this example, the research question is if the money that men spend on presents for women depends on the women’s attractiveness and their relationship status. To answer this research question, we will implement a multiple linear regression and start by loading the data and inspect its structure and properties.\n\n# load data\nmlrdata  <- base::readRDS(url(\"https://slcladal.github.io/data/mld.rda\", \"rb\"))\n\n\n\n\n\n\nFirst 15 rows of the mlrdata.\n\n\nstatusattractionmoneyRelationshipNotInterested86.33RelationshipNotInterested45.58RelationshipNotInterested68.43RelationshipNotInterested52.93RelationshipNotInterested61.86RelationshipNotInterested48.47RelationshipNotInterested32.79RelationshipNotInterested35.91RelationshipNotInterested30.98RelationshipNotInterested44.82RelationshipNotInterested35.05RelationshipNotInterested64.49RelationshipNotInterested54.50RelationshipNotInterested61.48RelationshipNotInterested55.51\n\n\nThe data set consist of three variables stored in three columns. The first column contains the relationship status of the present giver (in this study this were men), the second whether the man is interested in the woman (the present receiver in this study), and the third column represents the money spend on the present. The data set represents 100 cases and the mean amount of money spend on a present is 88.38 dollars. In a next step, we visualize the data to get a more detailed impression of the relationships between variables.\n\n# create plots\np1 <- ggplot(mlrdata, aes(status, money)) +\n  geom_boxplot() + \n  theme_bw()\n# plot 2\np2 <- ggplot(mlrdata, aes(attraction, money)) +\n  geom_boxplot() +\n  theme_bw()\n# plot 3\np3 <- ggplot(mlrdata, aes(x = money)) +\n  geom_histogram(aes(y=..density..)) +            \n  theme_bw() +         \n  geom_density(alpha=.2, fill = \"gray50\") \n# plot 4\np4 <- ggplot(mlrdata, aes(status, money)) +\n  geom_boxplot(aes(fill = factor(status))) + \n  scale_fill_manual(values = c(\"grey30\", \"grey70\")) + \n  facet_wrap(~ attraction) + \n  guides(fill = \"none\") +\n  theme_bw()\n# show plots\nvip::grid.arrange(grobs = list(p1, p2, p3, p4), widths = c(1, 1), layout_matrix = rbind(c(1, 2), c(3, 4)))\n\n\n\n\nThe upper left figure consists of a boxplot which shows how much money was spent by relationship status. The figure suggests that men spend more on women if they are not in a relationship. The next figure shows the relationship between the money spend on presents and whether or not the men were interested in the women.\nThe boxplot in the upper right panel suggests that men spend substantially more on women if the men are interested in them. The next figure depicts the distribution of the amounts of money spend on the presents for the women. In addition, the figure indicates the existence of two outliers (dots in the boxplot)\nThe histogram in the lower left panel shows that, although the mean amount of money spent on presents is 88.38 dollars, the distribution peaks around 50 dollars indicating that on average, men spend about 50 dollars on presents. Finally, we will plot the amount of money spend on presents against relationship status by attraction in order to check whether the money spent on presents is affected by an interaction between attraction and relationship status.\nThe boxplot in the lower right panel confirms the existence of an interaction (a non-additive term) as men only spend more money on women if the men single and they are interested in the women. If men are not interested in the women, then the relationship has no effect as they spend an equal amount of money on the women regardless of whether they are in a relationship or not.\nWe will now start to implement the regression model. In a first step, we create two saturated models that contain all possible predictors (main effects and interactions). The two models are identical but one is generated with the lm and the other with the glm function as these functions offer different model parameters in their output.\n\nm1.mlr = lm(                      # generate lm regression object\n  money ~ 1 + attraction*status,  # def. regression formula (1 = intercept)\n  data = mlrdata)                 # def. data\nm1.glm = glm(                     # generate glm regression object\n  money ~ 1 + attraction*status,  # def. regression formula (1 = intercept)\n  family = gaussian,              # def. linkage function\n  data = mlrdata)                 # def. data\n\nAfter generating the saturated models we can now start with the model fitting. Model fitting refers to a process that aims at find the model that explains a maximum of variance with a minimum of predictors [see 6]. Model fitting is therefore based on the principle of parsimony which is related to Occam’s razor according to which explanations that require fewer assumptions are more likely to be true.\n\n\nAutomatic Model Fitting and Why You Should Not Use It\nIn this section, we will use a step-wise step-down procedure that uses decreases in AIC (Akaike Information Criterion) as the criterion to minimize the model in a step-wise manner. This procedure aims at finding the model with the lowest AIC values by evaluating - step-by-step - whether the removal of a predictor (term) leads to a lower AIC value.\nWe use this method here just so that you know it exists and how to implement it but you should rather avoid using automated model fitting. The reason for avoiding automated model fitting is that the algorithm only checks if the AIC has decreased but not if the model is stable or reliable. Thus, automated model fitting has the problem that you can never be sure that the way that lead you to the final model is reliable and that all models were indeed stable. Imagine you want to climb down from a roof top and you have a ladder. The problem is that you do not know if and how many steps are broken. This is similar to using automated model fitting. In other sections, we will explore better methods to fit models (manual step-wise step-up and step-down procedures, for example).\nThe AIC is calculated using the equation below. The lower the AIC value, the better the balance between explained variance and the number of predictors. AIC values can and should only be compared for models that are fit on the same data set with the same (number of) cases (LL stands for logged likelihood or LogLikelihood and k represents the number of predictors in the model (including the intercept); the LL represents a measure of how good the model fits the data).\n\\[\\begin{equation}\nAkaike Information Criterion (AIC) = -2LL + 2k\n\\end{equation}\\]\nAn alternative to the AIC is the BIC (Bayesian Information Criterion). Both AIC and BIC penalize models for including variables in a model. The penalty of the BIC is bigger than the penalty of the AIC and it includes the number of cases in the model (LL stands for logged likelihood or LogLikelihood, k represents the number of predictors in the model (including the intercept), and N represents the number of cases in the model).\n\\[\\begin{equation}\nBayesian Information Criterion (BIC) = -2LL + 2k * log(N)\n\\end{equation}\\]\nInteractions are evaluated first and only if all insignificant interactions have been removed would the procedure start removing insignificant main effects (that are not part of significant interactions). Other model fitting procedures (forced entry, step-wise step up, hierarchical) are discussed during the implementation of other regression models. We cannot discuss all procedures here as model fitting is rather complex and a discussion of even the most common procedures would to lengthy and time consuming at this point. It is important to note though that there is not perfect model fitting procedure and automated approaches should be handled with care as they are likely to ignore violations of model parameters that can be detected during manual - but time consuming - model fitting procedures. As a general rule of thumb, it is advisable to fit models as carefully and deliberately as possible. We will now begin to fit the model.\n\n# automated AIC based model fitting\nstep(m1.mlr, direction = \"both\")\n\nStart:  AIC=592.52\nmoney ~ 1 + attraction * status\n\n                    Df   Sum of Sq         RSS         AIC\n<none>                             34557.56428 592.5211556\n- attraction:status  1 24947.25481 59504.81909 644.8642395\n\n\n\nCall:\nlm(formula = money ~ 1 + attraction * status, data = mlrdata)\n\nCoefficients:\n                         (Intercept)               attractionNotInterested  \n                             99.1548                              -47.6628  \n                        statusSingle  attractionNotInterested:statusSingle  \n                             57.6928                              -63.1788  \n\n\nThe automated model fitting procedure informs us that removing predictors has not caused a decrease in the AIC. The saturated model is thus also the final minimal adequate model. We will now inspect the final minimal model and go over the model report.\n\nm2.mlr = lm(                       # generate lm regression object\n  money ~ (status + attraction)^2, # def. regression formula\n  data = mlrdata)                  # def. data\nm2.glm = glm(                      # generate glm regression object\n  money ~ (status + attraction)^2, # def. regression formula\n  family = gaussian,               # def. linkage function\n  data = mlrdata)                  # def. data\n# inspect final minimal model\nsummary(m2.mlr)\n\n\nCall:\nlm(formula = money ~ (status + attraction)^2, data = mlrdata)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-45.0760 -14.2580   0.4596  11.9315  44.1424 \n\nCoefficients:\n                                         Estimate   Std. Error  t value\n(Intercept)                           99.15480000   3.79459947 26.13050\nstatusSingle                          57.69280000   5.36637403 10.75080\nattractionNotInterested              -47.66280000   5.36637403 -8.88175\nstatusSingle:attractionNotInterested -63.17880000   7.58919893 -8.32483\n                                                   Pr(>|t|)    \n(Intercept)                          < 0.000000000000000222 ***\nstatusSingle                         < 0.000000000000000222 ***\nattractionNotInterested                 0.00000000000003751 ***\nstatusSingle:attractionNotInterested    0.00000000000058085 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 18.9729973 on 96 degrees of freedom\nMultiple R-squared:  0.852041334,   Adjusted R-squared:  0.847417626 \nF-statistic: 184.276619 on 3 and 96 DF,  p-value: < 0.0000000000000002220446\n\n\nThe first element of the report is called Call and it reports the regression formula of the model. Then, the report provides the residual distribution (the range, median and quartiles of the residuals) which allows drawing inferences about the distribution of differences between observed and expected values. If the residuals are distributed non-normally, then this is a strong indicator that the model is unstable and unreliable because mathematical assumptions on which the model is based are violated.\nNext, the model summary reports the most important part: a table with model statistics of the fixed-effects structure of the model. The table contains the estimates (coefficients of the predictors), standard errors, t-values, and the p-values which show whether a predictor significantly correlates with the dependent variable that the model investigates.\nAll main effects (status and attraction) as well as the interaction between status and attraction is reported as being significantly correlated with the dependent variable (money). An interaction occurs if a correlation between the dependent variable and a predictor is affected by another predictor.\nThe top most term is called intercept and has a value of 99.15 which represents the base estimate to which all other estimates refer. To exemplify what this means, let us consider what the model would predict that a man would spend on a present if he interested in the woman but he is also in a relationship. The amount he would spend (based on the model would be 99.15 dollars (which is the intercept). This means that the intercept represents the predicted value if all predictors take the base or reference level. And since being in relationship but being interested are the case, and because the interaction does not apply, the predicted value in our example is exactly the intercept (see below).\n\n#intercept  Single  NotInterested  Single:NotInterested\n99.15     + 57.69  + 0           + 0     # 156.8 single + interested\n\n[1] 156.84\n\n99.15     + 57.69  - 47.66       - 63.18 # 46.00 single + not interested\n\n[1] 46\n\n99.15     - 0      + 0           - 0     # 99.15 relationship + interested\n\n[1] 99.15\n\n99.15     - 0      - 47.66       - 0     # 51.49 relationship + not interested\n\n[1] 51.49\n\n\nNow, let us consider what a man would spend if he is in a relationship and he is not attracted to the women. In that case, the model predicts that the man would spend only 51.49 dollars on a present: the intercept (99.15) minus 47.66 because the man is not interested (and no additional subtraction because the interaction does not apply).\nWe can derive the same results easier using the predict function.\n\n# make prediction based on the model for original data\nprediction <- predict(m2.mlr, newdata = mlrdata)\n# inspect predictions\ntable(round(prediction,2))\n\n\n 46.01  51.49  99.15 156.85 \n    25     25     25     25 \n\n\nBelow the table of coefficients, the regression summary reports model statistics that provide information about how well the model performs. The difference between the values and the values in the coefficients table is that the model statistics refer to the model as a whole rather than focusing on individual predictors.\nThe multiple R2-value is a measure of how much variance the model explains. A multiple R2-value of 0 would inform us that the model does not explain any variance while a value of .852 mean that the model explains 85.2 percent of the variance. A value of 1 would inform us that the model explains 100 percent of the variance and that the predictions of the model match the observed values perfectly. Multiplying the multiple R2-value thus provides the percentage of explained variance. Models that have a multiple R2-value equal or higher than .05 are deemed substantially significant [see 16]. It has been claimed that models should explain a minimum of 5 percent of variance but this is problematic as it is not uncommon for models to have very low explanatory power while still performing significantly and systematically better than chance. In addition, the total amount of variance is negligible in cases where one is interested in very weak but significant effects. It is much more important for model to perform significantly better than minimal base-line models because if this is not the case, then the model does not have any predictive and therefore no explanatory power.\nThe adjusted R2-value considers the amount of explained variance in light of the number of predictors in the model (it is thus somewhat similar to the AIC and BIC) and informs about how well the model would perform if it were applied to the population that the sample is drawn from. Ideally, the difference between multiple and adjusted R2-value should be very small as this means that the model is not overfitted. If, however, the difference between multiple and adjusted R2-value is substantial, then this would strongly suggest that the model is unstable and overfitted to the data while being inadequate for drawing inferences about the population. Differences between multiple and adjusted R2-values indicate that the data contains outliers that cause the distribution of the data on which the model is based to differ from the distributions that the model mathematically requires to provide reliable estimates. The difference between multiple and adjusted R2-value in our model is very small (85.2-84.7=.05) and should not cause concern.\nBefore continuing, we will calculate the confidence intervals of the coefficients.\n\n# extract confidence intervals of the coefficients\nconfint(m2.mlr)\n\n                                              2.5 %         97.5 %\n(Intercept)                           91.6225795890 106.6870204110\nstatusSingle                          47.0406317400  68.3449682600\nattractionNotInterested              -58.3149682600 -37.0106317400\nstatusSingle:attractionNotInterested -78.2432408219 -48.1143591781\n\n# create and compare baseline- and minimal adequate model\nm0.mlr <- lm(money ~1, data = mlrdata)\nanova(m0.mlr, m2.mlr)\n\nAnalysis of Variance Table\n\nModel 1: money ~ 1\nModel 2: money ~ (status + attraction)^2\n  Res.Df          RSS Df   Sum of Sq         F                 Pr(>F)    \n1     99 233562.28650                                                    \n2     96  34557.56428  3 199004.7222 184.27662 < 0.000000000000000222 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nNow, we compare the final minimal adequate model to the base-line model to test whether then final model significantly outperforms the baseline model.\n\n# compare baseline- and minimal adequate model\nAnova(m0.mlr, m2.mlr, type = \"III\")\n\nAnova Table (Type III tests)\n\nResponse: money\n                 Sum Sq Df    F value                 Pr(>F)    \n(Intercept) 781015.8300  1 2169.64133 < 0.000000000000000222 ***\nResiduals    34557.5643 96                                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe comparison between the two model confirms that the minimal adequate model performs significantly better (makes significantly more accurate estimates of the outcome variable) compared with the baseline model.\n\n\nOutlier Detection\nAfter implementing the multiple regression, we now need to look for outliers and perform the model diagnostics by testing whether removing data points disproportionately decreases model fit. To begin with, we generate diagnostic plots.\n\n# generate plots\nautoplot(m2.mlr) + \n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +\n  theme_bw()\n\n\n\n\nThe plots do not show severe problems such as funnel shaped patterns or drastic deviations from the diagonal line in Normal Q-Q plot (have a look at the explanation of what to look for and how to interpret these diagnostic plots in the section on simple linear regression) but data points 52, 64, and 83 are repeatedly indicated as potential outliers.\n\n# determine a cutoff for data points that have D-values higher than 4/(n-k-1)\ncutoff <- 4/((nrow(mlrdata)-length(m2.mlr$coefficients)-2))\n# start plotting\npar(mfrow = c(1, 2))           # display plots in 3 rows/2 columns\nqqPlot(m2.mlr, main=\"QQ Plot\") # create qq-plot\n\n[1] 52 83\n\nplot(m2.mlr, which=4, cook.levels = cutoff); par(mfrow = c(1, 1))\n\n\n\n\nThe graphs indicate that data points 52, 64, and 83 may be problematic. We will therefore statistically evaluate whether these data points need to be removed. In order to find out which data points require removal, we extract the influence measure statistics and add them to out data set.\n\n# extract influence statistics\ninfl <- influence.measures(m2.mlr)\n# add infl. statistics to data\nmlrdata <- data.frame(mlrdata, infl[[1]], infl[[2]])\n# annotate too influential data points\nremove <- apply(infl$is.inf, 1, function(x) {\n  ifelse(x == TRUE, return(\"remove\"), return(\"keep\")) } )\n# add annotation to data\nmlrdata <- data.frame(mlrdata, remove)\n# number of rows before removing outliers\nnrow(mlrdata)\n\n[1] 100\n\n# remove outliers\nmlrdata <- mlrdata[mlrdata$remove == \"keep\", ]\n# number of rows after removing outliers\nnrow(mlrdata)\n\n[1] 98\n\n\nThe difference in row in the data set before and after removing data points indicate that two data points which represented outliers have been removed.\n\n\n\nNOTEIn general, outliers should not simply be removed unless there are good reasons for it (this could be that the outliers represent measurement errors). If a data set contains outliers, one should rather switch to methods that are better at handling outliers, e.g. by using weights to account for data points with high leverage. One alternative would be to switch to a robust regression (see here). However, here we show how to proceed by removing outliers as this is a common, though potentially problematic, method of dealing with outliers.\n\n\n\n\n\n\n\n\nRerun Regression\nAs we have decided to remove the outliers which means that we are now dealing with a different data set, we need to rerun the regression analysis. As the steps are identical to the regression analysis performed above, the steps will not be described in greater detail.\n\n# recreate regression models on new data\nm0.mlr = lm(money ~ 1, data = mlrdata)\nm0.glm = glm(money ~ 1, family = gaussian, data = mlrdata)\nm1.mlr = lm(money ~ (status + attraction)^2, data = mlrdata)\nm1.glm = glm(money ~ status * attraction, family = gaussian,\n             data = mlrdata)\n# automated AIC based model fitting\nstep(m1.mlr, direction = \"both\")\n\nStart:  AIC=570.29\nmoney ~ (status + attraction)^2\n\n                    Df   Sum of Sq         RSS         AIC\n<none>                             30411.31714 570.2850562\n- status:attraction  1 21646.86199 52058.17914 620.9646729\n\n\n\nCall:\nlm(formula = money ~ (status + attraction)^2, data = mlrdata)\n\nCoefficients:\n                         (Intercept)                          statusSingle  \n                          99.1548000                            55.8535333  \n             attractionNotInterested  statusSingle:attractionNotInterested  \n                         -47.6628000                           -59.4613667  \n\n\n\n# create new final models\nm2.mlr = lm(money ~ (status + attraction)^2, data = mlrdata)\nm2.glm = glm(money ~ status * attraction, family = gaussian,\n             data = mlrdata)\n# inspect final minimal model\nsummary(m2.mlr)\n\n\nCall:\nlm(formula = money ~ (status + attraction)^2, data = mlrdata)\n\nResiduals:\n         Min           1Q       Median           3Q          Max \n-35.76416667 -13.50520000  -0.98948333  10.59887500  38.77166667 \n\nCoefficients:\n                                         Estimate   Std. Error  t value\n(Intercept)                           99.15480000   3.59735820 27.56323\nstatusSingle                          55.85353333   5.14015367 10.86612\nattractionNotInterested              -47.66280000   5.08743275 -9.36873\nstatusSingle:attractionNotInterested -59.46136667   7.26927504 -8.17982\n                                                   Pr(>|t|)    \n(Intercept)                          < 0.000000000000000222 ***\nstatusSingle                         < 0.000000000000000222 ***\nattractionNotInterested               0.0000000000000040429 ***\nstatusSingle:attractionNotInterested  0.0000000000013375166 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 17.986791 on 94 degrees of freedom\nMultiple R-squared:  0.857375902,   Adjusted R-squared:  0.852824069 \nF-statistic: 188.358387 on 3 and 94 DF,  p-value: < 0.0000000000000002220446\n\n\n\n# extract confidence intervals of the coefficients\nconfint(m2.mlr)\n\n                                              2.5 %         97.5 %\n(Intercept)                           92.0121609656 106.2974390344\nstatusSingle                          45.6476377202  66.0594289465\nattractionNotInterested              -57.7640169936 -37.5615830064\nstatusSingle:attractionNotInterested -73.8946826590 -45.0280506744\n\n\n\n# compare baseline with final model\nanova(m0.mlr, m2.mlr)\n\nAnalysis of Variance Table\n\nModel 1: money ~ 1\nModel 2: money ~ (status + attraction)^2\n  Res.Df          RSS Df   Sum of Sq         F                 Pr(>F)    \n1     97 213227.06081                                                    \n2     94  30411.31714  3 182815.7437 188.35839 < 0.000000000000000222 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n# compare baseline with final model\nAnova(m0.mlr, m2.mlr, type = \"III\")\n\nAnova Table (Type III tests)\n\nResponse: money\n                 Sum Sq Df    F value                 Pr(>F)    \n(Intercept) 760953.2107  1 2352.07181 < 0.000000000000000222 ***\nResiduals    30411.3171 94                                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nAdditional Model Diagnostics\nAfter rerunning the regression analysis on the updated data set, we again create diagnostic plots in order to check whether there are potentially problematic data points.\n\n# generate plots\nautoplot(m2.mlr) + \n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +\n  theme_bw()\n\n\n\n\n\n# determine a cutoff for data points that have\n# D-values higher than 4/(n-k-1)\ncutoff <- 4/((nrow(mlrdata)-length(m2.mlr$coefficients)-2))\n# start plotting\npar(mfrow = c(1, 2))           # display plots in 1 row/2 columns\nqqPlot(m2.mlr, main=\"QQ Plot\") # create qq-plot\n\n84 88 \n82 86 \n\nplot(m2.mlr, which=4, cook.levels = cutoff); par(mfrow = c(1, 1))\n\n\n\n\nAlthough the diagnostic plots indicate that additional points may be problematic, but these data points deviate substantially less from the trend than was the case with the data points that have already been removed. To make sure that retaining the data points that are deemed potentially problematic by the diagnostic plots, is acceptable, we extract diagnostic statistics and add them to the data.\n\n# add model diagnostics to the data\nmlrdata <- mlrdata %>%\n  dplyr::mutate(residuals = resid(m2.mlr),\n                standardized.residuals = rstandard(m2.mlr),\n                studentized.residuals = rstudent(m2.mlr),\n                cooks.distance = cooks.distance(m2.mlr),\n                dffit = dffits(m2.mlr),\n                leverage = hatvalues(m2.mlr),\n                covariance.ratios = covratio(m2.mlr),\n                fitted = m2.mlr$fitted.values)\n\nWe can now use these diagnostic statistics to create more precise diagnostic plots.\n\n# plot 5\np5 <- ggplot(mlrdata,\n             aes(studentized.residuals)) +\n  theme(legend.position = \"none\")+\n  geom_histogram(aes(y=..density..),\n                 binwidth = .2,\n                 colour=\"black\",\n                 fill=\"gray90\") +\n  labs(x = \"Studentized Residual\", y = \"Density\") +\n  stat_function(fun = dnorm,\n                args = list(mean = mean(mlrdata$studentized.residuals, na.rm = TRUE),\n                            sd = sd(mlrdata$studentized.residuals, na.rm = TRUE)),\n                colour = \"red\", size = 1) +\n  theme_bw(base_size = 8)\n# plot 6\np6 <- ggplot(mlrdata, aes(fitted, studentized.residuals)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", colour = \"Red\")+\n  theme_bw(base_size = 8)+\n  labs(x = \"Fitted Values\",\n       y = \"Studentized Residual\")\n# plot 7\np7 <- qplot(sample = mlrdata$studentized.residuals, stat=\"qq\") +\n  theme_bw(base_size = 8) +\n  labs(x = \"Theoretical Values\",\n       y = \"Observed Values\")\nvip::grid.arrange(p5, p6, p7, nrow = 1)\n\n\n\n\nThe new diagnostic plots do not indicate outliers that require removal. With respect to such data points the following parameters should be considered:\n\nData points with standardized residuals > 3.29 should be removed [6]\nIf more than 1 percent of data points have standardized residuals exceeding values > 2.58, then the error rate of the model is unacceptable [6].\nIf more than 5 percent of data points have standardized residuals exceeding values > 1.96, then the error rate of the model is unacceptable [6]\nIn addition, data points with Cook’s D-values > 1 should be removed [6]\nAlso, data points with leverage values higher than \\(3(k + 1)/N\\) or \\(2(k + 1)/N\\) (k = Number of predictors, N = Number of cases in model) should be removed [6]\nThere should not be (any) autocorrelation among predictors. This means that independent variables cannot be correlated with itself (for instance, because data points come from the same subject). If there is autocorrelation among predictors, then a Repeated Measures Design or a (hierarchical) mixed-effects model should be implemented instead.\nPredictors cannot substantially correlate with each other (multicollinearity) (see the subsection on (multi-)collinearity in the section of multiple binomial logistic regression for more details about (multi-)collinearity). If a model contains predictors that have variance inflation factors (VIF) > 10 the model is unreliable [17] and predictors causing such VIFs should be removed. Indeed, even VIFs of 2.5 can be problematic [16] Indeed, [18] propose that variables with VIFs exceeding 3 should be removed!\n\n\n\n\nNOTEHowever, (multi-)collinearity is only an issue if one is interested in interpreting regression results! If the interpretation is irrelevant because what is relevant is prediction(!), then it does not matter if the model contains collinear predictors! See [7] for a more elaborate explanation.\n\n\n\n\n\n\n\nThe mean value of VIFs should be ~ 1 [19].\n\nThe following code chunk evaluates these criteria.\n\n# 1: optimal = 0\n# (listed data points should be removed)\nwhich(mlrdata$standardized.residuals > 3.29)\n\nnamed integer(0)\n\n# 2: optimal = 1\n# (listed data points should be removed)\nstdres_258 <- as.vector(sapply(mlrdata$standardized.residuals, function(x) {\nifelse(sqrt((x^2)) > 2.58, 1, 0) } ))\n(sum(stdres_258) / length(stdres_258)) * 100\n\n[1] 0\n\n# 3: optimal = 5\n# (listed data points should be removed)\nstdres_196 <- as.vector(sapply(mlrdata$standardized.residuals, function(x) {\nifelse(sqrt((x^2)) > 1.96, 1, 0) } ))\n(sum(stdres_196) / length(stdres_196)) * 100\n\n[1] 6.12244897959\n\n# 4: optimal = 0\n# (listed data points should be removed)\nwhich(mlrdata$cooks.distance > 1)\n\nnamed integer(0)\n\n# 5: optimal = 0\n# (data points should be removed if cooks distance is close to 1)\nwhich(mlrdata$leverage >= (3*mean(mlrdata$leverage)))\n\nnamed integer(0)\n\n# 6: checking autocorrelation:\n# Durbin-Watson test (optimal: high p-value)\ndwt(m2.mlr)\n\n lag  Autocorrelation D-W Statistic p-value\n   1 -0.0143324675649  1.9680423527   0.672\n Alternative hypothesis: rho != 0\n\n# 7: test multicollinearity 1\nvif(m2.mlr)\n\n                        statusSingle              attractionNotInterested \n                                2.00                                 1.96 \nstatusSingle:attractionNotInterested \n                                2.96 \n\n# 8: test multicollinearity 2\n1/vif(m2.mlr)\n\n                        statusSingle              attractionNotInterested \n                      0.500000000000                       0.510204081633 \nstatusSingle:attractionNotInterested \n                      0.337837837838 \n\n# 9: mean vif should not exceed 1\nmean(vif(m2.mlr))\n\n[1] 2.30666666667\n\n\nExcept for the mean VIF value (2.307) which should not exceed 1, all diagnostics are acceptable. We will now test whether the sample size is sufficient for our model. With respect to the minimal sample size and based on [15], [6] offer the following rules of thumb for an adequate sample size (k = number of predictors; categorical predictors with more than two levels should be recoded as dummy variables):\n\nif you are interested in the overall model: 50 + 8k (k = number of predictors)\nif you are interested in individual predictors: 104 + k\nif you are interested in both: take the higher value!\n\n\n\nEvaluation of Sample Size\nAfter performing the diagnostics, we will now test whether the sample size is adequate and what the values of R would be based on a random distribution in order to be able to estimate how likely a \\(\\beta\\)-error is given the present sample size [see 6]. Beta errors (or \\(\\beta\\)-errors) refer to the erroneous assumption that a predictor is not significant (based on the analysis and given the sample) although it does have an effect in the population. In other words, \\(\\beta\\)-error means to overlook a significant effect because of weaknesses of the analysis. The test statistics ranges between 0 and 1 where lower values are better. If the values approximate 1, then there is serious concern as the model is not reliable given the sample size. In such cases, unfortunately, the best option is to increase the sample size.\n\n# load functions\nsource(\"https://slcladal.github.io/rscripts/SampleSizeMLR.r\")\nsource(\"https://slcladal.github.io/rscripts/ExpR.r\")\n# check if sample size is sufficient\nsmplesz(m2.mlr)\n\n[1] \"Sample too small: please increase your sample by  9  data points\"\n\n# check beta-error likelihood\nexpR(m2.mlr)\n\n[1] \"Based on the sample size expect a false positive correlation of 0.0309 between the predictors and the predicted\"\n\n\nThe function smplesz reports that the sample size is insufficient by 9 data points according to [15]. The likelihood of \\(\\beta\\)-errors, however, is very small (0.0309). As a last step, we summarize the results of the regression analysis.\n\n# tabulate model results\nsjPlot::tab_model(m0.glm, m2.glm)\n\n\n\n\n \nmoney\nmoney\n\n\nPredictors\nEstimates\nCI\np\nEstimates\nCI\np\n\n\n(Intercept)\n88.12\n78.72 – 97.52\n<0.001\n99.15\n92.10 – 106.21\n<0.001\n\n\nstatus [Single]\n\n\n\n55.85\n45.78 – 65.93\n<0.001\n\n\nattraction[NotInterested]\n\n\n\n-47.66\n-57.63 – -37.69\n<0.001\n\n\nstatus [Single] *attraction[NotInterested]\n\n\n\n-59.46\n-73.71 – -45.21\n<0.001\n\n\nObservations\n98\n98\n\n\nR2\n0.000\n0.857\n\n\n\n\n\n\n\n\n\n\nNOTEThe R2 values in this report is incorrect! As we have seen above, and is also shown in the table below, the correct R2 values are: multiple R2 0.8574, adjusted R2 0.8528.\n\n\n\n\n\n\nAdditionally, we can inspect the summary of the regression model as shown below to extract additional information.\n\nsummary(m2.mlr)\n\n\nCall:\nlm(formula = money ~ (status + attraction)^2, data = mlrdata)\n\nResiduals:\n         Min           1Q       Median           3Q          Max \n-35.76416667 -13.50520000  -0.98948333  10.59887500  38.77166667 \n\nCoefficients:\n                                         Estimate   Std. Error  t value\n(Intercept)                           99.15480000   3.59735820 27.56323\nstatusSingle                          55.85353333   5.14015367 10.86612\nattractionNotInterested              -47.66280000   5.08743275 -9.36873\nstatusSingle:attractionNotInterested -59.46136667   7.26927504 -8.17982\n                                                   Pr(>|t|)    \n(Intercept)                          < 0.000000000000000222 ***\nstatusSingle                         < 0.000000000000000222 ***\nattractionNotInterested               0.0000000000000040429 ***\nstatusSingle:attractionNotInterested  0.0000000000013375166 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 17.986791 on 94 degrees of freedom\nMultiple R-squared:  0.857375902,   Adjusted R-squared:  0.852824069 \nF-statistic: 188.358387 on 3 and 94 DF,  p-value: < 0.0000000000000002220446\n\n\nAlthough [6] suggest that the main effects of the predictors involved in the interaction should not be interpreted, they are interpreted here to illustrate how the results of a multiple linear regression can be reported.\nWe can use the reports package [14] to summarize the analysis.\n\nreport::report(m2.mlr)\n\nWe fitted a linear model (estimated using OLS) to predict money with status and attraction (formula: money ~ (status + attraction)^2). The model explains a statistically significant and substantial proportion of variance (R2 = 0.86, F(3, 94) = 188.36, p < .001, adj. R2 = 0.85). The model's intercept, corresponding to status = Relationship and attraction = Interested, is at 99.15 (95% CI [92.01, 106.30], t(94) = 27.56, p < .001). Within this model:\n\n  - The effect of status [Single] is statistically significant and positive (beta = 55.85, 95% CI [45.65, 66.06], t(94) = 10.87, p < .001; Std. beta = 1.19, 95% CI [0.97, 1.41])\n  - The effect of attraction [NotInterested] is statistically significant and negative (beta = -47.66, 95% CI [-57.76, -37.56], t(94) = -9.37, p < .001; Std. beta = -1.02, 95% CI [-1.23, -0.80])\n  - The interaction effect of attraction [NotInterested] on status [Single] is statistically significant and negative (beta = -59.46, 95% CI [-73.89, -45.03], t(94) = -8.18, p < .001; Std. beta = -1.27, 95% CI [-1.58, -0.96])\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using the Wald approximation.\n\n\nWe can use this output to write up a final report:\nA multiple linear regression was fitted to the data using an automated, step-wise, AIC-based (Akaike’s Information Criterion) procedure. The model fitting arrived at a final minimal model. During the model diagnostics, two outliers were detected and removed. Further diagnostics did not find other issues after the removal.\nThe final minimal adequate regression model is based on 98 data points and performs highly significantly better than a minimal baseline model (multiple R2: .857, adjusted R2: .853, F-statistic (3, 94): 154.4, AIC: 850.4, BIC: 863.32, p<.001\\(***\\)). The final minimal adequate regression model reports attraction and status as significant main effects. The relationship status of men correlates highly significantly and positively with the amount of money spend on the women’s presents (SE: 5.14, t-value: 10.87, p<.001\\(***\\)). This shows that men spend 156.8 dollars on presents if they are single while they spend 99,15 dollars if they are in a relationship. Whether men are attracted to women also correlates highly significantly and positively with the money they spend on women (SE: 5.09, t-values: -9.37, p<.001\\(***\\)). If men are not interested in women, they spend 47.66 dollar less on a present for women compared with women the men are interested in.\nFurthermore, the final minimal adequate regression model reports a highly significant interaction between relationship status and attraction (SE: 7.27, t-value: -8.18, p<.001\\(***\\)): If men are single but they are not interested in a women, a man would spend only 59.46 dollars on a present compared to all other constellations."
  },
  {
    "objectID": "regression.html#multiple-binomial-logistic-regression",
    "href": "regression.html#multiple-binomial-logistic-regression",
    "title": "Fixed- and Mixed-Effects Regression Models in R",
    "section": "Multiple Binomial Logistic Regression",
    "text": "Multiple Binomial Logistic Regression\nLogistic regression is a multivariate analysis technique that builds on and is very similar in terms of its implementation to linear regression but logistic regressions take dependent variables that represent nominal rather than numeric scaling [20]. The difference requires that the linear regression must be modified in certain ways to avoid producing non-sensical outcomes. The most fundamental difference between logistic and linear regressions is that logistic regression work on the probabilities of an outcome (the likelihood), rather than the outcome itself. In addition, the likelihoods on which the logistic regression works must be logged (logarithmized) in order to avoid produce predictions that produce values greater than 1 (instance occurs) and 0 (instance does not occur). You can check this by logging the values from -10 to 10 using the plogis function as shown below.\n\nround(plogis(-10:10), 5)\n\n [1] 0.00005 0.00012 0.00034 0.00091 0.00247 0.00669 0.01799 0.04743 0.11920\n[10] 0.26894 0.50000 0.73106 0.88080 0.95257 0.98201 0.99331 0.99753 0.99909\n[19] 0.99966 0.99988 0.99995\n\n\nIf we visualize these logged values, we get an S-shaped curve which reflects the logistic function.\n\n\n\n\n\nTo understand what this mean, we will use a very simple example. In this example, we want to see whether the height of men affect their likelihood of being in a relationship. The data we use represents a data set consisting of two variables: height and relationship.\n\n\n\n\n\nThe left panel of the Figure above shows that a linear model would predict values for the relationship status, which represents a factor (0 = Single and 1 = In a Relationship), that are nonsensical because values above 1 or below 0 do not make sense. In contrast to a linear regression, which predicts actual values, such as the frequencies of prepositions in a certain text, a logistic regression predicts probabilities of events (for example, being in a relationship) rather than actual values. The center panel shows the predictions of a logistic regression and we see that a logistic regression also has an intercept and a (very steep) slope but that the regression line also predicts values that are above 1 and below 0. However, when we log the predicted values we these predicted values are transformed into probabilities with values between 0 and 1. And the logged regression line has a S-shape which reflects the logistic function. Furthermore, we can then find the optimal line (the line with the lowest residual deviance) by comparing the sum of residuals - just as we did for a simple linear model and that way, we find the regression line for a logistic regression.\n\nExample 1: EH in Kiwi English\nTo exemplify how to implement a logistic regression in R [see 21, 22] for very good and thorough introductions to this topic], we will analyze the use of the discourse particle eh in New Zealand English and test which factors correlate with its occurrence. The data set represents speech units in a corpus that were coded for the speaker who uttered a given speech unit, the gender, ethnicity, and age of that speaker and whether or not the speech unit contained an eh. To begin with, we clean the current work space, set option, install and activate relevant packages, load customized functions, and load the example data set.\n\n# load data\nblrdata  <- base::readRDS(url(\"https://slcladal.github.io/data/bld.rda\", \"rb\"))\n\n\n\n\n\n\nFirst 15 rows of the blrdata.\n\n\nIDGenderAgeEthnicityEH<S1A-001#M>MenYoungPakeha0<S1A-001#M>MenYoungPakeha1<S1A-001#M>MenYoungPakeha0<S1A-001#M>MenYoungPakeha0<S1A-001#M>MenYoungPakeha1<S1A-001#M>MenYoungPakeha1<S1A-001#M>MenYoungPakeha0<S1A-001#M>MenYoungPakeha0<S1A-001#M>MenYoungPakeha0<S1A-001#M>MenYoungPakeha1<S1A-001#M>MenYoungPakeha0<S1A-001#M>MenYoungPakeha0<S1A-001#M>MenYoungPakeha1<S1A-001#M>MenYoungPakeha1<S1A-001#M>MenYoungPakeha0\n\n\nThe summary of the data show that the data set contains 25,821 observations of five variables. The variable ID contains strings that represent a combination file and speaker of a speech unit. The second variable represents the gender, the third the age, and the fourth the ethnicity of speakers. The fifth variable represents whether or not a speech unit contained the discourse particle eh.\nNext, we factorize the variables in our data set. In other words, we specify that the strings represent variable levels and define new reference levels because as a default R will use the variable level which first occurs in alphabet ordering as the reference level for each variable, we redefine the variable levels for Age and Ethnicity.\n\nblrdata <- blrdata %>%\n  # factorize variables\n  dplyr::mutate(Age = factor(Age),\n                Gender = factor(Gender),\n                Ethnicity = factor(Ethnicity),\n                ID = factor(ID),\n                EH = factor(EH)) %>%\n  # relevel Age (Reference = Young) and Ethnicity (Reference= Pakeha))\n  dplyr::mutate(Age = relevel(Age, \"Young\"),\n                Ethnicity = relevel(Ethnicity, \"Pakeha\"))\n\nAfter preparing the data, we will now plot the data to get an overview of potential relationships between variables.\n\nblrdata %>%\n  dplyr::mutate(EH = ifelse(EH == \"0\", 0, 1)) %>%\n  ggplot(aes(Age, EH, color = Gender)) +\n  facet_wrap(~Ethnicity) +\n  stat_summary(fun = mean, geom = \"point\") +\n  stat_summary(fun.data = mean_cl_boot, geom = \"errorbar\", width = 0.2) +\n  theme_set(theme_bw(base_size = 10)) +\n  theme(legend.position = \"top\") +\n  labs(x = \"\", y = \"Observed Probabilty of eh\") +\n  scale_color_manual(values = c(\"gray20\", \"gray70\"))\n\n\n\n\nWith respect to main effects, the Figure above indicates that men use eh more frequently than women, that young speakers use it more frequently compared with old speakers, and that speakers that are descendants of European settlers (Pakeha) use eh more frequently compared with Maori (the native inhabitants of New Zealand).\nThe plots in the lower panels do not indicate significant interactions between use of eh and the Age, Gender, and Ethnicity of speakers. In a next step, we will start building the logistic regression model.\n\n\nModel Building\nAs a first step, we need to define contrasts and use the datadist function to store aspects of our variables that can be accessed later when plotting and summarizing the model. Contrasts define what and how variable levels should be compared and therefore influences how the results of the regression analysis are presented. In this case, we use treatment contrasts which are in-built. Treatment contrasts mean that we assess the significance of levels of a predictor against a baseline which is the reference level of a predictor. [6] and [7] provide very good and accessible explanations of contrasts and how to manually define contrasts if you would like to know more.\n\n# set contrasts\noptions(contrasts  =c(\"contr.treatment\", \"contr.poly\"))\n# create distance matrix\nblrdata.dist <- datadist(blrdata)\n# include distance matrix in options\noptions(datadist = \"blrdata.dist\")\n\nNext, we generate a minimal model that predicts the use of eh solely based on the intercept.\n\n# baseline glm model\nm0.blr = glm(EH ~ 1, family = binomial, data = blrdata)\n\n\n\nModel fitting\nWe will now start with the model fitting procedure. In the present case, we will use a manual step-wise step-up procedure during which predictors are added to the model if they significantly improve the model fit. In addition, we will perform diagnostics as we fit the model at each step of the model fitting process rather than after the fitting.\nWe will test two things in particular: whether the data has incomplete information or complete separation and if the model suffers from (multi-)collinearity.\nIncomplete information or complete separation means that the data does not contain all combinations of the predictor or the dependent variable. This is important because if the data does not contain cases of all combinations, the model will assume that it has found a perfect predictor. In such cases the model overestimates the effect of that that predictor and the results of that model are no longer reliable. For example, if eh was only used by young speakers in the data, the model would jump on that fact and say Ha! If there is an old speaker, that means that that speaker will never ever and under no circumstances say eh* - I can therefore ignore all other factors!*\nMulticollinearity means that predictors correlate and have shared variance. This means that whichever predictor is included first will take all the variance that it can explain and the remaining part of the variable that is shared will not be attributed to the other predictor. This may lead to reporting that a factor is not significant because all of the variance it can explain is already accounted for. However, if the other predictor were included first, then the original predictor would be returned as insignificant. This means that- depending on the order in which predictors are added - the results of the regression can differ dramatically and the model is therefore not reliable. Multicollinearity is actually a very common problem and there are various ways to deal with it but it cannot be ignored (at least in regression analyses).\nWe will start by adding Age to the minimal adequate model.\n\n# check incomplete information\nifelse(min(ftable(blrdata$Age, blrdata$EH)) == 0, \"not possible\", \"possible\")\n\n[1] \"possible\"\n\n# add age to the model\nm1.blr = glm(EH ~ Age, family = binomial, data = blrdata)\n# check multicollinearity (vifs should have values of 3 or lower for main effects)\nifelse(max(vif(m1.blr)) <= 3,  \"vifs ok\", \"WARNING: high vifs!\") # VIFs ok\n\n[1] \"vifs ok\"\n\n# check if adding Age significantly improves model fit\nanova(m1.blr, m0.blr, test = \"Chi\")\n\nAnalysis of Deviance Table\n\nModel 1: EH ~ Age\nModel 2: EH ~ 1\n  Resid. Df  Resid. Dev Df     Deviance               Pr(>Chi)    \n1     25819 32376.86081                                           \n2     25820 33007.75469 -1 -630.8938871 < 0.000000000000000222 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nAs the data does not contain incomplete information, the vif values are below 3, and adding Age has significantly improved the model fit (the p-value of the ANOVA is lower than .05). We therefore proceed with Age included.\nWe continue by adding Gender. We add a second ANOVA test to see if including Gender affects the significance of other predictors in the model. If this were the case - if adding Gender would cause Age to become insignificant - then we could change the ordering in which we include predictors into our model.\n\nifelse(min(ftable(blrdata$Gender, blrdata$EH)) == 0, \"not possible\", \"possible\")\n\n[1] \"possible\"\n\nm2.blr <- update(m1.blr, . ~ . +Gender)\nifelse(max(vif(m2.blr)) <= 3,  \"vifs ok\", \"WARNING: high vifs!\") # VIFs ok\n\n[1] \"vifs ok\"\n\nanova(m2.blr, m1.blr, test = \"Chi\")\n\nAnalysis of Deviance Table\n\nModel 1: EH ~ Age + Gender\nModel 2: EH ~ Age\n  Resid. Df  Resid. Dev Df    Deviance               Pr(>Chi)    \n1     25818 32139.54089                                          \n2     25819 32376.86081 -1 -237.319914 < 0.000000000000000222 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nAnova(m2.blr, test = \"LR\")\n\nAnalysis of Deviance Table (Type II tests)\n\nResponse: EH\n          LR Chisq Df             Pr(>Chisq)    \nAge    668.6350712  1 < 0.000000000000000222 ***\nGender 237.3199140  1 < 0.000000000000000222 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nAgain, including Gender significantly improves model fit and the data does not contain incomplete information or complete separation. Also, including Gender does not affect the significance of Age. Now, we include Ethnicity.\n\nifelse(min(ftable(blrdata$Ethnicity, blrdata$EH)) == 0, \"not possible\", \"possible\")\n\n[1] \"possible\"\n\nm3.blr <- update(m2.blr, . ~ . +Ethnicity)\nifelse(max(vif(m3.blr)) <= 3,  \"vifs ok\", \"WARNING: high vifs!\") # VIFs ok\n\n[1] \"vifs ok\"\n\nanova(m3.blr, m2.blr, test = \"Chi\")\n\nAnalysis of Deviance Table\n\nModel 1: EH ~ Age + Gender + Ethnicity\nModel 2: EH ~ Age + Gender\n  Resid. Df  Resid. Dev Df      Deviance Pr(>Chi)\n1     25817 32139.27988                          \n2     25818 32139.54089 -1 -0.2610145387  0.60942\n\n\nSince adding Ethnicity does not significantly improve the model fit, we do not need to test if its inclusion affects the significance of other predictors. We continue without Ethnicity and include the interaction between Age and Gender.\n\nifelse(min(ftable(blrdata$Age, blrdata$Gender, blrdata$EH)) == 0, \"not possible\", \"possible\")\n\n[1] \"possible\"\n\nm4.blr <- update(m2.blr, . ~ . +Age*Gender)\nifelse(max(vif(m4.blr)) <= 3,  \"vifs ok\", \"WARNING: high vifs!\") # VIFs ok\n\n[1] \"vifs ok\"\n\nanova(m4.blr, m2.blr, test = \"Chi\")\n\nAnalysis of Deviance Table\n\nModel 1: EH ~ Age + Gender + Age:Gender\nModel 2: EH ~ Age + Gender\n  Resid. Df  Resid. Dev Df     Deviance Pr(>Chi)\n1     25817 32139.41665                         \n2     25818 32139.54089 -1 -0.124239923  0.72448\n\n\nThe interaction between Age and Gender is not significant which means that men and women do not behave differently with respect to their use of EH as they age. Also, the data does not contain incomplete information and the model does not suffer from multicollinearity - the predictors are not collinear. We can now include if there is a significant interaction between Age and Ethnicity.\n\nifelse(min(ftable(blrdata$Age, blrdata$Ethnicity, blrdata$EH)) == 0, \"not possible\", \"possible\")\n\n[1] \"possible\"\n\nm5.blr <- update(m2.blr, . ~ . +Age*Ethnicity)\nifelse(max(vif(m5.blr)) <= 3,  \"vifs ok\", \"WARNING: high vifs!\") # VIFs ok\n\n[1] \"vifs ok\"\n\nanova(m5.blr, m2.blr, test = \"Chi\")\n\nAnalysis of Deviance Table\n\nModel 1: EH ~ Age + Gender + Ethnicity + Age:Ethnicity\nModel 2: EH ~ Age + Gender\n  Resid. Df  Resid. Dev Df     Deviance Pr(>Chi)\n1     25816 32136.47224                         \n2     25818 32139.54089 -2 -3.068654514   0.2156\n\n\nAgain, no incomplete information or multicollinearity and no significant interaction. Now, we test if there exists a significant interaction between Gender and Ethnicity.\n\nifelse(min(ftable(blrdata$Gender, blrdata$Ethnicity, blrdata$EH)) == 0, \"not possible\", \"possible\")\n\n[1] \"possible\"\n\nm6.blr <- update(m2.blr, . ~ . +Gender*Ethnicity)\nifelse(max(vif(m6.blr)) <= 3,  \"vifs ok\", \"WARNING: high vifs!\") # VIFs ok\n\n[1] \"vifs ok\"\n\nanova(m6.blr, m2.blr, test = \"Chi\")\n\nAnalysis of Deviance Table\n\nModel 1: EH ~ Age + Gender + Ethnicity + Gender:Ethnicity\nModel 2: EH ~ Age + Gender\n  Resid. Df  Resid. Dev Df      Deviance Pr(>Chi)\n1     25816 32139.26864                          \n2     25818 32139.54089 -2 -0.2722521835  0.87273\n\n\nAs the interaction between Gender and Ethnicity is not significant, we continue without it. In a final step, we include the three-way interaction between Age, Gender, and Ethnicity.\n\nifelse(min(ftable(blrdata$Age, blrdata$Gender, blrdata$Ethnicity, blrdata$EH)) == 0, \"not possible\", \"possible\")\n\n[1] \"possible\"\n\nm7.blr <- update(m2.blr, . ~ . +Gender*Ethnicity)\nifelse(max(vif(m7.blr)) <= 3,  \"vifs ok\", \"WARNING: high vifs!\") # VIFs ok\n\n[1] \"vifs ok\"\n\nanova(m7.blr, m2.blr, test = \"Chi\")\n\nAnalysis of Deviance Table\n\nModel 1: EH ~ Age + Gender + Ethnicity + Gender:Ethnicity\nModel 2: EH ~ Age + Gender\n  Resid. Df  Resid. Dev Df      Deviance Pr(>Chi)\n1     25816 32139.26864                          \n2     25818 32139.54089 -2 -0.2722521835  0.87273\n\n\nWe have found our final minimal adequate model because the 3-way interaction is also insignificant. As we have now arrived at the final minimal adequate model (m2.blr), we generate a final minimal model using the lrm function.\n\nm2.lrm <- lrm(EH ~ Age+Gender, data = blrdata, x = T, y = T, linear.predictors = T)\nm2.lrm\n\nLogistic Regression Model\n \n lrm(formula = EH ~ Age + Gender, data = blrdata, x = T, y = T, \n     linear.predictors = T)\n \n                        Model Likelihood        Discrimination    Rank Discrim.    \n                              Ratio Test               Indexes          Indexes    \n Obs         25821    LR chi2     868.21        R2       0.046    C       0.602    \n  0          17114    d.f.             2      R2(2,25821)0.033    Dxy     0.203    \n  1           8707    Pr(> chi2) <0.0001    R2(2,17312.8)0.049    gamma   0.302    \n max |deriv| 3e-10                              Brier    0.216    tau-a   0.091    \n \n              Coef    S.E.   Wald Z Pr(>|Z|)\n Intercept    -0.2324 0.0223 -10.44 <0.0001 \n Age=Old      -0.8305 0.0335 -24.78 <0.0001 \n Gender=Women -0.4201 0.0273 -15.42 <0.0001 \n \n\n\n\nanova(m2.lrm)\n\n                Wald Statistics          Response: EH \n\n Factor     Chi-Square d.f. P     \n Age        614.04     1    <.0001\n Gender     237.65     1    <.0001\n TOTAL      802.65     2    <.0001\n\n\nAfter fitting the model, we validate the model to avoid arriving at a final minimal model that is overfitted to the data at hand.\n\n\nModel Validation\nTo validate a model, you can apply the validate function and apply it to a saturated model. The output of the validate function shows how often predictors are retained if the sample is re-selected with the same size but with placing back drawn data points. The execution of the function requires some patience as it is rather computationally expensive and it is, therefore, commented out below.\n\n# model validation (remove # to activate: output too long for website)\nm7.lrm <- lrm(EH ~ (Age+Gender+Ethnicity)^3, data = blrdata, x = T, y = T, linear.predictors = T)\n#validate(m7.lrm, bw = T, B = 200)\n\nThe validate function shows that retaining two predictors (Age and Gender) is the best option and thereby confirms our final minimal adequate model as the best minimal model. In addition, we check whether we need to include a penalty for data points because they have too strong of an impact of the model fit. To see whether a penalty is warranted, we apply the pentrace function to the final minimal adequate model.\n\npentrace(m2.lrm, seq(0, 0.8, by = 0.05)) # determine penalty\n\n\nBest penalty:\n\n penalty            df\n     0.8 1.99925395138\n\n penalty            df           aic           bic         aic.c\n    0.00 2.00000000000 864.213801108 847.895914321 864.213336316\n    0.05 1.99995335085 864.213893816 847.896387637 864.213429042\n    0.10 1.99990670452 864.213985335 847.896859740 864.213520579\n    0.15 1.99986006100 864.214075641 847.897330609 864.213610904\n    0.20 1.99981342030 864.214164764 847.897800270 864.213700044\n    0.25 1.99976678241 864.214252710 847.898268733 864.213788009\n    0.30 1.99972014734 864.214339446 847.898735961 864.213874762\n    0.35 1.99967351509 864.214424993 847.899201978 864.213960327\n    0.40 1.99962688564 864.214509360 847.899666792 864.214044712\n    0.45 1.99958025902 864.214592526 847.900130382 864.214127896\n    0.50 1.99953363520 864.214674504 847.900592761 864.214209892\n    0.55 1.99948701420 864.214755279 847.901053914 864.214290685\n    0.60 1.99944039601 864.214834874 847.901513865 864.214370299\n    0.65 1.99939378063 864.214913276 847.901972599 864.214448719\n    0.70 1.99934716807 864.214990480 847.902430112 864.214525941\n    0.75 1.99930055832 864.215066506 847.902886425 864.214601985\n    0.80 1.99925395138 864.215141352 847.903341534 864.214676849\n\n\nThe values are so similar that a penalty is unnecessary. In a next step, we rename the final models.\n\nlr.glm <- m2.blr  # rename final minimal adequate glm model\nlr.lrm <- m2.lrm  # rename final minimal adequate lrm model\n\nNow, we calculate a Model Likelihood Ratio Test to check if the final model performs significantly better than the initial minimal base-line model. The result of this test is provided as a default if we call a summary of the lrm object.\n\nmodelChi <- lr.glm$null.deviance - lr.glm$deviance\nchidf <- lr.glm$df.null - lr.glm$df.residual\nchisq.prob <- 1 - pchisq(modelChi, chidf)\nmodelChi; chidf; chisq.prob\n\n[1] 868.21380111\n\n\n[1] 2\n\n\n[1] 0\n\n\nThe code above provides three values: a \\(\\chi\\)2, the degrees of freedom, and a p-value. The p-value is lower than .05 and the results of the Model Likelihood Ratio Test therefore confirm that the final minimal adequate model performs significantly better than the initial minimal base-line model. Another way to extract the model likelihood test statistics is to use an ANOVA to compare the final minimal adequate model to the minimal base-line model.\nA handier way to get these statistics is by performing an ANOVA on the final minimal model which, if used this way, is identical to a Model Likelihood Ratio test.\n\nanova(m0.glm, lr.glm, test = \"Chi\") # Model Likelihood Ratio Test\n\nWarning in anova.glmlist(c(list(object), dotargs), dispersion = dispersion, :\nmodels with response '\"EH\"' removed because response differs from model 1\n\n\nAnalysis of Deviance Table\n\nModel: gaussian, link: identity\n\nResponse: money\n\nTerms added sequentially (first to last)\n\n     Df Deviance Resid. Df  Resid. Dev Pr(>Chi)\nNULL                    97 213227.0608         \n\n\nIn a next step, we calculate pseudo-R2 values which represent the amount of residual variance that is explained by the final minimal adequate model. We cannot use the ordinary R2 because the model works on the logged probabilities rather than the values of the dependent variable.\n\n# calculate pseudo R^2\n# number of cases\nncases <- length(fitted(lr.glm))\nR2.hl <- modelChi/lr.glm$null.deviance\nR.cs <- 1 - exp ((lr.glm$deviance - lr.glm$null.deviance)/ncases)\nR.n <- R.cs /( 1- ( exp (-(lr.glm$null.deviance/ ncases))))\n# function for extracting pseudo-R^2\nlogisticPseudoR2s <- function(LogModel) {\n  dev <- LogModel$deviance\n    nullDev <- LogModel$null.deviance\n    modelN <-  length(LogModel$fitted.values)\n    R.l <-  1 -  dev / nullDev\n    R.cs <- 1- exp ( -(nullDev - dev) / modelN)\n    R.n <- R.cs / ( 1 - ( exp (-(nullDev / modelN))))\n    cat(\"Pseudo R^2 for logistic regression\\n\")\n    cat(\"Hosmer and Lemeshow R^2  \", round(R.l, 3), \"\\n\")\n    cat(\"Cox and Snell R^2        \", round(R.cs, 3), \"\\n\")\n    cat(\"Nagelkerke R^2           \", round(R.n, 3),    \"\\n\") }\nlogisticPseudoR2s(lr.glm)\n\nPseudo R^2 for logistic regression\nHosmer and Lemeshow R^2   0.026 \nCox and Snell R^2         0.033 \nNagelkerke R^2            0.046 \n\n\nThe low pseudo-R2 values show that our model has very low explanatory power. For instance, the value of Hosmer and Lemeshow R2 (0.026) “is the proportional reduction in the absolute value of the log-likelihood measure and as such it is a measure of how much the badness of fit improves as a result of the inclusion of the predictor variables” [6]. In essence, all the pseudo-R2 values are measures of how substantive the model is (how much better it is compared to a baseline model). Next, we extract the confidence intervals for the coefficients of the model.\n\n# extract the confidence intervals for the coefficients\nconfint(lr.glm)\n\n                      2.5 %          97.5 %\n(Intercept) -0.276050866670 -0.188778707810\nAgeOld      -0.896486392279 -0.765095825382\nGenderWomen -0.473530977637 -0.366703827307\n\n\nDespite having low explanatory and predictive power, the age of speakers and their gender are significant as the confidence intervals of the coefficients do not overlap with 0.\n\n\nEffect Size\nIn a next step, we compute odds ratios and their confidence intervals. Odds Ratios represent a common measure of effect size and can be used to compare effect sizes across models. Odds ratios rang between 0 and infinity. Values of 1 indicate that there is no effect. The further away the values are from 1, the stronger the effect. If the values are lower than 1, then the variable level correlates negatively with the occurrence of the outcome (the probability decreases) while values above 1 indicate a positive correlation and show that the variable level causes an increase in the probability of the outcome (the occurrence of EH).\n\nexp(lr.glm$coefficients) # odds ratios\n\n   (Intercept)         AgeOld    GenderWomen \n0.792642499264 0.435815384592 0.656972294902 \n\nexp(confint(lr.glm))     # confidence intervals of the odds ratios\n\nWaiting for profiling to be done...\n\n\n                     2.5 %         97.5 %\n(Intercept) 0.758774333456 0.827969709653\nAgeOld      0.408000698619 0.465289342309\nGenderWomen 0.622799290871 0.693014866732\n\n\nThe odds ratios confirm that older speakers use eh significantly less often compared with younger speakers and that women use eh less frequently than men as the confidence intervals of the odds rations do not overlap with 1. In a next step, we calculate the prediction accuracy of the model.\n\n\nPrediction Accuracy\nIn order to calculate the prediction accuracy of the model, we generate a variable called Prediction that contains the predictions of pour model and which we add to the data. Then, we use the confusionMatrix function from the caret package [23] to extract the prediction accuracy.\n\n# create variable with contains the prediction of the model\nblrdata <- blrdata %>%\n  dplyr::mutate(Prediction = predict(lr.glm, type = \"response\"),\n                Prediction = ifelse(Prediction > .5, 1, 0),\n                Prediction = factor(Prediction, levels = c(\"0\", \"1\")),\n                EH = factor(EH, levels = c(\"0\", \"1\")))\n# create a confusion matrix with compares observed against predicted values\ncaret::confusionMatrix(blrdata$Prediction, blrdata$EH)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction     0     1\n         0 17114  8707\n         1     0     0\n                                                    \n               Accuracy : 0.66279385                \n                 95% CI : (0.656990096, 0.668560948)\n    No Information Rate : 0.66279385                \n    P-Value [Acc > NIR] : 0.5029107                 \n                                                    \n                  Kappa : 0                         \n                                                    \n Mcnemar's Test P-Value : < 0.00000000000000022     \n                                                    \n            Sensitivity : 1.00000000                \n            Specificity : 0.00000000                \n         Pos Pred Value : 0.66279385                \n         Neg Pred Value :        NaN                \n             Prevalence : 0.66279385                \n         Detection Rate : 0.66279385                \n   Detection Prevalence : 1.00000000                \n      Balanced Accuracy : 0.50000000                \n                                                    \n       'Positive' Class : 0                         \n                                                    \n\n\nWe can see that out model has never predicted the use of eh which is common when dealing with rare phenomena. This is expected as the event s so rare that the probability of it not occurring substantively outweighs the probability of it occurring. As such, the prediction accuracy of our model is not significantly better compared to the prediction accuracy of the baseline model which is the no-information rate (NIR)) (p = 0.5029).\nWe can use the plot_model function from the sjPlot package [13] to visualize the effects.\n\n# predicted probability\nefp1 <- plot_model(lr.glm, type = \"pred\", terms = c(\"Age\"), axis.lim = c(0, 1)) \n# predicted percentage\nefp2 <- plot_model(lr.glm, type = \"pred\", terms = c(\"Gender\"), axis.lim = c(0, 1)) \ngrid.arrange(efp1, efp2, nrow = 1)\n\n\n\n\nAnd we can also combine the visualization of the effects in a single plot as shown below.\n\nsjPlot::plot_model(lr.glm, type = \"pred\", terms = c(\"Age\", \"Gender\"), axis.lim = c(0, 1)) +\n  theme(legend.position = \"top\") +\n  labs(x = \"\", y = \"Predicted Probabilty of eh\", title = \"\") +\n  scale_color_manual(values = c(\"gray20\", \"gray70\"))\n\n\n\n\n\n\nModel Diagnostics\nWe are now in a position to perform model diagnostics and test if the model violates distributional requirements. In a first step, we test for the existence of multicollinearity.\n\n\nMulticollinearity\nMulticollinearity means that predictors in a model can be predicted by other predictors in the model (this means that they share variance with other predictors). If this is the case, the model results are unreliable because the presence of absence of one predictor has substantive effects on at least one other predictor.\nTo check whether the final minimal model contains predictors that correlate with each other, we extract variance inflation factors (VIF). If a model contains predictors that have variance inflation factors (VIF) > 10 the model is unreliable [17]. [7] shows that a VIF of 10 means that that predictor is explainable (predictable) from the other predictors in the model with an R2 of .9 (a VIF of 5 means that predictor is explainable (predictable) from the other predictors in the model with an R2 of .8).Indeed, predictors with VIF values greater than 4 are usually already problematic but, for large data sets, even VIFs greater than 2 can lead inflated standard errors (Jaeger 2013). Also, VIFs of 2.5 can be problematic [16] and [18] proposes that variables with VIFs exceeding 3 should be removed.\n\n\n\nNOTEHowever, (multi-)collinearity is only an issue if one is interested in interpreting regression results! If the interpretation is irrelevant because what is relevant is prediction(!), then it does not matter if the model contains collinear predictors! See [7] or the excursion below for a more elaborate explanation.\n\n\n\n\n\n\n\n\n\n\nEXCURSION\n\n\n\n\n\n\n`\nWhat is multicollinearity?\n\n\nAnswer\n\n\nDuring the workshop on mixed-effects modeling, we talked about (multi-)collinearity and someone asked if collinearity reflected shared variance (what I thought) or predictability of variables (what the other person thought). Both answers are correct! We will see below why…\n\n\n\n\n(Multi-)collinearity reflects the predictability of predictors based on the values of other predictors!\n\n\n\n\nTo test this, I generate a data set with 4 independent variables a, b, c, and d as well as two potential response variables r1 (which is random) and r2 (where the first 50 data points are the same as in r1 but for the second 50 data points I have added a value of 50 to the data points 51 to 100 from r1). This means that the predictors a and d should both strongly correlate with r2.\n::: {.cell}\n  # load packages\n  library(dplyr)\n  library(rms)\n  # create data set\n  # responses\n  # 100 random numbers\n  r1 <- rnorm(100, 50, 10)\n  # 50 smaller + 50 larger numbers\n  r2 <- c(r1[1:50], r1[51:100] + 50)\n  # predictors\n  a <- c(rep(\"1\", 50), rep (\"0\", 50))\n  b <- rep(c(rep(\"1\", 25), rep (\"0\", 25)), 2)\n  c <- rep(c(rep(\"1\", 10), rep(\"0\", 10)), 5)\n  d <- c(rep(\"1\", 47), rep (\"0\", 3), rep (\"0\", 47), rep (\"1\", 3))\n  # create data set\n  df <- data.frame(r1, r2, a, b, c, d)\n:::\n::: {.cell} ::: {.cell-output-display}\n\n  \n  First 10 rows of df data.\n   \n    \n      r1 \n      r2 \n      a \n      b \n      c \n      d \n    \n   \n  \n    \n      55.3805120133 \n      55.3805120133 \n      1 \n      1 \n      1 \n      1 \n    \n    \n      50.5169933942 \n      50.5169933942 \n      1 \n      1 \n      1 \n      1 \n    \n    \n      67.5135734932 \n      67.5135734932 \n      1 \n      1 \n      1 \n      1 \n    \n    \n      60.1733973728 \n      60.1733973728 \n      1 \n      1 \n      1 \n      1 \n    \n    \n      72.3678161278 \n      72.3678161278 \n      1 \n      1 \n      1 \n      1 \n    \n    \n      52.4309002788 \n      52.4309002788 \n      1 \n      1 \n      1 \n      1 \n    \n    \n      55.6904236277 \n      55.6904236277 \n      1 \n      1 \n      1 \n      1 \n    \n    \n      44.2366484882 \n      44.2366484882 \n      1 \n      1 \n      1 \n      1 \n    \n    \n      55.5987335816 \n      55.5987335816 \n      1 \n      1 \n      1 \n      1 \n    \n    \n      58.3120761737 \n      58.3120761737 \n      1 \n      1 \n      1 \n      1 \n    \n  \n  \n  \n::: :::\nHere are the visualizations of r1 and r2\n::: {.cell} ::: {.cell-output-display}  :::\n::: {.cell-output-display}  ::: :::\nFit first model\nNow, I fit a first model. As the response is random, we do not expect any of the predictors to have a significant effect and we expect the R2 to be rather low.\n::: {.cell}\n  m1 <- lm(r1 ~ a + b + c + d, data = df)\n  # inspect model\n  summary(m1)\n::: {.cell-output .cell-output-stdout} ```\nCall: lm(formula = r1 ~ a + b + c + d, data = df)\nResiduals: Min 1Q Median 3Q Max -25.076373713 -7.774694324 0.933388851 6.630369791 22.021652947\nCoefficients: Estimate Std. Error t value Pr(>|t|)\n(Intercept) 46.077236782 2.059313248 22.37505 < 0.0000000000000002 ** a1 5.957479586 4.593681927 1.29689 0.197812\nb1 5.148440925 2.098541787 2.45334 0.015977 \nc1 -0.213502165 2.188893729 -0.09754 0.922504\nd1 -5.232737413 4.515342995 -1.15888 0.249410\n— Signif. codes: 0 ‘’ 0.001 ’’ 0.01 ’’ 0.05 ‘.’ 0.1 ’ ’ 1\nResidual standard error: 10.4927089 on 95 degrees of freedom Multiple R-squared: 0.075627922, Adjusted R-squared: 0.0367069924 F-statistic: 1.94311705 on 4 and 95 DF, p-value: 0.109586389 ``` ::: :::\nWe now check for (multi-)collinearity using the vif function from the rms package [24]. Variables a and d should have high variance inflation factor values (vif-values) because they overlap very much!\n::: {.cell}\n  # extract vifs\n  rms::vif(m1)\n::: {.cell-output .cell-output-stdout} a1            b1            c1            d1    4.79166666667 1.00000000000 1.08796296296 4.62962962963 ::: :::\nVariables a and d do indeed have high vif-values.\nFit second model\nWe now fit a second model to the response which has higher values for the latter part of the response. Both a and d strongly correlate with the response. But because a and d are collinear, d should not be reported as being significant by the model. The R2 of the model should be rather high (given the correlation between the response r2 and a and d).\n::: {.cell}\n  m2 <- lm(r2 ~ a + b + c + d, data = df)\n  # inspect model\n  summary(m2)\n::: {.cell-output .cell-output-stdout} ```\nCall: lm(formula = r2 ~ a + b + c + d, data = df)\nResiduals: Min 1Q Median 3Q Max -25.076373713 -7.774694324 0.933388851 6.630369791 22.021652947\nCoefficients: Estimate Std. Error t value Pr(>|t|)\n(Intercept) 96.077236782 2.059313248 46.65499 < 0.000000000000000222  a1 -44.042520414 4.593681927 -9.58763 0.0000000000000012574  b1 5.148440925 2.098541787 2.45334 0.015977 *\nc1 -0.213502165 2.188893729 -0.09754 0.922504\nd1 -5.232737413 4.515342995 -1.15888 0.249410\n— Signif. codes: 0 ‘’ 0.001 ’’ 0.01 ’’ 0.05 ‘.’ 0.1 ’ ’ 1\nResidual standard error: 10.4927089 on 95 degrees of freedom Multiple R-squared: 0.851726565, Adjusted R-squared: 0.845483473 F-statistic: 136.427041 on 4 and 95 DF, p-value: < 0.0000000000000002220446 ``` ::: :::\nAgain, we extract the vif-values.\n::: {.cell}\n  # extract vifs\n  rms::vif(m2)\n::: {.cell-output .cell-output-stdout} a1            b1            c1            d1    4.79166666667 1.00000000000 1.08796296296 4.62962962963 ::: :::\nThe vif-values are identical which shows that what matters is if the variables are predictable. To understand how we arrive at vif-values, we inspect the model matrix.\n::: {.cell}\n  # inspect model matrix\n  mm <- model.matrix(m2)\n:::\n::: {.cell} ::: {.cell-output-display}\n\n  \n  First 15 rows of the model matrix.\n   \n    \n      (Intercept) \n      a1 \n      b1 \n      c1 \n      d1 \n    \n   \n  \n    \n      1 \n      1 \n      1 \n      1 \n      1 \n    \n    \n      1 \n      1 \n      1 \n      1 \n      1 \n    \n    \n      1 \n      1 \n      1 \n      1 \n      1 \n    \n    \n      1 \n      1 \n      1 \n      1 \n      1 \n    \n    \n      1 \n      1 \n      1 \n      1 \n      1 \n    \n    \n      1 \n      1 \n      1 \n      1 \n      1 \n    \n    \n      1 \n      1 \n      1 \n      1 \n      1 \n    \n    \n      1 \n      1 \n      1 \n      1 \n      1 \n    \n    \n      1 \n      1 \n      1 \n      1 \n      1 \n    \n    \n      1 \n      1 \n      1 \n      1 \n      1 \n    \n    \n      1 \n      1 \n      1 \n      0 \n      1 \n    \n    \n      1 \n      1 \n      1 \n      0 \n      1 \n    \n    \n      1 \n      1 \n      1 \n      0 \n      1 \n    \n    \n      1 \n      1 \n      1 \n      0 \n      1 \n    \n    \n      1 \n      1 \n      1 \n      0 \n      1 \n    \n  \n  \n  \n::: :::\nWe now fit a linear model in which we predict d from the other predictors in the model matrix.\n::: {.cell}\n  mt <- lm(mm[,5] ~ mm[,1:4])\n  summary(mt)$r.squared\n::: {.cell-output .cell-output-stdout} [1] 0.784 ::: :::\nThe R2 shows that the values of d are explained to 78.4 percent by the values of the other predictors in the model.\nNow, we can write a function [taken from 7] that converts this R2 value\n::: {.cell}\n  R2.to.VIF <- function(some.modelmatrix.r2) {\n  return(1/(1-some.modelmatrix.r2)) } \n  R2.to.VIF(0.784)\n::: {.cell-output .cell-output-stdout} [1] 4.62962962963 ::: :::\nThe function outputs the vif-value of d. This shows that the vif-value of d represents its predictability from the other predictors in the model matrix which represents the amount of shared variance between d and the other predictors in the model.\n\n\n`\n\n\nWe now extract and check the VIFs of the model.\n\nvif(lr.glm)\n\n       AgeOld   GenderWomen \n1.00481494539 1.00481494539 \n\n\nIn addition, predictors with 1/VIF values \\(<\\) .1 must be removed (data points with values above .2 are considered problematic) [25] and the mean value of VIFs should be \\(~\\) 1 [19].\n\nmean(vif(lr.glm))\n\n[1] 1.00481494539\n\n\n\n\nOutlier detection\nIn order to detect potential outliers, we will calculate diagnostic parameters and add these to our data set.\n\ninfl <- influence.measures(lr.glm) # calculate influence statistics\nblrdata <- data.frame(blrdata, infl[[1]], infl[[2]]) # add influence statistics\n\nIn a next step, we use these diagnostic parameters to check if there are data points which should be removed as they unduly affect the model fit.\n\n\nSample Size\nWe now check whether the sample size is sufficient for our analysis [15].\n\nif you are interested in the overall model: 50 + 8k (k = number of predictors)\nif you are interested in individual predictors: 104 + k\nif you are interested in both: take the higher value!\n\n\n# function to evaluate sample size\nsmplesz <- function(x) {\n  ifelse((length(x$fitted) < (104 + ncol(summary(x)$coefficients)-1)) == TRUE,\n    return(\n      paste(\"Sample too small: please increase your sample by \",\n      104 + ncol(summary(x)$coefficients)-1 - length(x$fitted),\n      \" data points\", collapse = \"\")),\n    return(\"Sample size sufficient\")) }\n# apply unction to model\nsmplesz(lr.glm)\n\n[1] \"Sample size sufficient\"\n\n\nAccording to rule of thumb provided in [15], the sample size is sufficient for our analysis.\n\n\nSummarizing Results\nAs a final step, we summarize our findings in tabulated form.\n\nsjPlot::tab_model(lr.glm)\n\n\n\n\n \nEH\n\n\nPredictors\nOdds Ratios\nCI\np\n\n\n(Intercept)\n0.79\n0.76 – 0.83\n<0.001\n\n\nAge [Old]\n0.44\n0.41 – 0.47\n<0.001\n\n\nGender [Women]\n0.66\n0.62 – 0.69\n<0.001\n\n\nObservations\n25821\n\n\nR2 Tjur\n0.032\n\n\n\n\n\n\n\nA more detailed summary table can be retrieved as follows:\n\n# load function\nsource(\"https://slcladal.github.io/rscripts/blrsummary.r\")\n# calculate accuracy \npredict.acc <- caret::confusionMatrix(blrdata$Prediction, blrdata$EH)\npredict.acc <- predict.acc[3]$overall[[1]]\n# create summary table\nblrsummarytb <- blrsummary(lr.glm, lr.lrm, predict.acc) \n\n\n\n\n\n\nResults of the binomial logistic regression analysis.\n\n\nStatisticsEstimateVIFOddsRatioCI(2.5%)CI(97.5%)Std. Errorz valuePr(>|z|)Significance(Intercept)-0.230.790.760.830.02-10.440p < .001***AgeOld-0.8310.440.410.470.03-24.780p < .001***GenderWomen-0.4210.660.620.690.03-15.420p < .001***Model statisticsValueNumber of cases in model25821Observed misses0 :17114Observed successes1 :8707Null deviance33007.75Residual deviance32139.54R2 (Nagelkerke)0.046R2 (Hosmer & Lemeshow)0.026R2 (Cox & Snell)0.033C0.602Somers' Dxy0.203AIC32145.54Prediction accuracy0.66%Model Likelihood Ratio TestModel L.R.: 868.21df: 2p-value: 0sig: p < .001***\n\n\nR2 (Hosmer & Lemeshow)\nHosmer and Lemeshow’s R2 “is the proportional reduction in the absolute value of the log-likelihood measure and as such it is a measure of how much the badness of fit improves as a result of the inclusion of the predictor variables. It can vary between 0 (indicating that the predictors are useless at predicting the outcome variable) and 1 (indicating that the model predicts the outcome variable perfectly)” [6].\nR2 (Cox & Snell)\n“Cox and Snell’s R2 (1989) is based on the deviance of the model (-2LL(new») and the deviance of the baseline model (-2LL(baseline), and the sample size, n […]. However, this statistic never reaches its theoretical maximum of 1.\nR2 (Nagelkerke)\nSince R2 (Cox & Snell) never reaches its theoretical maximum of 1, Nagelkerke (1991) suggested Nagelkerke’s R2 [6].\nSomers’ Dxy\nSomers’ Dxy is a rank correlation between predicted probabilities and observed responses ranges between 0 (randomness) and 1 (perfect prediction). Somers’ Dxy should have a value higher than .5 for the model to be meaningful [10].\nC\nC is an index of concordance between the predicted probability and the observed response. When C takes the value 0.5, the predictions are random, when it is 1, prediction is perfect. A value above 0.8 indicates that the model may have some real predictive capacity [10].\nAkaike information criteria (AIC)\nAkaike information criteria (AlC = -2LL + 2k) provide a value that reflects a ratio between the number of predictors in the model and the variance that is explained by these predictors. Changes in AIC can serve as a measure of whether the inclusion of a variable leads to a significant increase in the amount of variance that is explained by the model. “You can think of this as the price you pay for something: you get a better value of R2, but you pay a higher price, and was that higher price worth it? These information criteria help you to decide. The BIC is the same as the AIC but adjusts the penalty included in the AlC (i.e., 2k) by the number of cases: BlC = -2LL + 2k x log(n) in which n is the number of cases in the model” [6].\nWe can use the reports package [14] to summarize the analysis.\n\nreport::report(lr.glm)\n\nWe fitted a logistic model (estimated using ML) to predict EH with Age and Gender (formula: EH ~ Age + Gender). The model's explanatory power is weak (Tjur's R2 = 0.03). The model's intercept, corresponding to Age = Young and Gender = Men, is at -0.23 (95% CI [-0.28, -0.19], p < .001). Within this model:\n\n  - The effect of Age [Old] is statistically significant and negative (beta = -0.83, 95% CI [-0.90, -0.77], p < .001; Std. beta = -0.83, 95% CI [-0.90, -0.77])\n  - The effect of Gender [Women] is statistically significant and negative (beta = -0.42, 95% CI [-0.47, -0.37], p < .001; Std. beta = -0.42, 95% CI [-0.47, -0.37])\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using \n\n\nWe can use this output to write up a final report:\nWe fitted a logistic model (estimated using ML) to predict the use of the utterance-final discourse particle eh with Age and Gender (formula: EH ~ Age + Gender). The model’s explanatory power is weak (Tjur’s R2 = 0.03). The model’s intercept, corresponding to Age = Young and Gender = Men, is at -0.23 (95% CI [-0.28, -0.19], p < .001). Within this model:\n\nThe effect of Age [Old] is statistically significant and negative (beta = -0.83, 95% CI [-0.90, -0.77], p < .001; Std. beta = -0.83, 95% CI [-0.90, -0.77])\nThe effect of Gender [Women] is statistically significant and negative (beta = -0.42, 95% CI [-0.47, -0.37], p < .001; Std. beta = -0.42, 95% CI [-0.47, -0.37])\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using"
  },
  {
    "objectID": "regression.html#ordinal-regression",
    "href": "regression.html#ordinal-regression",
    "title": "Fixed- and Mixed-Effects Regression Models in R",
    "section": "Ordinal Regression",
    "text": "Ordinal Regression\nOrdinal regression is very similar to multiple linear regression but takes an ordinal dependent variable [26]. For this reason, ordinal regression is one of the key methods in analysing Likert data.\nTo see how an ordinal regression is implemented in R, we load and inspect the ´ordinaldata´ data set. The data set consists of 400 observations of students that were either educated at this school (Internal = 1) or not (Internal = 0). Some of the students have been abroad (Exchange = 1) while other have not (Exchange = 0). In addition, the data set contains the students’ final score of a language test (FinalScore) and the dependent variable which the recommendation of a committee for an additional, very prestigious program. The recommendation has three levels (very likely, somewhat likely, and unlikely) and reflects the committees’ assessment of whether the student is likely to succeed in the program.\n\n# load data\nordata  <- base::readRDS(url(\"https://slcladal.github.io/data/ord.rda\", \"rb\")) %>%\n  dplyr::rename(Recommend = 1, \n              Internal = 2, \n              Exchange = 3, \n              FinalScore = 4) %>%\n  dplyr::mutate(FinalScore = round(FinalScore, 2))\n\n\n\n\n\n\nFirst 15 rows of the ordata.\n\n\nRecommendInternalExchangeFinalScorevery likely003.26somewhat likely103.21unlikely113.94somewhat likely002.81somewhat likely002.53unlikely012.59somewhat likely002.56somewhat likely002.73unlikely003.00somewhat likely103.50unlikely113.65somewhat likely002.84very likely013.90somewhat likely002.68unlikely103.57\n\n\nIn a first step, we need to relevel the ordinal variable to represent an ordinal factor (or a progression from “unlikely” over “somewhat likely” to “very likely”. And we will also factorize Internal and Exchange to make it easier to interpret the output later on.\n\n# relevel data\nordata <- ordata %>%\n  dplyr::mutate(Recommend = factor(Recommend, \n                           levels=c(\"unlikely\", \"somewhat likely\", \"very likely\"),\n                           labels=c(\"unlikely\",  \"somewhat likely\",  \"very likely\"))) %>%\n  dplyr::mutate(Exchange = ifelse(Exchange == 1, \"Exchange\", \"NoExchange\")) %>%\n  dplyr::mutate(Internal = ifelse(Internal == 1, \"Internal\", \"External\"))\n\nNow that the dependent variable is releveled, we check the distribution of the variable levels by tabulating the data. To get a better understanding of the data we create frequency tables across variables rather than viewing the variables in isolation.\n\n## three way cross tabs (xtabs) and flatten the table\nftable(xtabs(~ Exchange + Recommend + Internal, data = ordata))\n\n                           Internal External Internal\nExchange   Recommend                                 \nExchange   unlikely                       25        6\n           somewhat likely                12        4\n           very likely                     7        3\nNoExchange unlikely                      175       14\n           somewhat likely                98       26\n           very likely                    20       10\n\n\nWe also check the mean and standard deviation of the final score as final score is a numeric variable and cannot be tabulated (unless we convert it to a factor).\n\nsummary(ordata$FinalScore); sd(ordata$FinalScore)\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n1.900000 2.720000 2.990000 2.998925 3.270000 4.000000 \n\n\n[1] 0.397940933861\n\n\nThe lowest score is 1.9 and the highest score is a 4.0 with a mean of approximately 3. Finally, we inspect the distributions graphically.\n\n# visualize data\nggplot(ordata, aes(x = Recommend, y = FinalScore)) +\n  geom_boxplot(size = .75) +\n  geom_jitter(alpha = .5) +\n  facet_grid(Exchange ~ Internal, margins = TRUE) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1))\n\n\n\n\nWe see that we have only few students that have taken part in an exchange program and there are also only few internal students overall. With respect to recommendations, only few students are considered to very likely succeed in the program. We can now start with the modeling by using the polr function. To make things easier for us, we will only consider the main effects here as this tutorial only aims to how to implement an ordinal regression but not how it should be done in a proper study - then, the model fitting and diagnostic procedures would have to be performed accurately, of course.\n\n# fit ordered logit model and store results 'm'\nm.or <- polr(Recommend ~ Internal + Exchange + FinalScore, data = ordata, Hess=TRUE)\n# summarize model\nsummary(m.or)\n\nCall:\npolr(formula = Recommend ~ Internal + Exchange + FinalScore, \n    data = ordata, Hess = TRUE)\n\nCoefficients:\n                          Value  Std. Error     t value\nInternalInternal   1.0476639460 0.265789134 3.941710973\nExchangeNoExchange 0.0586810767 0.297858822 0.197009698\nFinalScore         0.6157435926 0.260631275 2.362508462\n\nIntercepts:\n                            Value       Std. Error  t value    \nunlikely|somewhat likely    2.261997623 0.882173604 2.564118460\nsomewhat likely|very likely 4.357441880 0.904467838 4.817685824\n\nResidual Deviance: 717.024871356 \nAIC: 727.024871356 \n\n\nThe results show that having studied here at this school increases the chances of receiving a positive recommendation but that having been on an exchange has a negative but insignificant effect on the recommendation. The final score also correlates positively with a positive recommendation but not as much as having studied here.\n\n## store table\n(ctable <- coef(summary(m.or)))\n\n                                      Value     Std. Error        t value\nInternalInternal            1.0476639460469 0.265789134041 3.941710972596\nExchangeNoExchange          0.0586810766831 0.297858821982 0.197009698396\nFinalScore                  0.6157435925546 0.260631274948 2.362508462104\nunlikely|somewhat likely    2.2619976233665 0.882173604268 2.564118459704\nsomewhat likely|very likely 4.3574418799106 0.904467837679 4.817685824069\n\n\nAs the regression report does not provide p-values, we have to calculate them separately (after having calculated them, we add them to the coefficient table).\n\n## calculate and store p values\np <- pnorm(abs(ctable[, \"t value\"]), lower.tail = FALSE) * 2\n## combined table\n(ctable <- cbind(ctable, \"p value\" = p))\n\n                                      Value     Std. Error        t value\nInternalInternal            1.0476639460469 0.265789134041 3.941710972596\nExchangeNoExchange          0.0586810766831 0.297858821982 0.197009698396\nFinalScore                  0.6157435925546 0.260631274948 2.362508462104\nunlikely|somewhat likely    2.2619976233665 0.882173604268 2.564118459704\nsomewhat likely|very likely 4.3574418799106 0.904467837679 4.817685824069\n                                        p value\nInternalInternal            0.00008090242989074\nExchangeNoExchange          0.84381994829785212\nFinalScore                  0.01815172703306605\nunlikely|somewhat likely    0.01034382345525988\nsomewhat likely|very likely 0.00000145232812832\n\n\nAs predicted, Exchange does not have a significant effect but FinalScore and Internal both correlate significantly with the likelihood of receiving a positive recommendation.\n\n# extract profiled confidence intervals\nci <- confint(m.or)\n# calculate odds ratios and combine them with profiled CIs\nexp(cbind(OR = coef(m.or), ci))\n\n                              OR          2.5 %        97.5 %\nInternalInternal   2.85098328212 1.695837799597 4.81711408266\nExchangeNoExchange 1.06043698872 0.595033205649 1.91977108408\nFinalScore         1.85103250193 1.113625249822 3.09849059342\n\n\nThe odds ratios show that internal students are 2.85 times or 285 percent as likely as non-internal students to receive positive evaluations and that a 1-point increase in the test score lead to a 1.85 times or 185 percent increase in the chances of receiving a positive recommendation. The effect of an exchange is slightly negative but, as we have seen above, not significant."
  },
  {
    "objectID": "regression.html#poisson-regression",
    "href": "regression.html#poisson-regression",
    "title": "Fixed- and Mixed-Effects Regression Models in R",
    "section": "Poisson Regression",
    "text": "Poisson Regression\nThis section is based on this tutorials on how to perform a Poisson regression in R.\n\n\n\nNOTEPoisson regressions are used to analyze data where the dependent variable represents counts.\n\n\n\n\n\n\nThis applied particularly to counts that are based on observations of something that is measured in set intervals. For instances the number of pauses in two-minute-long conversations. Poisson regressions are particularly appealing when dealing with rare events, i.e. when something only occurs very infrequently. In such cases, normal linear regressions do not work because the instances that do occur are automatically considered outliers. Therefore, it is useful to check if the data conform to a Poisson distribution.\nHowever, the tricky thing about Poisson regressions is that the data has to conform to the Poisson distribution which is, according to my experience, rarely the case, unfortunately. The Gaussian Normal Distribution is very flexible because it is defined by two parameters, the mean (mu, i.e. \\(\\mu\\)) and the standard deviation (sigma, i.e. \\(\\sigma\\)). This allows the normal distribution to take very different shapes (for example, very high and slim (compressed) or very wide and flat). In contrast, the Poisson is defined by only one parameter (lambda, i.e. \\(\\lambda\\)) which mean that if we have a mean of 2, then the standard deviation is also 2 (actually we would have to say that the mean is \\(\\lambda\\) and the standard deviation is also \\(\\lambda\\) or \\(\\lambda\\) = \\(\\mu\\) = \\(\\sigma\\)). This is much trickier for natural data as this means that the Poisson distribution is very rigid.\n\n\n\n\n\nAs we can see, as \\(\\lambda\\) takes on higher values, the distribution becomes wider and flatter - a compressed distribution with a high mean can therefore not be Poisson-distributed. We will now start by loading the data.\n\n# load data\npoissondata  <- base::readRDS(url(\"https://slcladal.github.io/data/prd.rda\", \"rb\"))\n\n\n\n\n\n\nFirst 15 rows of the poissondata.\n\n\nIdPausesLanguageAlcohol450German411080Russian41150German44670German421530German40510Russian421640German461330German4020German33530German4610German401280English38160German441060German37890German40\n\n\nWe will clean the data by factorizing Id which is currently considered a numeric variable rather than a factor.\n\n# process data\npoissondata <- poissondata %>%\n  mutate(Id = factor(Id, levels = 1:200, labels = 1:200))\n# inspect data\nstr(poissondata)\n\n'data.frame':   200 obs. of  4 variables:\n $ Id      : Factor w/ 200 levels \"1\",\"2\",\"3\",\"4\",..: 45 108 15 67 153 51 164 133 2 53 ...\n $ Pauses  : int  0 0 0 0 0 0 0 0 0 0 ...\n $ Language: chr  \"German\" \"Russian\" \"German\" \"German\" ...\n $ Alcohol : int  41 41 44 42 40 42 46 40 33 46 ...\n\n\nFirst, we check if the conditions for a Poisson regression are met.\n\n# output the results\ngf = vcd::goodfit(poissondata$Pauses, \n                  type= \"poisson\", \n                  method= \"ML\")\n# inspect results\nsummary(gf)\n\n\n     Goodness-of-fit test for poisson distribution\n\n                           X^2 df            P(> X^2)\nLikelihood Ratio 33.0122916717  5 0.00000374234139957\n\n\nIf the p-values is smaller than .05, then data is not Poisson distributed which means that it differs significantly from a Poisson distribution and is very likely over-dispersed. We will check the divergence from a Poisson distribution visually by plotting the observed counts against the expected counts if the data were Poisson distributed.\n\nplot(gf,main=\"Count data vs Poisson distribution\")\n\n\n\n\nAlthough the goodfit function reported that the data differs significantly from the Poisson distribution, the fit is rather good. We can use an additional Levene’s test to check if variance homogeneity is given.\n\n\n\nNOTEThe use of Levene’s test to check if the model is heteroscedastic is generally not recommended as it is too lax when dealing with few observations (because in such cases it does not have the power to identify heteroscedasticity) while it is too harsh when dealing with many observations (when heteroscedasticity typically is not a severe problem).\n\n\n\n\n\n\n\n# check homogeneity\nleveneTest(poissondata$Pauses, poissondata$Language, center = mean)\n\nLevene's Test for Homogeneity of Variance (center = mean)\n       Df  F value        Pr(>F)    \ngroup   2 17.15274 0.00000013571 ***\n      197                           \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe Levene’s test indicates that variance homogeneity is also violated. Since both the approximation to a Poisson distribution and variance homogeneity are violated, we should switch either to a quasi-Poisson model or a negative binomial model. However, as we are only interested in how to implement a Poisson model here, we continue despite the fact that this could not be recommended if we were actually interested in accurate results based on a reliable model.\nIn a next step, we summarize Progression by inspecting the means and standard deviations of the individual variable levels.\n\n# extract mean and standard deviation\nwith(poissondata, tapply(Pauses, Language, function(x) {\n  sprintf(\"M (SD) = %1.2f (%1.2f)\", mean(x), sd(x))\n}))\n\n               English                 German                Russian \n\"M (SD) = 1.00 (1.28)\" \"M (SD) = 0.24 (0.52)\" \"M (SD) = 0.20 (0.40)\" \n\n\nNow, we visualize the data.\n\n# plot data\nggplot(poissondata, aes(Pauses, fill = Language)) +\n  geom_histogram(binwidth=.5, position=\"dodge\") +\n  scale_fill_manual(values=c(\"gray30\", \"gray50\", \"gray70\"))\n\n\n\n\n\n# calculate Poisson regression\nm1.poisson <- glm(Pauses ~ Language + Alcohol, family=\"poisson\", data=poissondata)\n# inspect model\nsummary(m1.poisson)\n\n\nCall:\nglm(formula = Pauses ~ Language + Alcohol, family = \"poisson\", \n    data = poissondata)\n\nDeviance Residuals: \n         Min            1Q        Median            3Q           Max  \n-2.204338080  -0.843641817  -0.510586515   0.255772098   2.679576560  \n\nCoefficients:\n                     Estimate    Std. Error  z value         Pr(>|z|)    \n(Intercept)     -4.1632652529  0.6628774832 -6.28060 0.00000000033728 ***\nLanguageGerman  -0.7140499158  0.3200148750 -2.23130         0.025661 *  \nLanguageRussian -1.0838591456  0.3582529824 -3.02540         0.002483 ** \nAlcohol          0.0701523975  0.0105992050  6.61865 0.00000000003625 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 287.6722345  on 199  degrees of freedom\nResidual deviance: 189.4496199  on 196  degrees of freedom\nAIC: 373.5045031\n\nNumber of Fisher Scoring iterations: 6\n\n\nIn addition to the Estimates for the coefficients, we could also calculate the confidence intervals for the coefficients (LL stands for lower limit and UL for upper limit in the table below).\n\n# calculate model\ncov.m1 <- sandwich::vcovHC(m1.poisson, type=\"HC0\")\n# extract standard error\nstd.err <- sqrt(diag(cov.m1))\n# extract robust estimates\nr.est <- cbind(Estimate= coef(m1.poisson), \n               \"Robust SE\" = std.err,\n               \"Pr(>|z|)\" = 2 * pnorm(abs(coef(m1.poisson)/std.err),\n                                      lower.tail=FALSE),\nLL = coef(m1.poisson) - 1.96 * std.err,\nUL = coef(m1.poisson) + 1.96 * std.err)\n# inspect data\nr.est\n\n                        Estimate       Robust SE                 Pr(>|z|)\n(Intercept)     -4.1632652529178 0.6480942940026 0.0000000001328637743948\nLanguageGerman  -0.7140499157783 0.2986422497410 0.0168031204011183932234\nLanguageRussian -1.0838591456208 0.3210481575684 0.0007354744824167306532\nAlcohol          0.0701523974937 0.0104351647012 0.0000000000178397516955\n                              LL               UL\n(Intercept)     -5.4335300691629 -2.8930004366727\nLanguageGerman  -1.2993887252708 -0.1287111062859\nLanguageRussian -1.7131135344549 -0.4546047567867\nAlcohol          0.0496994746793  0.0906053203082\n\n\nWe can now calculate the p-value of the model.\n\nwith(m1.poisson, cbind(res.deviance = deviance, df = df.residual,\n  p = pchisq(deviance, df.residual, lower.tail=FALSE)))\n\n     res.deviance  df              p\n[1,] 189.44961991 196 0.618227445717\n\n\nNow, we check, if removing Language leads to a significant decrease in model fit.\n\n# remove Language from the model\nm2.poisson <- update(m1.poisson, . ~ . -Language)\n# check if dropping Language causes a significant decrease in model fit\nanova(m2.poisson, m1.poisson, test=\"Chisq\")\n\nAnalysis of Deviance Table\n\nModel 1: Pauses ~ Alcohol\nModel 2: Pauses ~ Language + Alcohol\n  Resid. Df  Resid. Dev Df   Deviance   Pr(>Chi)    \n1       198 204.0213018                             \n2       196 189.4496199  2 14.5716819 0.00068517 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWe now calculate robust coefficients using the msm package [27].\n\n# get estimates\ns <- msm::deltamethod(list(~ exp(x1), ~ exp(x2), ~ exp(x3), ~ exp(x4)), \n                                                coef(m1.poisson), cov.m1)\n# exponentiate old estimates dropping the p values\nrexp.est <- exp(r.est[, -3])\n# replace SEs with estimates for exponentiated coefficients\nrexp.est[, \"Robust SE\"] <- s\n# display results\nrexp.est\n\n                       Estimate       Robust SE               LL\n(Intercept)     0.0155566784084 0.0100821945101 0.00436765044896\nLanguageGerman  0.4896571063601 0.1462322998451 0.27269843575906\nLanguageRussian 0.3382875026084 0.1086065794408 0.18030353649622\nAlcohol         1.0726716412682 0.0111935052470 1.05095521026088\n                             UL\n(Intercept)     0.0554097096208\nLanguageGerman  0.8792279322820\nLanguageRussian 0.6346987787641\nAlcohol         1.0948368101199\n\n\n\n# extract predicted values\n(s1 <- data.frame(Alcohol = mean(poissondata$Alcohol),\n  Language = factor(1:3, levels = 1:3, labels = names(table(poissondata$Language)))))\n\n  Alcohol Language\n1  52.645  English\n2  52.645   German\n3  52.645  Russian\n\n# show results\npredict(m1.poisson, s1, type=\"response\", se.fit=TRUE)\n\n$fit\n             1              2              3 \n0.624944591447 0.306008560283 0.211410945109 \n\n$se.fit\n              1               2               3 \n0.0862811728183 0.0883370633684 0.0705010813453 \n\n$residual.scale\n[1] 1\n\n\n\n## calculate and store predicted values\npoissondata$Predicted <- predict(m1.poisson, type=\"response\")\n## order by program and then by math\npoissondata <- poissondata[with(poissondata, order(Language, Alcohol)), ]\n\n\n## create the plot\nggplot(poissondata, aes(x = Alcohol, y = Predicted, colour = Language)) +\n  geom_point(aes(y = Pauses), alpha=.5, \n             position=position_jitter(h=.2)) +\n  geom_line(size = 1) +\n  labs(x = \"Alcohol (ml)\", y = \"Expected number of pauses\") +\n  scale_color_manual(values=c(\"gray30\", \"gray50\", \"gray70\"))"
  },
  {
    "objectID": "regression.html#robust-regression",
    "href": "regression.html#robust-regression",
    "title": "Fixed- and Mixed-Effects Regression Models in R",
    "section": "Robust Regression",
    "text": "Robust Regression\nRobust regression represent an alternative to simple linear models which can handle overly influential data points (outliers). Robust regressions allow us to retain outliers in the data rather than having to remove them from the data by adding weights [28]. Thus, robust regressions are used when there are outliers present in the data and we can thus not use traditional models but we have no good argument to remove these data points.\n\n\n\nNOTERobust regressions allow us to handle overly influential data points (outliers) by using weights. Thus, robust regressions enable us to retain all data points.\n\n\n\n\n\n\nWe begin by loading a data set (the mlrdata set which have used for multiple linear regression).\n\n# load data\nrobustdata  <- base::readRDS(url(\"https://slcladal.github.io/data/mld.rda\", \"rb\"))\n\n\n\n\n\n\nFirst 15 rows of the robustdata.\n\n\nstatusattractionmoneyRelationshipNotInterested86.33RelationshipNotInterested45.58RelationshipNotInterested68.43RelationshipNotInterested52.93RelationshipNotInterested61.86RelationshipNotInterested48.47RelationshipNotInterested32.79RelationshipNotInterested35.91RelationshipNotInterested30.98RelationshipNotInterested44.82RelationshipNotInterested35.05RelationshipNotInterested64.49RelationshipNotInterested54.50RelationshipNotInterested61.48RelationshipNotInterested55.51\n\n\nWe first fit an ordinary linear model (and although we know from the section on multiple regression that the interaction between status and attraction is significant, we will disregard this for now as this will help to explain the weighing procedure which is the focus of this section).\n\n# create model\nslm <- lm(money ~ status+attraction, data = robustdata)\n# inspect model\nsummary(slm)\n\n\nCall:\nlm(formula = money ~ status + attraction, data = robustdata)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-60.87070 -15.78645  -2.61010  13.88770  59.93710 \n\nCoefficients:\n                            Estimate   Std. Error   t value\n(Intercept)             114.94950000   4.28993616  26.79515\nstatusSingle             26.10340000   4.95359160   5.26959\nattractionNotInterested -79.25220000   4.95359160 -15.99894\n                                      Pr(>|t|)    \n(Intercept)             < 0.000000000000000222 ***\nstatusSingle                     0.00000082576 ***\nattractionNotInterested < 0.000000000000000222 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 24.767958 on 97 degrees of freedom\nMultiple R-squared:  0.745229335,   Adjusted R-squared:  0.739976331 \nF-statistic: 141.867286 on 2 and 97 DF,  p-value: < 0.0000000000000002220446\n\n\nWe now check whether the model is well fitted using diagnostic plots.\n\n# generate plots\nautoplot(slm) + \n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) \n\n\n\n\nThe diagnostic plots indicate that there are three outliers in the data (data points 52, 83 and possibly 64). Therefore, we need to evaluate if the outliers severely affect the fit of the model.\n\nrobustdata[c(52, 64, 83),]\n\n   status    attraction  money\n52 Single NotInterested   0.93\n64 Single NotInterested  84.28\n83 Single    Interested 200.99\n\n\nWe can now calculate Cook’s distance and standardized residuals check if the values of the potentially problematic points have unacceptably high values (-2 < ok < 2).\n\nCooksDistance <- cooks.distance(slm)\nStandardizedResiduals <- stdres(slm)\na <- cbind(robustdata, CooksDistance, StandardizedResiduals)\na[CooksDistance > 4/100, ]\n\n         status    attraction  money   CooksDistance StandardizedResiduals\n1  Relationship NotInterested  86.33 0.0444158829857         2.07565427025\n52       Single NotInterested   0.93 0.0641937458854        -2.49535435377\n65       Single NotInterested  12.12 0.0427613629698        -2.03662765573\n67       Single NotInterested  13.28 0.0407877963315        -1.98907421786\n83       Single    Interested 200.99 0.0622397126545         2.45708203516\n84       Single    Interested 193.69 0.0480020776213         2.15782333134\n88       Single    Interested 193.78 0.0481663678409         2.16151282221\n\n\nWe will calculate the absolute value and reorder the table so that it is easier to check the values.\n\nAbsoluteStandardizedResiduals <- abs(StandardizedResiduals)\na <- cbind(robustdata, CooksDistance, StandardizedResiduals, AbsoluteStandardizedResiduals)\nasorted <- a[order(-AbsoluteStandardizedResiduals), ]\nasorted[1:10, ]\n\n         status    attraction  money   CooksDistance StandardizedResiduals\n52       Single NotInterested   0.93 0.0641937458854        -2.49535435377\n83       Single    Interested 200.99 0.0622397126545         2.45708203516\n88       Single    Interested 193.78 0.0481663678409         2.16151282221\n84       Single    Interested 193.69 0.0480020776213         2.15782333134\n1  Relationship NotInterested  86.33 0.0444158829857         2.07565427025\n65       Single NotInterested  12.12 0.0427613629698        -2.03662765573\n67       Single NotInterested  13.28 0.0407877963315        -1.98907421786\n78       Single    Interested 188.76 0.0394313968241         1.95572122040\n21 Relationship NotInterested  81.90 0.0369837409025         1.89404933081\n24 Relationship NotInterested  81.56 0.0364414260634         1.88011125419\n   AbsoluteStandardizedResiduals\n52                 2.49535435377\n83                 2.45708203516\n88                 2.16151282221\n84                 2.15782333134\n1                  2.07565427025\n65                 2.03662765573\n67                 1.98907421786\n78                 1.95572122040\n21                 1.89404933081\n24                 1.88011125419\n\n\nAs Cook’s distance and the standardized residuals do have unacceptable values, we re-calculate the linear model as a robust regression and inspect the results\n\n# create robust regression model\nrmodel <- robustbase::lmrob(money ~ status + attraction, data = robustdata)\n# inspect model\nsummary(rmodel)\n\n\nCall:\nrobustbase::lmrob(formula = money ~ status + attraction, data = robustdata)\n \\--> method = \"MM\"\nResiduals:\n         Min           1Q       Median           3Q          Max \n-61.14269796 -15.20405781  -1.48712081  14.43502508  62.42342804 \n\nCoefficients:\n                            Estimate   Std. Error   t value\n(Intercept)             113.18405781   3.89777692  29.03811\nstatusSingle             25.38251415   5.08841106   4.98830\nattractionNotInterested -76.49387400   5.06626449 -15.09867\n                                      Pr(>|t|)    \n(Intercept)             < 0.000000000000000222 ***\nstatusSingle                      0.0000026725 ***\nattractionNotInterested < 0.000000000000000222 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nRobust residual standard error: 22.3497532 \nMultiple R-squared:  0.740716949,   Adjusted R-squared:  0.735370907 \nConvergence in 11 IRWLS iterations\n\nRobustness weights: \n 10 weights are ~= 1. The remaining 90 ones are summarized as\n       Min.     1st Qu.      Median        Mean     3rd Qu.        Max. \n0.415507215 0.856134403 0.947485769 0.889078657 0.986192099 0.998890516 \nAlgorithmic parameters: \n           tuning.chi                    bb            tuning.psi \n1.5476399999999999046 0.5000000000000000000 4.6850610000000001421 \n           refine.tol               rel.tol             scale.tol \n0.0000001000000000000 0.0000001000000000000 0.0000000001000000000 \n            solve.tol           eps.outlier                 eps.x \n0.0000001000000000000 0.0010000000000000000 0.0000000000018189894 \n    warn.limit.reject     warn.limit.meanrw \n0.5000000000000000000 0.5000000000000000000 \n     nResample         max.it       best.r.s       k.fast.s          k.max \n           500             50              2              1            200 \n   maxit.scale      trace.lev            mts     compute.rd fast.s.large.n \n           200              0           1000              0           2000 \n                  psi           subsampling                   cov \n           \"bisquare\"         \"nonsingular\"         \".vcov.avar1\" \ncompute.outlier.stats \n                 \"SM\" \nseed : int(0) \n\n\nThe output shows that both status and attraction are significant but, as we have seen above, the effect that really matters is the interaction between status and attraction.\nWe will briefly check the weights to understand the process of weighing better. The idea of weighing is to downgrade data points that are too influential while not punishing data points that have a good fit and are thus less influential. This means that the problematic data points should have lower weights than other data points (the maximum is 1 - so points can only be made “lighter”).\n\nhweights <- data.frame(status = robustdata$status, resid = rmodel$resid, weight = rmodel$rweights)\nhweights2 <- hweights[order(rmodel$rweights), ]\nhweights2[1:15, ]\n\n         status          resid         weight\n83       Single  62.4234280426 0.415507214894\n52       Single -61.1426979566 0.434323578289\n88       Single  55.2134280426 0.521220526760\n84       Single  55.1234280426 0.522529106622\n78       Single  50.1934280426 0.593234343069\n65       Single -49.9526979566 0.596626306571\n1  Relationship  49.6398161925 0.601024865582\n67       Single -48.7926979566 0.612874578464\n21 Relationship  45.2098161925 0.661914499988\n24 Relationship  44.8698161925 0.666467581490\n39 Relationship -43.8940578083 0.679427975993\n79       Single  40.8234280426 0.719104361372\n58       Single -40.5226979566 0.722893449575\n89       Single  39.9734280426 0.729766992775\n95       Single  39.8234280426 0.731633375015\n\n\nThe values of the weights support our assumption that those data points that were deemed too influential are made lighter as they now only have weights of 0.415507214894 and 0.434323578289 respectively. This was, however, not the focus of this sections as this section merely served to introduce the concept of weights and how they can be used in the context of a robust linear regression."
  },
  {
    "objectID": "regression.html#linear-mixed-effects-regression",
    "href": "regression.html#linear-mixed-effects-regression",
    "title": "Fixed- and Mixed-Effects Regression Models in R",
    "section": "Linear Mixed-Effects Regression",
    "text": "Linear Mixed-Effects Regression\nThe following focuses on an extension of ordinary multiple linear regressions: mixed-effects regression linear regression. Mixed-effects models have the following advantages over simpler statistical tests:\n\nMixed-effects models are multivariate, i.e. they test the effect of several predictors simultaneously while controlling for the effect of all other predictors.\nMixed models allow to statistically incorporate within-speaker variability and are thus fit to model hierarchical or nested data structures. This applies if several observations are produced by an individual speaker, for instance.\nMixed-models provide a wealth of diagnostic statistics which enables us to control e.g. (multi-)collinearity, i.e. correlations between predictors, and to test whether conditions or requirements are violated (e.g. homogeneity of variance, etc.).\n\nMajor disadvantages of mixed-effects regression modeling are that they are prone to producing high \\(\\beta\\)-errors [see 29] and that they require rather large data sets.\n\nIntroduction\nSo far, the regression models that we have used only had fixed-effects. Having only fixed-effects means that all data points are treated as if they are completely independent and thus on the same hierarchical level. However, it is very common that the data is nested in the sense that data points are not independent because they are, for instance produced by the same speaker or are grouped by some other characteristic. In such cases, the data is considered hierarchical and statistical models should incorporate such structural features of the data they work upon. Fortunately, modeling hierarchical or nested data structures is very easy thanks to the lme4 package [1].\nWith respect to regression modeling, hierarchical structures are incorporated by what is called random effects. When models only have a fixed-effects structure, then they make use of only a single intercept and/or slope (as in the left panel in the figure below), while mixed effects models have intercepts for each level of a random effect. If the random effect structure represents speakers then this would mean that a mixed-model would have a separate intercept and/or slope for each speaker (in addition to the overall intercept that is shown as an orange line in the figure below).\n\n\n\n\n\nThe idea behind regression analysis is expressed formally in the equation below where\\(f_{(x)}\\) is the y-value we want to predict, \\(\\alpha\\) is the intercept (the point where the regression line crosses the y-axis at x = 0), \\(\\beta\\) is the coefficient (the slope of the regression line), and x is the value of a predictor (e.g. 180cm - if we would like to predict the weight of a person based on their height). The \\(\\epsilon\\) is an error term that reflects the difference between the predicted value and the (actually) observed value (\\(\\epsilon\\) is thus a residual that is important as regressions assume that residuals are, e.g., normally distributed).\n\\[\\begin{equation}\nf_{(x)} = \\alpha + \\beta x + \\epsilon\n\\end{equation}\\]\nIn other words, to estimate how much some weights who is 180cm tall, we would multiply the coefficient (slope of the line) with 180 (\\(x\\)) and add the value of the intercept (point where line crosses the y-axis at x = 0).\nThe equation below represents a formal representation of a mixed-effects regression with varying intercepts [see 12].\n\\[\\begin{equation}\nf_{(x)} = \\alpha_{i} + \\beta x + \\epsilon\n\\end{equation}\\]\nIn this random intercept model, each level of a random variable has a different intercept. To predict the value of a data point, we would thus take the appropriate intercept value (the model intercept + the intercept of the random effect) and add the product of the predictor coefficient and the value of x.\nFinally, the equation below represents a formal representation of a mixed-effects regression with varying intercepts and varying slopes [see 12].\n\\[\\begin{equation}\nf_{(x)} = \\alpha_{i} + \\beta_{i}x + \\epsilon\n\\end{equation}\\]\nIn this last model, each level of a random variable has a different intercept and a different slope. To predict the value of a data point, we would thus take the appropriate intercept value (the model intercept + the intercept of the random effect) and add the coefficient of that random effect level multiplied by the value of x.\n\n\nRandom Effects\nRandom Effects can be visualized using two parameters: the intercept (the point where the regression line crosses the y-axis at x = 0) and the slope (the acclivity of the regression line). In contrast to fixed-effects models, that have only 1 intercept and one slope (left panel in the figure above), mixed-effects models can therefore have various random intercepts (center panel) or various random slopes, or both, various random intercepts and various random slopes (right panel).\nWhat features do distinguish random and fixed effects?\n\nRandom effects represent a higher level variable under which data points are grouped. This implies that random effects must be categorical (or nominal but they a´cannot be continuous!) [see 12].\nRandom effects represent a sample of an infinite number of possible levels. For instance, speakers, trials, items, subjects, or words represent a potentially infinite pool of elements from which many different samples can be drawn. Thus, random effects represent a random sample sample. Fixed effects, on the other hand, typically do not represent a random sample but a fixed set of variable levels (e.g. Age groups, or parts-of-speech).\nRandom effects typically represent many different levels while fixed effects typically have only a few. [30] propose that a variable may be used as a fixed effect if it has less than 5 levels while it should be treated as a random effect if it has more than 10 levels. Variables with 5 to 10 levels can be used as both. However, this is a rule of thumb and ignores the theoretical reasons (random sample and nestedness) for considering something as a random effect and it also is at odds with the way that repeated measures are models (namely as mixed effects) although they typically only have very few levels.\nFixed effects represent an effect that if we draw many samples, the effect would be consistent across samples [12] while random effects should vary for each new sample that is drawn.\n\nIn the following, we will only focus on models with random intercepts because this is the more common method and because including both random intercepts and random slopes requires larger data sets (but have a better fit because intercepts are not forced to be parallel and the lines therefore have a better fit). You should, however, always think about what random effects structure is appropriate for your model - a very recommendable explanation of how to chose which random effects structure is best (and about what the determining factors for this decision are) is give in [12]. Also, consider the center and the right plots above to understand what is meant by random intercepts and random slopes.\nAfter adding random intercepts, predictors (or fixed effects) are added to the model (just like with multiple regression). So mixed-effects are called mixed-effects because they contain both random and fixed effects.\nIn terms of general procedure, random effects are added first, and only after we have ascertained that including random effects is warranted, we test whether including fixed-effects is warranted [6]. We test whether including random effects is warranted by comparing a model, that bases its estimates of the depended variable solely on the base intercept (the mean), with a model, that bases its estimates of the dependent variable solely on the intercepts of the random effect. If the random-effect model explains significantly more variance than the simple model without random effect structure, then we continue with the mixed-effects model. In other words, including random effects is justified if they reduce residual deviance.\n\n\nExample: Preposition Use across Time by Genre\nTo explore how to implement a mixed-effects model in R we revisit the preposition data that contains relative frequencies of prepositions in English texts written between 1150 and 1913. As a first step, and to prepare our analysis, we load necessary R packages, specify options, and load as well as provide an overview of the data.\n\n# load data\nlmmdata  <- base::readRDS(url(\"https://slcladal.github.io/data/lmd.rda\", \"rb\")) %>%\n  # convert date into a numeric variable\n  dplyr::mutate(Date = as.numeric(Date))\n\n\n\n\n\n\nFirst 15 rows of the lmmdata.\n\n\nDateGenreTextPrepositionsRegion1,736Sciencealbin166.01North1,711Educationanon139.86North1,808PrivateLetterausten130.78North1,878Educationbain151.29North1,743Educationbarclay145.72North1,908Educationbenson120.77North1,906Diarybenson119.17North1,897Philosophyboethja132.96North1,785Philosophyboethri130.49North1,776Diaryboswell135.94North1,905Travelbradley154.20North1,711Educationbrightland149.14North1,762Sermonburton159.71North1,726Sermonbutler157.49North1,835PrivateLettercarlyle124.16North\n\n\nThe data set contains the date when the text was written (Date), the genre of the text (Genre), the name of the text (Text), the relative frequency of prepositions in the text (Prepositions), and the region in which the text was written (Region). We now plot the data to get a first impression of its structure.\n\np1 <- ggplot(lmmdata, aes(x = Date, y = Prepositions)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = F, color = \"red\", linetype = \"dashed\") +\n  theme_bw() +\n  labs(y = \"Frequency\\n(Prepositions)\")\np2 <- ggplot(lmmdata, aes(x = reorder(Genre, -Prepositions), y = Prepositions)) +\n  geom_boxplot() +\n  theme_bw() + \n  theme(axis.text.x = element_text(angle=90)) +\n  labs(x = \"Genre\", y = \"Frequency\\n(Prepositions)\")\np3 <- ggplot(lmmdata, aes(Prepositions)) +\n  geom_histogram() +\n  theme_bw() + \n  labs(y = \"Count\", x = \"Frequency (Prepositions)\")\ngrid.arrange(grobs = list(p1, p2, p3), widths = c(1, 1), layout_matrix = rbind(c(1, 1), c(2, 3)))\n\n\n\n\nThe scatter plot in the upper panel indicates that the use of prepositions has moderately increased over time while the boxplots in the lower left panel show that the genres differ quite substantially with respect to their median frequencies of prepositions per text. Finally, the histogram in the lower right panel show that preposition use is distributed normally with a mean of 132.2 prepositions per text.\n\np4 <- ggplot(lmmdata, aes(Date, Prepositions)) +\n  geom_point() +\n  labs(x = \"Year\", y = \"Prepositions per 1,000 words\") +\n  geom_smooth(method = \"lm\")  + \n  theme_bw()\np5 <- ggplot(lmmdata, aes(Region, Prepositions)) +\n  geom_boxplot() +\n  labs(x = \"Region\", y = \"Prepositions per 1,000 words\") +\n  geom_smooth(method = \"lm\")  + \n  theme_bw()\ngrid.arrange(p4, p5, nrow = 1)\n\n\n\n\n\nggplot(lmmdata, aes(Date, Prepositions)) +\n  geom_point() +\n  facet_wrap(~ Genre, nrow = 4) +\n  geom_smooth(method = \"lm\") +\n  theme_bw() +\n  labs(x = \"Date of composition\", y = \"Prepositions per 1,000 words\") +\n  coord_cartesian(ylim = c(0, 220))\n\n\n\n\nCentering or even scaling numeric variables is useful for later interpretation of regression models: if the date variable were not centered, the regression would show the effects of variables at year 0(!). If numeric variables are centered, other variables are variables are considered relative not to 0 but to the mean of that variable (in this case the mean of years in our data). Centering simply means that the mean of the numeric variable is subtracted from each value.\n\nlmmdata$DateUnscaled <- lmmdata$Date\nlmmdata$Date <- scale(lmmdata$Date, scale = F)\n\n\n\n\n\n\nFirst 15 rows of the lmmdata.\n\n\nDateGenreTextPrepositionsRegionDateUnscaled109.8696461825Sciencealbin166.01North1,73684.8696461825Educationanon139.86North1,711181.8696461825PrivateLetterausten130.78North1,808251.8696461825Educationbain151.29North1,878116.8696461825Educationbarclay145.72North1,743281.8696461825Educationbenson120.77North1,908279.8696461825Diarybenson119.17North1,906270.8696461825Philosophyboethja132.96North1,897158.8696461825Philosophyboethri130.49North1,785149.8696461825Diaryboswell135.94North1,776278.8696461825Travelbradley154.20North1,90584.8696461825Educationbrightland149.14North1,711135.8696461825Sermonburton159.71North1,76299.8696461825Sermonbutler157.49North1,726208.8696461825PrivateLettercarlyle124.16North1,835\n\n\nWe now set up a fixed-effects model with the glm function and a mixed-effects model using the glmer function from the lme4 package [1] with Genre as a random effect.\n\n# generate models\nm0.glm <- glm(Prepositions ~ 1, family = gaussian, data = lmmdata)\nm0.lmer = lmer(Prepositions ~ 1 + (1|Genre), REML = T, data = lmmdata)\n\nNow that we have created the base-line models, we will test whether including a random effect structure is mathematically justified. It is important to note here that we are not going to test if including a random effect structure is theoretically motivated but simply if it causes a decrease in variance.\n\n\nTesting Random Effects\nAs a first step in the modeling process, we now need to determine whether or not including a random effect structure is justified. We do so by comparing the AIC of the base-line model without random intercepts to the AIC of the model with random intercepts.\n\nAIC(logLik(m0.glm))\n\n[1] 4718.19031114\n\nAIC(logLik(m0.lmer))\n\n[1] 4497.77554693\n\n\nThe inclusion of a random effect structure with random intercepts is justified as the AIC of the model with random intercepts is substantially lower than the AIC of the model without random intercepts.\nWhile I do not how how to test if including a random effect is justified, there are often situations, which require to test exactly which random effect structure is best. When doing this, it is important to use restricted maximum likelihood (REML = TRUE or method = REML) rather than maximum likelihood [12, see 31].\n\n# generate models with 2 different random effect structures\nma.lmer = lmer(Prepositions ~ Date + (1|Genre), REML = T, data = lmmdata)\nmb.lmer = lmer(Prepositions ~ Date + (1 + Date | Genre), REML = T, data = lmmdata)\n# compare models\nanova(ma.lmer, mb.lmer, test = \"Chisq\", refit = F)\n\nData: lmmdata\nModels:\nma.lmer: Prepositions ~ Date + (1 | Genre)\nmb.lmer: Prepositions ~ Date + (1 + Date | Genre)\n        npar         AIC         BIC       logLik    deviance    Chisq Df\nma.lmer    4 4499.148092 4516.292084 -2245.574046 4491.148092            \nmb.lmer    6 4486.699509 4512.415498 -2237.349755 4474.699509 16.44858  2\n        Pr(>Chisq)    \nma.lmer               \nmb.lmer 0.00026806 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe model comparison shows that the model with the more complex random effect structure has a significantly better fit to the data compared with the model with the simpler random effect structure. However, we will continue with the model with the simpler structure because this is just an example.\n\n\n\nNOTEIn a real analysis, we would switch to a model with random intercepts and random slopes for Genre because it has a significantly better fit to the data.\n\n\n\n\n\n\n\n\nModel Fitting\nAfter having determined that including a random effect structure is justified, we can continue by fitting the model and including diagnostics as we go. Including diagnostics in the model fitting process can save time and prevent relying on models which only turn out to be unstable if we would perform the diagnostics after the fact.\nWe begin fitting our model by adding Date as a fixed effect and compare this model to our mixed-effects base-line model to see if Date improved the model fit by explaining variance and if Date significantly correlates with our dependent variable (this means that the difference between the models is the effect (size) of Date!)\n\nm1.lmer <- lmer(Prepositions ~ (1|Genre) + Date, REML = T, data = lmmdata)\nanova(m1.lmer, m0.lmer, test = \"Chi\")\n\nrefitting model(s) with ML (instead of REML)\n\n\nData: lmmdata\nModels:\nm0.lmer: Prepositions ~ 1 + (1 | Genre)\nm1.lmer: Prepositions ~ (1 | Genre) + Date\n        npar         AIC         BIC       logLik    deviance  Chisq Df\nm0.lmer    3 4501.947337 4514.805331 -2247.973668 4495.947337          \nm1.lmer    4 4495.017736 4512.161728 -2243.508868 4487.017736 8.9296  1\n        Pr(>Chisq)   \nm0.lmer              \nm1.lmer  0.0028059 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nrefitting model(s) with ML (instead of REML)\n\n\nThe model with Date is the better model (significant p-value and lower AIC). The significant p-value shows that Date correlates significantly with Prepositions (\\(\\chi\\)2(1): 8.929600937903, p = 0.00281) . The \\(\\chi\\)2 value here is labeled Chisq and the degrees of freedom are calculated by subtracting the smaller number of DFs from the larger number of DFs.\nWe now test whether Region should also be part of the final minimal adequate model. The easiest way to add predictors is by using the update function (it saves time and typing).\n\n# generate model\nm2.lmer <- update(m1.lmer, .~.+ Region)\n# test vifs\ncar::vif(m2.lmer)\n\n         Date        Region \n1.20287667936 1.20287667936 \n\n# compare models                \nanova(m2.lmer, m1.lmer, test = \"Chi\")\n\nrefitting model(s) with ML (instead of REML)\n\n\nData: lmmdata\nModels:\nm1.lmer: Prepositions ~ (1 | Genre) + Date\nm2.lmer: Prepositions ~ (1 | Genre) + Date + Region\n        npar         AIC         BIC       logLik    deviance   Chisq Df\nm1.lmer    4 4495.017736 4512.161728 -2243.508868 4487.017736           \nm2.lmer    5 4494.624343 4516.054333 -2242.312171 4484.624343 2.39339  1\n        Pr(>Chisq)\nm1.lmer           \nm2.lmer    0.12185\n\n\nThree things tell us that Region should not be included:\n\nthe AIC does not decrease,\nthe BIC increases(!), and\nthe p-value is higher than .05.\n\nThis means, that we will continue fitting the model without having Region included. Well… not quite - just as a note on including variables: while Region is not significant as a main effect, it must still be included in a model if it were part of a significant interaction. To test if this is indeed the case, we fit another model with the interaction between Date and Region as predictor.\n\n# generate model\nm3.lmer <- update(m1.lmer, .~.+ Region*Date)\n# extract vifs\ncar::vif(m3.lmer)\n\n         Date        Region   Date:Region \n1.96923042276 1.20324697637 1.78000887978 \n\n# compare models                \nanova(m3.lmer, m1.lmer, test = \"Chi\")\n\nrefitting model(s) with ML (instead of REML)\n\n\nData: lmmdata\nModels:\nm1.lmer: Prepositions ~ (1 | Genre) + Date\nm3.lmer: Prepositions ~ (1 | Genre) + Date + Region + Date:Region\n        npar         AIC         BIC       logLik    deviance   Chisq Df\nm1.lmer    4 4495.017736 4512.161728 -2243.508868 4487.017736           \nm3.lmer    6 4496.124872 4521.840861 -2242.062436 4484.124872 2.89286  2\n        Pr(>Chisq)\nm1.lmer           \nm3.lmer    0.23541\n\n\nAgain, the high p-value and the increase in AIC and BIC show that we have found our minimal adequate model with only contains Date as a main effect. In a next step, we can inspect the final minimal adequate model, i.e. the most parsimonious (the model that explains a maximum of variance with a minimum of predictors).\n\n# inspect results\nsummary(m1.lmer)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: Prepositions ~ (1 | Genre) + Date\n   Data: lmmdata\n\nREML criterion at convergence: 4491.1\n\nScaled residuals: \n         Min           1Q       Median           3Q          Max \n-3.734915441 -0.657038004  0.005865025  0.661298615  3.596659863 \n\nRandom effects:\n Groups   Name        Variance   Std.Dev.  \n Genre    (Intercept) 159.021120 12.6103576\n Residual             228.764179 15.1249522\nNumber of obs: 537, groups:  Genre, 16\n\nFixed effects:\n                   Estimate      Std. Error  t value\n(Intercept) 133.88516211469   3.24749296248 41.22724\nDate          0.01894493515   0.00632363682  2.99589\n\nCorrelation of Fixed Effects:\n     (Intr)\nDate 0.005 \n\n\n\n\nModel Diagnostics\nWe can now evaluate the goodness of fit of the model and check if mathematical requirements and assumptions have been violated. In a first step, we generate diagnostic plots that focus on the random effect structure.\n\nplot(m1.lmer, Genre ~ resid(.), abline = 0 ) # generate diagnostic plots\n\n\n\n\nThe plot shows that there are some outliers (points outside the boxes) and that the variability within letters is greater than in other genres we therefore examine the genres in isolation standardized residuals versus fitted values [31].\n\nplot(m1.lmer, resid(., type = \"pearson\") ~ fitted(.) | Genre, id = 0.05, \n     adj = -0.3, pch = 20, col = \"gray40\")\n\n\n\n\nThe plot shows the standardized residuals (or Pearson’s residuals) versus fitted values and suggests that there are outliers in the data (the names elements in the plots). To check if these outliers are a cause for concern, we will now use a Levene’s test to check if the variance is distributed homogeneously (homoscedasticity) or whether the assumption of variance homogeneity is violated (due to the outliers).\n\n\n\nNOTEThe use of Levene’s test to check if the model is heteroscedastic is generally not recommended as it is too lax when dealing with few observations (because in such cases it does not have the power to identify heteroscedasticity) while it is too harsh when dealing with many observations (when heteroscedasticity typically is not a severe problem).\n\n\n\n\n\n\nWe use Levene’s test here merely to check if it substantiates the impressions we got from the visual inspection.\n\n# check homogeneity\nleveneTest(lmmdata$Prepositions, lmmdata$Genre, center = mean)\n\nWarning in leveneTest.default(lmmdata$Prepositions, lmmdata$Genre, center =\nmean): lmmdata$Genre coerced to factor.\n\n\nLevene's Test for Homogeneity of Variance (center = mean)\n       Df F value   Pr(>F)  \ngroup  15 1.74289 0.039906 *\n      521                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe Levene’s test shows that the variance is distributed unevenly across genres which means that we do not simply continue but should either remove problematic data points (outliers) or use a weighing method.\nIn this case, we create a new model which uses weights to compensate for heterogeneity of variance and thus the influence of outliers - which is an alternative to removing the data points and rerunning the analysis [31]. However, to do so, we need to use a different function (the lme function) which means that we have to create two models: the old minimal adequate model and the new minimal adequate model with added weights. After we have created these models, we will compare them to see if including weights has improved the fit.\n\n# generate models\nm4.lme = lme(Prepositions ~ Date, random = ~1|Genre, data = lmmdata, method = \"ML\")\nm5.lme <- update(m4.lme, weights = varIdent(form = ~ 1 | Genre))\n# compare models\nanova(m5.lme, m4.lme)\n\n       Model df           AIC           BIC         logLik   Test       L.Ratio\nm5.lme     1 19 4485.84955689 4567.28352069 -2223.92477845                     \nm4.lme     2  4 4495.01773596 4512.16172834 -2243.50886798 1 vs 2 39.1681790667\n       p-value\nm5.lme        \nm4.lme  0.0006\n\n\nThe weight model (m5.lme) that uses weights to account for unequal variance is performing significantly better than the model without weights (m4.lme) and we therefore switch to the weight model and inspect its parameters.\n\n# inspect results\nsummary(m5.lme)        \n\nLinear mixed-effects model fit by maximum likelihood\n  Data: lmmdata \n            AIC           BIC         logLik\n  4485.84955689 4567.28352069 -2223.92477845\n\nRandom effects:\n Formula: ~1 | Genre\n          (Intercept)      Residual\nStdDev: 12.2631808124 14.3420265787\n\nVariance function:\n Structure: Different standard deviations per stratum\n Formula: ~1 | Genre \n Parameter estimates:\n          Bible       Biography           Diary       Education         Fiction \n 1.000000000000  0.340719922131  0.869529591941  0.788861415446  0.911718858006 \n       Handbook         History             Law      Philosophy   PrivateLetter \n 1.096586432734  0.978750069335  0.784977716011  0.736988751895  1.190652740883 \n   PublicLetter        Religion         Science          Sermon          Travel \n 1.218929184394  0.974646323962  0.848611735175  0.970873365315  1.086235097365 \nTrialProceeding \n 1.260207123026 \nFixed effects:  Prepositions ~ Date \n                     Value      Std.Error  DF       t-value p-value\n(Intercept) 133.9640139919 3.144348306618 520 42.6046992663  0.0000\nDate          0.0217418124 0.005454714153 520  3.9858756695  0.0001\n Correlation: \n     (Intr)\nDate 0.004 \n\nStandardized Within-Group Residuals:\n             Min               Q1              Med               Q3 \n-3.3190657058362 -0.6797224358638  0.0146860053179  0.6987149873619 \n             Max \n 3.1039114834876 \n\nNumber of Observations: 537\nNumber of Groups: 16 \n\n\nWe can also use an ANOVA display which is more to the point.\n\nanova(m5.lme)          \n\n            numDF denDF        F-value p-value\n(Intercept)     1   520 1813.907203146  <.0001\nDate            1   520   15.887204853  0.0001\n\n\nAs we did before, we now check, whether the final minimal model (with weights) outperforms an intercept-only base-line model.\n\n# generate base-line model\nm0.lme = lme(Prepositions ~ 1, random = ~1|Genre, data = lmmdata, method = \"ML\", weights = varIdent(form = ~ 1 | Genre))\nanova(m0.lme, m5.lme)  # test if date is significant\n\n       Model df           AIC           BIC         logLik   Test       L.Ratio\nm0.lme     1 18 4496.28563020 4573.43359590 -2230.14281510                     \nm5.lme     2 19 4485.84955689 4567.28352069 -2223.92477845 1 vs 2 12.4360733078\n       p-value\nm0.lme        \nm5.lme  0.0004\n\n\nOur final minimal adequate model with weights performs significantly better than an intercept only base-line model. Before doing the final diagnostics, we well inspect the estimates for the random effect structure to check if there are values which require further inspection (e.g. because they are drastically different from all other values).\n\n# extract estimates and sd for fixed and random effects\nintervals(m5.lme, which=\"fixed\")      \n\nApproximate 95% confidence intervals\n\n Fixed effects:\n                        lower              est.             upper\n(Intercept) 127.7983408437279 133.9640139919199 140.1296871401120\nDate          0.0110458012653   0.0217418124276   0.0324378235899\n\n\nThe random effect estimates do not show any outliers or drastically increased or decreased values which means that the random effect structure is fine.\n\n\nEffect Sizes\nWe will now extract effect sizes (in the example: the effect size of Date) and calculate normalized effect size measures (this effect size measure works for all fixed effects). When you have factorial design, you can take the square root of the squared t-value divided by the t-value squared plus the degrees of freedom to calculate the effect size:\n\\[\\begin{equation}\n\nr = \\sqrt{ \\frac{ t^2}{(t^2 + df) } } = \\sqrt{ \\frac{ 3.99^2}{(3.99^2 + 520) } } = 0.172\n\n\\end{equation}\\]\n\n\n\n\nNOTETwo words of warning though: br>1. In our case, the effect we are interested in is not factorial but continuous which means that we should not use this effect size measure. We only show this here as an example for how you can calculate the effect size measure r.2. Only apply this function to main effects that are not involved in interactions as they are meaningless because the amount of variance explained by main effects involved in interactions is unclear [6].\n\n\n\n\n\n\n\n\n\n\nsjPlot::tab_model(m5.lme)\n\n\n\n\n \nPrepositions\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n133.96\n127.80 – 140.13\n<0.001\n\n\nDate\n0.02\n0.01 – 0.03\n<0.001\n\n\nRandom Effects\n\n\n\nσ2\n205.69\n\n\n\nτ00 Genre\n150.39\n\n\nN Genre\n16\n\nObservations\n537\n\n\nMarginal R2 / Conditional R2\n0.030 / NA\n\n\n\n\n\n\n\n\n\n\n\nNOTEThe R2 values of the summary table are incorrect (as indicated by the missing conditional R2 value). The more appropriate conditional and marginal coefficient of determination for generalized mixed-effect models can be extracted using the r.squaredGLMM function from the MuMIn package [32].\n\n\n\n\n\n\nThe marginal R2 (marginal coefficient of determination) represents the variance explained by the fixed effects while the conditional R2 is interpreted as a variance explained by the entire model, including both fixed and random effects [33].\nThe respective call for the model is:\n\n# extract R2s\nr.squaredGLMM(m1.lmer)\n\n                 R2m            R2c\n[1,] 0.0121971160211 0.417270545308\n\n\nThe effects can be visualized using the plot_model function from the sjPlot package [13].\n\nsjPlot::plot_model(m5.lme, type = \"pred\", terms = c(\"Date\")) +\n  # show uncentered date rather than centered date\n  scale_x_continuous(name = \"Date\", \n                     breaks = seq(-500, 300, 100), \n                     labels = seq(1150, 1950, 100))\n\n\n\n\nWhile we have already shown that the effect of Date is significant, it is small which means that the number of prepositions per text does not correlate very strongly with time. This suggests that other factors that are not included in the model also impact the frequency of prepositions (and probably more meaningfully, too).\nBefore turning to the diagnostics, we will use the fitted (or predicted) and the observed values with a regression line for the predicted values. This will not only show how good the model fit the data but also the direction and magnitude of the effect.\n\n# extract predicted values\nlmmdata$Predicted <- predict(m5.lme, lmmdata)\n# plot predicted values\nggplot(lmmdata, aes(DateUnscaled, Predicted)) +\n  facet_wrap(~Genre) +\n  geom_point(aes(x = DateUnscaled, y = Prepositions), color = \"gray80\", size = .5) +\n  geom_smooth(aes(y = Predicted), color = \"gray20\", linetype = \"solid\", \n              se = T, method = \"lm\") +\n  guides(color=guide_legend(override.aes=list(fill=NA))) +  \n  theme_set(theme_bw(base_size = 10)) +\n  theme(legend.position=\"top\", legend.title = element_blank(),\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank()) + \n  xlab(\"Date of composition\")\n\n\n\n\n\n\nModel Diagnostics\nWe now create diagnostic plots. What we wish to see in the diagnostic plots is a cloud of dots in the middle of the window without any structure. What we do not want to see is a funnel-shaped cloud because this indicates an increase of the errors/residuals with an increase of the predictor(s) (because this would indicate heteroscedasticity) [31].\n\n# start plotting\npar(mfrow = c(2, 2))           # display plots in 2 rows and 2 columns\nplot(m5.lme, pch = 20, col = \"black\", lty = \"dotted\"); par(mfrow = c(1, 1))\n\n\n\n\nWhat a wonderful unstructured cloud - the lack of structure tells us that the model is “healthy” and does not suffer from heteroscedasticity. We will now create more diagnostic plots to find potential problems [31].\n\n# fitted values by Genre\nplot(m5.lme, form = resid(., type = \"p\") ~ fitted(.) | Genre, abline = 0, \n     cex = .5, pch = 20, col = \"black\")\n\n\n\n\nIn contrast to the unweight model, no data points are named which indicates that the outliers do no longer have unwarranted influence on the model. Now, we check the residuals of fitted values against observed values [31]. What we would like to see is a straight, upwards going line.\n\n# residuals of fitted values against observed\nqqnorm(m5.lme, pch = 20, col = \"black\")\n\n\n\n\nA beautiful, straight line! The qqplot does not indicate any problems. It is, unfortunately, rather common that the dots deviate from the straight line at the very bottom or the very top which means that the model is good at estimating values around the middle of the dependent variable but rather bad at estimating lower or higher values. Next, we check the residuals by “Genre” [31].\n\n# residuals by genre\nqqnorm(m5.lme, ~resid(.) | Genre, pch = 20, col = \"black\" )\n\n\n\n\nBeautiful straight lines - perfection! Now, we inspect the observed responses versus the within-group fitted values [31].\n\n# observed responses versus the within-group fitted values\nplot(m5.lme, Prepositions ~ fitted(.), id = 0.05, adj = -0.3, \n     xlim = c(80, 220), cex = .8, pch = 20, col = \"blue\")\n\n\n\n\nAlthough some data points are named, the plot does not show any structure, like a funnel, which would have been problematic.\n\n\nReporting Results\nBefore we do the write-up, we have a look at the model summary as this will provide us with at least some of the parameters that we want to report.\n\nsummary(m5.lme)\n\nLinear mixed-effects model fit by maximum likelihood\n  Data: lmmdata \n            AIC           BIC         logLik\n  4485.84955689 4567.28352069 -2223.92477845\n\nRandom effects:\n Formula: ~1 | Genre\n          (Intercept)      Residual\nStdDev: 12.2631808124 14.3420265787\n\nVariance function:\n Structure: Different standard deviations per stratum\n Formula: ~1 | Genre \n Parameter estimates:\n          Bible       Biography           Diary       Education         Fiction \n 1.000000000000  0.340719922131  0.869529591941  0.788861415446  0.911718858006 \n       Handbook         History             Law      Philosophy   PrivateLetter \n 1.096586432734  0.978750069335  0.784977716011  0.736988751895  1.190652740883 \n   PublicLetter        Religion         Science          Sermon          Travel \n 1.218929184394  0.974646323962  0.848611735175  0.970873365315  1.086235097365 \nTrialProceeding \n 1.260207123026 \nFixed effects:  Prepositions ~ Date \n                     Value      Std.Error  DF       t-value p-value\n(Intercept) 133.9640139919 3.144348306618 520 42.6046992663  0.0000\nDate          0.0217418124 0.005454714153 520  3.9858756695  0.0001\n Correlation: \n     (Intr)\nDate 0.004 \n\nStandardized Within-Group Residuals:\n             Min               Q1              Med               Q3 \n-3.3190657058362 -0.6797224358638  0.0146860053179  0.6987149873619 \n             Max \n 3.1039114834876 \n\nNumber of Observations: 537\nNumber of Groups: 16 \n\n\n\nsjPlot::tab_model(m5.lme)\n\n\n\n\n \nPrepositions\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n133.96\n127.80 – 140.13\n<0.001\n\n\nDate\n0.02\n0.01 – 0.03\n<0.001\n\n\nRandom Effects\n\n\n\nσ2\n205.69\n\n\n\nτ00 Genre\n150.39\n\n\nN Genre\n16\n\nObservations\n537\n\n\nMarginal R2 / Conditional R2\n0.030 / NA\n\n\n\n\n\n\n\n\n\n\n\nNOTEThe R2 values of the summary table are incorrect (as indicated by the missing conditional R2 value). The more appropriate conditional and marginal coefficient of determination for generalized mixed-effect models can be extracted using the r.squaredGLMM function from the MuMIn package [32].\n\n\n\n\n\n\nThe respective call for the model is:\n\nr.squaredGLMM(m5.lme)\n\n                 R2m            R2c\n[1,] 0.0174025274607 0.432390148426\n\n\nWe can use the reports package [14] to summarize the analysis.\n\nreport::report(m5.lme)\n\nRandom effect variances not available. Returned R2 does not account for random effects.\nRandom effect variances not available. Returned R2 does not account for random effects.\n\n\nWe fitted a linear mixed model (estimated using ML and nlminb optimizer) to predict Prepositions with Date (formula: Prepositions ~ Date). The model included Genre as random effect (formula: ~1 | Genre). The model's explanatory power related to the fixed effects alone (marginal R2) is 0.03. The model's intercept, corresponding to Date = 0, is at 133.96 (95% CI [127.80, 140.13], t(520) = 42.60, p < .001). Within this model:\n\n  - The effect of Date is statistically significant and positive (beta = 0.02, 95% CI [0.01, 0.03], t(520) = 3.99, p < .001; Std. beta = 0.13, 95% CI [0.07, 0.19])\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using \n\n\nWe can use this output to write up a final report:\nA mixed-effect linear regression model which contained the genre of texts as random effect was fit to the data in a step-wise-step up procedure. Due to the presence of outliers in the data, weights were included into the model which led to a significantly improved model fit compared to an un-weight model (\\(\\chi\\)2(2): 39.17, p: 0.0006). The final minimal adequate model performed significantly better than an intercept-only base-line model (\\(\\chi\\)2(1): 12.44, p =.0004) and showed that the frequency of prepositions increases significantly but only marginally with the date of composition (Estimate: 0.02, CI: 0.01-0.03, p < .001, marginal R2 = 0.0174, conditional R2 = 0.4324). Neither the region where the text was composed nor a higher order interaction between genre and region significantly correlated with the use of prepositions in the data.\n\n\nRemarks on Prediction\nWhile the number of intercepts, the model reports, and the way how mixed- and fixed-effects arrive at predictions differ, their predictions are extremely similar and almost identical (at least when dealing with a simple random effect structure). Consider the following example where we create analogous fixed and mixed effect models and plot their predicted frequencies of prepositions per genre across the un-centered date of composition. The predictions of the mixed-effects model are plotted as a solid red line, while the predictions of the fixed-effects model are plotted as dashed blue lines.\n\n# create lm model\nm5.lmeunweight <- lm(Prepositions ~ DateUnscaled + Genre, data = lmmdata)\nlmmdata$lmePredictions <- fitted(m5.lmeunweight, lmmdata)\nm5.lm <- lm(Prepositions ~ DateUnscaled + Genre, data = lmmdata)\nlmmdata$lmPredictions <- fitted(m5.lm, lmmdata)\n# plot predictions\nggplot(lmmdata, aes(x = DateUnscaled, y = lmePredictions, group = Genre)) +\n  geom_line(aes(y = lmmdata$lmePredictions), linetype = \"solid\", color = \"red\") +\n  geom_line(aes(y = lmmdata$lmPredictions), linetype = \"dashed\", color = \"blue\") +\n  facet_wrap(~ Genre, nrow = 4) +\n  theme_bw() +\n  labs(x = \"Date of composition\") +\n  labs(y = \"Prepositions per 1,000 words\") +\n  coord_cartesian(ylim = c(0, 220))\n\n\n\n\nThe predictions overlap almost perfectly which means that the predictions of both are almost identical - irrespective of whether genre is part of the mixed or the fixed effects structure."
  },
  {
    "objectID": "regression.html#mixed-effects-binomial-logistic-regression",
    "href": "regression.html#mixed-effects-binomial-logistic-regression",
    "title": "Fixed- and Mixed-Effects Regression Models in R",
    "section": "Mixed-Effects Binomial Logistic Regression",
    "text": "Mixed-Effects Binomial Logistic Regression\nWe now turn to an extension of binomial logistic regression: mixed-effects binomial logistic regression. As is the case with linear mixed-effects models logistic mixed effects models have the following advantages over simpler statistical tests:\n\nMixed-effects models are multivariate, i.e. they test the effect of several predictors simultaneously while controlling for the effect of all other predictors.\nMixed models allow to statistically incorporate within-speaker variability and are thus fit to model hierarchical or nested data structures. This applies if several observations are produced by an individual speaker, for instance.\nMixed-models provide a wealth of diagnostic statistics which enables us to control e.g. multicollinearity, i.e. correlations between predictors, and to test whether conditions or requirements are violated (e.g. homogeneity of variance, etc.).\n\nMajor disadvantages of regression modeling are that they are prone to producing high \\(\\beta\\)-errors [see 29] and that they require rather large data sets.\n\nIntroduction\nAs is the case with linear mixed-effects models, binomial logistic mixed-effect models are multivariate analyses that treat data points as hierarchical or grouped in some way. In other words, they take into account that the data is nested in the sense that data points are produced by the same speaker or are grouped by some other characteristics. In mixed-models, hierarchical structures are modelled as random effects. If the random effect structure represents speakers then this means that a mixed-model would have a separate intercept and/or slope for each speaker.\nRandom Effects in linear models can be visualized using two parameters: the intercept (the point where the regression line crosses the y-axis at x = 0) and the slope (the acclivity of the regression line). In contrast to linear mixed-effects models, random effects differ in the position and the slope of the logistic function that is applied to the likelihood of the dependent variable. random intercepts (center left panel \\(\\ref{fig:mem02}\\)) or various random slopes (center right panel \\(\\ref{fig:mem02}\\)), or both, various random intercepts and various random slopes (right panel \\(\\ref{fig:mem02}\\)). In the following, we will only focus on models with random intercepts because this is the by far more common method and because including both random intercepts and random slopes requires huge amounts of data. Consider the Figure below to understand what is meant by “random intercepts”.\n\n\n\n\n\nThe upper left panel merely shows the logistic curve representing the predictions of a fixed-effects logistic regression with a single intercept and slope. The upper right panel shows the logistic curves representing the predictions of a of a mixed-effects logistic regression with random intercepts for each level of a grouping variable. The lower left panel shows the logistic curves representing the predictions of a mixed-effects logistic regression with one intercept but random slopes for each level of a grouping variable. The lower right panel shows the logistic curves representing the predictions of a mixed-effects logistic regression with random intercepts and random slopes for each level of a grouping variable.\nAfter adding random intercepts, predictors (or fixed effects) are added to the model (just like with multiple regression). So mixed-effects are called mixed-effects because they contain both random and fixed effects.\nIn terms of general procedure, random effects are added first, and only after we have ascertained that including random effects is warranted, we test whether including fixed-effects is warranted [6]. We test whether including random effects is warranted by comparing a model, that bases its estimates of the dependent variable solely on the base intercept, with a model that bases its estimates of the dependent variable solely on the intercepts of the random effect. If the mixed-effects model explains significantly more variance than the fixed-effects model without random effect structure, then we continue with the mixed-effects model. In other words, including random effects is justified.\n\n\nExample: Discourse LIKE in Irish English\nIn this example we will investigate which factors correlate with the use of final discourse like (e.g. “The weather is shite, like!”) in Irish English. The data set represents speech units in a corpus that were coded for the speaker who uttered a given speech unit, the gender (Gender: Men versus Women) and age of that speaker (Age: Old versus Young), whether the interlocutors were of the same or a different gender (ConversationType: SameGender versus MixedGender), and whether another final discourse like had been used up to three speech units before (Priming: NoPrime versus Prime), whether or not the speech unit contained an final discourse like (SUFLike: 1 = yes, 0 = no. To begin with, we load the data and inspect the structure of the data set,\n\n# load data\nmblrdata  <- base::readRDS(url(\"https://slcladal.github.io/data/mbd.rda\", \"rb\"))\n\n\n\n\n\n\nFirst 15 rows of the mblrdata.\n\n\nIDGenderAgeConversationTypePrimingSUFlikeS1A-061$CWomenYoungMixedGenderNoPrime0S1A-023$BWomenYoungMixedGenderNoPrime0S1A-054$AWomenYoungSameGenderNoPrime0S1A-090$BWomenYoungMixedGenderNoPrime0S1A-009$BWomenOldSameGenderPrime0S1A-085$EMenYoungMixedGenderPrime1S1A-003$CWomenYoungMixedGenderNoPrime1S1A-084$CWomenYoungSameGenderNoPrime0S1A-076$AWomenYoungSameGenderNoPrime0S1A-083$DMenOldMixedGenderNoPrime1S1A-068$AWomenYoungSameGenderNoPrime0S1A-066$BWomenYoungSameGenderNoPrime0S1A-061$AMenOldMixedGenderNoPrime1S1A-049$AWomenYoungSameGenderNoPrime0S1A-022$BWomenYoungMixedGenderNoPrime0\n\n\nAs all variables except for the dependent variable (SUFlike) are character strings, we factorize the independent variables.\n\n# def. variables to be factorized\nvrs <- c(\"ID\", \"Age\", \"Gender\", \"ConversationType\", \"Priming\")\n# def. vector with variables\nfctr <- which(colnames(mblrdata) %in% vrs)     \n# factorize variables\nmblrdata[,fctr] <- lapply(mblrdata[,fctr], factor)\n# relevel Age (Young = Reference)\nmblrdata$Age <- relevel(mblrdata$Age, \"Young\")\n# order data by ID\nmblrdata <- mblrdata %>%\n  dplyr::arrange(ID)\n\n\n\n\n\n\nFirst 15 rows of the mblrdata arranged by ID.\n\n\nIDGenderAgeConversationTypePrimingSUFlikeS1A-001$AMenOldSameGenderNoPrime0S1A-001$AMenOldSameGenderNoPrime0S1A-001$AMenOldSameGenderNoPrime0S1A-001$AMenOldSameGenderNoPrime0S1A-001$AMenOldSameGenderNoPrime0S1A-001$AMenOldSameGenderNoPrime0S1A-001$AMenOldSameGenderNoPrime0S1A-001$AMenOldSameGenderNoPrime0S1A-001$BWomenOldMixedGenderNoPrime0S1A-001$BWomenOldMixedGenderNoPrime0S1A-001$BWomenOldMixedGenderNoPrime0S1A-001$BWomenOldMixedGenderPrime0S1A-001$BWomenOldMixedGenderNoPrime1S1A-001$BWomenOldMixedGenderNoPrime0S1A-001$BWomenOldMixedGenderNoPrime0\n\n\nBefore continuing, a few words about the minimum number of random effect levels and the minimum number of observations per random effect level are in order.\nWhile many data points per random variable level increases statistical power and thus to more robust estimates of the random effects [34], it has been shown that small numbers of observations per random effect variable level do not cause serious bias and it does not negatively affect the estimates of the fixed-effects coefficients [35–38]. The minimum number of observations per random effect variable level is therefore 1.\nIn simulation study, [35] tested the impact of random variable levels with only a single observation ranging from 0 to 70 percent. As long as there was a relatively high number of random effect variable levels (500 or more), small numbers of observations had almost no impact on bias and Type 1 error control.\nWe now plot the data to inspect the relationships within the data set.\n\nggplot(mblrdata, aes(Gender, SUFlike, color = Priming)) +\n  facet_wrap(Age~ConversationType) +\n  stat_summary(fun = mean, geom = \"point\") +\n  stat_summary(fun.data = mean_cl_boot, geom = \"errorbar\", width = 0.2) +\n  theme_set(theme_bw(base_size = 10)) +\n  theme(legend.position = \"top\") +\n  labs(x = \"\", y = \"Observed Probabilty of discourse like\") +\n  scale_color_manual(values = c(\"gray20\", \"gray70\"))\n\n\n\n\nThe upper left panel in the Figure above indicates that men use discourse like more frequently than women. The center right panel suggests that priming significantly increases the likelihood of discourse like being used. The center left panel suggests that speakers use discourse like more frequently in mixed-gender conversations. However, the lower right panel indicates an interaction between gender and conversation type as women appear to use discourse like less frequently in same gender conversations while the conversation type does not seem to have an effect on men. After visualizing the data, we will now turn to the model building process.\n\n\nModel Building\nIn a first step, we set the options.\n\n# set options\noptions(contrasts  =c(\"contr.treatment\", \"contr.poly\"))\nmblrdata.dist <- datadist(mblrdata)\noptions(datadist = \"mblrdata.dist\")\n\nIn a next step, we generate fixed-effects minimal base-line models and a base-line mixed-model using the “glmer” function with a random intercept for ID (a lmer object of the final minimal adequate model will be created later).\n\n# baseline model glm\nm0.glm = glm(SUFlike ~ 1, family = binomial, data = mblrdata) \n# base-line mixed-model\nm0.glmer = glmer(SUFlike ~ (1|ID), data = mblrdata, family = binomial) \n\n\n\nTesting the Random Effect\nNow, we check if including the random effect is permitted by comparing the AICs from the glm to AIC from the glmer model. If the AIC of the glmer object is smaller than the AIC of the glm object, then this indicates that including random intercepts is justified.\n\naic.glmer <- AIC(logLik(m0.glmer))\naic.glm <- AIC(logLik(m0.glm))\naic.glmer; aic.glm\n\n[1] 1828.49227107\n\n\n[1] 1838.17334856\n\n\nThe AIC of the glmer object is smaller which shows that including the random intercepts is justified. To confirm whether the AIC reduction is sufficient for justifying the inclusion of a random-effect structure, we also test whether the mixed-effects minimal base-line model explains significantly more variance by applying a Model Likelihood Ratio Test to the fixed- and the mixed effects minimal base-line models.\n\n# test random effects\nnull.id = -2 * logLik(m0.glm) + 2 * logLik(m0.glmer)\npchisq(as.numeric(null.id), df=1, lower.tail=F) \n\n[1] 0.000631389572445\n\n# sig m0.glmer better than m0.glm\n\nThe p-value of the Model Likelihood Ratio Test is lower than .05 which shows that the inclusion of the random-effects structure is warranted. We can now continue with the model fitting process.\n\n\nModel Fitting\nThe next step is to fit the model which means that we aim to find the “best” model, i.e. the minimal adequate model. In this case, we will use a manual step-wise step-up, forward elimination procedure. Before we begin with the model fitting process we need to add ´control = glmerControl(optimizer = “bobyqa”)´ to avoid unnecessary failures to converge.\n\nm0.glmer <- glmer(SUFlike ~ 1+ (1|ID), family = binomial, data = mblrdata, control=glmerControl(optimizer=\"bobyqa\"))\n\nDuring each step of the fitting procedure, we test whether certain assumptions on which the model relies are violated. To avoid incomplete information (a combination of variables does not occur in the data), we tabulate the variables we intend to include and make sure that all possible combinations are present in the data. Including variables although not all combinations are present in the data would lead to unreliable models that report (vastly) inaccurate results. A special case of incomplete information is complete separation which occurs if one predictor perfectly explains an outcome (in that case the incomplete information would be caused by a level of the dependent variable). In addition, we make sure that the VIFs do not exceed a maximum of 3 for main effects [18] - [39] suggest that VIFs should ideally be lower than 3 for as higher values would indicate multicollinearity and thus that the model is unstable. The value of 3 should be taken with a pinch of salt because there is no clear consensus about what the maximum VIF for interactions should be or if it should be considered at all. The reason is that we would, of course, expect the VIFs to increase when we are dealing with interactions as the main effects that are part of the interaction are very likely to correlate with the interaction itself. However, if the VIFs are too high, then this will still cause the issues with the attribution of variance. The value of 3 was chosen based on recommendations in the standard literature on multicollinearity [40, 41]. Only once we have confirmed that the incomplete information, complete separation, and multicollinearity are not a major concern, we generate the more saturated model and test whether the inclusion of a predictor leads to a significant reduction in residual deviance. If the predictor explains a significant amount of variance, it is retained in the model while being disregarded in case it does not explain a sufficient quantity of variance.\n\n# add Priming\nifelse(min(ftable(mblrdata$Priming, mblrdata$SUFlike)) == 0, \"incomplete information\", \"okay\")\n\n[1] \"okay\"\n\nm1.glmer <- update(m0.glmer, .~.+Priming)\nanova(m1.glmer, m0.glmer, test = \"Chi\") \n\nData: mblrdata\nModels:\nm0.glmer: SUFlike ~ 1 + (1 | ID)\nm1.glmer: SUFlike ~ (1 | ID) + Priming\n         npar         AIC         BIC       logLik    deviance     Chisq Df\nm0.glmer    2 1828.492271 1839.694076 -912.2461355 1824.492271             \nm1.glmer    3 1702.773341 1719.576048 -848.3866704 1696.773341 127.71893  1\n                     Pr(>Chisq)    \nm0.glmer                           \nm1.glmer < 0.000000000000000222 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nSince the tests do not show problems relating to incomplete information, because including Priming significantly improves the model fit (decrease in AIC and BIC values), and since it correlates significantly with our dependent variable, we include Priming into our model.\n\n# add Age\nifelse(min(ftable(mblrdata$Age, mblrdata$SUFlike)) == 0, \"incomplete information\", \"okay\")\n\n[1] \"okay\"\n\nm2.glmer <- update(m1.glmer, .~.+ Age)\nifelse(max(car::vif(m2.glmer)) <= 3,  \"VIFs okay\", \"VIFs unacceptable\") \n\n[1] \"VIFs okay\"\n\nanova(m2.glmer, m1.glmer, test = \"Chi\")   \n\nData: mblrdata\nModels:\nm1.glmer: SUFlike ~ (1 | ID) + Priming\nm2.glmer: SUFlike ~ (1 | ID) + Priming + Age\n         npar         AIC         BIC       logLik    deviance   Chisq Df\nm1.glmer    3 1702.773341 1719.576048 -848.3866704 1696.773341           \nm2.glmer    4 1704.210790 1726.614400 -848.1053950 1696.210790 0.56255  1\n         Pr(>Chisq)\nm1.glmer           \nm2.glmer    0.45323\n\nAnova(m2.glmer, test = \"Chi\")\n\nAnalysis of Deviance Table (Type II Wald chisquare tests)\n\nResponse: SUFlike\n            Chisq Df           Pr(>Chisq)    \nPriming 129.51509  1 < 0.0000000000000002 ***\nAge       0.56943  1              0.45049    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe ANOVAs show that Age is not significant and the first ANOVA also shows that the BIC has increased which indicates that Age does not decrease variance. In such cases, the variable should not be included.\nHowever, if the second ANOVA would report Age as being marginally significant, a case could be made for including it but it would be better to change the ordering in which predictors are added to the model. This is, however, just a theoretical issue here as Age is clearly not significant.\n\n# add Gender\nifelse(min(ftable(mblrdata$Gender, mblrdata$SUFlike)) == 0, \"incomplete information\", \"okay\")\n\n[1] \"okay\"\n\nm3.glmer <- update(m1.glmer, .~.+Gender)\nifelse(max(car::vif(m3.glmer)) <= 3,  \"VIFs okay\", \"VIFs unacceptable\") \n\n[1] \"VIFs okay\"\n\nanova(m3.glmer, m1.glmer, test = \"Chi\")\n\nData: mblrdata\nModels:\nm1.glmer: SUFlike ~ (1 | ID) + Priming\nm3.glmer: SUFlike ~ (1 | ID) + Priming + Gender\n         npar         AIC         BIC       logLik    deviance    Chisq Df\nm1.glmer    3 1702.773341 1719.576048 -848.3866704 1696.773341            \nm3.glmer    4 1679.397070 1701.800680 -835.6985349 1671.397070 25.37627  1\n            Pr(>Chisq)    \nm1.glmer                  \nm3.glmer 0.00000047168 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nAnova(m3.glmer, test = \"Chi\")\n\nAnalysis of Deviance Table (Type II Wald chisquare tests)\n\nResponse: SUFlike\n            Chisq Df             Pr(>Chisq)    \nPriming 124.40764  1 < 0.000000000000000222 ***\nGender   28.56705  1         0.000000090509 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nGender is significant and will therefore be included as a predictor (you can also observe that including Gender has substantially decreased both AIC and BIC).\n\n# add ConversationType\nifelse(min(ftable(mblrdata$ConversationType, mblrdata$SUFlike)) == 0, \"incomplete information\", \"okay\")\n\n[1] \"okay\"\n\nm4.glmer <- update(m3.glmer, .~.+ConversationType)\nifelse(max(car::vif(m4.glmer)) <= 3,  \"VIFs okay\", \"VIFs unacceptable\") \n\n[1] \"VIFs okay\"\n\nanova(m4.glmer, m3.glmer, test = \"Chi\") \n\nData: mblrdata\nModels:\nm3.glmer: SUFlike ~ (1 | ID) + Priming + Gender\nm4.glmer: SUFlike ~ (1 | ID) + Priming + Gender + ConversationType\n         npar         AIC         BIC       logLik    deviance    Chisq Df\nm3.glmer    4 1679.397070 1701.800680 -835.6985349 1671.397070            \nm4.glmer    5 1668.583222 1696.587734 -829.2916108 1658.583222 12.81385  1\n         Pr(>Chisq)    \nm3.glmer               \nm4.glmer 0.00034406 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nAnova(m4.glmer, test = \"Chi\")\n\nAnalysis of Deviance Table (Type II Wald chisquare tests)\n\nResponse: SUFlike\n                     Chisq Df             Pr(>Chisq)    \nPriming          130.68714  1 < 0.000000000000000222 ***\nGender            13.44456  1             0.00024572 ***\nConversationType  12.99243  1             0.00031275 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nConversationType improves model fit (AIC and BIC decrease and it is reported as being significant) and will, therefore, be included in the model.\n\n# add Priming*Age\nifelse(min(ftable(mblrdata$Priming, mblrdata$Age, mblrdata$SUFlike)) == 0, \"incomplete information\", \"okay\")\n\n[1] \"okay\"\n\nm5.glmer <- update(m4.glmer, .~.+Priming*Age)\nifelse(max(car::vif(m5.glmer)) <= 3,  \"VIFs okay\", \"WARNING: high VIFs!\") \n\n[1] \"VIFs okay\"\n\nanova(m5.glmer, m4.glmer, test = \"Chi\") \n\nData: mblrdata\nModels:\nm4.glmer: SUFlike ~ (1 | ID) + Priming + Gender + ConversationType\nm5.glmer: SUFlike ~ (1 | ID) + Priming + Gender + ConversationType + Age + Priming:Age\n         npar         AIC         BIC       logLik    deviance   Chisq Df\nm4.glmer    5 1668.583222 1696.587734 -829.2916108 1658.583222           \nm5.glmer    7 1671.599008 1710.805326 -828.7995042 1657.599008 0.98421  2\n         Pr(>Chisq)\nm4.glmer           \nm5.glmer    0.61134\n\n\nThe interaction between Priming and Age is not significant and we thus not be included.\n\n# add Priming*Gender\nifelse(min(ftable(mblrdata$Priming, mblrdata$Gender, mblrdata$SUFlike)) == 0, \"incomplete information\", \"okay\")\n\n[1] \"okay\"\n\nm6.glmer <- update(m4.glmer, .~.+Priming*Gender)\nifelse(max(car::vif(m6.glmer)) <= 3,  \"VIFs okay\", \"WARNING: high VIFs!\") \n\n[1] \"WARNING: high VIFs!\"\n\n\nWe get the warning that the VIFs are high (>= 3) which means that the model suffers from (multi-)collinearity. We thus check the VIFs to determine how to proceed. If the VIFs are > 10, then we definitely cannot use the model as the multicollinearity is excessive.\n\ncar::vif(m6.glmer)\n\n         Priming           Gender ConversationType   Priming:Gender \n   4.35644725184    1.43315974183    1.20288306932    4.58280387132 \n\n\nThe VIFs are below 5 which is not good (VIFs of 5 mean “that column in the model matrix is explainable from the others with an R2 of 0.8” [7]) but it is still arguably acceptable and we will thus check if including the interaction between Priming and Gender significantly improved model fit.\n\nanova(m6.glmer, m4.glmer, test = \"Chi\") \n\nData: mblrdata\nModels:\nm4.glmer: SUFlike ~ (1 | ID) + Priming + Gender + ConversationType\nm6.glmer: SUFlike ~ (1 | ID) + Priming + Gender + ConversationType + Priming:Gender\n         npar         AIC         BIC       logLik    deviance   Chisq Df\nm4.glmer    5 1668.583222 1696.587734 -829.2916108 1658.583222           \nm6.glmer    6 1663.164481 1696.769896 -825.5822404 1651.164481 7.41874  1\n         Pr(>Chisq)   \nm4.glmer              \nm6.glmer  0.0064548 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nAnova(m6.glmer, test = \"Chi\")\n\nAnalysis of Deviance Table (Type II Wald chisquare tests)\n\nResponse: SUFlike\n                     Chisq Df             Pr(>Chisq)    \nPriming          131.96244  1 < 0.000000000000000222 ***\nGender            13.57723  1             0.00022895 ***\nConversationType  10.70707  1             0.00106727 ** \nPriming:Gender     7.45362  1             0.00633089 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe interaction between Priming and Gender improved model fit (AIC and BIC reduction) and significantly correlates with the use of speech-unit final like. It will therefore be included in the model.\n\n# add Priming*ConversationType\nifelse(min(ftable(mblrdata$Priming, mblrdata$ConversationType, mblrdata$SUFlike)) == 0, \"incomplete information\", \"okay\")\n\n[1] \"okay\"\n\nm7.glmer <- update(m6.glmer, .~.+Priming*ConversationType)\nifelse(max(car::vif(m7.glmer)) <= 3,  \"VIFs okay\", \"WARNING: high VIFs!\") \n\n[1] \"WARNING: high VIFs!\"\n\n\nWhen including the interaction between Priming and ConversationType we get a warning that there are high VIFs (multicollinearity) so we inspect the VIFs in more detail.\n\n# check VIFs\ncar::vif(m7.glmer) \n\n                 Priming                   Gender         ConversationType \n           5.10717661703            1.49290487846            1.49158369697 \n          Priming:Gender Priming:ConversationType \n           4.93025403307            3.41077852978 \n\n\nThe VIF of Priming is above 5 so we would normally continue without checking if including the interaction between Priming and ConversationType leads to a significant improvement in model fit. However, given that this is just a practical example, we check if including this interaction significantly improves model fit.\n\nanova(m7.glmer, m6.glmer, test = \"Chi\")\n\nData: mblrdata\nModels:\nm6.glmer: SUFlike ~ (1 | ID) + Priming + Gender + ConversationType + Priming:Gender\nm7.glmer: SUFlike ~ (1 | ID) + Priming + Gender + ConversationType + Priming:Gender + Priming:ConversationType\n         npar         AIC         BIC       logLik    deviance   Chisq Df\nm6.glmer    6 1663.164481 1696.769896 -825.5822404 1651.164481           \nm7.glmer    7 1663.134968 1702.341285 -824.5674840 1649.134968 2.02951  1\n         Pr(>Chisq)\nm6.glmer           \nm7.glmer    0.15427\n\n\nThe interaction between Priming and ConversationType does not significantly correlate with the use of speech-unit final like and it does not explain much variance (AIC and BIC increase). It will be not be included in the model.\n\n# add Age*Gender\nifelse(min(ftable(mblrdata$Age, mblrdata$Gender, mblrdata$SUFlike)) == 0, \"incomplete information\", \"okay\")\n\n[1] \"okay\"\n\nm8.glmer <- update(m6.glmer, .~.+Age*Gender)\nifelse(max(car::vif(m8.glmer)) <= 3,  \"VIFs okay\", \"WARNING: high VIFs!\") \n\n[1] \"WARNING: high VIFs!\"\n\n\nWhen including the interaction between Age and Gender we get a warning that there are high VIFs (multicollinearity) so we inspect the VIFs in more detail.\n\n# check VIFs\ncar::vif(m8.glmer)\n\n         Priming           Gender ConversationType              Age \n   4.46161514975    2.14158283266    1.20633411411    3.25166979907 \n  Priming:Gender       Gender:Age \n   4.66786414860    3.21354842006 \n\n\nThe VIFs are all below 5 so we test if including the interaction between Gender and Age significantly improves model fit.\n\nanova(m8.glmer, m6.glmer, test = \"Chi\") \n\nData: mblrdata\nModels:\nm6.glmer: SUFlike ~ (1 | ID) + Priming + Gender + ConversationType + Priming:Gender\nm8.glmer: SUFlike ~ (1 | ID) + Priming + Gender + ConversationType + Age + Priming:Gender + Gender:Age\n         npar         AIC         BIC       logLik    deviance   Chisq Df\nm6.glmer    6 1663.164481 1696.769896 -825.5822404 1651.164481           \nm8.glmer    8 1665.634538 1710.441758 -824.8172690 1649.634538 1.52994  2\n         Pr(>Chisq)\nm6.glmer           \nm8.glmer    0.46535\n\n\nThe interaction between Age and Gender is not significant and will thus continue without it.\n\n# add Age*ConversationType\nifelse(min(ftable(mblrdata$Age, mblrdata$ConversationType, mblrdata$SUFlike)) == 0, \"incomplete information\", \"okay\")\n\n[1] \"okay\"\n\nm9.glmer <- update(m6.glmer, .~.+Age*ConversationType)\nifelse(max(car::vif(m9.glmer)) <= 3,  \"VIFs okay\", \"WARNING: high VIFs!\") \n\n[1] \"WARNING: high VIFs!\"\n\n\nWhen including the interaction between Age and ConversationType we get a warning that there are high VIFs (multicollinearity) so we inspect the VIFs in more detail.\n\n# check VIFs\ncar::vif(m9.glmer)\n\n             Priming               Gender     ConversationType \n       4.37760628012        1.50657679188        1.51468866164 \n                 Age       Priming:Gender ConversationType:Age \n       1.96937607096        4.61285236598        2.05894940257 \n\n\nThe VIFs are all below 5 so we test if including the interaction between ConversationType and Age significantly improves model fit.\n\nanova(m9.glmer, m6.glmer, test = \"Chi\") \n\nData: mblrdata\nModels:\nm6.glmer: SUFlike ~ (1 | ID) + Priming + Gender + ConversationType + Priming:Gender\nm9.glmer: SUFlike ~ (1 | ID) + Priming + Gender + ConversationType + Age + Priming:Gender + ConversationType:Age\n         npar         AIC         BIC       logLik    deviance   Chisq Df\nm6.glmer    6 1663.164481 1696.769896 -825.5822404 1651.164481           \nm9.glmer    8 1666.262166 1711.069386 -825.1310831 1650.262166 0.90231  2\n         Pr(>Chisq)\nm6.glmer           \nm9.glmer    0.63689\n\n\nThe interaction between Age and ConversationType is insignificant and does not improve model fit (AIC and BIC reduction). It will therefore not be included in the model.\n\n# add Gender*ConversationType\nifelse(min(ftable(mblrdata$Gender, mblrdata$ConversationType, mblrdata$SUFlike)) == 0, \"incomplete information\", \"okay\")\n\n[1] \"okay\"\n\nm10.glmer <- update(m6.glmer, .~.+Gender*ConversationType)\nifelse(max(car::vif(m10.glmer)) <= 3,  \"VIFs okay\", \"WARNING: high VIFs!\")\n\n[1] \"WARNING: high VIFs!\"\n\n\nWhen including the interaction between Gender and ConversationType we get a warning that there are high VIFs (multicollinearity) so we inspect the VIFs in more detail.\n\n# check VIFs\ncar::vif(m10.glmer) \n\n                Priming                  Gender        ConversationType \n          4.96223841163           1.73315963551           7.76458422494 \n         Priming:Gender Gender:ConversationType \n          5.11897075069           9.24867418732 \n\n\nThe highest VIF is almost 10 (9.248674187318) which is why the interaction between Gender and ConversationType will not be included in the model.\n\n# add Priming*Age*Gender\nifelse(min(ftable(mblrdata$Priming,mblrdata$Age, mblrdata$Gender, mblrdata$SUFlike)) == 0, \"incomplete information\", \"okay\")\n\n[1] \"okay\"\n\nm11.glmer <- update(m6.glmer, .~.+Priming*Age*Gender)\nifelse(max(car::vif(m11.glmer)) <= 3,  \"VIFs okay\", \"WARNING: high VIFs!\") \n\n[1] \"WARNING: high VIFs!\"\n\n\nWhen including the interaction between Priming, Age, and Gender we get a warning that there are high VIFs (multicollinearity) so we inspect the VIFs in more detail.\n\n# check VIFs\ncar::vif(m11.glmer)\n\n           Priming             Gender   ConversationType                Age \n     6.57538416079      2.28504186327      1.22325183699      3.66037124792 \n    Priming:Gender        Priming:Age         Gender:Age Priming:Gender:Age \n     6.66183402912      5.85367812826      3.75879850896      5.98139175840 \n\n\nThere are several VIFs with values greater than 5 and we will thus continue without including the interaction between Priming, Age, and Gender into the model.\n\n# add Priming*Age*ConversationType\nifelse(min(ftable(mblrdata$Priming,mblrdata$Age, mblrdata$ConversationType, mblrdata$SUFlike)) == 0, \"incomplete information\", \"okay\")\n\n[1] \"okay\"\n\nm12.glmer <- update(m6.glmer, .~.+Priming*Age*ConversationType)\nifelse(max(car::vif(m12.glmer)) <= 3,  \"VIFs okay\", \"WARNING: high VIFs!\") \n\n[1] \"WARNING: high VIFs!\"\n\n\nWhen including the interaction between Priming, Age, and Gender we get a warning that there are high VIFs (multicollinearity) so we inspect the VIFs in more detail.\n\n# check VIFs\ncar::vif(m12.glmer)\n\n                     Priming                       Gender \n               7.14771665816                1.61068940841 \n            ConversationType                          Age \n               1.83168707059                2.20184432953 \n              Priming:Gender                  Priming:Age \n               4.99337045657                3.33295681086 \n    Priming:ConversationType         ConversationType:Age \n               4.87448199535                2.38314262793 \nPriming:ConversationType:Age \n               2.99929551111 \n\n\nThe VIF of Priming is very high (7.147716658159) which is why we will thus continue without including the interaction between Priming, Age, and Gender in the model.\n\n# add Priming*Gender*ConversationType\nifelse(min(ftable(mblrdata$Priming,mblrdata$Gender, mblrdata$ConversationType, mblrdata$SUFlike)) == 0, \"incomplete information\", \"okay\")\n\n[1] \"okay\"\n\nm13.glmer <- update(m6.glmer, .~.+Priming*Gender*ConversationType)\nifelse(max(car::vif(m13.glmer)) <= 3,  \"VIFs okay\", \"WARNING: high VIFs!\") \n\n[1] \"WARNING: high VIFs!\"\n\n\nThe VIFs are excessive with a maximum value is 23.863401882333 which shows an unacceptable degree of multicollinearity so that we abort and move on the next model.\n\ncar::vif(m13.glmer)\n\n                        Priming                          Gender \n                  7.69529718176                   1.81201132511 \n               ConversationType                  Priming:Gender \n                 21.92712607379                  10.01452723810 \n       Priming:ConversationType         Gender:ConversationType \n                 21.62322307216                  23.86340188233 \nPriming:Gender:ConversationType \n                 22.67125500326 \n\n\nThe VIFs are excessive with a maximum value is 23.863401882333 which shows an unacceptable degree of multicollinearity so that we abort and move on the next model.\n\n# add Age*Gender*ConversationType\nifelse(min(ftable(mblrdata$Age,mblrdata$Gender, mblrdata$ConversationType, mblrdata$SUFlike)) == 0, \"incomplete information\", \"okay\")\n\n[1] \"okay\"\n\nm14.glmer <- update(m6.glmer, .~.+Age*Gender*ConversationType)\nifelse(max(car::vif(m14.glmer)) <= 3,  \"VIFs okay\", \"WARNING: high VIFs!\") \n\n[1] \"WARNING: high VIFs!\"\n\n\nWhen including the interaction between Age, Gender, ConversationType, we get a warning that there are high VIFs (multicollinearity) so we inspect the VIFs in more detail.\n\ncar::vif(m14.glmer)\n\n                    Priming                      Gender \n              5.33214037289               2.60055549758 \n           ConversationType                         Age \n             11.94207363385               3.80035469665 \n             Priming:Gender                  Gender:Age \n              5.45398766628               5.66107224954 \n       ConversationType:Age     Gender:ConversationType \n             16.98200292781              13.80649554087 \nGender:ConversationType:Age \n             19.07180232877 \n\n\nAgain, the VIFs are excessive with a maximum value of 19.071802328769 which shows an unacceptable degree of multicollinearity so that we abort and move on the next model.\n\n# add Priming*Age*Gender*ConversationType\nifelse(min(ftable(mblrdata$Priming,mblrdata$Age,mblrdata$Gender, mblrdata$ConversationType, mblrdata$SUFlike)) == 0, \"incomplete information\", \"okay\")\n\n[1] \"incomplete information\"\n\n\nThe model suffers from incomplete information! As this was the last possible model, we have found our final minimal adequate model in m6.glmer.\nIn a next step, we create an overview of model comparisons which serves as a summary for the model fitting process and provides AIC, BIC, and \\(\\chi\\)2 values.\n\nsource(\"https://slcladal.github.io/rscripts/ModelFittingSummarySWSU.r\") \n# comparisons of glmer objects\nm1.m0 <- anova(m1.glmer, m0.glmer, test = \"Chi\") \nm2.m1 <- anova(m2.glmer, m1.glmer, test = \"Chi\")   \nm3.m1 <- anova(m3.glmer, m1.glmer, test = \"Chi\")\nm4.m3 <- anova(m4.glmer, m3.glmer, test = \"Chi\") \nm5.m4 <- anova(m5.glmer, m4.glmer, test = \"Chi\") \nm6.m4 <- anova(m6.glmer, m4.glmer, test = \"Chi\") \nm7.m6 <- anova(m7.glmer, m6.glmer, test = \"Chi\")\nm8.m6 <- anova(m8.glmer, m6.glmer, test = \"Chi\") \nm9.m6 <- anova(m9.glmer, m6.glmer, test = \"Chi\") \n# create a list of the model comparisons\nmdlcmp <- list(m1.m0, m2.m1, m3.m1, m4.m3, m5.m4, m6.m4, m7.m6, m8.m6, m9.m6)\n# summary table for model fitting\nmdlft <- mdl.fttng.swsu(mdlcmp)\nmdlft <- mdlft[,-2]\n\n\n\n\n\n\nFirst 15 rows of the model fitting summary table.\n\n\nModelTerm AddedCompared to...DFAICBICLogLikelihoodResidual DevianceX2X2DFp-valueSignificancem1.glmer1+Primingm0.glmer31702.771719.58-848.391696.77127.7210p < .001***m2.glmerAgem1.glmer41704.211726.61-848.111696.210.5610.45323n.s.m3.glmerGenderm1.glmer41679.41701.8-835.71671.425.3810p < .001***m4.glmerConversationTypem3.glmer51668.581696.59-829.291658.5812.8110.00034p < .001***m5.glmerAge+Priming:Agem4.glmer71671.61710.81-828.81657.60.9820.61134n.s.m6.glmerPriming:Genderm4.glmer61663.161696.77-825.581651.167.4210.00645p <  .01 **m7.glmerPriming:ConversationTypem6.glmer71663.131702.34-824.571649.132.0310.15427n.s.m8.glmerAge+Gender:Agem6.glmer81665.631710.44-824.821649.631.5320.46535n.s.m9.glmerAge+ConversationType:Agem6.glmer81666.261711.07-825.131650.260.920.63689n.s.\n\n\nWe now rename our final minimal adequate model, test whether it performs significantly better than the minimal base-line model, and print the regression summary.\n\n# rename final minimal adequate model\nmlr.glmer <- m6.glmer \n# final model better than base-line model\nsigfit <- anova(mlr.glmer, m0.glmer, test = \"Chi\") \n# inspect\nsigfit\n\nData: mblrdata\nModels:\nm0.glmer: SUFlike ~ 1 + (1 | ID)\nmlr.glmer: SUFlike ~ (1 | ID) + Priming + Gender + ConversationType + Priming:Gender\n          npar         AIC         BIC       logLik    deviance     Chisq Df\nm0.glmer     2 1828.492271 1839.694076 -912.2461355 1824.492271             \nmlr.glmer    6 1663.164481 1696.769896 -825.5822404 1651.164481 173.32779  4\n                      Pr(>Chisq)    \nm0.glmer                            \nmlr.glmer < 0.000000000000000222 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n# inspect final minimal adequate model\nprint(mlr.glmer, corr = F)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: \nSUFlike ~ (1 | ID) + Priming + Gender + ConversationType + Priming:Gender\n   Data: mblrdata\n      AIC       BIC    logLik  deviance  df.resid \n1663.1645 1696.7699 -825.5822 1651.1645      1994 \nRandom effects:\n Groups Name        Std.Dev.   \n ID     (Intercept) 0.292489586\nNumber of obs: 2000, groups:  ID, 208\nFixed Effects:\n               (Intercept)                PrimingPrime  \n              -0.920849062                 1.060138443  \n               GenderWomen  ConversationTypeSameGender  \n              -0.867735748                -0.492309646  \n  PrimingPrime:GenderWomen  \n               1.035741994  \n\n\nTo extract the effect sizes of the significant fixed effects, we compare the model with that effect to a model without that effect. This can be problematic when checking the effect of main effects that are involved in significant interactions though [6].\n\n# effect of ConversationType\nef_conv <- anova(m4.glmer, m3.glmer, test = \"Chi\") \n# inspect\nef_conv\n\nData: mblrdata\nModels:\nm3.glmer: SUFlike ~ (1 | ID) + Priming + Gender\nm4.glmer: SUFlike ~ (1 | ID) + Priming + Gender + ConversationType\n         npar         AIC         BIC       logLik    deviance    Chisq Df\nm3.glmer    4 1679.397070 1701.800680 -835.6985349 1671.397070            \nm4.glmer    5 1668.583222 1696.587734 -829.2916108 1658.583222 12.81385  1\n         Pr(>Chisq)    \nm3.glmer               \nm4.glmer 0.00034406 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n# effect of Priming:Gender\nef_prigen <- anova(m6.glmer, m4.glmer, test = \"Chi\")\n# inspect\nef_prigen\n\nData: mblrdata\nModels:\nm4.glmer: SUFlike ~ (1 | ID) + Priming + Gender + ConversationType\nm6.glmer: SUFlike ~ (1 | ID) + Priming + Gender + ConversationType + Priming:Gender\n         npar         AIC         BIC       logLik    deviance   Chisq Df\nm4.glmer    5 1668.583222 1696.587734 -829.2916108 1658.583222           \nm6.glmer    6 1663.164481 1696.769896 -825.5822404 1651.164481 7.41874  1\n         Pr(>Chisq)   \nm4.glmer              \nm6.glmer  0.0064548 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nVisualizing Effects\nAs we will see the effects in the final summary, we visualize the effects here by showing the probability of discourse like based on the predicted values.\n\n# extract predicted values\nmblrdata$Predicted <- predict(m6.glmer, mblrdata, type = \"response\")\n# plot\nggplot(mblrdata, aes(ConversationType, Predicted)) +\n  stat_summary(fun = mean, geom = \"point\") +\n  stat_summary(fun.data = mean_cl_boot, geom = \"errorbar\", width = 0.2) +\n  theme_set(theme_bw(base_size = 10)) +\n  theme(legend.position = \"top\") +\n    ylim(0, .75) +\n  labs(x = \"\", y = \"Predicted Probabilty of discourse like\") +\n  scale_color_manual(values = c(\"gray20\", \"gray70\"))\n\n\n\n\nA proper visualization of the marginal effects can be extracted using the sjPlot package [13].\n\nplot_model(m6.glmer, type = \"pred\", terms = c(\"Priming\", \"Gender\"))\n\n\n\n\nWe can see that discourse like is more likely to surface in primed contexts but that in contrast to women and men in same-gender conversations as well as women in mixed-gender conversations, priming appears to affect the use of discourse like by men in mixed-gender conversations only very little.\n\n\nExtracting Model Fit Parameters\nWe now extract model fit parameters [10].\n\nprobs = 1/(1+exp(-fitted(mlr.glmer)))\nprobs = binomial()$linkinv(fitted(mlr.glmer))\nsomers2(probs, as.numeric(mblrdata$SUFlike))\n\n                C               Dxy                 n           Missing \n   0.758332104539    0.516664209078 2000.000000000000    0.000000000000 \n\n\nThe two lines that start with probs are simply two different ways to do the same thing (you only need one of these).\nThe model fit parameters indicate a suboptimal fit. Both the C-value and Somers’s Dxy show poor fit between predicted and observed occurrences of discourse like. If the C-value is 0.5, the predictions are random, while the predictions are perfect if the C-value is 1. C-values above 0.8 indicate real predictive capacity [10]. Somers’ Dxy is a value that represents a rank correlation between predicted probabilities and observed responses. Somers’ Dxy values range between 0, which indicates complete randomness, and 1, which indicates perfect prediction [10]. The C.value of 0.758332104539 suggests that the model has some predictive and explanatory power, but not at an optimal level. We will now perform the model diagnostics.\n\n\nModel Diagnostics\nWe begin the model diagnostics by generating a diagnostic that plots the fitted or predicted values against the residuals.\n\nplot(mlr.glmer, pch = 20, col = \"black\", lty = \"dotted\")\n\n\n\n\nAs a final step, we summarize our findings in tabulated form.\n\n# summarize final model\nsjPlot::tab_model(mlr.glmer)\n\n\n\n\n \nSUFlike\n\n\nPredictors\nOdds Ratios\nCI\np\n\n\n(Intercept)\n0.40\n0.29 – 0.54\n<0.001\n\n\nPriming [Prime]\n2.89\n1.50 – 5.56\n0.002\n\n\nGender [Women]\n0.42\n0.29 – 0.61\n<0.001\n\n\nConversationType[SameGender]\n0.61\n0.46 – 0.82\n0.001\n\n\nPriming [Prime] * Gender[Women]\n2.82\n1.34 – 5.93\n0.006\n\n\nRandom Effects\n\n\n\nσ2\n3.29\n\n\n\nτ00 ID\n0.09\n\n\nICC\n0.03\n\n\nN ID\n208\n\nObservations\n2000\n\n\nMarginal R2 / Conditional R2\n0.138 / 0.160\n\n\n\n\n\n\n\nWe can use the reports package [14] to summarize the analysis.\n\nreport::report(mlr.glmer)\n\nWe fitted a logistic mixed model (estimated using ML and BOBYQA optimizer) to predict SUFlike with Priming, Gender and ConversationType (formula: SUFlike ~ Priming + Gender + ConversationType + Priming:Gender). The model included ID as random effect (formula: ~1 | ID). The model's total explanatory power is moderate (conditional R2 = 0.16) and the part related to the fixed effects alone (marginal R2) is of 0.14. The model's intercept, corresponding to Priming = NoPrime, Gender = Men and ConversationType = MixedGender, is at -0.92 (95% CI [-1.22, -0.62], p < .001). Within this model:\n\n  - The effect of Priming [Prime] is statistically significant and positive (beta = 1.06, 95% CI [0.41, 1.71], p = 0.002; Std. beta = 1.06, 95% CI [0.41, 1.71])\n  - The effect of Gender [Women] is statistically significant and negative (beta = -0.87, 95% CI [-1.25, -0.49], p < .001; Std. beta = -0.87, 95% CI [-1.25, -0.49])\n  - The effect of ConversationType [SameGender] is statistically significant and negative (beta = -0.49, 95% CI [-0.79, -0.20], p = 0.001; Std. beta = -0.49, 95% CI [-0.79, -0.20])\n  - The interaction effect of Gender [Women] on Priming [Prime] is statistically significant and positive (beta = 1.04, 95% CI [0.29, 1.78], p = 0.006; Std. beta = 1.04, 95% CI [0.29, 1.78])\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using the Wald approximation.\n\n\nWe can use this output to write up a final report:\nA mixed-effect binomial logistic regression model with random intercepts for speakers was fit to the data in a step-wise-step up procedure. The final minimal adequate model performed significantly better than an intercept-only base line model (\\(\\chi\\)2(4): 173.327790313894, p = 0) and a good but not optimal fit (C: 0.758332104539, Somers’ Dxy: 0.516664209078). The final minimal adequate model reported that speakers use more discourse like in mixed-gender conversations compared to same-gender conversations (\\(\\chi\\)2(1): `r 12.813848196363, p = 0.00034) and that there is an interaction between priming and gender with men using more discourse like in un-primed contexts while this gender difference is not present in primed contexts where speakers more more likely to use discourse like regardless of gender (\\(\\chi\\)2(1): 7.418740944005, p = 0.00645)."
  },
  {
    "objectID": "regression.html#mixed-effects-quasi-poisson-and-negative-binomial-regression",
    "href": "regression.html#mixed-effects-quasi-poisson-and-negative-binomial-regression",
    "title": "Fixed- and Mixed-Effects Regression Models in R",
    "section": "Mixed-Effects (Quasi-)Poisson and Negative-Binomial Regression",
    "text": "Mixed-Effects (Quasi-)Poisson and Negative-Binomial Regression\nLike fixed-effects Poisson models, mixed-effects Poisson models take counts as dependent variables. The data for this analysis was collected on three separate evenings (Trial). The number of the filler uhm (UHM) was counted in two-minute conversations that were either in English, German, Russian, or Mandarin (Language). In addition, the number of shots that speakers drank before they talked was recorded (Shots).\n\n# load data\ncountdata  <- base::readRDS(url(\"https://slcladal.github.io/data/cld.rda\", \"rb\"))\n\n\n# inspect data\ncountdata %>%\n  as.data.frame() %>%\n  head(15) %>%\n  flextable() %>%\n  flextable::set_table_properties(width = .75, layout = \"autofit\") %>%\n  flextable::theme_zebra() %>%\n  flextable::fontsize(size = 12) %>%\n  flextable::fontsize(size = 12, part = \"header\") %>%\n  flextable::align_text_col(align = \"center\") %>%\n  flextable::set_caption(caption = \"First 15 rows of the countdata data.\")  %>%\n  flextable::border_outer()\n\n\n\n\nFirst 15 rows of the countdata data.\n\n\nIDTrialLanguageGenderUHMShots13RussianMan0023RussianMan0033GermanMan0541GermanMan0351GermanWoman2663GermanMan1571MandarinMan1183GermanWoman0493RussianWoman00102GermanMan02113RussianMan01122GermanMan01133RussianWoman01142RussianWoman44152EnglishMan04\n\n\nSince the data contains character variables, we need to factorize the data before we can analyse it further and we also remove the ID column.\n\n# factorize variables\ncountdata <- countdata %>%\n  dplyr::select(-ID) %>%\n  dplyr::mutate_if(is.character, factor)\n\n\n# inspect data\ncountdata %>%\n  as.data.frame() %>%\n  head(15) %>%\n  flextable() %>%\n  flextable::set_table_properties(width = .75, layout = \"autofit\") %>%\n  flextable::theme_zebra() %>%\n  flextable::fontsize(size = 12) %>%\n  flextable::fontsize(size = 12, part = \"header\") %>%\n  flextable::align_text_col(align = \"center\") %>%\n  flextable::set_caption(caption = \"First 15 rows of the countdata data.\")  %>%\n  flextable::border_outer()\n\n\n\n\nFirst 15 rows of the countdata data.\n\n\nTrialLanguageGenderUHMShots3RussianMan003RussianMan003GermanMan051GermanMan031GermanWoman263GermanMan151MandarinMan113GermanWoman043RussianWoman002GermanMan023RussianMan012GermanMan013RussianWoman012RussianWoman442EnglishMan04\n\n\nAfter the data is factorized, we can visualize the data.\n\ncountdata %>%\n  # prepare data\n  dplyr::select(Language, Shots) %>%\n  dplyr::group_by(Language) %>%\n  dplyr::mutate(Mean = round(mean(Shots), 1)) %>%\n  dplyr::mutate(SD = round(sd(Shots), 1)) %>%\n  # start plot\n  ggplot(aes(Language, Shots, color = Language, fill = Language)) +\n  geom_violin(trim=FALSE, color = \"gray20\")+ \n  geom_boxplot(width=0.1, fill=\"white\", color = \"gray20\") +\n  geom_text(aes(y=-4,label=paste(\"mean: \", Mean, sep = \"\")), size = 3, color = \"black\") +\n  geom_text(aes(y=-5,label=paste(\"SD: \", SD, sep = \"\")), size = 3, color = \"black\") +\n  scale_fill_manual(values=rep(\"grey90\",4)) + \n  theme_set(theme_bw(base_size = 10)) +\n  theme(legend.position=\"none\", legend.title = element_blank(),\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank()) + \n  ylim(-5, 15) +\n  labs(x = \"Language\", y = \"Shots\")\n\n\n\n\nThe violin plots show that the English speakers drank more shots than speakers of other languages with Mandarin speakers drinking the fewest shots.\nIn the present case, we will a Boruta variable selection procedure to streamline the model fitting process. Thus, before fitting the model, we will test which variables have any kind of relationship with the dependent variable and therefore deserve to be evaluated in the regression modeling. As this is just an example, we will only consider variables which are deemed important and disregard both unimportant and tentative variables. We start the Boruta analysis by setting a seed and running an initial Boruta analysis.\n\n# perform variable selection\nset.seed(20191220)\nboruta <- Boruta(UHM ~.,data=countdata)\nprint(boruta)\n\nBoruta performed 99 iterations in 7.04685592651 secs.\n 1 attributes confirmed important: Shots;\n 1 attributes confirmed unimportant: Gender;\n 2 tentative attributes left: Language, Trial;\n\n\nAs only Shots is confirmed as important, we will only check for the effect of Shots and include Language as a random effect in the regression modeling. Including Language as a random effect is probably not justified statistically (given that the Boruta analysis showed that it only has a tentative effect) but for theoretical reasons as the speakers are nested into Languages. Before we start with the modeling, however, we proceed by checking if the data does indeed approximate a Poisson distribution.\n\n# output the results\ngf = goodfit(countdata$UHM,type= \"poisson\", method= \"ML\")\nplot(gf, main=\"Count data vs Poisson distribution\")\n\n\n\n\nThe data does not perfectly match a distribution that would be expected if the data approximated a Poisson distribution. We will use a goodness-of-fit test to check if the data does indeed diverge significantly from being Poisson distributed. If the p-values of the goodness-of-fit test is smaller than .05, then the distribution of the data differs significantly from a Poisson distribution and, given the visualization is likely over-dispersed.\nIn case of overdispersion, we may have to use a quasi-Poisson or, even better, a negative binomial model but we will, for now continue with the Poisson model and perform diagnostics later to check if we have to switch to a more robust method. One effect of overdispersion is that the standard errors of a model are biased and quasi-Poisson models scale the standard errors to compensate bias. However, [30] suggest to use negative-binomial model instead. This is so because the scaling of the standard errors performed by quasi-Poisson models only affects the significance of coefficients (the p-values) but it does not affect the coefficients which, however, may be affected themselves by overdispersion. Thus, the coefficients of Poisson as well as quasi-Poisson models (which are identical) may be unreliable when dealing with overdispersion. Negative binomial models, in contrast, include an additional dispersion or heterogeneity parameter which accommodates overdispersion better than merely scaling the standard errors [see 30].\n\nsummary(gf)\n\n\n     Goodness-of-fit test for poisson distribution\n\n                           X^2 df                                     P(> X^2)\nLikelihood Ratio 153.422771085  5 0.000000000000000000000000000000249336691328\n\n\nThe p-value is indeed smaller than .05 which means that we should indeed use a negative-binomial model rather than a Poisson model. We will ignore this, for now, and proceed to fit a Poisson mixed-effects model and check what happens if a Poisson model is fit to over-dispersed data.\n\nMixed-Effects Poisson Regression\nIn a first step, we create mixed-effect intercept-only baseline models and then test if including “Shots” significantly improves model fit and, thus, has a significant impact on the number of uhms.\n\n# base-line mixed-model\nm0.glmer = glmer(UHM ~ 1 + (1 | Language), data = countdata, family = poisson,\n                 control=glmerControl(optimizer=\"bobyqa\"))\n# add Shots\nm1.glmer <- update(m0.glmer, .~.+ Shots)\nAnova(m1.glmer, test = \"Chi\")           \n\nAnalysis of Deviance Table (Type II Wald chisquare tests)\n\nResponse: UHM\n          Chisq Df             Pr(>Chisq)    \nShots 321.24968  1 < 0.000000000000000222 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe ANOVA confirms that Shots have a significant impact on the number of instances of uhm. However, we get the warning that the fitted mixed model is (almost / near) singular. In such cases, the model should not be reported. As this is only an example, we will continue by having a look at the model summary.\n\nsummary(m1.glmer)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: poisson  ( log )\nFormula: UHM ~ (1 | Language) + Shots\n   Data: countdata\nControl: glmerControl(optimizer = \"bobyqa\")\n\n     AIC      BIC   logLik deviance df.resid \n  1041.8   1054.5   -517.9   1035.8      497 \n\nScaled residuals: \n         Min           1Q       Median           3Q          Max \n-1.509633852 -0.666423093 -0.592950422  0.586114082  4.338639382 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n Language (Intercept) 0        0       \nNumber of obs: 500, groups:  Language, 4\n\nFixed effects:\n                 Estimate    Std. Error   z value               Pr(>|z|)    \n(Intercept) -1.2789168850  0.0893313713 -14.31655 < 0.000000000000000222 ***\nShots        0.2336279071  0.0130347699  17.92344 < 0.000000000000000222 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n      (Intr)\nShots -0.806\noptimizer (bobyqa) convergence code: 0 (OK)\nboundary (singular) fit: see help('isSingular')\n\n\nThe model summary confirms that the number of shots does have a significantly positive effect on the number of occurrences of uhm. Furthermore, the scaled residuals are distributed very unevenly which suggests overdispersion. Including Language as a random effect is not justified given that they have 0 variance and a standard deviation of 0 (which means that Language does not account for or explain any additional variance).\nWe now check if the model suffers from overdispersion following [30].\n\n# extract pearson residuals\nPearsonResiduals <- resid(m1.glmer, type = \"pearson\")\n# extract number of cases in model\nCases <- nrow(countdata)\n# extract number of predictors (plus intercept)\nNumberOfPredictors <- length(fixef(m1.glmer)) +1\n# calculate overdispersion\nOverdispersion <- sum(PearsonResiduals^2) / (Cases-NumberOfPredictors)\n# inspect overdispersion\nOverdispersion\n\n[1] 1.16901460967\n\n\nThe data is slightly over-dispersed. It would also be advisable to plot the Cook’s distance (which should not show data points with values > 1). If there are data points with high Cook’s D values, we could exclude them which would, very likely reduce the overdispersion [see 30]. We ignore this, for now, and use diagnostic plots to check if the plots indicate problems.\n\ndiag_data <- data.frame(PearsonResiduals, fitted(m1.glmer)) %>%\n  dplyr::rename(Pearson = 1,\n                Fitted = 2)\np9 <- ggplot(diag_data, aes(x = Fitted, y = Pearson)) +\n  geom_point() +\n  geom_hline(yintercept = 0, linetype = \"dotted\")\np10 <- ggplot(countdata, aes(x = Shots, y = diag_data$Pearson)) +\n  geom_point()  +\n  geom_hline(yintercept = 0, linetype = \"dotted\") +\n  labs(y = \"Pearson\")\np11 <- ggplot(countdata, aes(x = Language, y = diag_data$Pearson)) +\n  geom_boxplot() +\n  labs(y = \"Pearson\") + \n  theme(axis.text.x = element_text(angle=90))\ngrid.arrange(p9, p10, p11, nrow = 1)\n\n\n\n\nThe diagnostic plots show problems as the dots in the first two plots are not random but show a pattern in the lower left corner. In addition, the variance of English (left boxplot) is notable larger than the variance of Russian (right boxplot). As a final step, we plot the predicted vales of the model to check if the predictions make sense.\n\nplot_model(m1.glmer, type = \"pred\", terms = c(\"Shots\"))\n\n\n\n\nThe model predicts that the instances of uhm increase with the number of shots. Note that the increase is not homogeneous as the y-axis labels indicate! We now compare the predicted number of uhm with the actually observed instances of uhm to check if the results of the model make sense.\n\ncountdata %>%\n  mutate(Predicted = predict(m1.glmer, type = \"response\")) %>%\n  dplyr::rename(Observed = UHM) %>%\n  tidyr::gather(Type, Frequency, c(Observed, Predicted)) %>%\n  dplyr::mutate(Shots = factor(Shots),\n                Type = factor(Type)) %>%\n  dplyr::group_by(Shots, Type) %>%\n  dplyr::summarize(Frequency = mean(Frequency)) %>%\n  ggplot(aes(Shots, Frequency, group = Type, color = Type, linetype = Type)) +\n  geom_line(size = 1.2) +\n  scale_color_manual(values = c(\"orange\", \"lightblue\")) \n\n\n\n\nThe comparison between the observed and the predicted uses of uhm becomes somewhat volatile and shows fluctuations after eight shots. We will now summarize the results as if the violations (overdispersion measure > 1 and excessive multicollinearity (singular fit)) had NOT occurred(!) - again: this is only because we are practicing here - this would be absolutely unacceptable in a proper write-up of an analysis!\nThe summary of the model can be extracted using the tab_model function from the sjPlot package [13].\n\nsjPlot::tab_model(m1.glmer)\n\n\n\n\n \nUHM\n\n\nPredictors\nIncidence Rate Ratios\nCI\np\n\n\n(Intercept)\n0.28\n0.23 – 0.33\n<0.001\n\n\nShots\n1.26\n1.23 – 1.30\n<0.001\n\n\nRandom Effects\n\n\n\nσ2\n0.89\n\n\n\nτ00 Language\n0.00\n\n\nN Language\n4\n\nObservations\n500\n\n\nMarginal R2 / Conditional R2\n0.288 / NA\n\n\n\n\n\n\n\n\n\n\n\nNOTEThe R2 values of the summary table are incorrect (as indicated by the missing conditional R2 value). The more appropriate conditional and marginal coefficient of determination for generalized mixed-effect models can be extracted using the r.squaredGLMM function from the MuMIn package [32].\n\n\n\n\n\n\n\nr.squaredGLMM(m1.glmer)\n\n                     R2m            R2c\ndelta     0.208910674745 0.208910674745\nlognormal 0.294971460548 0.294971460548\ntrigamma  0.120419614835 0.120419614835\n\n\nAlso note that our model suffers from a serious problem (near singular fit). If this were not just an example, you should not(!) report this model!\n\n\nMixed-Effects Quasi-Possion Regression\nThe Quasi-Poisson Regression is a generalization of the Poisson regression and is used when modeling an overdispersed count variable. Poisson models are based on the Poisson distribution which is defined as a distribution where the variance is equal to the mean (which is very restrictive and not often the case). Quasi-Poisson models scale the standard errors which has a positive effect when dealing with overdispersed data.\nTherefore, when the variance is greater than the mean, a Quasi-Poisson model, which assumes that the variance is a linear function of the mean, is more appropriate as it handles over-dispersed data better than normal Poisson-models.\nWe begin the model fitting process by creating a mixed- and a fixed-effects intercept-only base-line model. Unfortunately, there is not yet a procedure in place for quasi-Poisson models to test if the inclusion of random effects is justified. However, here the Boruta also provides valuable information: Language was only considered tentative but not important which suggests that it will not explain variance which means that including Language as a random effect may not be justified. This would require further inspection. Because we are only dealing with an example here, we ignore this fact (which you should not do in proper analyses) and continue right away with adding shots.\n\n# base-line mixed-model\nm0.qp = glmmPQL(UHM ~ 1, random = ~ 1 | Language, data = countdata, \n                   family = quasipoisson(link='log'))\n\niteration 1\n\n\niteration 2\n\n\niteration 3\n\n\niteration 4\n\n# add Shots\nm1.qp <- update(m0.qp, .~.+ Shots)\n\niteration 1\n\nAnova(m1.qp, test = \"Chi\")           # SIG! (p<0.0000000000000002 ***)\n\nAnalysis of Deviance Table (Type II tests)\n\nResponse: zz\n         Chisq Df             Pr(>Chisq)    \nShots 276.4523  1 < 0.000000000000000222 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe ANOVA confirms that Shots have a significant impact on the number of instances of uhm. We will now have a look at the model summary.\n\nsummary(m1.qp)\n\nLinear mixed-effects model fit by maximum likelihood\n  Data: countdata \n  AIC BIC logLik\n   NA  NA     NA\n\nRandom effects:\n Formula: ~1 | Language\n               (Intercept)      Residual\nStdDev: 0.0000407595004855 1.07801257915\n\nVariance function:\n Structure: fixed weights\n Formula: ~invwt \nFixed effects:  UHM ~ Shots \n                      Value       Std.Error  DF        t-value p-value\n(Intercept) -1.278929721022 0.0964886326643 495 -13.2547190867       0\nShots        0.233630231741 0.0140795660799 495  16.5935676153       0\n Correlation: \n      (Intr)\nShots -0.806\n\nStandardized Within-Group Residuals:\n            Min              Q1             Med              Q3             Max \n-1.400417640404 -0.618228066949 -0.550071160973  0.543731905024  4.024828853746 \n\nNumber of Observations: 500\nNumber of Groups: 4 \n\n\nThe model summary does not provide much information such as,e.g. AIC or BIC values. The coefficient for Shots is highly significant (p <.001) and the data is notably over-dispersed (the Standardized Within-Group Residuals deviate substantially from a normal distribution with higher values having a thick tail). Also, in contrast to the Poisson model, Language does explain at least a minimal share of the variance now as the mean and standard deviation are no longer 0. Note also, that the coefficients are identical to the Poisson coefficients but the standard errors and p-values differ (the model provides t- rather than z-values).\nIn a next step, we will calculate the odds ratios of the coefficient (as we only have one). We will use the coefficients from the fixed-effects model as the coefficients for mixed- and fixed-effects models are identical (the random effect structure only affects the standard error and p-values but not the coefficients; you can check by uncommenting the summary command).\n\nm1.glm = glm(UHM ~ Shots, data = countdata, family = quasipoisson(link='log'))\nexp(coef(m1.glm))\n\n   (Intercept)          Shots \n0.278338612235 1.263174385573 \n\n\nThe standardized or \\(\\beta\\)-coefficient tells us that the likelihood of uhm increases by 1.26 (or 26.32 percent) with each additional shot.\nBefore inspecting the relationship between Shots and uhm, we will check if the overdispersion was reduced.\n\n# extract pearson residuals\nPearsonResiduals <- resid(m1.qp, type = \"pearson\")\n# extract number of cases in model\nCases <- nrow(countdata)\n# extract number of predictors (plus intercept)\nNumberOfPredictors <- length(fixef(m1.qp)) +1\n# calculate overdispersion\nOverdispersion <- sum(PearsonResiduals^2) / (Cases-NumberOfPredictors)\n# inspect overdispersion\nOverdispersion\n\n[1] 1.00603621722\n\n\nThe overdispersion has indeed decreased and is not so close to 1 that overdispersion is no longer an issue.\nWe continue to diagnose the model by plotting the Pearson’s residuals against fitted values. This diagnostic plot should not show a funnel-like structure or patterning as we observed in the case of the Poisson model.\n\n# diagnostic plot\nplot(m1.qp, pch = 20, col = \"black\", lty= \"dotted\", ylab = \"Pearson's residuals\")\n\n\n\n\nIndeed, the plot exhibits a (slight) funnel shape (but not drastically so) and thus indicates heteroscedasticity. However, the patterning that we observed with the Poisson model has disappeared. We continue by plotting the random effect adjustments.\n\n# generate diagnostic plots\nplot(m1.qp, Language ~ resid(.), abline = 0, fill = \"gray70\") \n\n\n\n\nThe adjustments by “Language” are marginal (which was somewhat expected given that Language was only deemed tentative), which shows that there is very little variation between the languages and that we have no statistical reason to include Language as a random effect.\nIn a final step, we plot the fixed-effect of Shots using the predictorEffects function from the effects package [42].\n\nplot_model(m1.qp, type = \"pred\", terms = c(\"Shots\"))\n\n\n\n\nThe effects plot shows that the number of uhms increases exponentially with the number of shots a speaker has had. We now compare the predicted number of uhm with the actually observed instances of uhm to check if the results of the model make sense.\n\ncountdata %>%\n  mutate(Predicted = predict(m1.qp, type = \"response\")) %>%\n  dplyr::rename(Observed = UHM) %>%\n  tidyr::gather(Type, Frequency, c(Observed, Predicted)) %>%\n  dplyr::mutate(Shots = factor(Shots),\n                Type = factor(Type)) %>%\n  dplyr::group_by(Shots, Type) %>%\n  dplyr::summarize(Frequency = mean(Frequency)) %>%\n  ggplot(aes(Shots, Frequency, group = Type, color = Type, linetype = Type)) +\n  geom_line(size = 1.2) +\n  scale_color_manual(values = c(\"orange\", \"lightblue\")) \n\n\n\n\nGiven that the overdispersion measure of this Quasi-Poisson model is close to 1, that the model did not suffer from excessive multicollinearity (singular fit), and because this model shows improvements compared to the Poisson model with respect to the model diagnostics (some adjustments by Language and less patterning in the diagnostic plots), we would choose this quasi-Poisson model over the Poisson model.\nFinally, we extract the summary table of this model.\n\nsjPlot::tab_model(m1.qp)\n\n\n\n\n \nUHM\n\n\nPredictors\nIncidence Rate Ratios\nCI\np\n\n\n(Intercept)\n0.28\n0.23 – 0.34\n<0.001\n\n\nShots\n1.26\n1.23 – 1.30\n<0.001\n\n\n\nN Language\n4\n\nObservations\n500\n\n\n\n\n\n\n\n\n\n\n\nNOTEThe R2 values of the summary table are incorrect (as indicated by the missing conditional R2 value). The more appropriate conditional and marginal coefficient of determination for generalized mixed-effect models can be extracted using the r.squaredGLMM function from the MuMIn package [32].\n\n\n\n\n\n\n\nr.squaredGLMM(m1.qp)\n\n                      R2m             R2c\ndelta     0.1856941878845 0.1856941887428\nlognormal 0.2752763979322 0.2752763992045\ntrigamma  0.0977040663474 0.0977040667989\n\n\n\n\nMixed-Effects Negative Binomial Regression\nNegative binomial regression models are a generalization of Poisson regression which loosens the restrictive assumption that the variance is equal to the mean made by the Poisson model. This is a major advantage as the most common issue that one faces with Poisson regressions is that the data deviate too substantially from the assumed Poisson distribution.\nTo implement a Negative-Binomial Mixed-Effects Regression, we first create a mixed-effects intercept-only baseline model and then test if including Shots significantly improves model fit and, thus, has a significant impact on the number of uhms.\n\n# base-line mixed-model\nm0.nb = glmer.nb(UHM ~ 1 + (1 | Language), data = countdata)\n# add Shots\nm1.nb <- update(m0.nb, .~.+ Shots)\n\nboundary (singular) fit: see help('isSingular')\n\nanova(m1.nb, m0.nb)           \n\nData: countdata\nModels:\nm0.nb: UHM ~ 1 + (1 | Language)\nm1.nb: UHM ~ (1 | Language) + Shots\n      npar         AIC         BIC       logLik    deviance     Chisq Df\nm0.nb    3 1159.000894 1171.644718 -576.5004470 1153.000894             \nm1.nb    4 1051.593288 1068.451721 -521.7966442 1043.593288 109.40761  1\n                  Pr(>Chisq)    \nm0.nb                           \nm1.nb < 0.000000000000000222 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe negative-binomial model also reports a significant impact of shots on the number of uhms. In a next step, we calculate the overdispersion.\n\n# extract pearson residuals\nPearsonResiduals <- resid(m1.nb, type = \"pearson\")\n# extract number of betas + predictors + sigma\nNumberOfPredictors <- 2+1+1\n# extract number of cases in model\nCases <- nrow(countdata)\n# calculate overdispersion parameter\nOverdispersion <- sum(PearsonResiduals^2) / (Cases / NumberOfPredictors)# show overdispersion parameter\nOverdispersion\n\n[1] 2.46949245808\n\n\nThe overdispersion has increased which is rather suboptimal. In this case, we would report the Quasi-Poisson Regression rather than the Negative Binomial Model (which is rather rare as Negative Binomial Models typically perform better than (Quasi-)Poisson models. However, this tutorial focuses merely on how to implement a Negative Binomial Mixed-Effects Regression and we thus continue with generating diagnostic plots to check for problems.\n\ndiag_data <- data.frame(PearsonResiduals, fitted(m1.nb)) %>%\n  dplyr::rename(Pearson = 1,\n                Fitted = 2)\np9 <- ggplot(diag_data, aes(x = Fitted, y = Pearson)) +\n  geom_point() +\n  geom_hline(yintercept = 0, linetype = \"dotted\")\np10 <- ggplot(countdata, aes(x = Shots, y = diag_data$Pearson)) +\n  geom_point()  +\n  geom_hline(yintercept = 0, linetype = \"dotted\") +\n  labs(y = \"Pearson\")\np11 <- ggplot(countdata, aes(x = Language, y = diag_data$Pearson)) +\n  geom_boxplot() +\n  labs(y = \"Pearson\") + \n  theme(axis.text.x = element_text(angle=90))\ngrid.arrange(p9, p10, p11, nrow = 1)\n\n\n\n\nThe diagnostics show patterning similar to the one we saw with the Poisson model which suggest that the negative binomial model is also not an optimal model for our data. We continue by plotting the predicted values and, subsequently, summarize the analysis.\n\nplot_model(m1.nb, type = \"pred\", terms = c(\"Shots\"))\n\n\n\n\nThe effect plot shows that the predicted number of shots increases exponentially with each shot. We now compare the predicted number of uhm with the actually observed instances of uhm to check if the results of the model make sense.\n\ncountdata %>%\n  mutate(Predicted = predict(m1.nb, type = \"response\")) %>%\n  dplyr::rename(Observed = UHM) %>%\n  tidyr::gather(Type, Frequency, c(Observed, Predicted)) %>%\n  dplyr::mutate(Shots = factor(Shots),\n                Type = factor(Type)) %>%\n  dplyr::group_by(Shots, Type) %>%\n  dplyr::summarize(Frequency = mean(Frequency)) %>%\n  ggplot(aes(Shots, Frequency, group = Type, color = Type, linetype = Type)) +\n  geom_line(size = 1.2) +\n  scale_color_manual(values = c(\"orange\", \"lightblue\")) \n\n\n\n\nThe comparison between the observed and the predicted uses of uhm becomes somewhat volatile and shows fluctuations after eight shots. We will now summarize the results as if the violations had NOT occurred(!) - again: this is only because we are practicing here - this would be absolutely unacceptable in a proper write-up of an analysis!\nA mixed-effect negative binomial regression model which contained the language in which the conversation took place as random effect was fit to the data. Prior to the regression modeling, a Boruta analysis was applied to determine whether any of the predictors had a meaningful relationship with the dependent variable (instances of uhm). Since the Boruta analysis indicated that only the number of shots a speaker had was important, only “Shots” was tested during model fitting. The final minimal adequate model showed that the number of uhm as fillers increases significantly, and near-linearly with the number of shots speakers had (\\(\\chi\\)2(1):83.0, p <.0001, \\(\\beta\\): 0.2782). An inspection of the random effect structure conveyed that there was almost no variability between languages and language did not contribute meaningfully to the model fit."
  },
  {
    "objectID": "regression.html#mixed-effects-multinomial-regression",
    "href": "regression.html#mixed-effects-multinomial-regression",
    "title": "Fixed- and Mixed-Effects Regression Models in R",
    "section": "Mixed-Effects Multinomial Regression",
    "text": "Mixed-Effects Multinomial Regression\nIn this section, we will focus on how to implement a mixed-effects multinomial regression model using the mblogit function from the mclogit package [see 43]. As we have already gone though model fitting and model validation procedures above, we will strictly see how to implement this type of model here - we will not go through all the other steps that a proper regression analysis would require.\nWe begin the analysis by loading the example data set. The data represents observations gathered during an experiment where speakers had to report what they saw. The responses are categorized into four groups:\n\n# description data\npict  <- base::readRDS(url(\"https://slcladal.github.io/data/pict.rda\", \"rb\"))\n# inspect\nhead(pict)\n\n  Id Participant       Group Item     Response Gender Age\n1  1        G001 German_Mono    1  NumeralNoun   Male  18\n2  2        G002 German_Mono    3  NumeralNoun   Male  18\n3  3        G003 German_Mono    4  NumeralNoun   Male  18\n4  4        G004 German_Mono    6 QuantAdjNoun   Male  18\n5  5        G005 German_Mono    8  NumeralNoun   Male  18\n6  6        G006 German_Mono    9 QuantAdjNoun   Male  18\n\n\nIn a first step, we generate a baseline model that we call m0. This model only contains the random effect structure and the intercept as the sole predictor.\n\nm0.mn <- mblogit(formula = Response ~ 1, \n              random = ~ 1 | Participant, \n              data = pict)\n\n\nIteration 1 - deviance = 3038.65479447 - criterion = 0.788153856566\nIteration 2 - deviance = 1039.57449206 - criterion = 0.176903386816\nIteration 3 - deviance = 547.058967782 - criterion = 0.0855470770333\nIteration 4 - deviance = 1256.54718254 - criterion = 0.0813989708889\nIteration 5 - deviance = 63.4217806776 - criterion = 0.0986885267217\nIteration 6 - deviance = -394.76643481 - criterion = 0.0776120932314\nIteration 7 - deviance = 5.62985445935 - criterion = 0.104610835582\nIteration 8 - deviance = -210.444206442 - criterion = 0.0603396486691\nIteration 9 - deviance = 596.528255883 - criterion = 0.135066522725\nIteration 10 - deviance = -124.162950554 - criterion = 0.0415097050311\nIteration 11 - deviance = 377.975373084 - criterion = 0.0328094246095\nIteration 12 - deviance = 140.095311688 - criterion = 0.133864208674\nIteration 13 - deviance = 1013.05642125 - criterion = 0.231316954023\nIteration 14 - deviance = -3.30112378888 - criterion = 0.161806646675\nIteration 15 - deviance = 937.452340502 - criterion = 0.0971884881609\nIteration 16 - deviance = -170.314936427 - criterion = 0.110950531568\nIteration 17 - deviance = 240.541276454 - criterion = 0.0895026719048\nIteration 18 - deviance = -1028.78181767 - criterion = 0.139902469189\nIteration 19 - deviance = -482.901482304 - criterion = 0.0704521880486\nIteration 20 - deviance = -217.021371436 - criterion = 0.0539352925313\nIteration 21 - deviance = 501.706207685 - criterion = 0.16311970482\nIteration 22 - deviance = -156.634283459 - criterion = 0.0469752281655\nIteration 23 - deviance = 284.371554236 - criterion = 0.0384622152383\nIteration 24 - deviance = 477.190984196 - criterion = 0.275130330768\nIteration 25 - deviance = 1091.09306626 - criterion = 0.137081279068\n\n\nIn this case, the algorithm did not converge properly - if this were a real analysis, we could not simply continue but would have to inspect possible causes for this. However, as this is just a showcase, we will ignore this and move on. Next, we add the fixed effects (Gender and Group).\n\nm1.mn <- mblogit(formula = Response ~ Gender + Group, \n              random = ~ 1 | Item, \n              data = pict)\n\n\nIteration 1 - deviance = 1667.89123406 - criterion = 0.801491607033\nIteration 2 - deviance = 1576.66992662 - criterion = 0.0818733799283\nIteration 3 - deviance = 1550.62063217 - criterion = 0.0307203102233\nIteration 4 - deviance = 1541.31639399 - criterion = 0.00976465320773\nIteration 5 - deviance = 1537.50858746 - criterion = 0.00251480377207\nIteration 6 - deviance = 1536.10265122 - criterion = 0.000306996036811\nIteration 7 - deviance = 1535.72249302 - criterion = 0.0000470947760996\nIteration 8 - deviance = 1535.59319629 - criterion = 0.00000732037942916\nIteration 9 - deviance = 1535.54664057 - criterion = 0.00000119558493383\nIteration 10 - deviance = 1535.52876695 - criterion = 0.000000199398597452\nIteration 11 - deviance = 1535.52165999 - criterion = 0.0000000334243542711\nIteration 12 - deviance = 1535.518789 - criterion = 0.00000000560129429554\nconverged\n\n\nNow, we can compare the models to see if including the fixed-effects into the model has significantly improved the model fit.\n\nanova(m0.mn, m1.mn)\n\nAnalysis of Deviance Table\n\nModel 1: Response ~ 1\nModel 2: Response ~ Gender + Group\n  Resid. Df  Resid. Dev Df     Deviance\n1      3261 1091.093066                \n2      3249 1535.518789 12 -444.4257227\n\n\nAs the second model is significantly better, we are justified to believe that our fixed effects have explanatory power. We can now use the getSummary.mmblogit function to get a summary of the model with the fixed effects.\n\n# inspect\nmclogit::getSummary.mmblogit(m1.mn)\n\n$coef\n, , NumeralNoun/BareNoun\n\n                                  est             se             stat\n(Intercept)           1.0874092952417 0.893295577758   1.217300658726\nGenderMale            0.0692427468008 0.197359653418   0.350845502623\nGroupGerman_Mono     -3.3053490899596 0.306602481360 -10.780568621958\nGroupL2_Advanced     -0.4607564819019 0.309284978199  -1.489747366926\nGroupL2_Intermediate -1.1782785696607 0.322487307276  -3.653720760718\n                                                            p             lwr\n(Intercept)          0.22348984377707278858871120519324904308 -0.663417864712\nGenderMale           0.72570425820753281520580912911100313067 -0.317575065899\nGroupGerman_Mono     0.00000000000000000000000000425248830713 -3.906278910995\nGroupL2_Advanced     0.13629067498365624033773713108530500904 -1.066943900131\nGroupL2_Intermediate 0.00025846735930856122023455134772973452 -1.810342077392\n                                 upr\n(Intercept)           2.838236455195\nGenderMale            0.456060559501\nGroupGerman_Mono     -2.704419268924\nGroupL2_Advanced      0.145430936328\nGroupL2_Intermediate -0.546215061929\n\n, , QuantAdjNoun/BareNoun\n\n                                 est             se             stat\n(Intercept)           0.571176946931 0.692608541009   0.824674997653\nGenderMale            0.298987635166 0.248639839890   1.202492872013\nGroupGerman_Mono     -3.875407740288 0.378808841683 -10.230510257033\nGroupL2_Advanced     -2.198916269400 0.349918227201  -6.284086104881\nGroupL2_Intermediate -3.025120759319 0.400642905543  -7.550665985766\n                                                         p             lwr\n(Intercept)          0.40955613002243967946114366895926651 -0.786310848832\nGenderMale           0.22917262613964711759351189357403200 -0.188337496140\nGroupGerman_Mono     0.00000000000000000000000144754650331 -4.617859427011\nGroupL2_Advanced     0.00000000032978789549465373397243521 -2.884743392248\nGroupL2_Intermediate 0.00000000000004330381141474033699687 -3.810366424844\n                                 upr\n(Intercept)           1.928664742695\nGenderMale            0.786312766471\nGroupGerman_Mono     -3.132956053565\nGroupL2_Advanced     -1.513089146551\nGroupL2_Intermediate -2.239875093794\n\n, , QuantNoun/BareNoun\n\n                                 est             se            stat\n(Intercept)          -2.464598002828 0.998541739748 -2.468197276811\nGenderMale           -0.377291526526 0.434033290744 -0.869268635775\nGroupGerman_Mono     -2.485588291752 0.818754737248 -3.035815463011\nGroupL2_Advanced      0.781434518047 0.689498783889  1.133337050486\nGroupL2_Intermediate -0.177522551612 0.744857214668 -0.238330982255\n                                    p            lwr             upr\n(Intercept)          0.01357954576529 -4.42170384979 -0.507492155862\nGenderMale           0.38470021375801 -1.22798114448  0.473398091424\nGroupGerman_Mono     0.00239886133501 -4.09031808893 -0.880858494574\nGroupL2_Advanced     0.25707273828459 -0.56995826576  2.132827301853\nGroupL2_Intermediate 0.81162439319654 -1.63741586599  1.282370762763\n\n\n$Item\n, , 1\n\n                                              est            se stat  p lwr upr\nNumeralNoun/BareNoun: VCov(~1,~1)   7.04843777880 61.5809194674   NA NA  NA  NA\nQuantAdjNoun/BareNoun: VCov(~1,~1) -1.45406596713 84.3719670062   NA NA  NA  NA\nQuantNoun/BareNoun: VCov(~1,~1)     1.91291563134 87.9836147377   NA NA  NA  NA\n\n, , 2\n\n                                              est            se stat  p lwr upr\nNumeralNoun/BareNoun: VCov(~1,~1)  -1.45406596713 84.3719670062   NA NA  NA  NA\nQuantAdjNoun/BareNoun: VCov(~1,~1)  3.83022239605 48.3623057477   NA NA  NA  NA\nQuantNoun/BareNoun: VCov(~1,~1)     3.17487495693 17.9994914254   NA NA  NA  NA\n\n, , 3\n\n                                             est            se stat  p lwr upr\nNumeralNoun/BareNoun: VCov(~1,~1)  1.91291563134 87.9836147377   NA NA  NA  NA\nQuantAdjNoun/BareNoun: VCov(~1,~1) 3.17487495693 17.9994914254   NA NA  NA  NA\nQuantNoun/BareNoun: VCov(~1,~1)    5.35288426315 41.9596189735   NA NA  NA  NA\n\n\n$Groups\nGroups by Item \n            10 \n\n$sumstat\n               LR                df          deviance          McFadden \n1486.602918242263   21.000000000000 1535.518788999098    0.491907031633 \n        Cox.Snell        Nagelkerke               AIC               BIC \n   0.744326972782    0.793948770968 1577.518788999098 1682.391381478785 \n                N \n1090.000000000000 \n\n$call\nmblogit(formula = Response ~ Gender + Group, data = pict, random = ~1 | \n    Item)\n\n$contrasts\n$contrasts$Gender\n[1] \"contr.treatment\"\n\n$contrasts$Group\n[1] \"contr.treatment\"\n\n\n$xlevels\n$xlevels$Gender\n[1] \"Female\" \"Male\"  \n\n$xlevels$Group\n[1] \"English_Mono\"    \"German_Mono\"     \"L2_Advanced\"     \"L2_Intermediate\"\n\n\nThe NAs (not available information) is a result of the model having a bad fit to the data and, optimally, we would need to inspect why the model has a bad fit. Again, we ignore this and move on. Next, we check the VIFs to see if the model does not violate multicollinearity assumptions.\n\ncar::vif(m1.mn) # maybe use cut-off of 5 (maybe 10)\n\n                GVIF Df GVIF^(1/(2*Df))\nGender 3.12579996640  1   1.76799320316\nGroup  5.37692515348  3   1.32359666797\n\n\nThe VIFs are a bit high - especially the GVIF for Group would be a cause for concern if this was not just a demo analysis! However, as we only want to implement a multinomial mixed-effects model here and not provide a proper, clean analysis, we will ignore this issue here.\nIn a next step, we visualize effects to get a better understanding of how the predictors that are part of the fixed-effect structure of the mode affect the outcome (the response variable).\n\nsjPlot::plot_model(m1.mn)\n\n\n\n\nFinally, we can extract an alternative summary table produced by the tab_model function from the sjPlot package [see 13].\n\nsjPlot::tab_model(m1.mn)\n\n\n\n\n \nResponse\n\n\nPredictors\nEstimates\nCI\np\n\n\nNumeralNoun~(Intercept)\n1.09\n-0.66 – 2.84\n0.224\n\n\nQuantAdjNoun~(Intercept)\n0.57\n-0.79 – 1.93\n0.410\n\n\nQuantNoun~(Intercept)\n-2.46\n-4.42 – -0.51\n0.014\n\n\nNumeralNoun~GenderMale\n0.07\n-0.32 – 0.46\n0.726\n\n\nQuantAdjNoun~GenderMale\n0.30\n-0.19 – 0.79\n0.229\n\n\nQuantNoun~GenderMale\n-0.38\n-1.23 – 0.47\n0.385\n\n\nNumeralNoun~GroupGermanMono\n-3.31\n-3.91 – -2.70\n<0.001\n\n\nQuantAdjNoun~GroupGermanMono\n-3.88\n-4.62 – -3.13\n<0.001\n\n\nQuantNoun~GroupGermanMono\n-2.49\n-4.09 – -0.88\n0.002\n\n\nNumeralNoun~GroupL2Advanced\n-0.46\n-1.07 – 0.15\n0.136\n\n\nQuantAdjNoun~GroupL2Advanced\n-2.20\n-2.88 – -1.51\n<0.001\n\n\nQuantNoun~GroupL2Advanced\n0.78\n-0.57 – 2.13\n0.257\n\n\nNumeralNoun~GroupL2Intermediate\n-1.18\n-1.81 – -0.55\n<0.001\n\n\nQuantAdjNoun~GroupL2Intermediate\n-3.03\n-3.81 – -2.24\n<0.001\n\n\nQuantNoun~GroupL2Intermediate\n-0.18\n-1.64 – 1.28\n0.812\n\n\n\nN Item\n10\n\nObservations\n1090\n\n\n\n\n\n\n\nThis is the final step in implementing a a mixed-effects multinomial regression model using the mblogit function from the mclogit package [see 43]. We are aware that the analysis shown here is supervifial(!) - but please keep in mind that we just wanted to showcase the implementation here rather than providing a properly and carefully done analysis."
  },
  {
    "objectID": "regression.html#mixed-effects-ordinal-regression",
    "href": "regression.html#mixed-effects-ordinal-regression",
    "title": "Fixed- and Mixed-Effects Regression Models in R",
    "section": "Mixed-Effects Ordinal Regression",
    "text": "Mixed-Effects Ordinal Regression\nIn this section, we will strictly focus on how to implement a mixed-effects ordinal regression model using the clmm function from the ordinal package [see 44]. This type of regression model is extremely useful when dealing with Likert data or other types of questionnaire and survey data where the responses have some kind of hierarchical structure (i.e. responses are not truly independent because they come from different points in time or from different regions). load data\n\n# rating experiment data\nratex  <- base::readRDS(url(\"https://slcladal.github.io/data/ratex.rda\", \"rb\"))\n# inspect data\nhead(ratex)\n\n  Rater Child Group       Accent AccentNumeric       Family\n1    R1  C001 Child StrongAccent             2 DomBilingual\n2    R2  C001 Child StrongAccent             2 DomBilingual\n3    R3  C001 Child StrongAccent             2 DomBilingual\n4    R4  C001 Child StrongAccent             2 DomBilingual\n5    R5  C001 Child StrongAccent             2 DomBilingual\n6    R6  C001 Child StrongAccent             2 DomBilingual\n\n\nWe now tabulate the data to get a better understanding of the data structure.\n\nratex %>%\n  dplyr::group_by(Family, Accent) %>%\n  dplyr::summarise(Frequency = n()) %>%\n  tidyr::spread(Accent, Frequency)\n\n`summarise()` has grouped output by 'Family'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 3 × 4\n# Groups:   Family [3]\n  Family         NoAccent StrongAccent WeakAccent\n  <fct>             <int>        <int>      <int>\n1 DomBilingual         80          145        174\n2 EqualBilingual       20           22         63\n3 Monolingual         209            1         41\n\n\nNext, we visualize the data to inspect its properties.\n\nratex %>%\n  ggplot(aes(Family, AccentNumeric, color = Group)) + \n  stat_summary(fun = mean, geom = \"point\") +          \n  stat_summary(fun.data = mean_cl_boot, geom = \"errorbar\", width = 0.2) +\n  theme_bw() +\n  theme(legend.position = \"top\") +\n  scale_color_manual(values = c(\"gray20\", \"gray70\"))\n\n\n\n\nAn alternative plot shows other properties of the data.\n\nratex %>%\n  dplyr::group_by(Family, Rater, Group) %>%\n  dplyr::summarise(AccentMean = mean(AccentNumeric)) %>%\n  ggplot(aes(Family, AccentMean, fill = Group)) + \n  geom_boxplot() +\n  theme_bw() +\n  theme(legend.position = \"top\") +\n  scale_fill_manual(values = c(\"gray50\", \"gray85\"))\n\n`summarise()` has grouped output by 'Family', 'Rater'. You can override using\nthe `.groups` argument.\n\n\n\n\n\nWe now start the modeling by generating a model with Family as the sole predictor.\n\n# fit baseline model\nm1.or <- clmm(Accent ~ (1|Rater) + Family, link=\"logit\", data = ratex)\n# test for incomplete information\nifelse(min(ftable(ratex$Accent, ratex$Family)) == 0, \"incomplete information\", \"okay\")\n\n[1] \"okay\"\n\n# extract aic\naic.glmer <- AIC(logLik(m1.or))\n# inspect aic\naic.glmer\n\n[1] 1380.25888675\n\n# summarize model\nsummary(m1.or)\n\nCumulative Link Mixed Model fitted with the Laplace approximation\n\nformula: Accent ~ (1 | Rater) + Family\ndata:    ratex\n\n link  threshold nobs logLik  AIC     niter    max.grad cond.H \n logit flexible  755  -685.13 1380.26 371(435) 3.04e-07 2.9e+05\n\nRandom effects:\n Groups Name        Variance            Std.Dev.       \n Rater  (Intercept) 0.00000000353434196 0.0000594503318\nNumber of groups:  Rater 21 \n\nCoefficients:\n                         Estimate   Std. Error   z value             Pr(>|z|)\nFamilyEqualBilingual  0.477744668  0.214314867   2.22917             0.025802\nFamilyMonolingual    -2.550224082  0.198852092 -12.82473 < 0.0000000000000002\n                        \nFamilyEqualBilingual *  \nFamilyMonolingual    ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThreshold coefficients:\n                             Estimate    Std. Error   z value\nNoAccent|StrongAccent   -1.0798112480  0.1058010986 -10.20605\nStrongAccent|WeakAccent  0.1060945025  0.0951357711   1.11519\n\n\nWe can now perform Post-Hoc tests to see which comparisons are significant.\n\nlsmeans(m1.or, pairwise~Family, adjust=\"tukey\")\n\n$lsmeans\n Family         lsmean     SE  df asymp.LCL asymp.UCL\n DomBilingual    0.487 0.0914 Inf     0.308     0.666\n EqualBilingual  0.965 0.1954 Inf     0.582     1.347\n Monolingual    -2.063 0.1740 Inf    -2.404    -1.722\n\nConfidence level used: 0.95 \n\n$contrasts\n contrast                      estimate    SE  df z.ratio p.value\n DomBilingual - EqualBilingual   -0.478 0.214 Inf  -2.229  0.0664\n DomBilingual - Monolingual       2.550 0.199 Inf  12.825  <.0001\n EqualBilingual - Monolingual     3.028 0.265 Inf  11.438  <.0001\n\nP value adjustment: tukey method for comparing a family of 3 estimates \n\n\nFinally, we can summarize the model.\n\nsjPlot::tab_model(m1.or)\n\n\n\n\n \nAccent\n\n\nPredictors\nOdds Ratios\nCI\np\n\n\nNoAccent|StrongAccent\n0.34\n0.28 – 0.42\n<0.001\n\n\nStrongAccent|WeakAccent\n1.11\n0.92 – 1.34\n0.265\n\n\nFamily [EqualBilingual]\n1.61\n1.06 – 2.45\n0.026\n\n\nFamily [Monolingual]\n0.08\n0.05 – 0.12\n<0.001\n\n\nRandom Effects\n\n\n\nσ2\n3.29\n\n\n\nτ00 Rater\n0.00\n\n\nN Rater\n21\n\nObservations\n755\n\n\nMarginal R2 / Conditional R2\n0.325 / NA\n\n\n\n\n\n\n\nAnd we can visualize the effects.\n\nplot_model(m1.or, type = \"pred\", terms = c(\"Family\"))\n\n\n\n\nThat’s it for this tutorial. We hope that you have enjoyed this tutorial and learned how to perform regression analysis including model fitting and model diagnostics as well as reporting regression results."
  },
  {
    "objectID": "reinfnlp.html#preparation-and-session-set-up",
    "href": "reinfnlp.html#preparation-and-session-set-up",
    "title": "Reinforcement Learning and Text Summarization in R",
    "section": "Preparation and session set up",
    "text": "Preparation and session set up\nThis tutorial is based on R. If you have not installed R or are new to it, you will find an introduction to and more information how to use R here. For this tutorials, we need to install certain packages from an R library so that the scripts in this tutorial are executed without errors. Before continuing, please install the packages by running the code below this paragraph. If you have already installed the packages mentioned below, then you can skip ahead ignore this section. To install the necessary packages, simply run the following code - it may take some time (between 1 and 5 minutes to install all of the libraries so you do not need to worry if it takes some time).\nFor this tutorial we will be primarily requiring four packages: tidytext for text manipulations, tidyverse for general tasks, textrank for the implementation of the TextRank algorithm and rvest to scrape through an article to use as an example. For this analysis an article for Time has been selected.\n\n# set options\noptions(stringsAsFactors = F)\n# install libraries\ninstall.packages(c(\"tidytext\",\"tidyverse\",\"textrank\",\"rvest\",\"ggplot2\"))\n# install klippy for copy-to-clipboard button in code chunks\ninstall.packages(\"remotes\")\nremotes::install_github(\"rlesur/klippy\")\n\nNow that we have installed the packages, we activate them as shown below.\n\n# set options\noptions(stringsAsFactors = F)          # no automatic data transformation\noptions(\"scipen\" = 100, \"digits\" = 12) # suppress math annotation\n# activate packages\nlibrary(tidytext)\nlibrary(tidyverse)\nlibrary(textrank)\nlibrary(rvest)\nlibrary(ggplot2)\n# activate klippy for copy-to-clipboard button\nklippy::klippy()\n\n\n\n\nOnce you have installed R and RStudio and also initiated the session by executing the code shown above, you are good to go."
  },
  {
    "objectID": "reinfnlp.html#text-summarisation",
    "href": "reinfnlp.html#text-summarisation",
    "title": "Reinforcement Learning and Text Summarization in R",
    "section": "Text Summarisation",
    "text": "Text Summarisation\nA deep reinforced model for text summarisation involves sequence of input tokens x={x1,x2,…,xn} and produces a sequence of output (summary) tokens. A schematic presentation of the process is shown below:\n\n\n\n\n\nFor the article summarisation objective the deep RL has the following components:\n\nAction which involves a function ut which copies and generates summary output yt\nState it encapsulates the hidden states of encoder and previous outputs\nReward which generates a rough score determining the performance of the summarisation\n\nText summarisation [see 4] is highly critical in extracting important information from large texts.\nIn case of text summarisation there are broadly two categories:\n\nExtractive Summarisation\nAbstractive Summarisation\n\nIn case of Extractive Summarisation words and sentences are scored according to a specific metric and then utilizing that information for summarizing based copying or pasting the most informative parts of the text.\nOn the other hand Abstractive Summarisation involves building a semantic representation of the text and then incorporating natural language generation techniques to generate text highlighting the informative parts of the parent text document.\nHere, we will be focusing on an extractive summarisation method called TextRank which is hinged upon the PageRank algorithm which was developed by Google to rank websites based on their importance.\nThe TextRank Algorithm\nTextRank is a graph-based ranking algorithm for NLP. Graph-based ranking algorithms evaluate the importance of a vertex within a graph, based on global information extracted recursively from the entire graph. When one vertex is associated with another it is actually casting a vote for that vertex. The higher the number of votes cast for a vertex, the higher importance of that vertex.\nIn the NLP case it is necessary to define vertices and edges. In this tutorial we will be using sentences as vertices and words as edges. Thus sentences with words present in many other sentences will have higher priority\n\nlibrary(tidyverse)\nlibrary(tidytext)\nlibrary(textrank)\nlibrary(rvest)\n# define url\nurl <- \"http://time.com/5196761/fitbit-ace-kids-fitness-tracker/\"\n# read in data\narticle <- read_html(url) %>%\n  html_nodes('div[class=\"padded\"]') %>%\n  html_text()\n\nNext the article is loaded into a tibble. Then tokenisation is implemented according to sentences. Although this tokenisation is fully perfect it has a lower number of dependencies and is suitable for this case. Finally we add column for sentence number and switch the order of the columns.\n\narticle_sentences <- tibble(text = article) %>%\n  tidytext::unnest_tokens(sentence, text, token = \"sentences\") %>%\n  dplyr::mutate(sentence_id = row_number()) %>%\n  dplyr::select(sentence_id, sentence)\narticle_sentences\n\n# A tibble: 21 × 2\n   sentence_id sentence                                                         \n         <int> <chr>                                                            \n 1           1 \"fitbit is launching a new fitness tracker designed for children…\n 2           2 \"the [tempo-ecommerce src=”http://www.amazon.com/fitbit-activity…\n 3           3 \"the most important of which is fitbit’s new family account opti…\n 4           4 \"parents must approve who their child can connect with via the f…\n 5           5 \"but while fitbit’s default move goal is 30 minutes for adult us…\n 6           6 \"fitbit says the tracker is designed for children eight years ol…\n 7           7 \"fitbit will also be introducing a family faceoff feature that l…\n 8           8 \"the app also will reward children with in-app badges for achiev…\n 9           9 \"fitbit’s new child-friendly fitness band will be available in b…\n10          10 \"the ace launch is part of fitbit’s broader goal of branching ou…\n# … with 11 more rows\n\n\nNext we will tokenize based on words.\n\narticle_words <- article_sentences %>%\n  tidytext::unnest_tokens(word, sentence)\narticle_words\n\n# A tibble: 587 × 2\n   sentence_id word     \n         <int> <chr>    \n 1           1 fitbit   \n 2           1 is       \n 3           1 launching\n 4           1 a        \n 5           1 new      \n 6           1 fitness  \n 7           1 tracker  \n 8           1 designed \n 9           1 for      \n10           1 children \n# … with 577 more rows\n\n\nWe have one last step left is to remove the stop words in article_words as they are prone to result in redundancy.\n\narticle_words <- article_words %>%\n  dplyr::anti_join(stop_words, by = \"word\")\narticle_words\n\n# A tibble: 297 × 2\n   sentence_id word     \n         <int> <chr>    \n 1           1 fitbit   \n 2           1 launching\n 3           1 fitness  \n 4           1 tracker  \n 5           1 designed \n 6           1 children \n 7           1 called   \n 8           1 fitbit   \n 9           1 ace      \n10           1 sale     \n# … with 287 more rows\n\n\nUsing the textrank package it is really easy to implement the TextRank algorithm. The textrank_sentences function requires only 2 inputs:\n\nA data frame with sentences\nA data frame with tokens which are part of each sentence\n\n\narticle_summary <- textrank_sentences(data = article_sentences, \n                                      terminology = article_words)\n# inspect the summary\narticle_summary\n\nTextrank on sentences, showing top 5 most important sentences found:\n  1. fitbit is launching a new fitness tracker designed for children called the fitbit ace, which will go on sale for $99.95 in the second quarter of this year.\n  2. fitbit says the tracker is designed for children eight years old and up.\n  3. above all else, the ace is an effort to get children up and moving.\n  4. but while fitbit’s default move goal is 30 minutes for adult users, the ace’s will be 60 minutes, in line with the world health organization’s recommendation that children between the ages of five and 17 get an hour of daily physical activity per day.\n  5. the most important of which is fitbit’s new family account option, which gives parents control over how their child uses their tracker and is compliant with the children’s online privacy protection act, or coppa.\n\n\nLets have a look where these important sentences appear in the article:\n\nlibrary(ggplot2)\narticle_summary[[\"sentences\"]] %>%\n  ggplot(aes(textrank_id, textrank, fill = textrank_id)) +\n  geom_col() +\n  theme_minimal() +\n  scale_fill_viridis_c() +\n  guides(fill = \"none\") +\n  labs(x = \"Sentence\",\n       y = \"TextRank score\",\n       title = \"Most informative sentences appear within first half of sentences\",\n       subtitle = 'In article \"Fitbits Newest Fitness Tracker Is Just for Kids\"',\n       caption = \"Source: http://time.com/5196761/fitbit-ace-kids-fitness-tracker/\")\n\n\n\n\nPosition of Important Sentences in the Article"
  },
  {
    "objectID": "reinfnlp.html#other-applications-of-rl",
    "href": "reinfnlp.html#other-applications-of-rl",
    "title": "Reinforcement Learning and Text Summarization in R",
    "section": "Other Applications of RL",
    "text": "Other Applications of RL\n\nDialogue Generation\nIn today’s digital world dialogue generation is a widely used application especially in chatbots. One widely used model in this regard is the Long Short Term Memory (LSTM) sequence-to-sequence (SEQ2SEQ) model. It is a neural generative model that maximizes the probability of generating a response given the previous dialogue. However SEQ2SEQ model has some constraints:\n\nThey tend to generate highly generic responses\nOften they are stuck in an infinite loop of repetitive responses\n\nThis is where deep RL is much more efficient as it can integrate developer-defined rewards which efficiently mimics the true goal of chatbot development. In case of dialogue generation the component:\n\nAction which involves a function that generates sequences of arbitrary lengths\nState it comprises of previous 2 dialogue turns [pi,qi]\nReward which determines the ease of answering, information flow and semantic coherence\n\nThe schematic diagram highlighting the dialogue simulation between 2 agents using deep RL is shown below:\n\n\n\n\n\n\n\nNeural Machine Translation\nMost of Neural Machine Translation (NMT) models are based encoder-decoder framework with attention mechanism. The encoder initially maps a source sentence x={x1,x2,…,xn} to a set of continuous representations z={z1,z2,…,zn} . Given z the decoder then generates a target sentence y={y1,y2,…,ym} of word tokens one by one. RL is used to bridge the gap between training and inference of of NMT by directly optimizing the loss function at training time. In this scenario the NMT model acts as the agent which interacts with the environment which in this case are the previous words and the context vector z available at each step t. This is a a policy based RL and in place of a state a policy will be assigned in every iteration. The critical components of the RL for NMT are discussed below:\n\nPolicy which is a conditional probability defined by the parameters of the agent\nAction is decided by the agent based on the policy and it will pick up a candidate word from the vocabulary\nReward is evaluated once the agent generates a complete sequence which in case of machine translation is Bilingual Evaluation Understudy (BLEU).BLEU is defined by comparing the generated sequence with the ground truth sequence.\n\nThe schematic of the overall process is depicted below:"
  },
  {
    "objectID": "relearn.html",
    "href": "relearn.html",
    "title": "Reinforcement Learning in NLP",
    "section": "",
    "text": "Citation & Session Info\nMajumdar, Dattatreya. 2020. Reinforcement Learning in NLP. Brisbane: The University of Queensland. url: https://slcladal.github.io/reinfnlp.html (Version 2020.11.20).\n@manual{Majumdar2020ta,\n  author = {Majumdar, Dattatreya},\n  title = {Reinforcement Learning in NLP},\n  note = {https://slcladal.github.io/reinfnlp.html},\n  year = {2020},\n  organization = \"The University of Queensland, Australia. School of Languages and Cultures},\n  address = {Brisbane},\n  edition = {2020/11/20}\n}\n\nsessionInfo()\n\nR version 4.2.1 (2022-06-23)\nPlatform: x86_64-pc-linux-gnu (64-bit)\nRunning under: Ubuntu 22.04.1 LTS\n\nMatrix products: default\nBLAS:   /usr/lib/x86_64-linux-gnu/blas/libblas.so.3.10.0\nLAPACK: /usr/lib/x86_64-linux-gnu/lapack/liblapack.so.3.10.0\n\nlocale:\n [1] LC_CTYPE=en_AU.UTF-8       LC_NUMERIC=C              \n [3] LC_TIME=en_AU.UTF-8        LC_COLLATE=en_AU.UTF-8    \n [5] LC_MONETARY=en_AU.UTF-8    LC_MESSAGES=en_AU.UTF-8   \n [7] LC_PAPER=en_AU.UTF-8       LC_NAME=C                 \n [9] LC_ADDRESS=C               LC_TELEPHONE=C            \n[11] LC_MEASUREMENT=en_AU.UTF-8 LC_IDENTIFICATION=C       \n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nloaded via a namespace (and not attached):\n [1] digest_0.6.29     jsonlite_1.8.0    magrittr_2.0.3    evaluate_0.15    \n [5] rlang_1.0.4       stringi_1.7.8     cli_3.3.0         rstudioapi_0.13  \n [9] rmarkdown_2.14    tools_4.2.1       stringr_1.4.0     htmlwidgets_1.5.4\n[13] xfun_0.31         yaml_2.3.5        fastmap_1.1.0     compiler_4.2.1   \n[17] htmltools_0.5.2   knitr_1.39       \n\n\n\nMain page\n\n\n\n\n\n\n\n\n\nReferences\n\n1. Sutton, R.S., Barto, A.G.: Reinforcement learning: An introduction. MIT press (2018).\n\n\n2. Wu, L., Tian, F., Qin, T., Lai, J., Liu, T.-Y.: A study of reinforcement learning for neural machine translation. arXiv preprint arXiv:1808.08866. (2018).\n\n\n3. Paulus, R., Xiong, C., Socher, R.: A deep reinforced model for abstractive summarization. arXiv preprint arXiv:1705.04304. (2017)."
  },
  {
    "objectID": "repro.html#folder-structure",
    "href": "repro.html#folder-structure",
    "title": "Data Management",
    "section": "Folder structure",
    "text": "Folder structure\nThe UQ Library offers the Digital Essentials module Working with Files. This module contains information on storage options, naming conventions, back up options, metadata, and file formats. Some of these issues are dealt with below but the materials provided by the library offer a more extensive introduction into these topics.\n\n\n\n\n\nAll of the ways to organize your folders have different advantages and problems but they all have in common that they rely on a tree-structure - more general folders contain more specialized ones. For example, if you want to find any file with as few clicks as possible, an alphabetical folder structure would be a good solution. Organized in this way, everything that starts with a certain letter will be stored by its initial letter (e.g. everything starting with a t such as travel under T or everything related to your courses under C). However, organizing your data alphabetically is not intuitive and completely unrelated topics will be located in the same folder.\nA more common and intuitive way to organize your data is to separate your data into meaningful aspects of your life such as Work (containing, e.g., teaching and research), Living (including rent, finances, and insurances), and Media (including Movies, Music, and Audiobooks)."
  },
  {
    "objectID": "repro.html#naming-conventions",
    "href": "repro.html#naming-conventions",
    "title": "Data Management",
    "section": "Naming conventions",
    "text": "Naming conventions\nA File Naming Convention (FNC) is a framework or protocol if you like for naming your files in a way that describes what files contain and importantly, how they relate to other files. It is essential prior to collecting data to establish an agreed FNC.\n\n\n\n\n\nFolders (as well as files) should be labeled in a meaningful way. This means that you avoid names like Stuff or Document for folders and doc2 or homework for files.\nNaming files consistently, logically and in a predictable manner will prevent against unorganized files, misplaced or lost data. It could also prevent possible backlogs or project delays. A file naming convention will ensure files are:\n\nEasier to process - All team members won’t have to over think the file naming process\nEasier to facilitate access, retrieval and storage of files\nEasier to browse through files saving time and effort\nHarder to lose!\nCheck for obsolete or duplicate records\n\nThe University of Edinburgh has a comprehensive and easy to follow list (with examples and explanations) of 13 Rules for file naming conventions. You can also use the recommendations of the\nAustralian National Data Services (ANDS) guide of file wrangling. Some of the suggestions are summarized below.\n\n\n\n\n\nLike the different conventions for folders, there are different conventions for files. As a general rule, any special character symbols, such as +, !, “, -, ., ö, ü, ä, %, &, (, ), [, ], &, $, =, ?, ’, #, or / but also including white spaces, should be avoided (while _ is also a special character, it is still common practice to include them in file names for readability). One reason for this is that you will may encounter problems when sharing files if you avoid white spaces and special characters. Also, some software applications automatically replace or collapse white spaces. A common practice to avoid this issue is to capitalize initial letters in file names which allows you avoid white spaces. An example would be TutorialIntroComputerSkills or Tutorial_IntroComputerSkills.\nWhen you want to include time-stamps in file names, the best way to do this is to use the YYYYMMDD format (rather than DDMMYYYY or even D.M.YY). The reason is that if dates are added this way, the files can be easily sorted in ascending or descending order. To elaborate on the examples shown before, we may use TutorialIntroComputerSkills20200413 or Tutorial_IntroComputerSkills_20200413\n\n\nGOING FURTHER\n\nFor Beginners\n\nLet’s look at some easy naming convention for your data files and documents. Any dates are best stored with YYYY-MM-DD. Try to avoid spaces in your file names\n\nFor Intermediates\n\nMake sure you follow 13 Rules for file naming conventions\n\nFor Advanced file namers\n\nDo you have a policy in your team/section/cluster around naming conventions? If not, this is a great way of getting everyone on the same page."
  },
  {
    "objectID": "repro.html#documentation-and-the-bus-factor",
    "href": "repro.html#documentation-and-the-bus-factor",
    "title": "Data Management",
    "section": "Documentation and the Bus Factor",
    "text": "Documentation and the Bus Factor\nDocumentation is the idea of documenting your work so that outsiders (or yourself after a long time) can understand what you did and how you did it. As a general rule, you should document where to find what as if you informed someone else how to find stuff on your computer.\n\n\n\n\n\nDocumentation can include where your results and data are saved but it can also go far beyond this. Documentation does not have to be complex and it can come in different forms, depending on your needs and your work. Documenting what you do can also include photos, word documents with descriptions, or websites that detail how you work.\nThe idea behind documentation is to keep a log of the contents of folders so that you at a later point in time or someone else can continue your work in case you or someone else is run over by a bus (hence the Bus Factor).\nIn fact, documentation is all about changing the Bus Factor - how many people on a project would need to be hit by a bus to make a project fail. Many times, projects can have a bus factor of one. Adding documentation means when someone goes on leave, needs to take leave suddenly or finishes their study, their work is preserved for your project.\nIf you work in a collaborative project, it is especially useful to have a log of where one can find relevant information and who to ask for help with what. Ideally you want to document anything that someone coming on board would need to know. Thus, if you have not created a log for on-boarding, the perfect person to create a log would be the last person who joined the project. Although there is no fixed rule, it is recommendable to store the log either as a ReadMe document or in a ReadMe folder on the top level of the project.\n\n\nGOING FURTHER\n\nFor Beginners\n\nRead this first: How to start Documenting and more by CESSDA ERIC\nStart with documenting in a text file or document- any start is a good start\nHave this document automatically synced to the cloud with your data or keep this in a shared place such as Google docs, Microsoft teams, or Owncloud. If you collaborate on a project and use UQ’s RDM, you should store a copy of your documentation there.\n\nFor Intermediates\n\nOnce you have the basics in place, go into detail on how your workflow goes from your raw data to the finished results. This can be anything from a detailed description of how you analyse your data, over R Notebooks, to downloaded function lists from Virtual Lab.\n\nFor Advanced documentarians\n\nLearn about Git Repositories and wikis."
  },
  {
    "objectID": "repro.html#keep-copies",
    "href": "repro.html#keep-copies",
    "title": "Data Management",
    "section": "Keep copies",
    "text": "Keep copies\nKeeping a copy of all your data (working, raw and completed) in the cloud is incredibly important. This ensures that if you have a computer failure, accidentally delete your data or your data is corrupted, your research is recoverable and restorable.\nThe 3-2-1 backup rule\nTry to have at least three copies of your project that are in different locations. The rule is: keep at least three (3) copies of your data, and store backup-copies on two (2) different storage media , with one (1) of them located offsite. Although this is just a personal preference, I always safe my projects\n\n\n\n\n\n\non my personal notebook\non at least one additional hard drive (that you keep in a secure location)\nin an online repository (for example, UQ’s Research Data Management system (RDM) OneDrive, MyDrive, GitHub, or GitLab)\n\nUsing online repositories ensures that you do not lose any data in case your computer crashes (or in case you spill lemonade over it - don’t ask…) but it comes at the cost that your data can be more accessible to (criminal or other) third parties. Thus, if you are dealing with sensitive data, I suggest to store it on an additional external hard drive and do not keep cloud-based back-ups. If you trust tech companies with your data (or think that they are not interested in stealing your data), cloud-based solutions such as OneDrive, Google’s MyDrive, or Dropbox are ideal and easy to use options (however, UQ’s RDM is a safer option).\nThe UQ library also offers additional information on complying with ARC and NHMRC data management plan requirements and that UQ RDM meets these requirements for sensitive data (see here).\n\n\nGOING FURTHER\n\nFor Beginners\n\nGet your data into UQ’s RDM or Cloud Storage - If you need help, talk to the library or your tech/eResearch/QCIF Support\n\nFor Advanced backupers\n\nBuild a policy for your team or group on where things are stored. Make sure the location of your data is saved in your documentation"
  },
  {
    "objectID": "repro.html#avoid-duplicates",
    "href": "repro.html#avoid-duplicates",
    "title": "Data Management",
    "section": "Avoid duplicates",
    "text": "Avoid duplicates\n\n\n\n\n\nMany of the files on our computers have several duplicates or copies on the same machine. Optimally, each file should only be stored once (on one machine). Thus, to minimize the superfluous files on your computer, we can delete any duplicates of files.\nYou can, of course, do that manually but a better way to do this is to use programs that detect files that are identical in name, file size, and date of creation. One of these programs is the Douplicate Cleaner. A tutorial on how to use it can be found here."
  },
  {
    "objectID": "repro.html#how-to-organize-projects",
    "href": "repro.html#how-to-organize-projects",
    "title": "Data Management",
    "section": "How to organize projects",
    "text": "How to organize projects\nThis section focuses on how to organize your projects and how to use your computer in doing so.\nProject folder design\nHaving a standard folder structure can keep your files neat and tidy and save you time looking for data. It can also help if you are sharing files with colleagues and having a standard place to put working data and documentation.\nStore your projects in a separate folder. For instance, if you are creating a folder for a research project, create the project folder within a separate project folder that is within a research folder. If you are creating a folder for a course, create the course folder within a courses folder within a teaching folder, etc.\n\n\n\n\n\nWhenever you create a folder for a new project, try to have a set of standard folders. For example, when I create research project folders, I always have folders called archive, data, docs, and images. When I create course folders, I always have folders called slides, assignments, exam, studentmaterials, and correspondence. However, you are, of course, free to modify and come up or create your own basic project design. Also, by prefixing the folder names with numbers, you can force your files to be ordered by the steps in your workflow.\n\nHaving different sub folders allows you to avoid having too many files and many different file types in a single folder. Folders with many different files and file types tend to be chaotic and can be confusing. In addition, I have one ReadMe file on the highest level (which only contains folders except for this one single file) in which I describe very briefly what the folder is about and which folders contain which documents as well as some basic information about the folder (e.g. why and when I created it). This ReadMe file is intended both for me as a reminder what this folder is about and what it contains but also for others in case I hand the project over to someone else who continues the project or for someone who takes over my course and needs to use my materials.\n\nIf you work in a team or share files and folders regularly, it makes sense to develop a logical structure for your team, you need to consider the following points:\n\nCheck to make sure there are no pre-existing folder structure agreements\nName folders appropriately and in a meaningful manner. Don’t use staff names and consider using the type of work\nConsistency - make sure you use the agreed structure/hierarchy\nStructure folders hierarchically - start with a limited number of folders for the broader topics, and then create more specific folders within these\nSeparate ongoing and completed work - as you start to create lots of folders and files, it is a good idea to start thinking about separating your older documents from those you are currently working on\nBackup – ensure folders and files are backed up and retrievable in the event of a disaster. UQ like most universities, have safe storage solutions.\nClean up folders and files post project.\n\n\n\nGOING FURTHER\n\nFor Beginners\n\nPick some of your projects and illustrate how you currently organize your files. See if you can devise a better naming convention or note one or two improvements you could make to how you name your files.\nThere are some really good folder template shapes around. Here is one you can download.\n\nFor Advanced Folder designers\n\nCome up with a policy for your team for folder structures. You could create a template and put it in a downloadable location for them to get them started."
  },
  {
    "objectID": "repro.html#how-to-organize-data",
    "href": "repro.html#how-to-organize-data",
    "title": "Data Management",
    "section": "How to organize data",
    "text": "How to organize data\nThis section will elaborate on who to organize and handle (research) data and introduce some basic principles that may help you to keep your data tidy.\nNever delete data!\n\nThe first thing to keep in mind when dealing with data is to keep at least one copy of the original data. The original data should never be deleted but, rather, you should copy the data and delete only sections of the copy while retaining the original data intact.\n\nTips for sensitive data\n\nSensitive data are data that can be used to identify an individual, species, object, or location that introduces a risk of discrimination, harm, or unwanted attention. Major, familiar categories of sensitive data are: personal data - health and medical data - ecological data that may place vulnerable species at risk.\n\nSeparating identifying variables from your data\n\nSeparating or deidentifying your data has the purpose to protect an individual’s privacy. According to the Australian Privacy Act 1988, “personal information is deidentified if the information is no longer about an identifiable individual or an individual who is reasonably identifiable”. Deidentified information is no longer considered personal information and can be shared. More information on the Commonwealth Privacy Act can be located here.\nDeidentifying aims to allow data to be used by others for publishing, sharing and reuse without the possibility of individuals/location being re-identified. It may also be used to protect the location of archaeological findings, cultural data of location of endangered species.\nAny identifiers (name, date of birth, address or geospatial locations etc) should be removed from main data set and replaced with a code/key. The code/key is then preferably encrypted and stored separately. By storing deidentified data in a secure solution, you are meeting safety, controlled, ethical, privacy and funding agency requirements.\nRe-identifying an individual is possible by recombining the deidentifiable data set and the identifiers.\n\nTips for managing deidentification (ARDC)\n\nPlan deidentification early in the research as part of your data management planning\nRetain original unedited versions of data for use within the research team and for preservation\nCreate a deidentification log of all replacements, aggregations or removals made\nStore the log separately from the deidentified data files\nIdentify replacements in text in a meaningful way, e.g. in transcribed interviews indicate replaced text with [brackets] or use XML mark-up tags.\n\nManagement of identifiable data (ARDC)\nData may often need to be identifiable (i.e. contains personal information) during the process of research, e.g. for analysis. If data is identifiable then ethical and privacy requirements can be met through access control and data security. This may take the form of:\n\nControl of access through physical or digital means (e.g. passwords)\nEncryption of data, particularly if it is being moved between locations\nEnsuring data is not stored in an identifiable and unencrypted format when on easily lost items such as USB keys, laptops and external hard drives.\nTaking reasonable actions to prevent the inadvertent disclosure, release or loss of sensitive personal information.\n\n\n\nSafely sharing sensitive data guide (ARDC)\n\n\nANDS’ Deidentification Guide collates a selection of Australian and international practical guidelines and resources on how to deidentify data sets. You can find more information about deidentification here and information about safely sharing sensitive data here.\n\n\n\n\nAustralian practical guidance for Deidentification (ARDC)\n\n\nAustralian Research Data Commons (ARDC) formerly known as Australian National Data Service (ANDS) released a fabulous guide on Deidentification. The Deidentification guide is intended for researchers who own a data set and wish to share safely with fellow researchers or for publishing of data. The guide can be located here.\n\n\n\n\nNationally available guidelines for handling sensitive data\n\n\nThe Australian Government’s Office of the Australian Information Commissioner (OAIC) and CSIRO Data61 have released a Deidentification Decision Making Framework, which is a “practical guide to deidentification, focusing on operational advice”. The guide will assist organisations that handle personal information to deidentify their data effectively.\nThe OAIC also provides high-level guidance on deidentification of data and information, outlining what deidentification is, and how it can be achieved.\nThe Australian Government’s guidelines for the disclosure of health information, includes techniques for making a data set non-identifiable and example case studies.\nAustralian Bureau of Statistics’ National Statistical Service Handbook. Chapter 11 contains a summary of methods to maintain privacy.\nmed.data.edu.au gives information about anonymisation\nThe Office of the Information Commissioner Queensland’s guidance on deidentification techniques can be found here"
  },
  {
    "objectID": "repro.html#data-as-publications",
    "href": "repro.html#data-as-publications",
    "title": "Data Management",
    "section": "Data as publications",
    "text": "Data as publications\nMore recently, regarding data as a form of publications has gain a lot of traction. This has the advantage that it rewards researchers who put a lot of work into compiling data and it has created an incentive for making data available, e.g. for replication. The UQ RDM and UQ eSpace can help with the process of publishing a dataset.\nThere are many platforms where data can be published and made available in a sustainable manner. Below are listed just some options that are recommendable:\n\n\n\n\n\n\n\nUQ Research Data Manager\n\nThe UQ Research Data Manager (RDM) system is a robust, world-leading system designed and developed here at UQ. The UQ RDM provides the UQ research community with a collaborative, safe and secure large-scale storage facility to practice good stewardship of research data. The European Commission report “Turning FAIR into Reality” cites UQ’s RDM as an exemplar of, and approach to, good research data management practice. The disadvantage of RDM is that it is not available to everybody but restricted to UQ staff, affiliates, and collaborators.\n\n\n\nOpen Science Foundation\n\nThe Open Science Foundation (OSF) is a free, global open platform to support your research and enable collaboration.\n\n\n\nTROLLing\n\nTROLLing | DataverseNO (The Tromsø Repository of Language and Linguistics) is a repository of data, code, and other related materials used in linguistic research. The repository is open access, which means that all information is available to everyone. All postings are accompanied by searchable metadata that identify the researchers, the languages and linguistic phenomena involved, the statistical methods applied, and scholarly publications based on the data (where relevant).\n\n\n\nGit\n\nGitHub offers the distributed version control using Git. While GitHub is not designed to host research data, it can be used to share share small collections of research data and make them available to the public. The size restrictions and the fact that GitHub is a commercial enterprise owned by Microsoft are disadvantages of this as well as alternative, but comparable platforms such as GitLab.\n\n\nDigital Object Identifier (DOI) and Persistent identifier (PiD)\nOnce you’ve completed your project, help make your research data discoverable, accessible and possibly re-usable using a PiD such as a DOI! A Digital Object Identifier (DOI) is a unique alphanumeric string assigned by either a publisher, organisation or agency that identifies content and provides a PERSISTENT link to its location on the internet, whether the object is digital or physical. It might look something like this http://dx.doi.org/10.4225/01/4F8E15A1B4D89.\n\n\n\n\n\nDOIs are also considered a type of persistent identifiers (PiDs). An identifier is any label used to name some thing uniquely (whether digital or physical). URLs are an example of an identifier. So are serial numbers, and personal names. A persistent identifier is guaranteed to be managed and kept up to date over a defined time period.\nJournal publishers assign DOIs to electronic copies of individual articles. DOIs can also be assigned by an organisation, research institutes or agencies and are generally managed by the relevant organisation and relevant policies. DOIs not only uniquely identify research data collections, it also supports citation and citation metrics.\nA DOI will also be given to any data set published in UQ eSpace, whether added manually or uploaded from UQ RDM. For information on how cite data, have a look here.\nKey points\n\nDOIs are a persistent identifier and as such carry expectations of curation, persistent access and rich metadata\nDOIs can be created for DATA SETS and associated outputs (e.g. grey literature, workflows, algorithms, software etc) - DOIs for data are equivalent with DOIs for other scholarly publications\nDOIs enable accurate data citation and bibliometrics (both metrics and altmetrics)\nResolvable DOIs provide easy online access to research data for discovery, attribution and reuse\n\n\n\nGOING FURTHER\n\nFor Beginners\n\nEnsure data you associate with a publication has a DOI- your library is the best group to talk to for this.\n\nFor Intermediates\n\nLearn more about how your DOI can potentially increase your citation rates by watching this 4m:51s video\nLearn more about how your DOI can potentially increase your citation rate by reading the ANDS Data Citation Guide\n\nFor Advanced identifiers\n\nLearn more about PiDs and DOIs\nContact the Library for advice on how to obtain a DOI upon project completion.\nHave a look at ANDS/ARDC - Citation and Identifiers\nCheck out the DOI system for research data"
  },
  {
    "objectID": "repro.html#rproj",
    "href": "repro.html#rproj",
    "title": "Data Management",
    "section": "Rproj",
    "text": "Rproj\nIf you’re using RStudio, you have the option of creating a new R project. A project is simply a working directory designated with a .RProj file. When you open a project (using File/Open Project in RStudio or by double–clicking on the .RProj file outside of R), the working directory will automatically be set to the directory that the .RProj file is located in.\n\n\n\n\n\nI recommend creating a new R Project whenever you are starting a new research project. Once you’ve created a new R project, you should immediately create folders in the directory which will contain your R code, data files, notes, and other material relevant to your project (you can do this outside of R on your computer, or in the Files window of RStudio). For example, you could create a folder called R that contains all of your R code, a folder called data that contains all your data (etc.).\nBefore working with Rprojects, I set my working directories with setwd() but this is not optimal because it takes an absolute file path as an input then sets it as the current working directory of the R process. However, this makes scripts break easily and it makes it more difficult to share my analyses and projects with others. Hence, the setwd() approach makes it very difficult to share work and make it transparent and reproducible."
  },
  {
    "objectID": "repro.html#folder-structure-1",
    "href": "repro.html#folder-structure-1",
    "title": "Data Management",
    "section": "Folder structure",
    "text": "Folder structure\nLike with all (project) folders, it is recommendable to use a common folder structure as to make it easier to find information or particular files."
  },
  {
    "objectID": "repro.html#solving-dependency-issues-renv",
    "href": "repro.html#solving-dependency-issues-renv",
    "title": "Data Management",
    "section": "Solving dependency issues: renv",
    "text": "Solving dependency issues: renv\n\n\n\n\n\nThe renv package is a new way to make Rprojects independent and thus to remove outside dependencies from Rprojects. The renv package creates a new library within your project so that your Rproject is independent of your personal library and when you share your project, you automatically also share the packages that you have used. The idea behind renv is to be a robust, stable replacement for the packrat package which was rather unsatisfactory to work with (speaking from experience here).\nI have used renv and, while it took some time to generate the local library on first use, it was very easy to use, did not cause any issues and is overall very recommendable to get rid of outside dependencies and thus make Rprojects easier to share for transparency and reproducibility.\nUnderlying the philosophy of renv is that all existing work flows should just work as they did before – renv helps to isolate your project’s R dependencies (like package versioning).\nYou can get more information about renv and how it works, as well as how you can use it here."
  },
  {
    "objectID": "repro.html#version-control-with-git",
    "href": "repro.html#version-control-with-git",
    "title": "Data Management",
    "section": "Version Control with Git",
    "text": "Version Control with Git\n\n\n\n\n\nGetting started with Git\nTo connect your Rproject with GitHub, you need to have Git installed (if you have not yet downloaded and installed Git, simply search for download git in your favorite search engine and follow the instructions) and you need to have a GitHub account. If you do not have a GitHub account, here is a video tutorial showing how you can do this. If you have trouble with this, you can also check out Happy Git and GitHub for the useR at happygitwithr.com.\n\n\n\n\n\nJust as a word of warning: when I set up my connection to Git and GitHUb things worked very differently, so things may be a bit different on your machine. In any case, I highly recommend this YouTube tutorial which shows how to connect to Git and GitHub using the usethis package or this, slightly older, YouTube tutorial on how to get going with Git and GitHub when working with RStudio.\nOld school\nWhile many people use the usethis package to connect RStudio to GitHub, I still use a somewhat old school way to connect my projects with GitHub. I have decided to show you how to connect RStudio to GitHub using this method, as I actually think it is easier once you have installed Git and created a gitHub account.\nBefore you can use Git with R, you need to tell RStudio that you want to use Git. To do this, go to Tools, then Global Options and then to Git/SVN and make sure that the box labeled Enable version control interface for RStudio projects. is checked. You need to then browse to the Git executable file (for Window’s users this is called the Git.exe file).\n\n\n\n\n\n\n\nNow, we can connect our project to Git (not to GitHub yet). To do this, go to Tools, then to Project Options... and in the Git/SVN tab, change Version Control System to Git (from None). Once you have accepted these changes, a new tab called Git appears in the upper right pane of RStudio (you may need to / be asked to restart RStudio at this point). You can now commit files and changes in files to Git.\n\n\n\n\n\nTo commit files, go to the Git tab and check the boxes next to the files you would like to commit (this is called staging; meaning that these files are now ready to be committed). Then, click on Commit and enter a message in the pop-up window that appears. Finally, click on the commit button under the message window.\nConnecting your Rproj with GitHub\nTo connect your Rproject and GitHub, we go to our GitHub page and create a new GitHub repository (repo) that we call test (or whatever you want to call your repository). To create a new repository on GitHub, simply click on New Repository after you have clicked on the New icon. I recommend that you check Add a Readme in which you can describe what the repo contains, but you do not have to do this.\nOnce you have created a new GitHub repo, we need to connect this repo to our computer. To do this, we need to clone the repo which we do by clicking on the green Code icon. When you click on the green Code icon, a dropdown menu appears and you copy the url in the section clone with HTTPS.\nThen, we go to the Terminal (in-between Console and Jobs) and we include the path we got from the git repository after the command git remote add origin. We then use the command git branch -M main (to merge a master and a main branch) and then, finally, push our files into the remote GitHub repo by using the command git push -u origin main.\n\n# initiate the upstream tracking of the project on the GitHub repo\ngit remote add origin https://github.com/YourGitHUbUserName/YouGitHubRepositoryName.git\n# set main as main branch (rather than master)\ngit branch -M main\n# push content to main\ngit push -u origin main\n\nWe can then commit changes and push them to the remote GitHub repo.\nYou can then go to your GitHub repo and check if the documents that we pushed are now in the remote repo.\nFrom now on, you can simply commit all changes that you make to the GitHub repo associated with that Rproject. Other projects can, of course, be connected and push to other GitHub repos."
  },
  {
    "objectID": "repro.html#solving-path-issues-here",
    "href": "repro.html#solving-path-issues-here",
    "title": "Data Management",
    "section": "Solving path issues: here",
    "text": "Solving path issues: here\nThe goal of the here package is to enable easy file referencing in project-oriented workflows. In contrast to using setwd(), which is fragile and dependent on the way you organize your files, here uses the top-level directory of a project to easily build paths to files.\nThis makes your projects more robust as the paths will still work if you put your project into another location or on another computer. Also, moving between Mac and Windows (which would require different kind of path specifications) is no longer an issue with the here package.\n\n# define path\nexample_path_full <- \"D:\\\\Uni\\\\Konferenzen\\\\ISLE\\\\ISLE_2021\\\\isle6_reprows/repro.Rmd\"\n# show path\nexample_path_full\n\n[1] \"D:\\\\Uni\\\\Konferenzen\\\\ISLE\\\\ISLE_2021\\\\isle6_reprows/repro.Rmd\"\n\n\nWith the here package, the path starts in folder where the Rproj file is. As the Rmd file is in the same folder, we only need to specify the Rmd file and the here package will add the rest.\n\n# load package\nlibrary(here)\n# define path using here\nexample_path_here <- here::here(\"repro.Rmd\")\n#show path\nexample_path_here\n\n[1] \"/home/sam/programming/SLCLADAL.github.io/repro.Rmd\""
  },
  {
    "objectID": "repro.html#reproducible-randomness-set.seed",
    "href": "repro.html#reproducible-randomness-set.seed",
    "title": "Data Management",
    "section": "Reproducible randomness: set.seed",
    "text": "Reproducible randomness: set.seed\nThe set.seed function in R sets the seed of R‘s random number generator, which is useful for creating simulations or random objects that can be reproduced. This means that when you call a function that uses some form of randomness (e.g. when using random forests), using the set.seed function allows you to replicate results.\nBelow is an example of what I mean. First, we generate a random sample from a vector.\n\nnumbers <- 1:10\nrandomnumbers1 <- sample(numbers, 10)\nrandomnumbers1\n\n [1]  6  8  3  2  7  9  5  4 10  1\n\n\nWe now draw another random sample using the same sample call.\n\nrandomnumbers2 <- sample(numbers, 10)\nrandomnumbers2\n\n [1]  1 10  6  7  2  3  8  4  5  9\n\n\nAs you can see, we now have a different string of numbers although we used the same call. However, when we set the seed and then generate a string of numbers as show below, we create a reproducible random sample.\n\nset.seed(123)\nrandomnumbers3 <- sample(numbers, 10)\nrandomnumbers3\n\n [1]  3 10  2  8  6  9  1  7  5  4\n\n\nTo show that we can reproduce this sample, we call the same seed and then generate another random sample which will be the same as the previous one because we have set the seed.\n\nset.seed(123)\nrandomnumbers4 <- sample(numbers, 10)\nrandomnumbers4\n\n [1]  3 10  2  8  6  9  1  7  5  4"
  },
  {
    "objectID": "repro.html#r-markdown",
    "href": "repro.html#r-markdown",
    "title": "Data Management",
    "section": "R Markdown",
    "text": "R Markdown\n\n\n\n\n\nR Markdown provides an authoring framework for data science. You can use a single R Markdown file to both\n\nsave and execute code\ngenerate high quality reports that can be shared with an audience\n\nR Markdown documents are fully reproducible and support dozens of static and dynamic output formats. This 1-minute video provides a quick tour of what’s possible with R Markdown:\nCheck out this introduction to R Markdown by RStudio and have a look at this R Markdown cheat sheet."
  },
  {
    "objectID": "repro.html#tidy-data-principles",
    "href": "repro.html#tidy-data-principles",
    "title": "Data Management",
    "section": "Tidy data principles",
    "text": "Tidy data principles\nThe same (underlying) data can be represented in multiple ways. The following three tables are show the same data but in different ways.\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 1.\n \n  \n    country \n    continent \n    2002 \n    2007 \n  \n \n\n  \n    Afghanistan \n    Asia \n    42.129 \n    43.828 \n  \n  \n    Australia \n    Oceania \n    80.370 \n    81.235 \n  \n  \n    China \n    Asia \n    72.028 \n    72.961 \n  \n  \n    Germany \n    Europe \n    78.670 \n    79.406 \n  \n  \n    Tanzania \n    Africa \n    49.651 \n    52.517 \n  \n\n\n\n\n\n\n\n\n\n\n\n\nTable 2.\n \n  \n    year \n    Afghanistan (Asia) \n    Australia (Oceania) \n    China (Asia) \n    Germany (Europe) \n    Tanzania (Africa) \n  \n \n\n  \n    2002 \n    42.129 \n    80.370 \n    72.028 \n    78.670 \n    49.651 \n  \n  \n    2007 \n    43.828 \n    81.235 \n    72.961 \n    79.406 \n    52.517 \n  \n\n\n\n\n\n\n\n\n\n\n\n\nTable 3.\n \n  \n    country \n    year \n    continent \n    lifeExp \n  \n \n\n  \n    Afghanistan \n    2002 \n    Asia \n    42.129 \n  \n  \n    Afghanistan \n    2007 \n    Asia \n    43.828 \n  \n  \n    Australia \n    2002 \n    Oceania \n    80.370 \n  \n  \n    Australia \n    2007 \n    Oceania \n    81.235 \n  \n  \n    China \n    2002 \n    Asia \n    72.028 \n  \n  \n    China \n    2007 \n    Asia \n    72.961 \n  \n  \n    Germany \n    2002 \n    Europe \n    78.670 \n  \n  \n    Germany \n    2007 \n    Europe \n    79.406 \n  \n  \n    Tanzania \n    2002 \n    Africa \n    49.651 \n  \n  \n    Tanzania \n    2007 \n    Africa \n    52.517 \n  \n\n\n\n\n\nTable 3 should be the easiest to parse and understand. This is so because only Table 3 is tidy. Unfortunately, however, most data that you will encounter will be untidy. There are two main reasons:\n\nMost people aren’t familiar with the principles of tidy data, and it’s hard to derive them yourself unless you spend a lot of time working with data.\nData is often organised to facilitate some use other than analysis. For example, data is often organised to make entry as easy as possible.\n\nThis means that for most real analyses, you’ll need to do some tidying. The first step is always to figure out what the variables and observations are. Sometimes this is easy; other times you’ll need to consult with the people who originally generated the data. The second step is to resolve one of two common problems:\n\nOne variable might be spread across multiple columns.\nOne observation might be scattered across multiple rows.\n\nTo avoid structuring in ways that make it harder to parse, there are three interrelated principles which make a data set tidy:\n\nEach variable must have its own column.\nEach observation must have its own row.\nEach value must have its own cell.\n\nAn additional advantage of tidy data is that is can be transformed more easily into any other format when needed."
  },
  {
    "objectID": "repro.html#how-to-minimize-storage-space",
    "href": "repro.html#how-to-minimize-storage-space",
    "title": "Data Management",
    "section": "How to minimize storage space",
    "text": "How to minimize storage space\nMost people use or rely on data that comes in spreadsheets and use software such as Microsoft Excel or OpenOffice Calc. However, spreadsheets produced by these software applications take up a lot of storage space.\nOne way to minimize the space, that your data takes up, is to copy the data and paste it into a simple editor or txt-file. The good thing about txt files is that they take up only very little space and they can be viewed easily so that you can open the file to see what the data looks like. You could then delete the spread sheet because you can copy and paste the content of the txt file right back into a spread sheet when you need it.\nIf you work with R, you may also consider to save your data as .rda files which is a very efficient way to save and story data in an R environment\nBelow is an example for how you can load, process, and save your data as .rda in RStudio.\n\n# load data\nlmm <- read.delim(\"https://slcladal.github.io/data/lmmdata.txt\", header = TRUE)\n# convert strings to factors\nlmm <- lmm %>%\n  mutate(Genre = factor(Genre),\n         Text = factor(Text),\n         Region = factor(Region))\n# save data\nbase::saveRDS(lmm, file = here::here(\"data\", \"lmm_out.rda\"))\n# remove lmm object\nrm(lmm)\n# load .rda data\nlmm  <- base::readRDS(file = here::here(\"data\", \"lmm_out.rda\"))\n# or from web\nlmm  <- base::readRDS(url(\"https://slcladal.github.io/data/lmm.rda\", \"rb\"))\n# inspect data\nstr(lmm)\n\n'data.frame':   537 obs. of  5 variables:\n $ Date        : int  1736 1711 1808 1878 1743 1908 1906 1897 1785 1776 ...\n $ Genre       : Factor w/ 16 levels \"Bible\",\"Biography\",..: 13 4 10 4 4 4 3 9 9 3 ...\n $ Text        : Factor w/ 271 levels \"abott\",\"albin\",..: 2 6 12 16 17 20 20 24 26 27 ...\n $ Prepositions: num  166 140 131 151 146 ...\n $ Region      : Factor w/ 2 levels \"North\",\"South\": 1 1 1 1 1 1 1 1 1 1 ...\n\n\nTo compare, the lmmdata.txt requires 19.2KB while the lmmdata.rda only requires 5.2KB (and only 4.1KB with xz compression). If stored as an Excel spreadsheet, the same file requires 28.6KB."
  },
  {
    "objectID": "sentiment.html#preparation-and-session-set-up",
    "href": "sentiment.html#preparation-and-session-set-up",
    "title": "Sentiment Analysis in R",
    "section": "Preparation and session set up",
    "text": "Preparation and session set up\nThis tutorial is based on R. If you have not installed R or are new to it, you will find an introduction to and more information how to use R here. For this tutorials, we need to install certain packages from an R library so that the scripts shown below are executed without errors. Before turning to the code below, please install the packages by running the code below this paragraph. If you have already installed the packages mentioned below, then you can skip ahead and ignore this section. To install the necessary packages, simply run the following code - it may take some time (between 1 and 5 minutes to install all of the packages so you do not need to worry if it takes some time).\n\n# install packages\ninstall.packages(\"tidyverse\")\ninstall.packages(\"readr\")\ninstall.packages(\"tidytext\")\ninstall.packages(\"textdata\")\ninstall.packages(\"zoo\")\ninstall.packages(\"Hmisc\")\ninstall.packages(\"sentimentr\")\ninstall.packages(\"flextable\")\n# install klippy for copy-to-clipboard button in code chunks\ninstall.packages(\"remotes\")\nremotes::install_github(\"rlesur/klippy\")\n\nNow that we have installed the packages, we activate them as shown below.\n\n# set options\noptions(stringsAsFactors = F)          # no automatic data transformation\noptions(\"scipen\" = 100, \"digits\" = 12) # suppress math annotation\n# activate packages\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(readr)\nlibrary(tidytext)\nlibrary(textdata)\nlibrary(zoo)\nlibrary(Hmisc)\nlibrary(sentimentr)\nlibrary(flextable)\n# activate klippy for copy-to-clipboard button\nklippy::klippy()\n\n\n\n\nOnce you have installed R and RStudio and initiated the session by executing the code shown above, you are good to go."
  },
  {
    "objectID": "sentiment.html#getting-started",
    "href": "sentiment.html#getting-started",
    "title": "Sentiment Analysis in R",
    "section": "Getting started",
    "text": "Getting started\nIn the following, we will perform a SA to investigate the emotionality of five different novels. We will start with the first example and load five pieces of literature.\n\ndarwin <- base::readRDS(url(\"https://slcladal.github.io/data/origindarwin.rda\", \"rb\"))\ntwain <- base::readRDS(url(\"https://slcladal.github.io/data/twainhuckfinn.rda\", \"rb\"))\norwell <- base::readRDS(url(\"https://slcladal.github.io/data/orwell.rda\", \"rb\"))\nlovecraft <- base::readRDS(url(\"https://slcladal.github.io/data/lovecraftcolor.rda\", \"rb\"))\n\n\n\n\n\n\nFirst 10 lines of the darwin data.\n\n\n.THE ORIGIN OF SPECIES BY CHARLES DARWIN AN HISTORICAL SKETCH OF THE PROGRESS OF OPINION ON THE ORIGIN OF SPECIES INTRODUCTION When on board H.M.S. 'Beagle,' as naturalist, I was much struck with certain facts in the distribution of the organic beings in- habiting South America, and in the geological relations of the \n\n\nWrite function to clean data\n\ntxtclean <- function(x, title){\n  require(dplyr)\n  require(stringr)\n  x <- x %>%\n    iconv(to = \"UTF-8\") %>%\n    base::tolower() %>%\n    paste0(collapse = \" \") %>%\n    stringr::str_squish()%>%\n    stringr::str_split(\" \") %>%\n    unlist() %>%\n    tibble() %>%\n    dplyr::select(word = 1, everything()) %>%\n    dplyr::mutate(novel = title) %>%\n    dplyr::anti_join(stop_words) %>%\n    dplyr::mutate(word = str_remove_all(word, \"\\\\W\")) %>%\n    dplyr::filter(word != \"\")\n}\n\nProcess and clean texts.\n\n# process text data\ndarwin_clean <- txtclean(darwin, \"darwin\")\nlovecraft_clean <- txtclean(lovecraft, \"lovecraft\")\norwell_clean <- txtclean(orwell, \"orwell\")\ntwain_clean <- txtclean(twain, \"twain\")\n\n\n\n\n\n\nFirst 10 lines of the cleaned darwin data.\n\n\nwordnovelorigindarwinspeciesdarwincharlesdarwindarwindarwinhistoricaldarwinsketchdarwinprogressdarwinopiniondarwinorigindarwinspeciesdarwin"
  },
  {
    "objectID": "sentiment.html#binning",
    "href": "sentiment.html#binning",
    "title": "Sentiment Analysis in R",
    "section": "Binning",
    "text": "Binning\nThe following code chunk uses binning to determine the polarity and subsequently displaying changes in polarity across the development of the novels’ plots.\n\nnovels_bin <- novels_anno %>%\n  dplyr::group_by(novel) %>%\n  dplyr::filter(is.na(sentiment) | sentiment == \"negative\" | sentiment == \"positive\") %>%\n  dplyr::mutate(sentiment = as.character(sentiment),\n         sentiment = case_when(is.na(sentiment) ~ \"0\", \n                               TRUE ~ sentiment),\n         sentiment= case_when(sentiment == \"0\" ~ 0,\n                              sentiment == \"positive\" ~ 1,\n                              TRUE ~ -1),\n         id = 1:n(),\n         index = as.numeric(cut2(id, m=100))) %>%\n  dplyr::group_by(novel, index) %>%\n  dplyr::summarize(index = unique(index),\n            polarity = mean(sentiment))\n\n\n\n\n\n\nFirst 10 lines of the novels_bin data.\n\n\nnovelindexpolaritydarwin10.039603960396darwin20.120000000000darwin30.110000000000darwin40.090000000000darwin50.010000000000darwin60.110000000000darwin70.030000000000darwin80.080000000000darwin90.040000000000darwin100.010000000000\n\n\nWe now have an average polarity for each bin and can plot this polarity over the development of the story.\n\nggplot(novels_bin, aes(index, polarity)) + \n  facet_wrap(vars(novel), scales=\"free_x\") +\n  geom_smooth(se = F, col = \"black\") + \n  theme_bw() +\n  labs(y = \"polarity ratio (mean by bin)\",\n       x = \"index (bin)\")"
  },
  {
    "objectID": "sentiment.html#moving-average",
    "href": "sentiment.html#moving-average",
    "title": "Sentiment Analysis in R",
    "section": "Moving average",
    "text": "Moving average\nAnother method for tracking changes in polarity over time is to calculate rolling or moving means. It should be noted thought that rolling means are not an optimal method for tracking changes over time and rather represent a method for smoothing chaotic time-series data. However, they can be used to complement the analysis of changes that are detected by binning.\nTo calculate moving averages, we will assign words with positive polarity a value +1 and words with negative polarity a value of -1 (neutral words are coded as 0). A rolling mean calculates the mean over a fixed window span. Once the initial mean is calculated, the window is shifted to the next position and the mean is calculated for that window of values, and so on. We set the window size to 100 words which represents an arbitrary value.\n\nnovels_change <- novels_anno %>%\n  dplyr::filter(is.na(sentiment) | sentiment == \"negative\" | sentiment == \"positive\") %>%\n  dplyr::group_by(novel) %>%\n  dplyr::mutate(sentiment = as.character(sentiment),\n         sentiment = case_when(is.na(sentiment) ~ \"0\", \n                               TRUE ~ sentiment),\n         sentiment= case_when(sentiment == \"0\" ~ 0,\n                              sentiment == \"positive\" ~ 1,\n                              TRUE ~ -1),\n         id = 1:n()) %>%\n  dplyr::summarise(id = id,\n                 rmean=rollapply(sentiment, 100, mean, align='right', fill=NA)) %>%\n  na.omit()\n\n\n\n\n\n\nFirst 10 lines of the novels_change data.\n\n\nnovelidrmeandarwin1000.04darwin1010.04darwin1020.04darwin1030.04darwin1040.04darwin1050.04darwin1060.04darwin1070.04darwin1080.04darwin1090.04\n\n\nWe will now display the values of the rolling mean to check if three are notable trends in how the polarity shifts over the course of the unfolding of the story within George Orwell’s Nineteen Eighty-Four.\n\nggplot(novels_change, aes(id, rmean)) +    \n  facet_wrap(vars(novel), scales=\"free_x\") +\n  geom_smooth(se = F, col = \"black\") + \n  theme_bw() +\n  labs(y = \"polarity ratio (rolling mean, k = 100)\",\n       x = \"index (word in monograph)\")\n\n\n\n\nThe difference between the rolling mean and the binning is quite notable and results from the fact, that rolling means represent a smoothing method rather than a method to track changes over time."
  },
  {
    "objectID": "string.html",
    "href": "string.html",
    "title": "String Processing in R",
    "section": "",
    "text": "Introduction\n\n\n\n\n\nThis tutorial introduces string processing and it is aimed at beginners and intermediate users of R with the aim of showcasing how to work with and process textual data using R. The aim is not to provide a fully-fledged analysis but rather to show and exemplify selected useful functions and methods associated with text processing.\n\n\n\nThe entire R Notebook for the tutorial can be downloaded here. If you want to render the R Notebook on your machine, i.e. knitting the document to html or a pdf, you need to make sure that you have R and RStudio installed and you also need to download the bibliography file and store it in the same folder where you store the Rmd file.  Here is a link to an interactive version of this tutorial on Google Colab. The interactive tutorial is based on a Jupyter notebook of this tutorial. This interactive Jupyter notebook allows you to execute code yourself and - if you copy the Jupyter notebook - you can also change and edit the notebook, e.g. you can change code and upload your own data.\n\n\n\n\n\n\nPreparation and session set up\nThis tutorial is based on R. If you have not installed R or are new to it, you will find an introduction to and more information how to use R here. For this tutorials, we need to install certain packages from an R library so that the scripts shown below are executed without errors. Before turning to the code below, please install the packages by running the code below this paragraph. If you have already installed the packages mentioned below, then you can skip ahead ignore this section. To install the necessary packages, simply run the following code - it may take some time (between 1 and 5 minutes to install all of the packages so you do not need to worry if it takes some time).\n\n# install packages\ninstall.packages(\"tidyverse\")\ninstall.packages(\"htmlwidgets\")\n# install klippy for copy-to-clipboard button in code chunks\ninstall.packages(\"remotes\")\nremotes::install_github(\"rlesur/klippy\")\n\nNow that we have installed the packages, we can activate them as shown below.\n\n# load packages for website\nlibrary(tidyverse)\n# activate klippy for copy-to-clipboard button\nklippy::klippy()\n\n\n\n\nOnce you have installed RStudio and initiated the session by executing the code shown above, you are good to go.\nBefore we start with string processing, we will load some example texts on which we will perform the processing.\n\n# read in text\nexampletext  <- base::readRDS(url(\"https://slcladal.github.io/data/tx1.rda\", \"rb\"))\n# inspect\nexampletext\n\n[1] \"Grammar is a system of rules which governs the production and use of utterances in a given language. These rules apply to sound as well as meaning, and include componential subsets of rules, such as those pertaining to phonology (the organisation of phonetic sound systems), morphology (the formation and composition of words), and syntax (the formation and composition of phrases and sentences). Many modern theories that deal with the principles of grammar are based on Noam Chomsky's framework of generative linguistics.\"\n\n\nThis first example text represents a paragraph about grammar.\n\n# read in text\nsplitexampletext  <- base::readRDS(url(\"https://slcladal.github.io/data/tx2.rda\", \"rb\"))\n# inspect\nsplitexampletext\n\n[1] \"Grammar is a system of rules which governs the production and use of utterances in a given language.\"                                                                                                                                                                                                   \n[2] \"These rules apply to sound as well as meaning, and include componential subsets of rules, such as those pertaining to phonology (the organisation of phonetic sound systems), morphology (the formation and composition of words), and syntax (the formation and composition of phrases and sentences).\"\n[3] \"Many modern theories that deal with the principles of grammar are based on Noam Chomsky's framework of generative linguistics.\"                                                                                                                                                                         \n\n\nThis second example text represents the same paragraph about grammar, but split into individual sentences.\n\nadditionaltext  <- base::readRDS(url(\"https://slcladal.github.io/data/tx3.rda\", \"rb\"))\n# inspect\nadditionaltext\n\n[1] \"In the early 20th century, Ferdinand de Saussure distinguished between the notions of langue and parole in his formulation of structural linguistics. According to him, parole is the specific utterance of speech, whereas langue refers to an abstract phenomenon that theoretically defines the principles and system of rules that govern a language. This distinction resembles the one made by Noam Chomsky between competence and performance in his theory of transformative or generative grammar. According to Chomsky, competence is an individual's innate capacity and potential for language (like in Saussure's langue), while performance is the specific way in which it is used by individuals, groups, and communities (i.e., parole, in Saussurean terms). \"\n\n\nThe third example text represents a paragraph about Ferdinand de Saussure - the founder of modern linguistics.\n\nsentences  <- base::readRDS(url(\"https://slcladal.github.io/data/tx4.rda\", \"rb\"))\n# inspect\nsentences\n\n[1] \"This is a first sentence.\"     \"This is a second sentence.\"   \n[3] \"And this is a third sentence.\"\n\n\nThe third example text consist of 3 short plain sentences.\nIn the following, we will perform various operations on the example texts.\n\n\nBasic String Processing\nBefore turning to functions provided in the stringr, let us just briefly focus on some base functions that are extremely useful when working with texts.\nA very useful function is, e.g. tolower which converts everything to lower case.\n\ntolower(exampletext)\n\n[1] \"grammar is a system of rules which governs the production and use of utterances in a given language. these rules apply to sound as well as meaning, and include componential subsets of rules, such as those pertaining to phonology (the organisation of phonetic sound systems), morphology (the formation and composition of words), and syntax (the formation and composition of phrases and sentences). many modern theories that deal with the principles of grammar are based on noam chomsky's framework of generative linguistics.\"\n\n\nConversely, toupper converts everything to upper case.\n\ntoupper(exampletext)\n\n[1] \"GRAMMAR IS A SYSTEM OF RULES WHICH GOVERNS THE PRODUCTION AND USE OF UTTERANCES IN A GIVEN LANGUAGE. THESE RULES APPLY TO SOUND AS WELL AS MEANING, AND INCLUDE COMPONENTIAL SUBSETS OF RULES, SUCH AS THOSE PERTAINING TO PHONOLOGY (THE ORGANISATION OF PHONETIC SOUND SYSTEMS), MORPHOLOGY (THE FORMATION AND COMPOSITION OF WORDS), AND SYNTAX (THE FORMATION AND COMPOSITION OF PHRASES AND SENTENCES). MANY MODERN THEORIES THAT DEAL WITH THE PRINCIPLES OF GRAMMAR ARE BASED ON NOAM CHOMSKY'S FRAMEWORK OF GENERATIVE LINGUISTICS.\"\n\n\nThe stringr package (see here is part of the so-called tidyverse - a collection of packages that allows to write R code in a readable manner - and it is the most widely used package for string processing in . The advantage of using stringr is that it makes string processing very easy. All stringr functions share a common structure:\nstr_function(string, pattern)\nThe two arguments in the structure of stringr functions are: string which is the character string to be processed and a pattern which is either a simple sequence of characters, a regular expression, or a combination of both. Because the string comes first, the stringr functions are ideal for piping and thus use in tidyverse style R.\nAll function names of stringr begin with str, then an underscore and then the name of the action to be performed. For example, to replace the first occurrence of a pattern in a string, we should use str_replace(). In the following, we will use stringr functions to perform various operations on the example text. As we have already loaded the tidyverse package, we can start right away with using stringr functions as shown below.\nLike nchar in base, str_count provides the number of characters of a text.\n\nstr_count(splitexampletext)\n\n[1] 100 295 126\n\n\nThe function str_detect informs about whether a pattern is present in a text and outputs a logical vector with TRUE if the pattern occurs and FALSE if it does not.\n\nstr_detect(splitexampletext, \"and\")\n\n[1]  TRUE  TRUE FALSE\n\n\nThe function str_extract extracts the first occurrence of a pattern, if that pattern is present in a text.\n\nstr_extract(exampletext, \"and\")\n\n[1] \"and\"\n\n\nThe function str_extract_all extracts all occurrences of a pattern, if that pattern is present in a text.\n\nstr_extract_all(exampletext, \"and\")\n\n[[1]]\n[1] \"and\" \"and\" \"and\" \"and\" \"and\" \"and\"\n\n\nThe function str_locate provides the start and end position of the match of the pattern in a text.\n\nstr_locate(exampletext, \"and\") \n\n     start end\n[1,]    59  61\n\n\nThe function str_locate_all provides the start and end positions of the match of the pattern in a text and displays the result in matrix-form.\n\nstr_locate_all(exampletext, \"and\")\n\n[[1]]\n     start end\n[1,]    59  61\n[2,]   149 151\n[3,]   302 304\n[4,]   329 331\n[5,]   355 357\n[6,]   382 384\n\n\nThe function str_match extracts the first occurrence of the pattern in a text.\n\nstr_match(exampletext, \"and\") \n\n     [,1] \n[1,] \"and\"\n\n\nThe function str_match_all extracts the all occurrences of the pattern from a text.\n\nstr_match_all(exampletext, \"and\")\n\n[[1]]\n     [,1] \n[1,] \"and\"\n[2,] \"and\"\n[3,] \"and\"\n[4,] \"and\"\n[5,] \"and\"\n[6,] \"and\"\n\n\nThe function str_remove removes the first occurrence of a pattern in a text.\n\nstr_remove(exampletext, \"and\") \n\n[1] \"Grammar is a system of rules which governs the production  use of utterances in a given language. These rules apply to sound as well as meaning, and include componential subsets of rules, such as those pertaining to phonology (the organisation of phonetic sound systems), morphology (the formation and composition of words), and syntax (the formation and composition of phrases and sentences). Many modern theories that deal with the principles of grammar are based on Noam Chomsky's framework of generative linguistics.\"\n\n\nThe function str_remove_all removes all occurrences of a pattern from a text.\n\nstr_remove_all(exampletext, \"and\")\n\n[1] \"Grammar is a system of rules which governs the production  use of utterances in a given language. These rules apply to sound as well as meaning,  include componential subsets of rules, such as those pertaining to phonology (the organisation of phonetic sound systems), morphology (the formation  composition of words),  syntax (the formation  composition of phrases  sentences). Many modern theories that deal with the principles of grammar are based on Noam Chomsky's framework of generative linguistics.\"\n\n\nThe function str_replace replaces the first occurrence of a pattern with something else in a text.\n\nstr_replace(exampletext, \"and\", \"AND\")\n\n[1] \"Grammar is a system of rules which governs the production AND use of utterances in a given language. These rules apply to sound as well as meaning, and include componential subsets of rules, such as those pertaining to phonology (the organisation of phonetic sound systems), morphology (the formation and composition of words), and syntax (the formation and composition of phrases and sentences). Many modern theories that deal with the principles of grammar are based on Noam Chomsky's framework of generative linguistics.\"\n\n\nThe function str_replace_all replaces all occurrences of a pattern with something else in a text.\n\nstr_replace_all(exampletext, \"and\", \"AND\")\n\n[1] \"Grammar is a system of rules which governs the production AND use of utterances in a given language. These rules apply to sound as well as meaning, AND include componential subsets of rules, such as those pertaining to phonology (the organisation of phonetic sound systems), morphology (the formation AND composition of words), AND syntax (the formation AND composition of phrases AND sentences). Many modern theories that deal with the principles of grammar are based on Noam Chomsky's framework of generative linguistics.\"\n\n\nThe function str_starts tests whether a given text begins with a certain pattern and outputs a logical vector.\n\nstr_starts(exampletext, \"and\") \n\n[1] FALSE\n\n\nThe function str_ends tests whether a text ends with a certain pattern and outputs a logical vector.\n\nstr_ends(exampletext, \"and\")\n\n[1] FALSE\n\n\nLike strsplit, the function str_split splits a text when a given pattern occurs. If no pattern is provided, then the text is split into individual symbols.\n\nstr_split(exampletext, \"and\") \n\n[[1]]\n[1] \"Grammar is a system of rules which governs the production \"                                                                                            \n[2] \" use of utterances in a given language. These rules apply to sound as well as meaning, \"                                                               \n[3] \" include componential subsets of rules, such as those pertaining to phonology (the organisation of phonetic sound systems), morphology (the formation \"\n[4] \" composition of words), \"                                                                                                                              \n[5] \" syntax (the formation \"                                                                                                                               \n[6] \" composition of phrases \"                                                                                                                              \n[7] \" sentences). Many modern theories that deal with the principles of grammar are based on Noam Chomsky's framework of generative linguistics.\"           \n\n\nThe function str_split_fixed splits a text when a given pattern occurs but only so often as is indicated by the argument n. So, even if the patter occur more often than n, str_split_fixed will only split the text n times.\n\nstr_split_fixed(exampletext, \"and\", n = 3)\n\n     [,1]                                                        \n[1,] \"Grammar is a system of rules which governs the production \"\n     [,2]                                                                                     \n[1,] \" use of utterances in a given language. These rules apply to sound as well as meaning, \"\n     [,3]                                                                                                                                                                                                                                                                                                                                                                                  \n[1,] \" include componential subsets of rules, such as those pertaining to phonology (the organisation of phonetic sound systems), morphology (the formation and composition of words), and syntax (the formation and composition of phrases and sentences). Many modern theories that deal with the principles of grammar are based on Noam Chomsky's framework of generative linguistics.\"\n\n\nThe function str_subset extracts those subsets of a text that contain a certain pattern.\n\nstr_subset(splitexampletext, \"and\") \n\n[1] \"Grammar is a system of rules which governs the production and use of utterances in a given language.\"                                                                                                                                                                                                   \n[2] \"These rules apply to sound as well as meaning, and include componential subsets of rules, such as those pertaining to phonology (the organisation of phonetic sound systems), morphology (the formation and composition of words), and syntax (the formation and composition of phrases and sentences).\"\n\n\nThe function str_which provides a vector with the indices of the texts that contain a certain pattern.\n\nstr_which(splitexampletext, \"and\")\n\n[1] 1 2\n\n\nThe function str_view shows the locations of the first instances of a pattern in a text or vector of texts.\n\nstr_view(splitexampletext, \"and\")\n\n\n\n\n\nThe function str_view_all shows the locations of all instances of a pattern in a text or vector of texts.\n\nstr_view_all(exampletext, \"and\")\n\n\n\n\n\nThe function str_pad adds white spaces to a text or vector of texts so that they reach a given number of symbols.\n\n# create text with white spaces\ntext <- \" this    is a    text   \"\nstr_pad(text, width = 30)\n\n[1] \"       this    is a    text   \"\n\n\nThe function str_trim removes white spaces from the beginning(s) and end(s) of a text or vector of texts.\n\nstr_trim(text) \n\n[1] \"this    is a    text\"\n\n\nThe function str_squish removes white spaces that occur within a text or vector of texts.\n\nstr_squish(text)\n\n[1] \"this is a text\"\n\n\nThe function str_wrap removes white spaces from the beginning(s) and end(s) of a text or vector of texts and also those white spaces that occur within a text or vector of texts.\n\nstr_wrap(text)\n\n[1] \"this is a text\"\n\n\nThe function str_order provides a vector that represents the order of a vector of texts according to the lengths of texts in that vector.\n\nstr_order(splitexampletext)\n\n[1] 1 3 2\n\n\nThe function str_sort orders of a vector of texts according to the lengths of texts in that vector.\n\nstr_sort(splitexampletext)\n\n[1] \"Grammar is a system of rules which governs the production and use of utterances in a given language.\"                                                                                                                                                                                                   \n[2] \"Many modern theories that deal with the principles of grammar are based on Noam Chomsky's framework of generative linguistics.\"                                                                                                                                                                         \n[3] \"These rules apply to sound as well as meaning, and include componential subsets of rules, such as those pertaining to phonology (the organisation of phonetic sound systems), morphology (the formation and composition of words), and syntax (the formation and composition of phrases and sentences).\"\n\n\nThe function str_to_upper converts all symbols in a text or vector of texts to upper case.\n\nstr_to_upper(exampletext) \n\n[1] \"GRAMMAR IS A SYSTEM OF RULES WHICH GOVERNS THE PRODUCTION AND USE OF UTTERANCES IN A GIVEN LANGUAGE. THESE RULES APPLY TO SOUND AS WELL AS MEANING, AND INCLUDE COMPONENTIAL SUBSETS OF RULES, SUCH AS THOSE PERTAINING TO PHONOLOGY (THE ORGANISATION OF PHONETIC SOUND SYSTEMS), MORPHOLOGY (THE FORMATION AND COMPOSITION OF WORDS), AND SYNTAX (THE FORMATION AND COMPOSITION OF PHRASES AND SENTENCES). MANY MODERN THEORIES THAT DEAL WITH THE PRINCIPLES OF GRAMMAR ARE BASED ON NOAM CHOMSKY'S FRAMEWORK OF GENERATIVE LINGUISTICS.\"\n\n\nThe function str_to_lower converts all symbols in a text or vector of texts to lower case.\n\nstr_to_lower(exampletext) \n\n[1] \"grammar is a system of rules which governs the production and use of utterances in a given language. these rules apply to sound as well as meaning, and include componential subsets of rules, such as those pertaining to phonology (the organisation of phonetic sound systems), morphology (the formation and composition of words), and syntax (the formation and composition of phrases and sentences). many modern theories that deal with the principles of grammar are based on noam chomsky's framework of generative linguistics.\"\n\n\nThe function str_c combines texts into one text\n\nstr_c(exampletext, additionaltext)\n\n[1] \"Grammar is a system of rules which governs the production and use of utterances in a given language. These rules apply to sound as well as meaning, and include componential subsets of rules, such as those pertaining to phonology (the organisation of phonetic sound systems), morphology (the formation and composition of words), and syntax (the formation and composition of phrases and sentences). Many modern theories that deal with the principles of grammar are based on Noam Chomsky's framework of generative linguistics.In the early 20th century, Ferdinand de Saussure distinguished between the notions of langue and parole in his formulation of structural linguistics. According to him, parole is the specific utterance of speech, whereas langue refers to an abstract phenomenon that theoretically defines the principles and system of rules that govern a language. This distinction resembles the one made by Noam Chomsky between competence and performance in his theory of transformative or generative grammar. According to Chomsky, competence is an individual's innate capacity and potential for language (like in Saussure's langue), while performance is the specific way in which it is used by individuals, groups, and communities (i.e., parole, in Saussurean terms). \"\n\n\nThe function str_conv converts a text into a certain type of encoding, e.g. into UTF-8 or Latin1.\n\nstr_conv(exampletext, encoding = \"UTF-8\")\n\n[1] \"Grammar is a system of rules which governs the production and use of utterances in a given language. These rules apply to sound as well as meaning, and include componential subsets of rules, such as those pertaining to phonology (the organisation of phonetic sound systems), morphology (the formation and composition of words), and syntax (the formation and composition of phrases and sentences). Many modern theories that deal with the principles of grammar are based on Noam Chomsky's framework of generative linguistics.\"\n\n\nThe function str_dup reduplicates a text or a vector of texts n times.\n\nstr_dup(exampletext, times=2)\n\n[1] \"Grammar is a system of rules which governs the production and use of utterances in a given language. These rules apply to sound as well as meaning, and include componential subsets of rules, such as those pertaining to phonology (the organisation of phonetic sound systems), morphology (the formation and composition of words), and syntax (the formation and composition of phrases and sentences). Many modern theories that deal with the principles of grammar are based on Noam Chomsky's framework of generative linguistics.Grammar is a system of rules which governs the production and use of utterances in a given language. These rules apply to sound as well as meaning, and include componential subsets of rules, such as those pertaining to phonology (the organisation of phonetic sound systems), morphology (the formation and composition of words), and syntax (the formation and composition of phrases and sentences). Many modern theories that deal with the principles of grammar are based on Noam Chomsky's framework of generative linguistics.\"\n\n\nThe function str_flatten combines a vector of texts into one text. The argument collapse defines the symbol that occurs between the combined texts. If the argument collapse is left out, the texts will be combined without any symbol between the combined texts.\n\nstr_flatten(sentences, collapse = \" \")\n\n[1] \"This is a first sentence. This is a second sentence. And this is a third sentence.\"\n\n\nIf the argument collapse is left out, the texts will be combined without any symbol between the combined texts.\n\nstr_flatten(sentences)\n\n[1] \"This is a first sentence.This is a second sentence.And this is a third sentence.\"\n\n\nThe function str_length provides the length of texts in characters.\n\nstr_length(exampletext)\n\n[1] 523\n\n\nThe function str_replace_na replaces NA in texts. It is important to note that NA, if it occurs within a string, is considered to be the literal string NA.\n\n# create sentences with NA\nsentencesna <- c(\"Some text\", NA, \"Some more text\", \"Some NA text\")\n# apply str_replace_na function\nstr_replace_na(sentencesna, replacement = \"Something new\")\n\n[1] \"Some text\"      \"Something new\"  \"Some more text\" \"Some NA text\"  \n\n\nThe function str_trunc ends strings with … after a certain number of characters.\n\nstr_trunc(sentences, width = 20)\n\n[1] \"This is a first s...\" \"This is a second ...\" \"And this is a thi...\"\n\n\nThe function str_sub extracts a string from a text from a start location to an end position (expressed as character positions).\n\nstr_sub(exampletext, 5, 25)\n\n[1] \"mar is a system of ru\"\n\n\nThe function word extracts words from a text (expressed as word positions).\n\nword(exampletext, 2:7)\n\n[1] \"is\"     \"a\"      \"system\" \"of\"     \"rules\"  \"which\" \n\n\nThe function str_glue combines strings and allows to input variables.\n\nname <- \"Fred\"\nage <- 50\nanniversary <- as.Date(\"1991-10-12\")\nstr_glue(\n  \"My name is {name}, \",\n  \"my age next year is {age + 1}, \",\n  \"and my anniversary is {format(anniversary, '%A, %B %d, %Y')}.\"\n)\n\nMy name is Fred, my age next year is 51, and my anniversary is Saturday, October 12, 1991.\n\n\nThe function str_glue_data is particularly useful when it is used in data pipelines. The data set mtcars is a build in data set that is loaded automatically when starting R.\n\nmtcars %>% \n  str_glue_data(\"{rownames(.)} has {hp} hp\")\n\nMazda RX4 has 110 hp\nMazda RX4 Wag has 110 hp\nDatsun 710 has 93 hp\nHornet 4 Drive has 110 hp\nHornet Sportabout has 175 hp\nValiant has 105 hp\nDuster 360 has 245 hp\nMerc 240D has 62 hp\nMerc 230 has 95 hp\nMerc 280 has 123 hp\nMerc 280C has 123 hp\nMerc 450SE has 180 hp\nMerc 450SL has 180 hp\nMerc 450SLC has 180 hp\nCadillac Fleetwood has 205 hp\nLincoln Continental has 215 hp\nChrysler Imperial has 230 hp\nFiat 128 has 66 hp\nHonda Civic has 52 hp\nToyota Corolla has 65 hp\nToyota Corona has 97 hp\nDodge Challenger has 150 hp\nAMC Javelin has 150 hp\nCamaro Z28 has 245 hp\nPontiac Firebird has 175 hp\nFiat X1-9 has 66 hp\nPorsche 914-2 has 91 hp\nLotus Europa has 113 hp\nFord Pantera L has 264 hp\nFerrari Dino has 175 hp\nMaserati Bora has 335 hp\nVolvo 142E has 109 hp\n\n\n\n\n\n\nEXERCISE TIME!\n\n\n\n\n\n\n`\n\nLoad the text linguistics04. How many words does the text consist of?\n\n\n\nAnswer\n\n::: {.cell}\n  readLines(\"https://slcladal.github.io/data/testcorpus/linguistics04.txt\") %>%\n  paste0(collapse = \" \") %>%\n  strsplit(\" \") %>%\n  unlist() %>%\n  length()\n::: {.cell-output .cell-output-stdout} [1] 101 ::: :::\n\n\nHow many characters does the text consist of?\n\n\n\nAnswer\n\n::: {.cell}\n  readLines(\"https://slcladal.github.io/data/testcorpus/linguistics04.txt\") %>%\n  paste0(collapse = \" \") %>%\n  strsplit(\"\") %>%\n  unlist() %>%\n  length()\n::: {.cell-output .cell-output-stdout} [1] 673 ::: :::\n\n\n`\n\n\n\nAdvanced String Processing\nAbove, we have used functions and regular expressions to extract and find patters in textual data. Here, we will focus on common methods for cleaning text data that are applied before implementing certain methods.\nWe start by installing and then loading some additional packages, e.g., the quanteda (see here for a cheat sheet for the quanteda package), the tm, and the udpipe package, which are extremely useful when dealing with more advanced text processing.\n\ninstall.packages(\"quanteda\")\ninstall.packages(\"tm\")\ninstall.packages(\"udpipe\")\n\n\nlibrary(quanteda)\nlibrary(tm)\nlibrary(udpipe) \n\nOne common procedure is to split texts into sentences which we can do by using, e.g., the tokenize_sentence function from the quanteda package. I also unlist the data to have a vector wot work with (rather than a list).\n\net_sent <- quanteda::tokenize_sentence(exampletext) %>%\n  unlist()\n# inspect\net_sent\n\n[1] \"Grammar is a system of rules which governs the production and use of utterances in a given language.\"                                                                                                                                                                                                   \n[2] \"These rules apply to sound as well as meaning, and include componential subsets of rules, such as those pertaining to phonology (the organisation of phonetic sound systems), morphology (the formation and composition of words), and syntax (the formation and composition of phrases and sentences).\"\n[3] \"Many modern theories that deal with the principles of grammar are based on Noam Chomsky's framework of generative linguistics.\"                                                                                                                                                                         \n\n\nAnother common procedure is to remove stop words, i.e., words that do not have semantic or referential meaning (like nouns such as tree or cat, or verbs like sit or speak or adjectives such as green or loud) but that indicate syntactic relations, roles, or features.(e.g., articles and pronouns). We can remove stopwords using, e.g., the removeWords function from the tm package\n\net_wostop <-  tm::removeWords(exampletext, tm::stopwords(\"english\"))\n# inspect\net_wostop\n\n[1] \"Grammar   system  rules  governs  production  use  utterances   given language. These rules apply  sound  well  meaning,  include componential subsets  rules,    pertaining  phonology ( organisation  phonetic sound systems), morphology ( formation  composition  words),  syntax ( formation  composition  phrases  sentences). Many modern theories  deal   principles  grammar  based  Noam Chomsky's framework  generative linguistics.\"\n\n\nTo remove the superfluous whote spaces, we can use, e.g., the stripWhitespace function from the tm package.\n\net_wows <-  tm::stripWhitespace(et_wostop)\n# inspect\net_wows\n\n[1] \"Grammar system rules governs production use utterances given language. These rules apply sound well meaning, include componential subsets rules, pertaining phonology ( organisation phonetic sound systems), morphology ( formation composition words), syntax ( formation composition phrases sentences). Many modern theories deal principles grammar based Noam Chomsky's framework generative linguistics.\"\n\n\nIt can also be useful to remove numbers. We can do this using, e.g., the removeNumbers function from the tm package.\n\net_wonum <-  tm::removeNumbers(\"This is the 1 and only sentence I will write in 2022.\")\n# inspect\net_wonum\n\n[1] \"This is the  and only sentence I will write in .\"\n\n\nWe may also want to remove any type of punctuation using, e.g., the removePunctuation function from the tm package.\n\net_wopunct <-  tm::removePunctuation(exampletext)\n# inspect\net_wopunct\n\n[1] \"Grammar is a system of rules which governs the production and use of utterances in a given language These rules apply to sound as well as meaning and include componential subsets of rules such as those pertaining to phonology the organisation of phonetic sound systems morphology the formation and composition of words and syntax the formation and composition of phrases and sentences Many modern theories that deal with the principles of grammar are based on Noam Chomskys framework of generative linguistics\"\n\n\nWe may also want to stem the words in a document, i.e. removing the ends of words to be able to group together semantically related words such as walk, walks, walking, walked which would all be stemmed into walk. We can stem a text using, e.g., the stemDocument function from the tm package.\n\net_stem <-  tm::stemDocument(exampletext, language = \"en\")\n# inspect\net_stem\n\nTokenization, lemmatization, pos-tagging, and dependency parsing\nA far better option than stemming is lemmatization as lemmatization is based on proper morphological information and vocabularies. For lemmatization, we can use the udpipe package which also tokenizes texts, adds part-of-speech tags, and provides information about dependency relations.\nBefore we can tokenize, lemmatize, pos-tag and parse though, we need to download a pre-trained language model.\n\n# download language model\nm_eng   <- udpipe::udpipe_download_model(language = \"english-ewt\")\n\nIf you have downloaded a model once, you can also load the model directly from the place where you stored it on your computer. In my case, I have stored the model in a folder called udpipemodels\n\n# load language model from your computer after you have downloaded it once\nm_eng <- udpipe_load_model(file = here::here(\"udpipemodels\",\n                                             \"english-ewt-ud-2.5-191206.udpipe\"))\n\nWe can now use the model to annotate out text.\n\n# tokenise, tag, dependency parsing\ntext_anndf <- udpipe::udpipe_annotate(m_eng, x = exampletext) %>%\n  as.data.frame() %>%\n  dplyr::select(-sentence)\n# inspect\nhead(text_anndf, 10)\n\n   doc_id paragraph_id sentence_id token_id      token      lemma  upos xpos\n1    doc1            1           1        1    Grammar    Grammar PROPN  NNP\n2    doc1            1           1        2         is         be   AUX  VBZ\n3    doc1            1           1        3          a          a   DET   DT\n4    doc1            1           1        4     system     system  NOUN   NN\n5    doc1            1           1        5         of         of   ADP   IN\n6    doc1            1           1        6      rules       rule  NOUN  NNS\n7    doc1            1           1        7      which      which  PRON  WDT\n8    doc1            1           1        8    governs     govern  VERB  VBZ\n9    doc1            1           1        9        the        the   DET   DT\n10   doc1            1           1       10 production production  NOUN   NN\n                                                   feats head_token_id\n1                                            Number=Sing             4\n2  Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin             4\n3                              Definite=Ind|PronType=Art             4\n4                                            Number=Sing             0\n5                                                   <NA>             6\n6                                            Number=Plur             4\n7                                           PronType=Rel             8\n8  Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin             4\n9                              Definite=Def|PronType=Art            10\n10                                           Number=Sing             8\n     dep_rel deps misc\n1      nsubj <NA> <NA>\n2        cop <NA> <NA>\n3        det <NA> <NA>\n4       root <NA> <NA>\n5       case <NA> <NA>\n6       nmod <NA> <NA>\n7      nsubj <NA> <NA>\n8  acl:relcl <NA> <NA>\n9        det <NA> <NA>\n10       obj <NA> <NA>\n\n\nWe could, of course, perform many more manipulations of textual data but this should suffice to get you started.\n\n\nCitation & Session Info\nSchweinberger, Martin. 2022. String processing in R. Brisbane: The University of Queensland. url: https://slcladal.github.io/string.html (Version 2022.08.31).\n@manual{schweinberger2022string,\n  author = {Schweinberger, Martin},\n  title = {String processing in R},\n  note = {https://slcladal.github.io/string.html},\n  year = {2022},\n  organization = {The University of Queensland, School of Languages and Cultures},\n  address = {Brisbane},\n  edition = {2022.08.31}\n}\n\nsessionInfo()\n\nR version 4.2.1 (2022-06-23)\nPlatform: x86_64-pc-linux-gnu (64-bit)\nRunning under: Ubuntu 22.04.1 LTS\n\nMatrix products: default\nBLAS:   /usr/lib/x86_64-linux-gnu/blas/libblas.so.3.10.0\nLAPACK: /usr/lib/x86_64-linux-gnu/lapack/liblapack.so.3.10.0\n\nlocale:\n [1] LC_CTYPE=en_AU.UTF-8       LC_NUMERIC=C              \n [3] LC_TIME=en_AU.UTF-8        LC_COLLATE=en_AU.UTF-8    \n [5] LC_MONETARY=en_AU.UTF-8    LC_MESSAGES=en_AU.UTF-8   \n [7] LC_PAPER=en_AU.UTF-8       LC_NAME=C                 \n [9] LC_ADDRESS=C               LC_TELEPHONE=C            \n[11] LC_MEASUREMENT=en_AU.UTF-8 LC_IDENTIFICATION=C       \n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] udpipe_0.8.9    tm_0.7-8        NLP_0.2-1       quanteda_3.2.1 \n [5] forcats_0.5.1   stringr_1.4.0   dplyr_1.0.9     purrr_0.3.4    \n [9] readr_2.1.2     tidyr_1.2.0     tibble_3.1.7    ggplot2_3.3.6  \n[13] tidyverse_1.3.2\n\nloaded via a namespace (and not attached):\n [1] httr_1.4.3          jsonlite_1.8.0      here_1.0.1         \n [4] modelr_0.1.8        RcppParallel_5.1.5  assertthat_0.2.1   \n [7] googlesheets4_1.0.0 cellranger_1.1.0    yaml_2.3.5         \n[10] slam_0.1-50         pillar_1.7.0        backports_1.4.1    \n[13] lattice_0.20-45     glue_1.6.2          digest_0.6.29      \n[16] rvest_1.0.2         colorspace_2.0-3    htmltools_0.5.2    \n[19] Matrix_1.4-1        pkgconfig_2.0.3     broom_1.0.0        \n[22] haven_2.5.0         scales_1.2.0        tzdb_0.3.0         \n[25] googledrive_2.0.0   generics_0.1.3      ellipsis_0.3.2     \n[28] withr_2.5.0         klippy_0.0.0.9500   cli_3.3.0          \n[31] magrittr_2.0.3      crayon_1.5.1        readxl_1.4.0       \n[34] evaluate_0.15       stopwords_2.3       fs_1.5.2           \n[37] fansi_1.0.3         xml2_1.3.3          tools_4.2.1        \n[40] data.table_1.14.2   hms_1.1.1           gargle_1.2.0       \n[43] lifecycle_1.0.1     munsell_0.5.0       reprex_2.0.1       \n[46] compiler_4.2.1      rlang_1.0.4         grid_4.2.1         \n[49] rstudioapi_0.13     htmlwidgets_1.5.4   rmarkdown_2.14     \n[52] gtable_0.3.0        DBI_1.1.3           R6_2.5.1           \n[55] lubridate_1.8.0     knitr_1.39          fastmap_1.1.0      \n[58] utf8_1.2.2          fastmatch_1.1-3     rprojroot_2.0.3    \n[61] stringi_1.7.8       parallel_4.2.1      Rcpp_1.0.8.3       \n[64] vctrs_0.4.1         dbplyr_2.2.1        tidyselect_1.1.2   \n[67] xfun_0.31          \n\n\n\nBack to top\nBack to HOME\n\n\n\nReferences"
  },
  {
    "objectID": "support.html",
    "href": "support.html",
    "title": "SUPPORT & FEEDBACK",
    "section": "",
    "text": "The LADAL is a collaborative effort that is sponsored by the School of Languages and Cultures at the University of Queensland. If you are interested in becoming an affiliate member or even a contributor contact the LADAL team via email (ladal@uq.edu.au).\n\n\n\nSUPPORT\n\n\n\nFEEDBACK\n\n\n\nBECOME A CONTRIBUTOR\nIf you are interested in becoming an affiliate member or even a contributor contact the LADAL team via email (ladal@uq.edu.au).\n\nMain page"
  },
  {
    "objectID": "surex.html#creating-instructions",
    "href": "surex.html#creating-instructions",
    "title": "Creating Surveys, Questionnaires, and Experiments in R",
    "section": "Creating instructions",
    "text": "Creating instructions\nIn most experiments, the first thing we want to do is present some instructions to the participant. jsPsych contains an instructions plugin that will present multiple pages of instructions to the participant, and there is a corresponding function in jaysire called trial_instructions that we can use:\n\ninstructions <- jaysire::trial_instructions(\n  pages = c(\n    \"Welcome! Use the arrow buttons to browse these instructions\",\n    \"Your task is to decide if an equation like '2 + 2 = 4' is true or false\",\n    \"You will respond by clicking a button\",\n    \"Press the 'Next' button to begin!\"\n  ),\n  show_clickable_nav = TRUE,\n  post_trial_gap = 1000\n)\n\nIn this code, the pages argument specifies four very short pages of text that will be displayed to the participants. By setting show_clickable_nav to TRUE, we are telling jsPsych to display a pair of buttons that participants can click to move forward or backward within the instructions. By default, these are labeled “Next” and “Previous”, but you can change this if you like. The third thing I’ve specified here is the post_trial_gap, which is the length of time (in milliseconds) that the experiment will pause between the end of this trial (the instructions) and the start of the next one. During this “gap” period a blank screen is shown."
  },
  {
    "objectID": "surex.html#creating-simple-trials",
    "href": "surex.html#creating-simple-trials",
    "title": "Creating Surveys, Questionnaires, and Experiments in R",
    "section": "Creating simple trials",
    "text": "Creating simple trials\nOur next job is to write some experimental trials! We’ll keep it simple in this first example, and create two variables trial1 and trial2. In both cases we’ll present people with a piece of text, and then ask them to respond by clicking a button with the mouse. We can do this with the trial_html_button_response function. This function will use the html-button-response plugin within jsPsych. The plugin does exactly what you might expect given the name: it displays some HTML as the stimulus (in this case, just some regular text), and collects responses using buttons! Here’s the code to create trial1:\n\ntrial1 <- jaysire::trial_html_button_response(\n  stimulus = \"13 + 23 = 36\",\n  choices = c(\"true\", \"false\"),\n  post_trial_gap = 1000\n)\n\nIn this code the stimulus argument specifies the text that will be displayed on screen to the participant, and the choices argument specifies the labels that will be shown on the response buttons. Again, the post_trial_gap argument is used to tell jsPsych how long to pause before starting the next trial. We can create trial2 in much the same way:\n\ntrial2 <- jaysire::trial_html_button_response(\n  stimulus = \"17 - 9 = 6\",\n  choices = c(\"true\", \"false\"), \n  post_trial_gap = 1000\n)\n\nIn fact, the code for trial2 is so similar to the code for trial1 that it feels inefficient. There should be a way to create both trials at the same time, and indeed there is, which I’ll talk about that later."
  },
  {
    "objectID": "surex.html#creating-a-timeline",
    "href": "surex.html#creating-a-timeline",
    "title": "Creating Surveys, Questionnaires, and Experiments in R",
    "section": "Creating a timeline",
    "text": "Creating a timeline\nNow that we have our trial objects, instructions, trial1 and trial2, our next task is to bind together into a timeline. The build_timeline function allows us to do this:\n\nall_trials <- jaysire::build_timeline(instructions, trial1, trial2)\n\nAt this point we have a complete timeline for our simple experiment! Yay!"
  },
  {
    "objectID": "surex.html#building-the-experiment",
    "href": "surex.html#building-the-experiment",
    "title": "Creating Surveys, Questionnaires, and Experiments in R",
    "section": "Building the experiment",
    "text": "Building the experiment\nAt the moment we have a complete timeline, but it is stored in an abstract form as the all_trials variable. What we really want to do is build” an experiment from this timeline: we want to write the files that will run the experiment, and save those files somewhere. That is the job of the build_experiment function. First, let’s specify the location of the experiment. Normally, we would build the experiment into a sensible location (e.g., somewhere inside an RStudio project), but for the purposes of this demonstration – which has to be reproducible on any computer, not just mine – I’ll use the temporary_folder function for this purposes. Let’s check if there already exists a exp_path folder, and create it if it doesn’t exist:\n\nif (!exists(\"exp_path\")) {\n  exp_path <- jaysire::temporary_folder()\n}\n# inspect location of temp output folder\nexp_path\n\n[1] \"/tmp/RtmpmeYlBg/jaysire_3fefe\"\n\n\nNow all we have to do is write the experiment into this folder build_experiment function, specifying the timeline and path arguments to tell R what to write and where to write it:\n\njaysire::build_experiment(\n  timeline = all_trials,\n  path = exp_path,\n  on_finish = save_locally()\n)\n\nWarning in dir.create(path): '/tmp/RtmpmeYlBg/jaysire_3fefe' already exists\n\n\nThe other thing that is specified is the on_finish argument, which tells jsPsych what do to when the experiment ends. In the present case, the only thing we want to do is save the data. To keep it simple, we’ll assume the experiment is going to be run on the same computer where the experiment is stored, and so we can use the save_locally function here."
  },
  {
    "objectID": "surex.html#what-have-we-created",
    "href": "surex.html#what-have-we-created",
    "title": "Creating Surveys, Questionnaires, and Experiments in R",
    "section": "What have we created?",
    "text": "What have we created?\nWhen we run the build_experiment function, two subfolders within the exp_path folder are created, one called data (which is initially empty) and another called experiment, which contains all the source files required to run the experiment. Here are the files we have just created:\n\nlist.files(exp_path, recursive = TRUE)\n\n[1] \"experiment/experiment.js\"                                  \n[2] \"experiment/index.html\"                                     \n[3] \"experiment/resource/script/jspsych-html-button-response.js\"\n[4] \"experiment/resource/script/jspsych-instructions.js\"        \n[5] \"experiment/resource/script/jspsych.js\"                     \n[6] \"experiment/resource/style/jspsych.css\""
  },
  {
    "objectID": "surex.html#running-the-experiment",
    "href": "surex.html#running-the-experiment",
    "title": "Creating Surveys, Questionnaires, and Experiments in R",
    "section": "Running the experiment",
    "text": "Running the experiment\nTo run the experiment on the local machine (and save the data to the data folder) all we have to do is call the run_locally function, specifying the location of the experiment to run:\n\njaysire::run_locally(path = exp_path)"
  },
  {
    "objectID": "surex.html#trial-templates",
    "href": "surex.html#trial-templates",
    "title": "Creating Surveys, Questionnaires, and Experiments in R",
    "section": "Trial templates",
    "text": "Trial templates\nIn the original example, our next step was to define a specific trial using trial_html_button response, in which the stimulus argument corresponded to a specific equation like 2 + 2 = 4, and the choices offered to participants were to judge the equation to be “true” or “false”. This time around, we’ll do the same thing except we won’t specify the stimulus directly. Instead, what we’ll do is define a timeline variable using the variable function. The insert_variable function has only a single argument name, which tells jsPsych what to to call the timeline variable. So we can create a kind of “trial template” like this:\n\ntrial_template <- jaysire::trial_html_button_response(\n  stimulus = insert_variable(name = \"my_stimulus\"),\n  choices = c(\"true\", \"false\"),\n  post_trial_gap = 1000\n)"
  },
  {
    "objectID": "surex.html#attaching-variables-to-timelines",
    "href": "surex.html#attaching-variables-to-timelines",
    "title": "Creating Surveys, Questionnaires, and Experiments in R",
    "section": "Attaching variables to timelines",
    "text": "Attaching variables to timelines\nOur next step is to specify what possible values the variable can have. Let’s do that by creating a character vector that lists all the equations we want to show people:\n\nequations <- c(\n  \"13 + 23 = 36\",  \n  \"17 - 9 = 6\", \n  \"125 / 5 = 25\", \n  \"2 - 4 = 6\", \n  \"12 + 39 = 43\", \n  \"4 * 23 = 92\"\n)\n\nNow that we have specified the stimuli as an ordinary R variable, what we need to do is to link it to the trial template we defined earlier. To do this, we first “wrap” trial_template into a timeline, and use the set_variables function to attach a variable to that timeline:\n\ntrials <- jaysire::build_timeline(trial_template) %>%\n  jaysire::set_variables(my_stimulus = equations)\n\nThis might seem slightly odd, because in our first example we created all the trials individually and only wrapped them into a timeline at the very end. However, jsPsych allows you to nest timelines within other timelines, and this can be extremely useful. In this situation, what we have create is essentially a new mini-timeline that consists of six copies of the trial_template, each one using a different equation. This templating approach produces code that is much easier to write than if we had to use trial_html_button_response six times, and as you might imagine this is much more efficient when there are hundreds of different stimuli!"
  },
  {
    "objectID": "surex.html#randomisation",
    "href": "surex.html#randomisation",
    "title": "Creating Surveys, Questionnaires, and Experiments in R",
    "section": "Randomisation",
    "text": "Randomisation\nSo far, so good. However, one limitation with our code is that the experiment will run the six trials in the same order every time. If our experiment were running in R, what we could to is use the sample function to shuffle stimuli, but that won’t work in this situation because the experiment will (eventually) run through a web browser. So if we want to randomise the trial ordering we need to use the randomisation tools built into jsPsych. We can do this by attaching parameters to the timeline that we have just defined using the set_parameters function:\n\ntrials <- jaysire::build_timeline(trial_template) %>%\n  jaysire::set_variables(stimulus = equations) %>%\n  jaysire::set_parameters(randomize_order = TRUE)\n\nIn this version, the trials timeline contains the same six copies of the template, but jsPsych will show them in a random order when the experiment is run in the browser."
  },
  {
    "objectID": "surex.html#repetition",
    "href": "surex.html#repetition",
    "title": "Creating Surveys, Questionnaires, and Experiments in R",
    "section": "Repetition",
    "text": "Repetition\nWe’re almost there. Suppose we want to create a block randomised design, in which all six items are presented in a random order, and then the same six items are repeated in a different randomised order. This kind of design is quite common in experimental psychology, and we can create one by setting the repetitions parameter for the timeline:\n\ntrials <- jaysire::build_timeline(trial_template) %>%\n  jaysire::set_variables(stimulus = equations) %>%\n  jaysire::set_parameters(randomize_order = TRUE, repetitions = 2)"
  },
  {
    "objectID": "surex.html#add-final-page",
    "href": "surex.html#add-final-page",
    "title": "Creating Surveys, Questionnaires, and Experiments in R",
    "section": "Add final page",
    "text": "Add final page\nWe can also add a final good bye screen to inform participants that the experiment is over by defining a finish trial.\n\nfinish <- jaysire::trial_html_keyboard_response(\n  stimulus = \"All done! Press any key to finish\",\n  choices = respond_any_key()\n)"
  },
  {
    "objectID": "surex.html#build-experiment",
    "href": "surex.html#build-experiment",
    "title": "Creating Surveys, Questionnaires, and Experiments in R",
    "section": "Build experiment",
    "text": "Build experiment\n\nbuild_experiment(\n  timeline = build_timeline(instructions, trials, finish),\n  path = exp_path,\n  on_finish = save_locally()\n)\n\nWarning in dir.create(path): '/tmp/RtmpmeYlBg/jaysire_3fefe' already exists\n\n\nWarning in dir.create(file.path(path, experiment_folder)): '/tmp/RtmpmeYlBg/\njaysire_3fefe/experiment' already exists\n\n\nWarning in dir.create(file.path(path, experiment_folder, \"resource\")): '/tmp/\nRtmpmeYlBg/jaysire_3fefe/experiment/resource' already exists\n\n\nWarning in dir.create(file.path(path, experiment_folder, \"resource\", \"script\")):\n'/tmp/RtmpmeYlBg/jaysire_3fefe/experiment/resource/script' already exists\n\n\nWarning in dir.create(file.path(path, experiment_folder, \"resource\", \"style\")):\n'/tmp/RtmpmeYlBg/jaysire_3fefe/experiment/resource/style' already exists\n\n\nWarning in dir.create(file.path(path, experiment_folder, \"resource\", \"audio\")):\n'/tmp/RtmpmeYlBg/jaysire_3fefe/experiment/resource/audio' already exists\n\n\nWarning in dir.create(file.path(path, experiment_folder, \"resource\", \"video\")):\n'/tmp/RtmpmeYlBg/jaysire_3fefe/experiment/resource/video' already exists\n\n\nWarning in dir.create(file.path(path, experiment_folder, \"resource\", \"image\")):\n'/tmp/RtmpmeYlBg/jaysire_3fefe/experiment/resource/image' already exists\n\n\nWarning in dir.create(file.path(path, experiment_folder, \"resource\", \"other\")):\n'/tmp/RtmpmeYlBg/jaysire_3fefe/experiment/resource/other' already exists\n\n\nWarning in dir.create(file.path(path, data_folder)): '/tmp/RtmpmeYlBg/\njaysire_3fefe/data' already exists"
  },
  {
    "objectID": "surex.html#running-the-experiment-1",
    "href": "surex.html#running-the-experiment-1",
    "title": "Creating Surveys, Questionnaires, and Experiments in R",
    "section": "Running the experiment",
    "text": "Running the experiment\nTo run the experiment on the local machine (and save the data to the data folder) all we have to do is call the run_locally function, specifying the location of the experiment to run:\n\njaysire::run_locally(path = exp_path)"
  },
  {
    "objectID": "surex.html#the-resource-folder",
    "href": "surex.html#the-resource-folder",
    "title": "Creating Surveys, Questionnaires, and Experiments in R",
    "section": "The resource folder",
    "text": "The resource folder\nThe way jaysire works is to assume that there is a folder on your computer that contains all the files that your experiment needs to use. The current implementation in jaysire is a bit crude, so this needs to be a flat folder (i.e., it cannot contain any subfolders). Normally, this folder would be located somewhere sensible, but for the purposes of this example I’ll create one in a temporary folder:\n\n# specify the path\nresource_folder <- file.path(exp_path, \"images\")\nresource_folder\n\n[1] \"/tmp/RtmpmeYlBg/jaysire_3fefe/images\"\n\n# create the empty folder if necessary\nif(!dir.exists(resource_folder)) {\n  dir.create(resource_folder)\n}\n\nOkay, so now we have a resource folder. Note that this folder is not part of our experiment (it can’t be, since we haven’t created the experiment yet!) it’s somewhere else."
  },
  {
    "objectID": "surex.html#creating-resources-in-r",
    "href": "surex.html#creating-resources-in-r",
    "title": "Creating Surveys, Questionnaires, and Experiments in R",
    "section": "Creating resources in R",
    "text": "Creating resources in R\nThe next step is to place resources in the folder. One way to do this is just to copy and paste existing files from your computer, but because we’re in R we might as well use R to create some images for us to use in our experiment.\nThe experimental task that we’re going to design is one that asks people to look at images containing red and blue dots and judge whether there are more red dots or more blue dots in the image. To do this, here’s a function that will generate an image file that plots nred red dots and nblue blue dots, randomly located.\n\nmake_image <- function(nblue, nred, filename) {\n  points <- tibble(\n    x = runif(nblue + nred),\n    y = runif(nblue + nred),\n    col = c(rep(\"blue\", nblue), rep(\"red\", nred))\n  )\n  img <- ggplot(points, aes(x, y, color = col)) + \n    geom_point(size = 4, show.legend = FALSE) + \n    scale_color_identity() + \n    theme_void()\n  ggsave(filename, img, width = 4, height = 4, dpi = 100)\n}\n\nNow that we have our make_image function, let’s create stimuli that vary in the number of red dots and the number of blue dots! To do that, the first step is to specify how many red and blue dots will be in each image:\n\nnblue <-c(10, 20, 30, 40, 20, 30, 40, 50)\nnred <- c(20, 30, 40, 50, 10, 20, 30, 40)\nimage_files <- paste0(\"stimulus\", 1:8, \".png\")\n\nNow we create the files:\n\nfor(s in 1:8) {\n  make_image(\n    nblue = nblue[s], \n    nred = nred[s], \n    filename = file.path(resource_folder, image_files[s])\n  )\n}\n\nCheck that it worked:\n\nlist.files(resource_folder)\n\n[1] \"stimulus1.png\" \"stimulus2.png\" \"stimulus3.png\" \"stimulus4.png\"\n[5] \"stimulus5.png\" \"stimulus6.png\" \"stimulus7.png\" \"stimulus8.png\"\n\n\nOkay, so now we have our stimulus materials, so we’re ready to start incorporating them into an experiment! However, before we do so, there’s something important to note. At this point we have two separate variables resource_folder that specifies the path to the resource folder, and image_files which specifies the names of the images that are contained within that folder. That is:\n\nresource_folder\n\n[1] \"/tmp/RtmpmeYlBg/jaysire_3fefe/images\"\n\nimage_files\n\n[1] \"stimulus1.png\" \"stimulus2.png\" \"stimulus3.png\" \"stimulus4.png\"\n[5] \"stimulus5.png\" \"stimulus6.png\" \"stimulus7.png\" \"stimulus8.png\"\n\n\nThese are the two variables we’ll need to use when building our experiment."
  },
  {
    "objectID": "surex.html#the-trial-template",
    "href": "surex.html#the-trial-template",
    "title": "Creating Surveys, Questionnaires, and Experiments in R",
    "section": "The trial template",
    "text": "The trial template\nBecause we have several different stimuli to incorporate into the experiment, what we’ll do is specify a template just as we did in the second tutorial article. However, for our new experiment, the stimulus is an image rather than text, so we’ll use the trial_image_button_response function this time.\n\ntrial_template <- jaysire::trial_image_button_response(\n  stimulus = insert_variable(name = \"my_stimulus\"),\n  stimulus_height = 400,\n  stimulus_width = 400,\n  choices = c(\"there are more red dots\", \"there are more blue dots\"),\n  post_trial_gap = 1000\n)\n\nNotice that again I’ve used the insert_variable function to indicate the blank space that will need to be filled in when the experiment gets built. Another thing to notice is that the function allows you to manually specify the height and the width of the image (in pixels), so I’ve done that here."
  },
  {
    "objectID": "surex.html#using-resources-in-a-timeline",
    "href": "surex.html#using-resources-in-a-timeline",
    "title": "Creating Surveys, Questionnaires, and Experiments in R",
    "section": "Using resources in a timeline",
    "text": "Using resources in a timeline\nMuch like we did in the previous example, we can construct the timeline by first transforming trial_template into a timeline object using the build_timeline function, and then use the set_variables function to specify the timeline variable my_stimulus that the template requires. The only difference between this time and the last one is that we need to use the insert_resource function so that jaysire knows that it needs to treat image_files as filenames. So our code looks like this:\n\ntrials <- jaysire::build_timeline(trial_template) %>%\n  set_variables(my_stimulus = insert_resource(image_files)) \n\nOf course, in a real experiment we would probably want to randomise the order in which the stimuli are presented, so it would make more sense to add an extra step like this:\n\ntrials <- jaysire::build_timeline(trial_template) %>%\n  set_variables(my_stimulus = insert_resource(image_files)) %>%\n  set_parameters(randomize_order = TRUE)\n\nIn a complete experiment, the trials timeline would need to have other things added to it. We’d need some instructions at the start and a thank you message at the end. But I won’t bother with that in this case, and so we’ll pretend that trials is the entire timeline for the experiment.\n##Adding the resources to the experiment build{-}\nThe last step is, as always, building the experiment using the build_experiment function. Again, for the purposes of the tutorial I’ll set the experiment path to be a temporary folder, but in real life you’d put it somewhere sensible. Here’s how I do that:\n\njaysire::build_experiment(\n  timeline = trials,\n  path = file.path(exp_path, \"images_exp\"), \n  resources = build_resources(resource_folder),\n  on_finish = save_locally()\n)\n\nNotice the difference from the previous tutorials. This time around there is an extra argument called resources, and I’ve used the build_resources function. It’s worth unpacking this a little bit. To understand what is happening here, let’s have a look at the output of the build_resources function:\n\njaysire::build_resources(resource_folder)\n\n# A tibble: 8 × 4\n  name          type  from                                               to     \n  <chr>         <chr> <chr>                                              <chr>  \n1 stimulus1.png image /tmp/RtmpmeYlBg/jaysire_3fefe/images/stimulus1.png resour…\n2 stimulus2.png image /tmp/RtmpmeYlBg/jaysire_3fefe/images/stimulus2.png resour…\n3 stimulus3.png image /tmp/RtmpmeYlBg/jaysire_3fefe/images/stimulus3.png resour…\n4 stimulus4.png image /tmp/RtmpmeYlBg/jaysire_3fefe/images/stimulus4.png resour…\n5 stimulus5.png image /tmp/RtmpmeYlBg/jaysire_3fefe/images/stimulus5.png resour…\n6 stimulus6.png image /tmp/RtmpmeYlBg/jaysire_3fefe/images/stimulus6.png resour…\n7 stimulus7.png image /tmp/RtmpmeYlBg/jaysire_3fefe/images/stimulus7.png resour…\n8 stimulus8.png image /tmp/RtmpmeYlBg/jaysire_3fefe/images/stimulus8.png resour…\n\n\nWhat the build_resources function does is scan the resource folder and construct a tibble containing all the information that jaysire needs to be able to structure the experiment appropriately. The first column is just the filename, the second column is the type of resource (in this case, everything is an image), the third column indicates where the original file is located, and the final column indicates where copy of that file will be created.\nThis probably seems unnecessarily complicated. The reason it is structured like this is because there are different kinds of resources, and the browser needs to be given different instructions for how to handle each type. That is, the HTML code for incorporating images is different to the code for audio, video, or javascript code, and so the input to build_experiment needs to be explicit about what kind of resource each file corresponds to. By default the build_resources function assumes that an .mp3 file is audio, a .png file is an image, etc, but you can override the defaults if you need to. Alternatively, there’s nothing stopping you from constructing this tibble manually if you wanted to: the build_resources function is just there to make life slightly less annoying."
  },
  {
    "objectID": "surex.html#images",
    "href": "surex.html#images",
    "title": "Creating Surveys, Questionnaires, and Experiments in R",
    "section": "Images",
    "text": "Images\nFirst, let’s write an trial using trial_image_button_response to handle display the image file and ask people to rate its pleasantness on a 3-point scale:\n\nimage_trial <- jaysire::trial_image_button_response(\n  stimulus = insert_resource(\"heart.png\"), \n  stimulus_height = 400,\n  stimulus_width = 400,\n  choices = c(\"Unpleasant\", \"Neutral\", \"Pleasant\") \n)\n\nThis is familiar to us from the previous article. I’ve specified the stimulus itself using the insert_resource function, listed the choices available to participants, and some ancillary information about the size of the image."
  },
  {
    "objectID": "surex.html#videos",
    "href": "surex.html#videos",
    "title": "Creating Surveys, Questionnaires, and Experiments in R",
    "section": "Videos",
    "text": "Videos\nEmbedding videos in an experiment is, in principle, exactly the same as embedding images. Instead of using trial_image_button_response we use trial_video_button_response. However, there are a few complexities to note here. First, video files often require codecs to run, not every browser supports every form of video, and so on. As a consequence, it is often a good idea to have multiple versions of your video file, and let the browser find a version that it can display. As a consequence, instead of having a stimulus argument, we specify the video file using a vector of sources, like so:\n\nvideo_trial <- jaysire::trial_video_button_response(\n  sources = insert_resource(c(\"heart.mpg\", \"heart.webm\")), \n  choices = c(\"Unpleasant\", \"Neutral\", \"Pleasant\"), \n  trial_ends_after_video = FALSE,\n  response_ends_trial = TRUE\n)\n\nBy default, trial_video_button_response will end the trial as soon as the participant makes a response or the video ends. However, there are arguments that you can specify that allow you to make this decision yourself, as this example illustrates."
  },
  {
    "objectID": "surex.html#audio",
    "href": "surex.html#audio",
    "title": "Creating Surveys, Questionnaires, and Experiments in R",
    "section": "Audio",
    "text": "Audio\nThe structure of the audio trial is essentially the same as what we’ve seen previously.\n\naudio_trial <- jaysire::trial_audio_button_response(\n  stimulus = insert_resource(\"lukewarm_banjo.mp3\"),\n  choices = c(\"Unpleasant\", \"Neutral\", \"Pleasant\"), \n  trial_ends_after_audio = FALSE,\n  response_ends_trial = TRUE\n)\n\nHowever, there is one nuance to audio files if you are playing them locally (i.e., on your own computer, rather than from a remote server), which I’ll explain in the next section."
  },
  {
    "objectID": "surex.html#building-the-experiment-1",
    "href": "surex.html#building-the-experiment-1",
    "title": "Creating Surveys, Questionnaires, and Experiments in R",
    "section": "Building the experiment",
    "text": "Building the experiment\nTo build the experiment into the exp_path folder, I’ll call the experiment function:\n\njaysire::build_experiment(\n  timeline = build_timeline(image_trial, video_trial, audio_trial),\n  path = exp_path, \n  resources = build_resources(resources),\n  use_webaudio = FALSE,\n  on_finish = save_locally()\n)\n\nMost of this should look familiar. I’ve specified the timeline that describes the experiment, the path to the folder where it should be located, I’ve used the build_resources function to tell jaysire how to handle each of the resource files, and I’ve indicated that when the experiment finishes we should save_locally into the data folder (see the first article for more information about the data folder).\nThere is one thing that is new. I’ve specified the use_webaudio argument and set it to FALSE. The reason for this is that when you’re running the experiment locally, the web browser treats locally stored audio files as though they were being hosted on a different server to the webpage itself, and that triggers a “cross origin request error” (it is treated as a security risk). Fortunately, jsPsych includes a method to disable this and by setting use_webaudio = FALSE you can get around this issue and run your audio files even when the experiment is running locally!"
  },
  {
    "objectID": "surex.html#running-the-experiment-2",
    "href": "surex.html#running-the-experiment-2",
    "title": "Creating Surveys, Questionnaires, and Experiments in R",
    "section": "Running the experiment",
    "text": "Running the experiment\nTo run the experiment on the local machine (and save the data to the “data” folder) all we have to do is call the run_locally function, specifying the location of the experiment to run:\n\njaysire::run_locally(path = exp_path)"
  },
  {
    "objectID": "surex.html#what-have-we-created-1",
    "href": "surex.html#what-have-we-created-1",
    "title": "Creating Surveys, Questionnaires, and Experiments in R",
    "section": "What have we created?",
    "text": "What have we created?\nThere is one last thing worth discussing, namely the file structure of the experiment that we have build. To start with, let’s take a look at what build_resources creates:\n\njaysire::build_resources(resources)\n\n# A tibble: 4 × 4\n  name               type  from                                            to   \n  <chr>              <chr> <chr>                                           <chr>\n1 heart.mpg          video /home/sam/programming/SLCLADAL.github.io/renv/… reso…\n2 heart.png          image /home/sam/programming/SLCLADAL.github.io/renv/… reso…\n3 heart.webm         video /home/sam/programming/SLCLADAL.github.io/renv/… reso…\n4 lukewarm_banjo.mp3 audio /home/sam/programming/SLCLADAL.github.io/renv/… reso…\n\n\nNow take a look at where all the files have ended up:\n\nlist.files(exp_path, recursive = TRUE)\n\n [1] \"experiment/experiment.js\"                                              \n [2] \"experiment/index.html\"                                                 \n [3] \"experiment/resource/audio/lukewarm_banjo.mp3\"                          \n [4] \"experiment/resource/image/heart.png\"                                   \n [5] \"experiment/resource/script/jspsych-audio-button-response.js\"           \n [6] \"experiment/resource/script/jspsych-html-button-response.js\"            \n [7] \"experiment/resource/script/jspsych-html-keyboard-response.js\"          \n [8] \"experiment/resource/script/jspsych-image-button-response.js\"           \n [9] \"experiment/resource/script/jspsych-instructions.js\"                    \n[10] \"experiment/resource/script/jspsych-video-button-response.js\"           \n[11] \"experiment/resource/script/jspsych.js\"                                 \n[12] \"experiment/resource/style/jspsych.css\"                                 \n[13] \"experiment/resource/video/heart.mpg\"                                   \n[14] \"experiment/resource/video/heart.webm\"                                  \n[15] \"images_exp/experiment/experiment.js\"                                   \n[16] \"images_exp/experiment/index.html\"                                      \n[17] \"images_exp/experiment/resource/image/stimulus1.png\"                    \n[18] \"images_exp/experiment/resource/image/stimulus2.png\"                    \n[19] \"images_exp/experiment/resource/image/stimulus3.png\"                    \n[20] \"images_exp/experiment/resource/image/stimulus4.png\"                    \n[21] \"images_exp/experiment/resource/image/stimulus5.png\"                    \n[22] \"images_exp/experiment/resource/image/stimulus6.png\"                    \n[23] \"images_exp/experiment/resource/image/stimulus7.png\"                    \n[24] \"images_exp/experiment/resource/image/stimulus8.png\"                    \n[25] \"images_exp/experiment/resource/script/jspsych-image-button-response.js\"\n[26] \"images_exp/experiment/resource/script/jspsych.js\"                      \n[27] \"images_exp/experiment/resource/style/jspsych.css\"                      \n[28] \"images/stimulus1.png\"                                                  \n[29] \"images/stimulus2.png\"                                                  \n[30] \"images/stimulus3.png\"                                                  \n[31] \"images/stimulus4.png\"                                                  \n[32] \"images/stimulus5.png\"                                                  \n[33] \"images/stimulus6.png\"                                                  \n[34] \"images/stimulus7.png\"                                                  \n[35] \"images/stimulus8.png\""
  },
  {
    "objectID": "surex.html#creating-a-question",
    "href": "surex.html#creating-a-question",
    "title": "Creating Surveys, Questionnaires, and Experiments in R",
    "section": "Creating a question",
    "text": "Creating a question\nBecause a survey page can include more than one question, the jaysire package provides convenience functions that allow you to build questions. For instance, if you want a likert scale question you could do something like this:\n\njaysire::question_likert(\n  prompt = \"Kittens are awesome\",\n  labels = c(\"strong disagree\", \"somewhat disagree\", \"neutral\",\n             \"somewhat agree\", \"strongly agree\"),\n  required = TRUE\n)\n\nwhich defines a 5 point Likert scale item, and marks it as a required question. If you want to create a multiple choice or multiple selection question, then you can use question_multi, which (following the convention in jsPsych) refers to response options for multiple choice trials, in contrast to the scale labels used by question_likert. Here is an example:\n\njaysire::question_multi(\n  prompt = \"Choose your awesomeness\",\n  options = c(\"Kittens\", \"Puppies\", \"Otters\", \"Ducks\", \"Axolotls\")\n)\n\nNote that you don’t need to specify whether this is a multiple choice question or a multiple selection question: if you embed it in a trial_survey_multi_choice page it will be presented as a multiple choice question, but if you embed it in trial_survey_multi_select it will appear in a “choose as many as apply” format.\nFinally, you can define a free response question using question_text like so:\n\njaysire::question_text(\n  prompt = \"Explain the awesomeness of kittens\",\n  placeholder = \"Type your answer here\",\n  rows = 10,\n  columns = 60\n)\n\nWhen embedded on a trial_survey_text this question will appear with a text response box underneath that spans 10 rows and 60 columns. The placeholder text will be shown greyed out within the box."
  },
  {
    "objectID": "surex.html#composing-a-page-of-questions",
    "href": "surex.html#composing-a-page-of-questions",
    "title": "Creating Surveys, Questionnaires, and Experiments in R",
    "section": "Composing a page of questions",
    "text": "Composing a page of questions\nAt this point, our next task in writing a survey is to put together a list of questions that can be passed to one of the survey items. For example, suppose we wanted a page that asks for some basic demographic information. This page would contain a short preamble that would be displayed at the top of the page, and then show a list of multiple choice questions underneath. Once all of the required questions are answered, a button can be pressed to continue to the next page. Here is how we could do that:\n\npage1 <- jaysire::trial_survey_multi_choice(\n  preamble = \"Welcome! We'd like to ask some demographic questions\",\n  questions = list(\n    question_multi(\n      prompt = \"Please select the option that best matches your gender\",\n      options = c(\"Male\", \"Female\", \"Nonbinary\", \"Other\", \"Prefer not to say\"),\n      name = \"gender\"\n    ),\n    question_multi(\n      prompt = \"Do you consider yourself to be LGBTIQ+?\",\n      options = c(\"Yes\", \"No\", \"Unsure\", \"Prefer not to say\")\n    )\n  )\n)\n\nNotice that the questions argument is a list of questions. When you have two or more questions to include, this is mandatory, but if there is only one question jaysire will allow you to pass the question directly like so:\n\npage2 <- jaysire::trial_survey_multi_select(\n  questions = question_multi(\n    prompt = \"Which of the following R packages to you use?\",\n    options = c(\n      \"ggplot2\", \"dplyr\", \"purrr\", \"janitor\", \"data.table\", \"testthat\",\n      \"usethis\", \"tibble\", \"magrittr\", \"rlang\", \"babynames\", \"janeaustenr\"\n    )\n  )\n)\n\nWhen creating Likert pages, it is very common to reuse the same labels for every question, so it can be convenient to create a page of Likert items like this:\n\n# define the scale\nconfidence_scale <- c(\n  \"Very unconfident\", \n  \"Somewhat unconfident\", \n  \"Somewhat confident\", \n  \"Very confident\"\n)\n# a page of questions that all use the same scale\npage3 <- jaysire::trial_survey_likert(\n  preamble = \"How confident in you R skills?\",\n  questions = list(\n    question_likert(\"Data wrangling?\", confidence_scale),\n    question_likert(\"Data visualisation?\", confidence_scale),\n    question_likert(\"Statistical modelling?\", confidence_scale),\n    question_likert(\"Designing experiments?\", confidence_scale),\n    question_likert(\"R markdown documents?\", confidence_scale)\n  )\n)"
  },
  {
    "objectID": "surex.html#example-experiment",
    "href": "surex.html#example-experiment",
    "title": "Creating Surveys, Questionnaires, and Experiments in R",
    "section": "Example experiment",
    "text": "Example experiment\n\nlibrary(jaysire)\npage1 <- jaysire::trial_survey_multi_choice(\n  preamble = \"Welcome! We'd like to ask some demographic questions\",\n  questions = list(\n    question_multi(\n      prompt = \"Please select the option that best matches your gender\",\n      options = c(\"Male\", \"Female\", \"Nonbinary\", \"Other\", \"Prefer not to say\"),\n      name = \"gender\"\n    ),\n    question_multi(\n      prompt = \"Do you consider yourself to be LGBTIQ+?\",\n      options = c(\"Yes\", \"No\", \"Unsure\", \"Prefer not to say\")\n    )\n  )\n)\npage2 <- jaysire::trial_survey_multi_select(\n  questions = question_multi(\n    prompt = \"Which of the following R packages to you use?\",\n    options = c(\n      \"ggplot2\", \"dplyr\", \"purrr\", \"janitor\", \"data.table\", \"testthat\",\n      \"usethis\", \"tibble\", \"magrittr\", \"rlang\", \"babynames\", \"janeaustenr\"\n    )\n  )\n)\nconfidence_scale <- c(\n  \"Very unconfident\", \n  \"Somewhat unconfident\", \n  \"Somewhat confident\", \n  \"Very confident\"\n)\npage3 <- jaysire::trial_survey_likert(\n  preamble = \"How confident in you R skills?\",\n  questions = list(\n    question_likert(\"Data wrangling?\", confidence_scale),\n    question_likert(\"Data visualisation?\", confidence_scale),\n    question_likert(\"Statistical modelling?\", confidence_scale),\n    question_likert(\"Designing experiments?\", confidence_scale),\n    question_likert(\"R markdown documents?\", confidence_scale)\n  )\n)\npage4 <- jaysire::trial_survey_text(\n  questions = question_text(\n    prompt = \"Anything else you would like to mention?\",\n    placeholder = \"Type your answer here\",\n    rows = 8,\n    columns = 60\n  )\n)\njaysire::build_experiment(\n  timeline = jaysire::build_timeline(page1, page2, page3, page4),\n  path = exp_path, \n  on_finish = jaysire::save_locally()\n)\n\nWarning in dir.create(path): '/tmp/RtmpmeYlBg/jaysire_3fefe' already exists\n\n\nWarning in dir.create(file.path(path, experiment_folder)): '/tmp/RtmpmeYlBg/\njaysire_3fefe/experiment' already exists\n\n\nWarning in dir.create(file.path(path, experiment_folder, \"resource\")): '/tmp/\nRtmpmeYlBg/jaysire_3fefe/experiment/resource' already exists\n\n\nWarning in dir.create(file.path(path, experiment_folder, \"resource\", \"script\")):\n'/tmp/RtmpmeYlBg/jaysire_3fefe/experiment/resource/script' already exists\n\n\nWarning in dir.create(file.path(path, experiment_folder, \"resource\", \"style\")):\n'/tmp/RtmpmeYlBg/jaysire_3fefe/experiment/resource/style' already exists\n\n\nWarning in dir.create(file.path(path, experiment_folder, \"resource\", \"audio\")):\n'/tmp/RtmpmeYlBg/jaysire_3fefe/experiment/resource/audio' already exists\n\n\nWarning in dir.create(file.path(path, experiment_folder, \"resource\", \"video\")):\n'/tmp/RtmpmeYlBg/jaysire_3fefe/experiment/resource/video' already exists\n\n\nWarning in dir.create(file.path(path, experiment_folder, \"resource\", \"image\")):\n'/tmp/RtmpmeYlBg/jaysire_3fefe/experiment/resource/image' already exists\n\n\nWarning in dir.create(file.path(path, experiment_folder, \"resource\", \"other\")):\n'/tmp/RtmpmeYlBg/jaysire_3fefe/experiment/resource/other' already exists\n\n\nWarning in dir.create(file.path(path, data_folder)): '/tmp/RtmpmeYlBg/\njaysire_3fefe/data' already exists"
  },
  {
    "objectID": "surex.html#branching-timelines",
    "href": "surex.html#branching-timelines",
    "title": "Creating Surveys, Questionnaires, and Experiments in R",
    "section": "Branching timelines",
    "text": "Branching timelines\nSuppose the first question in our experiment asks people to indicate whether they identify as LGBTIQ+, like so:\n\npage1 <- jaysire::trial_html_button_response(\n  \"Do you identify as LGBTIQ+?\",\n  c(\"Yes\", \"No\", \"Prefer not to say\")\n)\n\nIf the user response Yes, we might want to ask a follow-up question, one that asks them to indicate (if they are willing to do so) which subcategories they belong to. However, we wouldn’t want to show that question to anyone who responds “No” or “Prefer not to say”, since the question isn’t relevant to those people. How do we do this? First, let’s just build the trial the same way we normally would:\n\nfollowup <- jaysire::trial_survey_multi_select(\n  jaysire::question_multi(\n    prompt = \"Select all that apply\",\n    options = c(\n      \"Lesbian\", \n      \"Gay\", \n      \"Bisexual/Pansexual\", \n      \"Transgender\", \n      \"Nonbinary\",\n      \"Genderqueer\", \n      \"Intersex\", \n      \"Asexual\",\n      \"Other\"\n    )\n  )\n)\n\nIf we add followup to our timeline “as is”, it will be shown to everybody regardless of what answer they provide. To fix this, what we do is wrap followup into a timeline, and then pipe that timeline through a display_if statement. What that looks like is this:\n\npage1a <- jaysire::build_timeline(followup) %>%\n  jaysire::display_if(jaysire::fn_data_condition(button_pressed == \"0\"))\n\nWhat we have done here is defined page1a as a conditional timeline. When the participant actually completes the experiment, this trial will only be shown if the condition fn_data_condition(button_pressed == \"0\") is true. This solves our problem, but it may not be obvious how we’re solving the problem, so it is worth unpacking this a little…"
  },
  {
    "objectID": "surex.html#what-is-going-on-here",
    "href": "surex.html#what-is-going-on-here",
    "title": "Creating Surveys, Questionnaires, and Experiments in R",
    "section": "What is going on here?",
    "text": "What is going on here?\nFirstly, let’s take a look at the button_pressed == \"0\" part. Within jsPsych, a button response is recorded numerically: it creates an internal “button_pressed” variable to store the results of the trial, where a value of 0 indicates that the user pressed the first button, 1 refers to the second button, and so on. However, everything in jsPsych is internally stored as text, so if we want to check the response we have to use \"0\" rather than 0. This is the reason why the condition statement takes the form button_pressed == \"0\": the “Yes” response is the first response option for the page1 trial, so we only want page1a to execute if the response on that trial (i.e., the value of button_pressed within the jsPsych data storage) is equal to \"0\".\nNext, I’ll expand on what is going on with the fn_data_condition function, but to do so I need to explain the display_if function in a little more detail. First, let’s rewrite the code above without using the pipe, and name our arguments explicitly:\n\npage1a <- jaysire::display_if(\n  timeline = jaysire::build_timeline(followup), \n  conditional_function = jaysire::fn_data_condition(button_pressed == \"0\")\n)\n\nThis isn’t quite so pretty to look at, but it is helpful for the purposes of understanding. The display_if function takes two arguments, the timeline object itself (i.e., the followup page), and a conditional_function that is used to determine whether or not the timeline should be executed. Because the experiment is eventually run in javascript through the web browser, this function must be a javascript function, not an R function. If you know javascript, then it may be useful to note that jaysire contains an insert_javascript function that means that any text you include will be passed as unfiltered javascript, so you can in fact pass anything you like here:\n\npage1a <- jaysire::display_if(\n  timeline = jaysire::build_timeline(followup), \n  conditional_function = insert_javascript(\"/* your javascript function here */\")\n)\n\nIf your javascript function returns true – logical values in javascript are true or false unlike in R where they are TRUE or FALSE – then the timeline will execute. If it returns false the timeline will not execute.\nHowever, one of the goals of jaysire is to minimise the amount of javascript you have to write when building a behavioural experiment, so there is a helper function called fn_data_condition that will create the javascript function that you want. By default, what it does is inspect the contents of the jsPsych data store for the preceding trial, and allows the user to construct an expression that will be tested against that data (e.g., button_pressed == \"0\"). This simplifies matters a little, but my intention is to extend this functionality over time to allow you to deal with the most common use cases without ever having to write your own javascript. Nevertheless, this is a work in progress."
  },
  {
    "objectID": "surex.html#looping-timelines",
    "href": "surex.html#looping-timelines",
    "title": "Creating Surveys, Questionnaires, and Experiments in R",
    "section": "Looping timelines",
    "text": "Looping timelines\nHaving gone into all this detail for display_if, it is very simple to provide an illustration of display_while because it works in exactly the same fashion. Suppose I want to force people to tell me that this picture of a heart looks pleasant. Let’s construct a trial:\n\nresources <- system.file(\"extdata\", \"resources\", package = \"jaysire\")\nquery <- jaysire::trial_image_button_response(\n  stimulus = jaysire::insert_resource(\"heart.png\"), \n  stimulus_height = 400,\n  stimulus_width = 400,\n  choices = c(\"Unpleasant\", \"Neutral\", \"Pleasant\"),\n  prompt = \"You will not be allowed to continue unless you select 'Pleasant'\"\n)\n\nNow what we do is take this query trial, wrap into a timeline using build_timeline and then keep repeating that trial until the user responds by pressing button \"2\" (i.e., selects \"Pleasant\"):\n\npage2 <- jaysire::build_timeline(query) %>%\n  jaysire::display_while(fn_data_condition(button_pressed != \"2\"))\n\nAt this point we are done! Let’s wrap all this up in a single timeline, add the resources, and build it as an experiment:\n\njaysire::build_experiment(\n  timeline = jaysire::build_timeline(page1, page1a, page2),\n  resources = jaysire::build_resources(resources),\n  path = exp_path, \n  on_finish = jaysire::save_locally()\n)\n\nWarning in dir.create(path): '/tmp/RtmpmeYlBg/jaysire_3fefe' already exists\n\n\nWarning in dir.create(file.path(path, experiment_folder)): '/tmp/RtmpmeYlBg/\njaysire_3fefe/experiment' already exists\n\n\nWarning in dir.create(file.path(path, experiment_folder, \"resource\")): '/tmp/\nRtmpmeYlBg/jaysire_3fefe/experiment/resource' already exists\n\n\nWarning in dir.create(file.path(path, experiment_folder, \"resource\", \"script\")):\n'/tmp/RtmpmeYlBg/jaysire_3fefe/experiment/resource/script' already exists\n\n\nWarning in dir.create(file.path(path, experiment_folder, \"resource\", \"style\")):\n'/tmp/RtmpmeYlBg/jaysire_3fefe/experiment/resource/style' already exists\n\n\nWarning in dir.create(file.path(path, experiment_folder, \"resource\", \"audio\")):\n'/tmp/RtmpmeYlBg/jaysire_3fefe/experiment/resource/audio' already exists\n\n\nWarning in dir.create(file.path(path, experiment_folder, \"resource\", \"video\")):\n'/tmp/RtmpmeYlBg/jaysire_3fefe/experiment/resource/video' already exists\n\n\nWarning in dir.create(file.path(path, experiment_folder, \"resource\", \"image\")):\n'/tmp/RtmpmeYlBg/jaysire_3fefe/experiment/resource/image' already exists\n\n\nWarning in dir.create(file.path(path, experiment_folder, \"resource\", \"other\")):\n'/tmp/RtmpmeYlBg/jaysire_3fefe/experiment/resource/other' already exists\n\n\nWarning in dir.create(file.path(path, data_folder)): '/tmp/RtmpmeYlBg/\njaysire_3fefe/data' already exists"
  },
  {
    "objectID": "surex.html#creating-the-resource-files",
    "href": "surex.html#creating-the-resource-files",
    "title": "Creating Surveys, Questionnaires, and Experiments in R",
    "section": "Creating the resource files",
    "text": "Creating the resource files\n\n# where should the images be stored initially?\nresources <- exp_path  \nblue_image <- file.path(resources, \"blue.png\")\norange_image <- file.path(resources, \"orange.png\")\n# create the blue circle plot\nblue_pic <- ggplot() + \n  theme_void() +\n  annotate(geom = \"point\", x = 0, y = 0, colour = \"blue\", size = 60)\n# create the orange circle plot\norange_pic <- ggplot() + \n  theme_void() +\n  annotate(geom = \"point\", x = 0, y = 0, colour = \"orange\", size = 60)\n# save the images to files in the resource folder\nggsave(filename = blue_image, plot = blue_pic, width = 2, height = 2)\nggsave(filename = orange_image, plot = orange_pic, width = 2, height = 2)\n# check that it worked\nlist.files(resources)\n\n[1] \"blue.png\"   \"data\"       \"experiment\" \"images\"     \"images_exp\"\n[6] \"orange.png\""
  },
  {
    "objectID": "surex.html#defining-instructions",
    "href": "surex.html#defining-instructions",
    "title": "Creating Surveys, Questionnaires, and Experiments in R",
    "section": "Defining instructions",
    "text": "Defining instructions\n\nwelcome <- jaysire::trial_html_keyboard_response(\n  stimulus = \"Welcome to the experiment! Press any key to begin\",\n  data = insert_property(stage = \"start\")\n)\nfinish <- jaysire::trial_html_keyboard_response(\n  stimulus = \"Thank you!\", \n  choices = respond_no_key(),\n  trial_duration = 2000,\n  data = jaysire::insert_property(stage = \"end\")\n)\npage1 <- paste0(\n  \"To navigate these instructions, use the arrow keys on your keyboard. \",\n  \"The right arrow will move you forward one page, and the left arrow will \",\n  \"move you back one page. Press the right arrow key to continue.\"\n)\npage2 <- paste0(\n  \"In this experiment, a circle will appear in the centre of the screen. \",\n  \"If the circle is <b>blue</b>, press the letter F on the keyboard as fast \",\n  \"as you can. If the circle is <b>orange</b>, press the letter J as fast as\",\n  \"you can.<br>\"\n)\npage3 <- paste0(\n  \"If you see this blue circle, you should press F. <br>\", \n  \"<img src = '\", insert_resource(\"blue.png\"), \"' width = 300px>\"\n)\npage4 <- paste0(\n  \"If you see this orange circle, you should press J. <br>\", \n  \"<img src = '\", insert_resource(\"orange.png\"), \"' width = 300px>\"\n)\npage5 <- \"When you are ready to begin, press the right arrow key.\"\ninstructions <- trial_instructions(\n  pages = c(page1, page2, page3, page4, page5), \n  show_clickable_nav = FALSE,\n  allow_keys = TRUE,\n  post_trial_gap = 2000,\n  data = jaysire::insert_property(stage = \"instruction\")\n)"
  },
  {
    "objectID": "surex.html#defining-the-experiment-trials",
    "href": "surex.html#defining-the-experiment-trials",
    "title": "Creating Surveys, Questionnaires, and Experiments in R",
    "section": "Defining the experiment trials",
    "text": "Defining the experiment trials\n\nintervals <- c(250, 500, 750, 1000, 1250, 1500, 1750, 2000)\nfixation <- jaysire::trial_html_keyboard_response(\n  stimulus = '<div style=\"font-size:60px;\">+</div>',\n  choices = jaysire::respond_no_key(),\n  trial_duration = fn_sample(intervals, 1),\n  data = jaysire::insert_property(stage = \"fixation\")\n)\ntest <- jaysire::trial_image_keyboard_response(\n  stimulus = insert_variable(\"circle\"), \n  stimulus_height = 300,\n  stimulus_width = 300,\n  choices = c(\"f\", \"j\"),\n  data = jaysire::insert_property(\n    stage = \"choice\", \n    colour = insert_variable(\"colour\"),\n    correct_key = insert_variable(\"correct_key\")\n  )\n)\ntrials <- jaysire::build_timeline(fixation, test) %>%\n  jaysire::set_variables(\n    circle = jaysire::insert_resource(c(\"orange.png\", \"blue.png\")),\n    colour = c(\"orange\", \"blue\"),\n    correct_key = keycode(c(\"f\",\"j\"))\n  ) %>%\n  set_parameters(\n    repetitions = 5,\n    randomize_order = TRUE\n  )"
  },
  {
    "objectID": "surex.html#building-the-experiment-2",
    "href": "surex.html#building-the-experiment-2",
    "title": "Creating Surveys, Questionnaires, and Experiments in R",
    "section": "Building the experiment",
    "text": "Building the experiment\n\njaysire::build_experiment(\n  timeline = jaysire::build_timeline(welcome, instructions, trials, finish),\n  resources = build_resources(resources),\n  columns = jaysire::insert_property(experiment = \"choice_rt\"),  \n  path = exp_path,\n  on_finish = jaysire::save_locally()\n)\n\nWorking version of the experiment here.\n\nfinish <- jaysire::trial_html_keyboard_response(\n  stimulus = \"All done! Click <a href='../../../articles/experiment01.html'>here</a> to return to the vignette.\",\n  choices = jaysire::respond_no_key()\n)\n# write a working copy of the experiment into the docs folder\njaysire::build_experiment(\n  timeline = jaysire::build_timeline(welcome, instructions, trials, finish),\n  resources = jaysire::build_resources(resources),\n  columns = jaysire::insert_property(experiment = \"choice_rt\"),  \n  path =  here::here(\"docs\", \"demos\", \"example08\")\n)"
  },
  {
    "objectID": "surex.html#running-the-experiment-3",
    "href": "surex.html#running-the-experiment-3",
    "title": "Creating Surveys, Questionnaires, and Experiments in R",
    "section": "Running the experiment",
    "text": "Running the experiment\nTo run the experiment on the local machine (and save the data to the data folder) all we have to do is call the run_locally function, specifying the location of the experiment to run:\n\njaysire::run_locally(path = exp_path)"
  },
  {
    "objectID": "surveys.html#line-graphs-for-likert-scaled-data",
    "href": "surveys.html#line-graphs-for-likert-scaled-data",
    "title": "Questionnaires and Surveys: Analyses with R",
    "section": "Line graphs for Likert-scaled data",
    "text": "Line graphs for Likert-scaled data\nA special case of line graphs is used when dealing with Likert-scaled variables (we will talk about Likert scales in more detail below). In such cases, the line graph displays the density of cumulative frequencies of responses. The difference between the cumulative frequencies of responses displays differences in preferences. We will only focus on how to create such graphs using the ggplot environment here as it has an in-build function (ecdf) which is designed to handle such data.\nIn a first step, we load a data set (ldat) which contains Likert-scaled variables. This data set represents fictitious rating of students from courses about how satisfied they were with their learning experience. The response to the Likert item is numeric so that strongly disagree/very dissatisfied would get the lowest (1) and strongly agree/very satisfied the highest numeric value (5).\n\n# define color vectors\nclrs3 <- c(\"firebrick4\",  \"gray70\", \"darkblue\")\nclrs5 <- c(\"firebrick4\", \"firebrick1\", \"gray70\", \"blue\", \"darkblue\")\n# load data\nldat <- base::readRDS(url(\"https://slcladal.github.io/data/lid.rda\", \"rb\"))\n\nLet’s briefly inspect the ldat data set.\n\n\n\n\n\nFirst 10 rows of the ldat data set.\n\n\nCourseSatisfactionChinese1Chinese1Chinese1Chinese1Chinese1Chinese1Chinese1Chinese1Chinese1Chinese1\n\n\nThe ldat data set has only two columns: a column labeled Course which has three levels (German, Japanese, and Chinese) and a column labeled Satisfaction which contains values from 1 to 5 which represent values ranging from very dissatisfied to very satisfied. Now that we have data resembling a Likert-scaled item from a questionnaire, we will display the data in a cumulative line graph.\n\n# create cumulative density plot\nldat %>%\n  ggplot(aes(x = Satisfaction, color = Course)) + \n  geom_step(aes(y = ..y..), stat = \"ecdf\") +\n  labs(y = \"Cumulative Density\") + \n  scale_x_discrete(limits = c(\"1\",\"2\",\"3\",\"4\",\"5\"), \n                   breaks = c(1,2,3,4,5),\n                   labels=c(\"very dissatisfied\", \"dissatisfied\", \n                            \"neutral\", \"satisfied\", \"very satisfied\")) + \n  scale_colour_manual(values = clrs3) + \n  theme_bw() \n\n\n\n\nThe satisfaction of the German course was the lowest as the red line shows the highest density (frequency of responses) of very dissatisfied and dissatisfied ratings. The students in our fictitious data set were most satisfied with the Chinese course as the blue line is the lowest for very dissatisfied and “dissatisfied” ratings while the difference between the courses shrinks for “satisfied” and very satisfied. The Japanese language course is in-between the German and the Chinese course."
  },
  {
    "objectID": "surveys.html#pie-charts",
    "href": "surveys.html#pie-charts",
    "title": "Questionnaires and Surveys: Analyses with R",
    "section": "Pie charts",
    "text": "Pie charts\nMost commonly, the data for visualization comes from tables of absolute frequencies associated with a categorical or nominal variable. The default way to visualize such frequency tables are pie charts and bar plots. In a first step, we modify the data to get counts and percentages.\n\n# create bar plot data\nbdat <- ldat %>%\n  dplyr::group_by(Satisfaction) %>%\n  dplyr::summarise(Frequency = n()) %>%\n  dplyr::mutate(Percent = round(Frequency/sum(Frequency)*100, 1)) %>%\n  # order the levels of Satisfaction manually so that the order is not alphabetical\n  dplyr::mutate(Satisfaction = factor(Satisfaction, \n                                      levels = 1:5,\n                                      labels = c(\"very dissatisfied\",\n                                                 \"dissatisfied\", \n                                                 \"neutral\", \n                                                 \"satisfied\", \n                                                 \"very satisfied\")))\n\nLet’s briefly inspect the new data set.\n\n\n\n\n\nOverview of the bardata data set.\n\n\nSatisfactionFrequencyPercentvery dissatisfied7023.3dissatisfied7023.3neutral6020.0satisfied5016.7very satisfied5016.7\n\n\nBefore creating bar plots, we will briefly turn to pie charts because pie charts are very common despite suffering from certain shortcomings. Consider the following example which highlights some of the issues that arise when using pie charts.\n# create pie chart\nbdat %>%\n  ggplot(aes(\"\", Percent, fill = Satisfaction)) + \n  geom_bar(stat=\"identity\", width=1, color = \"white\") +\n  coord_polar(\"y\", start=0) +\n  scale_fill_manual(values = clrs5) +\n  theme_void()\n\n\n\nIf the slices of the pie chart are not labelled, it is difficult to see which slices are smaller or bigger compared to other slices. This problem can easily be avoided when using a bar plot instead. This issue can be avoided by adding labels to pie charts. The labeling of pie charts is, however, somewhat tedious as the positioning is tricky. Below is an example for adding labels without specification.\n# create pie chart\nbdat %>%\n  ggplot(aes(\"\", Percent, fill = Satisfaction)) +\n  geom_bar(stat=\"identity\", width=1, color = \"white\") +\n  coord_polar(\"y\", start=0) +\n  scale_fill_manual(values = clrs5) +\n  theme_void() +\n  geom_text(aes(y = Percent, label = Percent), color = \"white\", size=6)\n\n\n\nTo place the labels where they make sense, we will add another variable to the data called “Position”.\npdat <- bdat %>%\n  dplyr::arrange(desc(Satisfaction)) %>%\n  dplyr::mutate(Position = cumsum(Percent)- 0.5*Percent)\nLet’s briefly inspect the new data set.\n\n\n\n\n\nOverview of the new data.\n\n\nSatisfactionFrequencyPercentPositionvery satisfied5016.78.35satisfied5016.725.05neutral6020.043.40dissatisfied7023.365.05very dissatisfied7023.388.35\n\n\nNow that we have specified the position, we can include it into the pie chart.\n# create pie chart\npdat %>%\n  ggplot(aes(\"\", Percent, fill = Satisfaction)) + \n  geom_bar(stat=\"identity\", width=1, color = \"white\") +\n  coord_polar(\"y\", start=0) +\n  scale_fill_manual(values = clrs5) +\n  theme_void() +\n  geom_text(aes(y = Position, label = Percent), color = \"white\", size=6)\n\n\n\nWe will now create separate pie charts for each course. In a first step, we create a data set that does not only contain the Satisfaction levels and their frequency but also the course.\n\n# create grouped pie data\ngldat <- ldat %>%\n  dplyr::group_by(Course, Satisfaction) %>%\n  dplyr::summarise(Frequency = n()) %>%\n  dplyr::mutate(Percent = round(Frequency/sum(Frequency)*100, 1),\n                Satisfaction = factor(Satisfaction, \n                                      levels = 1:5,\n                                      labels = c(\"very dissatisfied\",\n                                                 \"dissatisfied\", \n                                                 \"neutral\", \n                                                 \"satisfied\", \n                                                 \"very satisfied\"))) %>%\n  dplyr::arrange(desc(Satisfaction)) %>%\n  dplyr::mutate(Position = cumsum(Percent)- 0.5*Percent)\n\nLet’s briefly inspect the new data set.\n\n\n\n\n\nOverview of the grouped piedata.\n\n\nCourseSatisfactionFrequencyPercentPositionChinesevery satisfied15157.5Germanvery satisfied552.5Japanesevery satisfied303015.0Chinesesatisfied101020.0Germansatisfied151512.5Japanesesatisfied252542.5Chineseneutral252537.5Germanneutral151527.5Japaneseneutral202065.0Chinesedissatisfied303065.0Germandissatisfied252547.5Japanesedissatisfied151582.5Chinesevery dissatisfied202090.0Germanvery dissatisfied404080.0Japanesevery dissatisfied101095.0\n\n\nNow that we have created the data, we can plot separate pie charts for each course.\n# create pie chart\ngldat %>%\n  ggplot(aes(\"\", Percent, fill = Satisfaction)) + \n  facet_wrap(~Course) +\n  geom_bar(stat=\"identity\", width=1, color = \"white\") +\n  coord_polar(\"y\", start=0) +\n  scale_fill_manual(values = clrs5) +\n  theme_void() +\n  geom_text(aes(y = Position, label = Percent), color = \"white\", size=4)"
  },
  {
    "objectID": "surveys.html#bar-plots",
    "href": "surveys.html#bar-plots",
    "title": "Questionnaires and Surveys: Analyses with R",
    "section": "Bar plots",
    "text": "Bar plots\nLike pie charts, bar plot display frequency information across categorical variable levels.\n\n# bar plot\nbdat %>%\n  ggplot(aes(Satisfaction, Percent, fill = Satisfaction)) +\n  # determine type of plot\n  geom_bar(stat=\"identity\") +          \n  # use black & white theme\n  theme_bw() +                         \n  # add and define text\n  geom_text(aes(y = Percent-5, label = Percent), color = \"white\", size=3) + \n  # add colors\n  scale_fill_manual(values = clrs5) +\n  # suppress legend\n  theme(legend.position=\"none\")\n\n\n\n\nCompared with the pie chart, it is much easier to grasp the relative size and order of the percentage values which shows that pie charts are unfit to show relationships between elements in a graph and, as a general rule of thumb, should be avoided.\nBar plots can be grouped which adds another layer of information that is particularly useful when dealing with frequency counts across multiple categorical variables. But before we can create grouped bar plots, we need to create an appropriate data set.\n\n# create bar plot data\ngldat <- ldat %>%\n  dplyr::group_by(Course, Satisfaction) %>%\n  dplyr::summarise(Frequency = n()) %>%\n  dplyr::mutate(Percent = round(Frequency/sum(Frequency)*100, 1)) %>%\n  dplyr::mutate(Satisfaction = factor(Satisfaction, \n                                      levels = 1:5,\n                                      labels = c(\"very dissatisfied\",\n                                                 \"dissatisfied\", \n                                                 \"neutral\", \n                                                 \"satisfied\", \n                                                 \"very satisfied\")))\n\nLet’s briefly inspect the data set.\n\n\n\n\n\nOverview of the grouped bardata.\n\n\nCourseSatisfactionFrequencyPercentChinesevery dissatisfied2020Chinesedissatisfied3030Chineseneutral2525Chinesesatisfied1010Chinesevery satisfied1515Germanvery dissatisfied4040Germandissatisfied2525Germanneutral1515Germansatisfied1515Germanvery satisfied55Japanesevery dissatisfied1010Japanesedissatisfied1515Japaneseneutral2020Japanesesatisfied2525Japanesevery satisfied3030\n\n\nWe have now added Course as an additional categorical variable and will include Course as the “fill” argument in our bar plot. To group the bars, we use the command “position=position_dodge()”.\n\n# bar plot\ngldat %>%\n  ggplot(aes(Satisfaction, Frequency, fill = Course)) + \n  geom_bar(stat=\"identity\", position = position_dodge()) +\n  # define colors\n  scale_fill_manual(values = clrs3) + \n  # add text\n  geom_text(aes(label=Frequency), vjust=1.6, color=\"white\", \n            # define text position and size\n            position = position_dodge(0.9),  size=3.5) + \n  theme_bw()                         \n\n\n\n\nBar plots are particularly useful when visualizing data obtained through Likert items. As this is a very common issue that empirical researchers face. There are two basic ways to display Likert items using bar plots: grouped bar plots and more elaborate scaled bar plots.\nAlthough we have seen above how to create grouped bar plots, we will repeat it here with the language course example used above when we used cumulative density line graphs to visualize how to display Likert data.\nIn a first step, we recreate the data set which we have used above. The data set consists of a Likert-scaled variable (Satisfaction) which represents rating of students from three courses about how satisfied they were with their language-learning course. The response to the Likert item is numeric so that “strongly disagree/very dissatisfied” would get the lowest and “strongly agree/very satisfied” the highest numeric value.\nAgain, we can also plot separate bar graphs for each class by specifying “facets”.\n\n# create grouped bar plot\ngldat %>%\n  ggplot(aes(Satisfaction, Frequency, \n             fill = Satisfaction, \n             color = Satisfaction)) +\n  facet_grid(~Course) +\n  geom_bar(stat=\"identity\", position=position_dodge()) +\n  geom_line() +\n  # define colors\n  scale_fill_manual(values=clrs5) +\n  scale_color_manual(values=clrs5) +\n  # add text and define color\n  geom_text(aes(label=Frequency), vjust=1.6, color=\"white\", \n            # define text position and size\n            position = position_dodge(0.9),  size=3.5) +     \n  theme_bw() +\n  theme(axis.text.x=element_blank())\n\n\n\n\nAnother and very interesting way to display such data is by using the Likert package. In a first step, we need to activate the package, clean the data, and extract a subset for the data visualization example.\nOne aspect that is different to previous visualizations is that, when using the Likert package, we need to transform the data into a “likert” object (which is, however, very easy and is done by using the “likert()” function as shown below).\n\nsdat  <- base::readRDS(url(\"https://slcladal.github.io/data/sdd.rda\", \"rb\"))\n\n\n\n\n\n\nFirst 10 rows of the survey data.\n\n\nGroupRespondentHow.did.you.like.the.course.How.did.you.like.the.teacher.Was.the.content.intersting.Was.the.content.adequate.for.the.course.Were.there.enough.discussions.Was.the.use.of.online.materials.appropriate.Was.the.teacher.appropriately.prepared.Was.the.workload.of.the.course.appropriate.Was.the.course.content.enganging.Were.there.enough.interactive.exerceises.included.in.the.sessions.GermanG14444444444GermanG24534445313GermanG35342434431GermanG43333333333GermanG51111111111GermanG63132333333GermanG75342433445GermanG85555555555GermanG95133445531GermanG103333333333\n\n\nAs you can see, we need to clean and adapt the column names. To do this, we will\n\nadd an identifier which shows which question we are dealing with (e.g. Q 1: question text)\nremove the dots between words with spaces\nadd a question mark at the end of questions\nremove superfluous white spaces\n\n\n# clean column names\ncolnames(sdat)[3:ncol(sdat)] <- paste0(\"Q \", str_pad(1:10, 2, \"left\", \"0\"), \": \", colnames(sdat)[3:ncol(sdat)]) %>%\n  stringr::str_replace_all(\"\\\\.\", \" \") %>%\n  stringr::str_squish() %>%\n  stringr::str_replace_all(\"$\", \"?\")\n# inspect column names\ncolnames(sdat)\n\n [1] \"Group\"                                                                   \n [2] \"Respondent\"                                                              \n [3] \"Q 01: How did you like the course?\"                                      \n [4] \"Q 02: How did you like the teacher?\"                                     \n [5] \"Q 03: Was the content intersting?\"                                       \n [6] \"Q 04: Was the content adequate for the course?\"                          \n [7] \"Q 05: Were there enough discussions?\"                                    \n [8] \"Q 06: Was the use of online materials appropriate?\"                      \n [9] \"Q 07: Was the teacher appropriately prepared?\"                           \n[10] \"Q 08: Was the workload of the course appropriate?\"                       \n[11] \"Q 09: Was the course content enganging?\"                                 \n[12] \"Q 10: Were there enough interactive exerceises included in the sessions?\"\n\n\nNow, that we have nice column names, we will replace the numeric values (1 to 5) with labels ranging from disagree to agree and convert our data into a data frame.\n\nlbs <- c(\"disagree\", \"somewhat disagree\", \"neither agree nor disagree\",  \"somewhat agree\", \"agree\")\nsurvey <- sdat %>%\n  dplyr::mutate_if(is.character, factor) %>%\n  dplyr::mutate_if(is.numeric, factor, levels = 1:5, labels = lbs) %>%\n  drop_na() %>%\n  as.data.frame()\n\n\n\n\n\n\nFirst 10 rows of the edited survey data.\n\n\nGroupRespondentQ 01: How did you like the course?Q 02: How did you like the teacher?Q 03: Was the content intersting?Q 04: Was the content adequate for the course?Q 05: Were there enough discussions?Q 06: Was the use of online materials appropriate?Q 07: Was the teacher appropriately prepared?Q 08: Was the workload of the course appropriate?Q 09: Was the course content enganging?Q 10: Were there enough interactive exerceises included in the sessions?GermanG1somewhat agreesomewhat agreesomewhat agreesomewhat agreesomewhat agreesomewhat agreesomewhat agreesomewhat agreesomewhat agreesomewhat agreeGermanG2somewhat agreeagreeneither agree nor disagreesomewhat agreesomewhat agreesomewhat agreeagreeneither agree nor disagreedisagreeneither agree nor disagreeGermanG3agreeneither agree nor disagreesomewhat agreesomewhat disagreesomewhat agreeneither agree nor disagreesomewhat agreesomewhat agreeneither agree nor disagreedisagreeGermanG4neither agree nor disagreeneither agree nor disagreeneither agree nor disagreeneither agree nor disagreeneither agree nor disagreeneither agree nor disagreeneither agree nor disagreeneither agree nor disagreeneither agree nor disagreeneither agree nor disagreeGermanG5disagreedisagreedisagreedisagreedisagreedisagreedisagreedisagreedisagreedisagreeGermanG6neither agree nor disagreedisagreeneither agree nor disagreesomewhat disagreeneither agree nor disagreeneither agree nor disagreeneither agree nor disagreeneither agree nor disagreeneither agree nor disagreeneither agree nor disagreeGermanG7agreeneither agree nor disagreesomewhat agreesomewhat disagreesomewhat agreeneither agree nor disagreeneither agree nor disagreesomewhat agreesomewhat agreeagreeGermanG8agreeagreeagreeagreeagreeagreeagreeagreeagreeagreeGermanG9agreedisagreeneither agree nor disagreeneither agree nor disagreesomewhat agreesomewhat agreeagreeagreeneither agree nor disagreedisagreeGermanG10neither agree nor disagreeneither agree nor disagreeneither agree nor disagreeneither agree nor disagreeneither agree nor disagreeneither agree nor disagreeneither agree nor disagreeneither agree nor disagreeneither agree nor disagreeneither agree nor disagree\n\n\nNow, we can use the plot and the likert function to visualize the survey data.\n\nplot(likert(survey[,3:12]), ordered = F, wrap= 60)\n\n\n\n\nTo save this plot, you can use the save_plot function from the cowplot package as shown below.\n\nsurvey_p1 <- plot(likert(survey[,3:12]), ordered = F, wrap= 60)\n# save plot\ncowplot::save_plot(here(\"images\", \"stu_p1.png\"), # where to save the plot\n                   survey_p1,        # object to plot\n                   base_asp = 1.5,  # ratio of space fro questions vs space for plot\n                   base_height = 8) # size! higher for smaller font size\n\nAn additional and very helpful feature is that the likert package enables grouping the data as shown below. The display columns 3 to 8 and use column 1 for grouping.\n\n# create plot\nplot(likert(survey[,3:8], grouping = survey[,1]))"
  },
  {
    "objectID": "surveys.html#evaluating-the-reliability-of-questions",
    "href": "surveys.html#evaluating-the-reliability-of-questions",
    "title": "Questionnaires and Surveys: Analyses with R",
    "section": "Evaluating the reliability of questions",
    "text": "Evaluating the reliability of questions\n\nCronbach’s Alpha\nOftentimes several questions in one questionnaire aim to tap into the same cognitive concept or attitude or whatever we are interested in. The answers to these related questions should be internally consistent, i.e. the responses should correlate strongly and positively.\nCronbach’s \\(\\alpha\\) [2] is measure of internal consistency or reliability that provides information on how strongly the responses to a set of questions correlate. The formula for Cronbach’s \\(\\alpha\\) is shown below (N: number of items, \\(\\bar c\\): average inter-item co-variance among items, \\(\\bar v\\): average variance).\n\\(\\alpha = \\frac{N*\\bar c}{\\bar v + (N-1)\\bar c}\\)\nIf the values for Cronbach’s \\(\\alpha\\) are low (below .7), then this indicates that the questions are not internally consistent (and do not tap into the same concept) or that the questions are not uni-dimensional (as they should be).\nWhile Cronbach’s \\(\\alpha\\) is the most frequently used measures of reliability (probably because it is conceptually simple and can be computed very easily), it underestimates the reliability of a test and overestimates the first factor saturation. This can be a problem is the data is lumpy. Thus, various other measures of reliability have been proposed. Also,Cronbach’s \\(\\alpha\\) assumes that scale items are repeated measurements, an assumption that is often violated.\nAn alternative reliability measure that takes the amount of variance per item into account and thus performs better when dealing with lumpy data (although it is still affected by lumpiness) is Guttman’s Lambda 6 (G6) [3]. In contrast to Cronbach’s \\(\\alpha\\), G6 is mostly used to evaluate the reliability of individual test items though. This means that it provides information about how well individual questions reflect the concept that they aim to tap into.\nProbably the best measures of reliability are \\(\\omega\\) (omega) measures. Hierarchical \\(\\omega\\) provides more appropriate estimates of the general factor saturation while total \\(\\omega\\) is a better estimate of the reliability of the total test compared to both Cronbach’s \\(\\alpha\\) and G6 [4].\nCalculating Cronbach’s alpha in R\nWe will now calculate Cronbach’s \\(\\alpha\\) in R. In a first step, we activate the “psych” package and load as well as inspect the data.\n\n# load data\nsurveydata <- base::readRDS(url(\"https://slcladal.github.io/data/sud.rda\", \"rb\"))\n\n\n\n\n\n\nOverview of the survey data.\n\n\nRespondentQ01_OutgoingQ02_OutgoingQ03_OutgoingQ04_OutgoingQ05_OutgoingQ06_IntelligenceQ07_IntelligenceQ08_IntelligenceQ09_IntelligenceQ10_IntelligenceQ11_AttitudeQ12_AttitudeQ13_AttitudeQ14_AttitudeQ15_AttitudeRespondent_01454452332233233Respondent_02545442221244454Respondent_03544552112254444Respondent_04555451111154555Respondent_05454552212145455Respondent_06555545452212121Respondent_07454554544521121Respondent_08445455454512112Respondent_09554445544512212Respondent_10455444454521221Respondent_11222114555412221Respondent_12332325555412111Respondent_13322234345422122Respondent_14222113435412111Respondent_15122211222543445Respondent_16111122321434434Respondent_17222222222211122Respondent_18111111222555555Respondent_19223232111545545Respondent_20111122322555555\n\n\nThe inspection of the data shows that the responses of participants represent the rows and that the questions represent columns. The column names show that we have 15 questions and that the first five questions aim to test how outgoing respondents are. To check if the first five questions reliably test “outgoingness” (or “extraversion”), we calculate Cronbach’s alpha for these five questions.\nThus, we use the “alpha()” function and provide the questions that tap into the concept we want to assess. In addition to Cronbach’s \\(\\alpha\\), the “alpha()” function also reports Guttman’s lambda_6 which is an alternative measure for reliability. This is an advantage because Cronbach’s \\(\\alpha\\) underestimates the reliability of a test and overestimates the first factor saturation.\n\n# calculate cronbach's alpha\nCronbach <- psych::alpha(surveydata[c(\"Q01_Outgoing\",   \n                   \"Q02_Outgoing\",  \n                   \"Q03_Outgoing\",  \n                   \"Q04_Outgoing\",  \n                   \"Q05_Outgoing\")], check.keys=F)\n# inspect results\nCronbach\n\n\nReliability analysis   \nCall: psych::alpha(x = surveydata[c(\"Q01_Outgoing\", \"Q02_Outgoing\", \n    \"Q03_Outgoing\", \"Q04_Outgoing\", \"Q05_Outgoing\")], check.keys = F)\n\n  raw_alpha std.alpha G6(smc) average_r S/N    ase mean  sd median_r\n      0.98      0.98    0.97      0.89  42 0.0083  3.1 1.5      0.9\n\n    95% confidence boundaries \n         lower alpha upper\nFeldt     0.96  0.98  0.99\nDuhachek  0.96  0.98  0.99\n\n Reliability if an item is dropped:\n             raw_alpha std.alpha G6(smc) average_r S/N alpha se   var.r med.r\nQ01_Outgoing      0.97      0.97    0.97      0.89  33   0.0108 0.00099  0.89\nQ02_Outgoing      0.97      0.97    0.96      0.89  31   0.0116 0.00054  0.89\nQ03_Outgoing      0.97      0.97    0.97      0.90  35   0.0104 0.00095  0.90\nQ04_Outgoing      0.97      0.97    0.96      0.89  31   0.0115 0.00086  0.89\nQ05_Outgoing      0.98      0.98    0.97      0.91  41   0.0088 0.00034  0.91\n\n Item statistics \n              n raw.r std.r r.cor r.drop mean  sd\nQ01_Outgoing 20  0.96  0.96  0.95   0.94  3.1 1.5\nQ02_Outgoing 20  0.97  0.97  0.96   0.95  3.2 1.6\nQ03_Outgoing 20  0.95  0.95  0.94   0.93  3.1 1.5\nQ04_Outgoing 20  0.97  0.97  0.96   0.95  3.0 1.6\nQ05_Outgoing 20  0.94  0.94  0.91   0.90  3.2 1.6\n\nNon missing response frequency for each item\n                1   2    3    4    5 miss\nQ01_Outgoing 0.20 0.2 0.10 0.25 0.25    0\nQ02_Outgoing 0.15 0.3 0.05 0.15 0.35    0\nQ03_Outgoing 0.15 0.3 0.05 0.25 0.25    0\nQ04_Outgoing 0.25 0.2 0.05 0.30 0.20    0\nQ05_Outgoing 0.20 0.2 0.10 0.20 0.30    0\n\n\nThe output of the “alpha()” function is rather extensive and we will only interpret selected output here.\nThe value under alpha is Cronbach’s \\(\\alpha\\) and it should be above 0.7. The values to its left and right are the lower and upper bound of its confidence interval. The values in the column with the header “G6” show how well each question represents the concept it aims to reflect. Low values indicate that the question does not reflect the underlying concept while high values (.7 and higher) indicate that the question captures that concept well (or to an acceptable degree).\n\n\nOmega\nThe omega (\\(\\omega\\)) coefficient is also a reliability measure of internal consistency. \\(\\omega\\) represents an estimate of the general factor saturation of a test that was proposed by McDonald. [5] compare McDonald’s Omega to Cronbach’s \\(\\alpha\\) and Revelle’s \\(\\beta\\). They conclude that omega is the best estimate [6].\nA very handy way to calculate McDonald’s \\(\\omega\\) is to use the scaleReliability() function from the userfriendlyscience package (which also provides Cronbach’s \\(\\alpha\\) and the Greatest Lower Bound (GLB) estimate which is also a very good and innovative measure of reliability) [see also 7].\n\n# activate package\nlibrary(ufs)\n# extract reliability measures\nreliability <- ufs::scaleStructure(surveydata[c(\"Q01_Outgoing\", \n                                                \"Q02_Outgoing\", \n                                                \"Q03_Outgoing\", \n                                                \"Q04_Outgoing\", \n                                                \"Q05_Outgoing\")])\n# inspect results\nprint(reliability)\n\n\nInformation about this analysis:\n\n                 Dataframe: surveydata[c(\"Q01_Outgoing\", \"Q02_Outgoing\", \"Q03_Outgoing\", \n                     Items: all\n              Observations: 20\n     Positive correlations: 10 out of 10 (100%)\n\nEstimates assuming interval level:\n \nInformation about this analysis:\n\n                 Dataframe:     \"Q04_Outgoing\", \"Q05_Outgoing\")]\n                     Items: all\n              Observations: 20\n     Positive correlations: 10 out of 10 (100%)\n\nEstimates assuming interval level:\n\n             Omega (total): 0.98\n      Omega (hierarchical): 0.95\n   Revelle's omega (total): 0.98\nGreatest Lower Bound (GLB): 0.99\n             Coefficient H: 0.98\n         Coefficient alpha: 0.98\n\n(Estimates assuming ordinal level not computed, as the polychoric correlation matrix has missing values.)\n\nNote: the normal point estimate and confidence interval for omega are based on the procedure suggested by Dunn, Baguley & Brunsden (2013) using the MBESS function ci.reliability, whereas the psych package point estimate was suggested in Revelle & Zinbarg (2008). See the help ('?scaleStructure') for more information."
  },
  {
    "objectID": "surveys.html#factor-analysis",
    "href": "surveys.html#factor-analysis",
    "title": "Questionnaires and Surveys: Analyses with R",
    "section": "Factor analysis",
    "text": "Factor analysis\nWhen dealing with many variables it is often the case that several variables are related and represent a common, underlying factor. To find such underlying factors, we can use a factor analysis.\nFactor analysis is a method that allows to find commonalities or structure in data. This is particularly useful when dealing with many variables. Factors can be considered hidden latent variables or driving forces that affect or underlie several variables at once.\nThis becomes particularly apparent when considering socio-demographic variables as behaviors are not only dependent on single variables, e.g., economic status, but on the interaction of several additional variables such as education level, marital status, number of children, etc. All of these variables can be combined into a single factor (or hidden latent variable).\n\n# remove respondent\nsurveydata <- surveydata %>% \n  dplyr::select(-Respondent)\nfactoranalysis <- factanal(surveydata, 3, rotation=\"varimax\")\nprint(factoranalysis, digits=2, cutoff=.2, sort=TRUE)\n\n\nCall:\nfactanal(x = surveydata, factors = 3, rotation = \"varimax\")\n\nUniquenesses:\n    Q01_Outgoing     Q02_Outgoing     Q03_Outgoing     Q04_Outgoing \n            0.09             0.06             0.12             0.07 \n    Q05_Outgoing Q06_Intelligence Q07_Intelligence Q08_Intelligence \n            0.14             0.10             0.13             0.10 \nQ09_Intelligence Q10_Intelligence     Q11_Attitude     Q12_Attitude \n            0.28             0.41             0.08             0.14 \n    Q13_Attitude     Q14_Attitude     Q15_Attitude \n            0.04             0.09             0.06 \n\nLoadings:\n                 Factor1 Factor2 Factor3\nQ06_Intelligence -0.82    0.25    0.41  \nQ07_Intelligence -0.80            0.47  \nQ08_Intelligence -0.85            0.42  \nQ09_Intelligence -0.79            0.29  \nQ11_Attitude      0.96                  \nQ12_Attitude      0.92                  \nQ13_Attitude      0.97                  \nQ14_Attitude      0.95                  \nQ15_Attitude      0.96                  \nQ01_Outgoing              0.94          \nQ02_Outgoing              0.96          \nQ03_Outgoing              0.93          \nQ04_Outgoing              0.96          \nQ05_Outgoing              0.92          \nQ10_Intelligence -0.22   -0.46    0.57  \n\n               Factor1 Factor2 Factor3\nSS loadings       7.29    4.78    1.02\nProportion Var    0.49    0.32    0.07\nCumulative Var    0.49    0.80    0.87\n\nTest of the hypothesis that 3 factors are sufficient.\nThe chi square statistic is 62.79 on 63 degrees of freedom.\nThe p-value is 0.484 \n\n\nThe results of a factor analysis can be visualized so that questions which reflect the same underlying factor are grouped together.\n\n# plot factor 1 by factor 2\nload <- factoranalysis$loadings[,1:2]\n# set up plot\nplot(load, type=\"n\", xlim = c(-1.5, 1.5)) \n# add variable names\ntext(load,\n     # define labels\n     labels=names(surveydata),\n     # define font size \n     # (smaller than default = values smaller than 1)\n     cex=.7)  \n\n\n\n\nThe plot shows that the questions form groups which indicates that the questions do a rather good job at reflecting the concepts that they aim to tap into. The only problematic question is question 10 (Q10) which aimed to tap into the intelligence of respondents but appears not to correlate strongly with the other questions that aim to extract information about the respondents intelligence. In such cases, it makes sense, to remove a question (in this case Q10) from the survey as it does not appear to reflect what we wanted it to."
  },
  {
    "objectID": "surveys.html#principle-component-analysis",
    "href": "surveys.html#principle-component-analysis",
    "title": "Questionnaires and Surveys: Analyses with R",
    "section": "Principle component analysis",
    "text": "Principle component analysis\nPrincipal component analysis is used when several questions or variables reflect a common factor and they should be combined into a single variable, e.g. during the statistical analysis of the data. Thus, principal component analysis can be used to collapse different variables (or questions) into one.\nImagine you have measured lengths of sentences in different ways (in words, syllables, characters, time it takes to pronounce, etc.). You could combine all these different measures of length by applying a PCA to those measures and using the first principal component as a single proxy for all these different measures.\n\n# entering raw data and extracting PCs  from the correlation matrix\nPrincipalComponents <- princomp(surveydata[c(\"Q01_Outgoing\",    \n                   \"Q02_Outgoing\",  \n                   \"Q03_Outgoing\",  \n                   \"Q04_Outgoing\",  \n                   \"Q05_Outgoing\")], cor=TRUE)\nsummary(PrincipalComponents) # print variance accounted for\n\nImportance of components:\n                       Comp.1  Comp.2  Comp.3  Comp.4  Comp.5\nStandard deviation     2.1399 0.41221 0.33748 0.29870 0.21818\nProportion of Variance 0.9159 0.03398 0.02278 0.01784 0.00952\nCumulative Proportion  0.9159 0.94986 0.97264 0.99048 1.00000\n\n\nThe output shows that the first component (Comp.1) explains 91.58 percent of the variance. This shows that we only lose 8.42 percent of the variance if we use this component as a proxy for “outgoingness” if we use the collapsed component rather than the five individual items.\n\nloadings(PrincipalComponents) # pc loadings\n\n\nLoadings:\n             Comp.1 Comp.2 Comp.3 Comp.4 Comp.5\nQ01_Outgoing  0.448  0.324         0.831       \nQ02_Outgoing  0.453  0.242 -0.408 -0.360  0.663\nQ03_Outgoing  0.446  0.405  0.626 -0.405 -0.286\nQ04_Outgoing  0.452 -0.191 -0.568 -0.114 -0.650\nQ05_Outgoing  0.437 -0.798  0.342         0.230\n\n               Comp.1 Comp.2 Comp.3 Comp.4 Comp.5\nSS loadings       1.0    1.0    1.0    1.0    1.0\nProportion Var    0.2    0.2    0.2    0.2    0.2\nCumulative Var    0.2    0.4    0.6    0.8    1.0\n\n\nWe now check if the five questions that are intended to tap into “outgoingness” represent one (and not more) underlying factors. Do check this, we create a scree plot.\n\nplot(PrincipalComponents,type=\"lines\") # scree plot\n\n\n\n\nThe scree plot shown above indicates that we only need a single component to explain the variance as there is a steep decline from the first to the second component. This confirms that the questions that tap into “outgoingness” represent one (and not more) underlying factors.\n\nPrincipalComponents$scores # the principal components\n\n       Comp.1   Comp.2   Comp.3    Comp.4   Comp.5\n [1,]  1.8382 -0.36615 -0.05472 -0.185983  0.45152\n [2,]  1.8663  0.49141  0.43588  0.293521 -0.29327\n [3,]  2.1436 -0.43110 -0.14566  0.529200 -0.37631\n [4,]  2.4440  0.12865  0.39433  0.093406  0.28596\n [5,]  2.1362 -0.49210 -0.42957 -0.260901  0.02262\n [6,]  2.4574  0.52188 -0.20319 -0.014602 -0.29283\n [7,]  2.1362 -0.49210 -0.42957 -0.260901  0.02262\n [8,]  1.8506 -0.24517  0.63889 -0.230286 -0.17380\n [9,]  1.8538  0.37043 -0.25773  0.337824  0.33205\n[10,]  1.8589  0.43041  0.15198 -0.496581  0.10566\n[11,] -2.2853  0.62953  0.07366  0.047246  0.18177\n[12,] -0.8112  0.23229 -0.69790  0.254192 -0.06639\n[13,] -1.1176 -0.31735  0.16386  0.595405  0.08306\n[14,] -2.2853  0.62953  0.07366  0.047246  0.18177\n[15,] -2.2876  0.28617 -0.32086 -0.584569 -0.27755\n[16,] -2.8995 -0.54086  0.11152  0.034152  0.06787\n[17,] -1.7026 -0.01559 -0.07850  0.005418 -0.09725\n[18,] -3.1841 -0.02169 -0.11116  0.001061 -0.08202\n[19,] -1.1125 -0.25737  0.57357 -0.238999 -0.14334\n[20,] -2.8995 -0.54086  0.11152  0.034152  0.06787\n\n\nYou could now replace the five items which tap into “outgoingness” with the single first component shown in the table above."
  },
  {
    "objectID": "surveys.html#ordinal-regression",
    "href": "surveys.html#ordinal-regression",
    "title": "Questionnaires and Surveys: Analyses with R",
    "section": "Ordinal Regression",
    "text": "Ordinal Regression\nOrdinal regression is very similar to multiple linear regression but takes an ordinal dependent variable [8]. For this reason, ordinal regression is one of the key methods in analyzing Likert data.\nTo see how an ordinal regression is implemented in R, we load and inspect the “ordinaldata” data set. The data set consists of 400 observations of students that were either educated at this school (Internal = 1) or not (Internal = 0). Some of the students have been abroad (Exchange = 1) while other have not (Exchange = 0). In addition, the data set contains the students’ final score of a language test (FinalScore) and the dependent variable which the recommendation of a committee for an additional, very prestigious program. The recommendation has three levels (“very likely”, “somewhat likely”, and “unlikely”) and reflects the committees’ assessment of whether the student is likely to succeed in the program.\n\n# load data\nordata <- base::readRDS(url(\"https://slcladal.github.io/data/oda.rda\", \"rb\"))\n# inspect data\nstr(ordata)\n\n'data.frame':   400 obs. of  4 variables:\n $ Recommend : chr  \"very likely\" \"somewhat likely\" \"unlikely\" \"somewhat likely\" ...\n $ Internal  : int  0 1 1 0 0 0 0 0 0 1 ...\n $ Exchange  : int  0 0 1 0 0 1 0 0 0 0 ...\n $ FinalScore: num  3.26 3.21 3.94 2.81 2.53 ...\n\n\nIn a first step, we need to re-level the ordinal variable to represent an ordinal factor (or a progression from “unlikely” over “somewhat likely” to “very likely”. And we will also factorize Internal and Exchange to make it easier to interpret the output later on.\n\n# relevel data\nordata <- ordata %>%\ndplyr::mutate(Recommend = factor(Recommend, \n                           levels=c(\"unlikely\", \"somewhat likely\", \"very likely\"),\n                           labels=c(\"unlikely\",  \"somewhat likely\",  \"very likely\"))) %>%\n  dplyr::mutate(Exchange = ifelse(Exchange == 1, \"Exchange\", \"NoExchange\")) %>%\n  dplyr::mutate(Internal = ifelse(Internal == 1, \"Internal\", \"External\"))\n\nNow that the dependent variable is re-leveled, we check the distribution of the variable levels by tabulating the data. To get a better understanding of the data we create frequency tables across variables rather than viewing the variables in isolation.\n\n## three way cross tabs (xtabs) and flatten the table\nftable(xtabs(~ Exchange + Recommend + Internal, data = ordata))\n\n                           Internal External Internal\nExchange   Recommend                                 \nExchange   unlikely                       25        6\n           somewhat likely                12        4\n           very likely                     7        3\nNoExchange unlikely                      175       14\n           somewhat likely                98       26\n           very likely                    20       10\n\n\nWe also check the mean and standard deviation of the final score as final score is a numeric variable and cannot be tabulated (unless we convert it to a factor).\n\nsummary(ordata$FinalScore); sd(ordata$FinalScore)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   1.90    2.72    2.99    3.00    3.27    4.00 \n\n\n[1] 0.3979\n\n\nThe lowest score is 1.9 and the highest score is a 4.0 with a mean of approximately 3. Finally, we inspect the distributions graphically.\n\n# visualize data\nordata %>%\n  ggplot(aes(x = Recommend, y = FinalScore)) +\n  geom_boxplot(size = .75) +\n  geom_jitter(alpha = .5) +\n  facet_grid(Exchange ~ Internal, margins = TRUE) +\n  theme_bw() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1))\n\n\n\n\nWe see that we have only few students that have taken part in an exchange program and there are also only few internal students overall. With respect to recommendations, only few students are considered to very likely succeed in the program. We can now start with the modelling by using the “polr” function. To make things easier for us, we will only consider the main effects here as this tutorial only aims to how to implement an ordinal regression but not how it should be done in a proper study - then, the model fitting and diagnostic procedures would have to be performed accurately, of course.\n\n## fit ordered logit model and store results 'm'\nm <- polr(Recommend ~ Internal + Exchange + FinalScore, data = ordata, Hess=TRUE)\n## view a summary of the model\nsummary(m)\n\nCall:\npolr(formula = Recommend ~ Internal + Exchange + FinalScore, \n    data = ordata, Hess = TRUE)\n\nCoefficients:\n                    Value Std. Error t value\nInternalInternal   1.0477      0.266   3.942\nExchangeNoExchange 0.0587      0.298   0.197\nFinalScore         0.6157      0.261   2.363\n\nIntercepts:\n                            Value Std. Error t value\nunlikely|somewhat likely    2.262 0.882      2.564  \nsomewhat likely|very likely 4.357 0.904      4.818  \n\nResidual Deviance: 717.02 \nAIC: 727.02 \n\n\nThe results show that having studied here at this school increases the chances of receiving a positive recommendation but that having been on an exchange has a negative but insignificant effect on the recommendation. The final score also correlates positively with a positive recommendation but not as much as having studied here.\n\n## store table\n(ctable <- coef(summary(m)))\n\n                              Value Std. Error t value\nInternalInternal            1.04766     0.2658   3.942\nExchangeNoExchange          0.05868     0.2979   0.197\nFinalScore                  0.61574     0.2606   2.363\nunlikely|somewhat likely    2.26200     0.8822   2.564\nsomewhat likely|very likely 4.35744     0.9045   4.818\n\n\nAs the regression report does not provide p-values, we have to calculate them separately (after having calculated them, we add them to the coefficient table).\n\n## calculate and store p values\np <- pnorm(abs(ctable[, \"t value\"]), lower.tail = FALSE) * 2\n## combined table\n(ctable <- cbind(ctable, \"p value\" = p))\n\n                              Value Std. Error t value     p value\nInternalInternal            1.04766     0.2658   3.942 0.000080902\nExchangeNoExchange          0.05868     0.2979   0.197 0.843819939\nFinalScore                  0.61574     0.2606   2.363 0.018151727\nunlikely|somewhat likely    2.26200     0.8822   2.564 0.010343823\nsomewhat likely|very likely 4.35744     0.9045   4.818 0.000001452\n\n\nAs predicted, Exchange does not have a significant effect but FinalScore and Internal both correlate significantly with the likelihood of receiving a positive recommendation.\n\n# extract profiled confidence intervals\nci <- confint(m)\n# calculate odds ratios and combine them with profiled CIs\nexp(cbind(OR = coef(m), ci))\n\n                      OR 2.5 % 97.5 %\nInternalInternal   2.851 1.696  4.817\nExchangeNoExchange 1.060 0.595  1.920\nFinalScore         1.851 1.114  3.098\n\n\nThe odds ratios show that internal students are 2.85 or 285 percent more likely compared to non-internal students to receive positive evaluations and that a 1-point increase in the test score lead to a 1.85 or 185 percent increase in the chances of receiving a positive recommendation. The effect of an exchange is slightly negative but, as we have seen above, not significant.\nIn a final step, we will visualize the results of the ordinal regression model. To do that, we need to reformat the data and add the predictions.\n\n# extract predictions\npredictions <- predict(m, data = ordata, type = \"prob\")\n# add predictions to the data\nnewordata <- cbind(ordata, predictions)\n# rename columns\ncolnames(newordata)[6:7] <- c(\"somewhat_likely\", \"very_likely\")\n# reformat data\nnewordata <- newordata %>%\n  dplyr::select(-Recommend) %>%\n  tidyr::gather(Recommendation, Probability, unlikely:very_likely)  %>%\n  dplyr::mutate(Recommendation = factor(Recommendation, \n                                        levels = c(\"unlikely\",\n                                                   \"somewhat_likely\",\n                                                   \"very_likely\")))\n\n\nnewordata %>%\n  as.data.frame() %>%\n  head(10) %>%\n  flextable() %>%\n  flextable::set_table_properties(width = .5, layout = \"autofit\") %>%\n  flextable::theme_zebra() %>%\n  flextable::fontsize(size = 12) %>%\n  flextable::fontsize(size = 12, part = \"header\") %>%\n  flextable::align_text_col(align = \"center\") %>%\n  flextable::set_caption(caption = \"First 10 rows of the newordata.\")  %>%\n  flextable::border_outer()\n\n\n\n\nFirst 10 rows of the newordata.\n\n\nInternalExchangeFinalScoreRecommendationProbabilityExternalNoExchange3.26unlikely0.5488InternalNoExchange3.21unlikely0.3056InternalExchange3.94unlikely0.2294ExternalNoExchange2.81unlikely0.6161ExternalNoExchange2.53unlikely0.6560ExternalExchange2.59unlikely0.6609ExternalNoExchange2.56unlikely0.6518ExternalNoExchange2.73unlikely0.6277ExternalNoExchange3.00unlikely0.5881InternalNoExchange3.50unlikely0.2690\n\n\nWe can now visualize the predictions of the model.\n\n# bar plot\nnewordata %>%\n  ggplot(aes(x = FinalScore, Probability,\n             color = Recommendation, \n             group = Recommendation)) + \n  facet_grid(Exchange~Internal) +\n  geom_smooth() +  \n  # define colors\n  scale_fill_manual(values = clrs3) +\n  scale_color_manual(values = clrs3) +\n  theme_bw()  \n\n\n\n\nFor more information about regression modeling, model fitting, and model diagnostics, please have a look at the tutorial on fixed-effects regressions."
  },
  {
    "objectID": "svm.html#preparation-and-session-set-up",
    "href": "svm.html#preparation-and-session-set-up",
    "title": "Semantic Vector Space Models in R",
    "section": "Preparation and session set up",
    "text": "Preparation and session set up\nThis tutorial is based on R. If you have not installed R or are new to it, you will find an introduction to and more information how to use R here. For this tutorials, we need to install certain packages from an R library so that the scripts shown below are executed without errors. Before turning to the code below, please install the packages by running the code below this paragraph. If you have already installed the packages mentioned below, then you can skip ahead and ignore this section. To install the necessary packages, simply run the following code - it may take some time (between 1 and 5 minutes to install all of the libraries so you do not need to worry if it takes some time).\n\n# set options\noptions(stringsAsFactors = F)         # no automatic data transformation\noptions(\"scipen\" = 100, \"digits\" = 4) # suppress math annotation\n# install libraries\ninstall.packages(c(\"cluster\", \"factoextra\", \"cluster\", \n                   \"seriation\", \"pvclust\", \"ape\", \"vcd\", \n                   \"exact2x2\", \"factoextra\", \"seriation\", \n                   \"NbClust\", \"pvclust\"))\ninstall.packages(\"coop\")\ninstall.packages(\"tm\")\n\nOnce you have installed R and RStudio and initiated the session by executing the code shown above, you are good to go."
  },
  {
    "objectID": "table.html#preparation-and-session-set-up",
    "href": "table.html#preparation-and-session-set-up",
    "title": "Handling tables in R",
    "section": "Preparation and session set up",
    "text": "Preparation and session set up\nThis tutorial is based on R. If you have not installed R or are new to it, you will find an introduction to and more information how to use R here. For this tutorials, we need to install certain packages from an R library so that the scripts shown below are executed without errors. Before turning to the code below, please install the packages by running the code below this paragraph. If you have already installed the packages mentioned below, then you can skip ahead ignore this section. To install the necessary packages, simply run the following code - it may take some time (between 1 and 5 minutes to install all of the packages so you do not need to worry if it takes some time).\n\n# install packages\ninstall.packages(\"xlsx\")\ninstall.packages(\"tidyverse\")\ninstall.packages(\"flextable\")\ninstall.packages(\"openxlsx\")\ninstall.packages(\"here\")\n# install klippy for copy-to-clipboard button in code chunks\ninstall.packages(\"remotes\")\nremotes::install_github(\"rlesur/klippy\")\n\nNow that we have installed the packages, we can activate them as shown below.\n\n# set options\noptions(stringsAsFactors = F)         # no automatic data transformation\noptions(\"scipen\" = 100, \"digits\" = 4) # suppress math annotation\n# load packages\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(flextable)\nlibrary(xlsx)\nlibrary(openxlsx)\nlibrary(here)\n# activate klippy for copy-to-clipboard button\nklippy::klippy()\n\n\n\n\nOnce you have installed RStudio and initiated the session by executing the code shown above, you are good to go."
  },
  {
    "objectID": "table.html#piping",
    "href": "table.html#piping",
    "title": "Handling tables in R",
    "section": "Piping",
    "text": "Piping\nPiping, done with the sequence %>%, is a very easy, intuitive, quick, and handy way to process data. Essentially piping means that we take an element that is to the left of the piping symbol and then do something to it; that way, the piping symbol can be translated as and then.\nWe could, for example, load data and then capitalize the column names and then group the data by status and attraction and then get the mean of money spend on deleting all observations except for the first one. A more formal way to write this would be:\n\nload %>% capitalize %>% group %>% summarize.\n\nIn R this command would look like this:\n\n# example of a data processing pipeline\npipeddata <- base::readRDS(url(\"https://slcladal.github.io/data/mld.rda\", \"rb\")) %>%\n  dplyr::rename(Status = status, Attraction = attraction, Money = money) %>%\n  dplyr::group_by(Status, Attraction) %>%\n  dplyr::summarise(Mean = mean(Money))\n# inspect summarized data\npipeddata\n\n# A tibble: 4 × 3\n# Groups:   Status [2]\n  Status       Attraction     Mean\n  <chr>        <chr>         <dbl>\n1 Relationship Interested     99.2\n2 Relationship NotInterested  51.5\n3 Single       Interested    157. \n4 Single       NotInterested  46.0\n\n\nThe pipe has worked and we get the resulting summary which shows the mean of the money spend based on Attraction and Status."
  },
  {
    "objectID": "table.html#selecting-and-filtering",
    "href": "table.html#selecting-and-filtering",
    "title": "Handling tables in R",
    "section": "Selecting and filtering",
    "text": "Selecting and filtering\nAmong the most frequent procedures in data processing is selecting certain columns or extracting rows based on variable levels. In the tidyverse, this is done by using the select and filter functions. While select allows to extract columns, filter is used to extract rows, e.g. to get only observations that have a certain feature. Have a look at the example below.\n\n# select and filter\nreduceddata <- newdata %>%\n  # select the columns attraction and money\n  dplyr::select(attraction, money) %>%\n  # extract rows which represent cases where the person was interested in someone\n  dplyr::filter(attraction == \"Interested\")\n# inspect new table\nnrow(reduceddata); table(reduceddata$attraction)\n\n[1] 50\n\n\n\nInterested \n        50 \n\n\nWe have now reduced the data by excluding status (we have only selected attraction and money) and we have removed those 50 data rows of people who were not interested. The select function (like most other tidyverse functions) can also be used together with a minus sign which causes a column to be removed, thus dplyr::select(-money) would remove the money column (see below).\n\n# select and filter\ndatawithoutmoney <- newdata %>%\n  # remove money\n  dplyr::select(-money) \n# inspect data\nhead(datawithoutmoney)\n\n        status    attraction\n1 Relationship NotInterested\n2 Relationship NotInterested\n3 Relationship NotInterested\n4 Relationship NotInterested\n5 Relationship NotInterested\n6 Relationship NotInterested\n\n\nSelecting and filtering are extremely powerful functions that can also be combined with other functions. But before we discuss more complex issues, we will have a look at how we can change columns."
  },
  {
    "objectID": "table.html#changing-data-and-adding-columns",
    "href": "table.html#changing-data-and-adding-columns",
    "title": "Handling tables in R",
    "section": "Changing data and adding columns",
    "text": "Changing data and adding columns\nChanging and adding data is done with the mutate function. The mutate functions requires that we specify a column name - if we use the same name as the column we are changing, then we change the column but if we specify another column name, then a new column is created.\nWe will now create a new column (Spendalot) in which we encode if the person has spend a lot of money (100 AUD or more) on the present or not (less than 100 AUD).\n\n# creating a new column\nnewdata <- newdata %>%\n  dplyr::mutate(Spendalot = ifelse(money >= 100, \"Alot\", \"Alittle\")) \n# inspect data\nhead(newdata)\n\n        status    attraction money Spendalot\n1 Relationship NotInterested 86.33   Alittle\n2 Relationship NotInterested 45.58   Alittle\n3 Relationship NotInterested 68.43   Alittle\n4 Relationship NotInterested 52.93   Alittle\n5 Relationship NotInterested 61.86   Alittle\n6 Relationship NotInterested 48.47   Alittle\n\n\nThe table now has a new column (Spendalot) because we have specified a column name that did not exist yet - had we written dplyr::mutate(money = ifelse(money >= 100, \"Alot\", \"Alittle\")) then we would have changed the money column and replaced the money values with the labels Alot and Alittle."
  },
  {
    "objectID": "table.html#renaming-columns",
    "href": "table.html#renaming-columns",
    "title": "Handling tables in R",
    "section": "Renaming columns",
    "text": "Renaming columns\nOftentimes, column names are not really meaningful or incoherent which makes it easier to wrap your head around what the values in a column refer to. The easiest way around this is rename columns which is, fortunately very simple in the tidyverse. While the column names of our example table are meaningful, I want to capitalize the first letter of each column name. This can be done as follows.\n\n# renaming columns\nnewdata <- newdata  %>%\n  dplyr::rename(Status = status, Attraction = attraction, Money = money)\n# inspect data\nhead(newdata)\n\n        Status    Attraction Money Spendalot\n1 Relationship NotInterested 86.33   Alittle\n2 Relationship NotInterested 45.58   Alittle\n3 Relationship NotInterested 68.43   Alittle\n4 Relationship NotInterested 52.93   Alittle\n5 Relationship NotInterested 61.86   Alittle\n6 Relationship NotInterested 48.47   Alittle\n\n\nThe renaming was successful as all column names now begin with a capital letter."
  },
  {
    "objectID": "table.html#grouping-and-summarising",
    "href": "table.html#grouping-and-summarising",
    "title": "Handling tables in R",
    "section": "Grouping and summarising",
    "text": "Grouping and summarising\nIn contrast to mutate, which retains the number of rows, summarizing creates new columns but collapses rows and only provides the summary value (or values if more than one summary is specified). Also, columns that are not grouping variables are removed.\nSummarizing is particularly useful when we want to get summaries of groups. We will modify the example from above and extract the mean and the standard deviation of the money spend on presents by relationship status and whether the giver was attracted to the giv-ee.\n\n#grouping and summarizing data \ndatasummary <- newdata %>%\n  dplyr::group_by(Status, Attraction) %>%\n  dplyr::summarise(Mean = round(mean(Money), 2), SD = round(sd(Money), 1))\n# inspect summarized data\ndatasummary\n\n# A tibble: 4 × 4\n# Groups:   Status [2]\n  Status       Attraction     Mean    SD\n  <chr>        <chr>         <dbl> <dbl>\n1 Relationship Interested     99.2  14.7\n2 Relationship NotInterested  51.5  17  \n3 Single       Interested    157.   23.2\n4 Single       NotInterested  46.0  19.9"
  },
  {
    "objectID": "table.html#gathering-and-spreading",
    "href": "table.html#gathering-and-spreading",
    "title": "Handling tables in R",
    "section": "Gathering and Spreading",
    "text": "Gathering and Spreading\nOne very common problem is that data - or at least parts of it - have to be transformed from long to wide format or vice versa. In the tidyverse, this is done using the gather and spread function. We will convert the summary table shown above into a wide format (we also remove the SD column as it is no longer needed)\n\n# converting data to wide format \nwidedata <- datasummary %>%\n  # remove SD column\n  dplyr::select(-SD) %>% \n  # convert into wide format\n  tidyr::spread(Attraction, Mean)\n# inspect wide data\nwidedata\n\n# A tibble: 2 × 3\n# Groups:   Status [2]\n  Status       Interested NotInterested\n  <chr>             <dbl>         <dbl>\n1 Relationship       99.2          51.5\n2 Single            157.           46.0\n\n\nWe can re-convert the wide into a long format using the gather function.\n\n# converting data to long format \nlongdata <- widedata %>%\n  # convert into long format\n  tidyr::gather(Attraction, Money, Interested:NotInterested)\n# inspect wide data\nlongdata\n\n# A tibble: 4 × 3\n# Groups:   Status [2]\n  Status       Attraction    Money\n  <chr>        <chr>         <dbl>\n1 Relationship Interested     99.2\n2 Single       Interested    157. \n3 Relationship NotInterested  51.5\n4 Single       NotInterested  46.0\n\n\nThere are many more useful functions for processing, handling, and summarizing tables but this should suffice to get you started."
  },
  {
    "objectID": "tagging.html",
    "href": "tagging.html",
    "title": "POS-Tagging and Syntactic Parsing with R",
    "section": "",
    "text": "Introduction\n\n\n\n\n\nThis tutorial introduces part-of-speech tagging and syntactic parsing using R. This tutorial is aimed at beginners and intermediate users of R with the aim of showcasing how to annotate textual data with part-of-speech (pos) tags and how to syntactically parse textual data using R. The aim is not to provide a fully-fledged analysis but rather to show and exemplify selected useful methods associated with pos-tagging and syntactic parsing. Another highly recommendable tutorial on part-of-speech tagging in R with UDPipe is available here and another tutorial on pos-tagging and syntactic parsing by Andreas Niekler and Gregor Wiedemann can be found here [see 1].\n\n\n\nThe entire R Notebook for the tutorial can be downloaded here. If you want to render the R Notebook on your machine, i.e. knitting the document to html or a pdf, you need to make sure that you have R and RStudio installed and you also need to download the bibliography file and store it in the same folder where you store the Rmd file.  Here is a link to an interactive and simplified version of this tutorial on Google Colab. The interactive tutorial is based on a Jupyter notebook of this tutorial. This interactive Jupyter notebook allows you to execute code yourself and - if you copy the Jupyter notebook - you can also change and edit the notebook, e.g. you can change code and upload your own data.\n\n\n\n\n\n\n\n\nPart-Of-Speech Tagging\nMany analyses of language data require that we distinguish different parts of speech. In order to determine the word class of a certain word, we use a procedure which is called part-of-speech tagging (commonly referred to as pos-, pos-, or PoS-tagging). pos-tagging is a common procedure when working with natural language data. Despite being used quite frequently, it is a rather complex issue that requires the application of statistical methods that are quite advanced. In the following, we will explore different options for pos-tagging and syntactic parsing.\nParts-of-speech, or word categories, refer to the grammatical nature or category of a lexical item, e.g. in the sentence Jane likes the girl each lexical item can be classified according to whether it belongs to the group of determiners, verbs, nouns, etc. pos-tagging refers to a (computation) process in which information is added to existing text. This process is also called annotation. Annotation can be very different depending on the task at hand. The most common type of annotation when it comes to language data is part-of-speech tagging where the word class is determined for each word in a text and the word class is then added to the word as a tag. However, there are many different ways to tag or annotate texts.\nPos–tagging assigns part-of-speech tags to character strings (these represent mostly words, of course, but also encompass punctuation marks and other elements). This means that pos–tagging is one specific type of annotation, i.e. adding information to data (either by directly adding information to the data itself or by storing information in e.g. a list which is linked to the data). It is important to note that annotation encompasses various types of information such as pauses, overlap, etc. pos–tagging is just one of these many ways in which corpus data can be enriched. Sentiment Analysis, for instance, also annotates texts or words with respect to its or their emotional value or polarity.\nAnnotation is required in many machine-learning contexts because annotated texts are commonly used as training sets on which machine learning or deep learning models are trained that then predict, for unknown words or texts, what values they would most likely be assigned if the annotation were done manually. Also, it should be mentioned that by many online services offer pos-tagging (e.g. here or here.\nWhen pos–tagged, the example sentence could look like the example below.\n\nJane/NNP likes/VBZ the/DT girl/NN\n\nIn the example above, NNP stands for proper noun (singular), VBZ stands for 3rd person singular present tense verb, DT for determiner, and NN for noun(singular or mass). The pos-tags used by the openNLPpackage are the Penn English Treebank pos-tags. A more elaborate description of the tags can be found here which is summarised below:\n\n\n\n\n\nOverview of Penn English Treebank part-of-speech tags.\n\n\nTagDescriptionExamplesCCCoordinating conjunctionand, or, butCDCardinal numberone, two, threeDTDeterminera, theEXExistential thereThere/EX was a party in progressFWForeign wordpersona/FW non/FW grata/FWINPreposition or subordinating conuh, well, yesJJAdjectivegood, bad, uglyJJRAdjective, comparativebetter, nicerJJSAdjective, superlativebest, nicestLSList item markera., b., 1., 2.MDModalcan, would, willNNNoun, singular or masstree, chairNNSNoun, pluraltrees, chairsNNPProper noun, singularJohn, Paul, CIANNPSProper noun, pluralJohns, Pauls, CIAsPDTPredeterminerall/PDT this marble, many/PDT a soulPOSPossessive endingJohn/NNP 's/POS, the parentss/NNP '/POS distressPRPPersonal pronounI, you, hePRP$Possessive pronounmine, yoursRBAdverbevry, enough, notRBRAdverb, comparativelaterRBSAdverb, superlativelatestRPParticleRPSYMSymbolCO2TOtotoUHInterjectionuhm, uhVBVerb, base formgo, walkVBDVerb, past tensewalked, sawVBGVerb, gerund or present participwalking, seeingVBNVerb, past participlewalked, thoughtVBPVerb, non-3rd person singular prwalk, thinkVBZVerb, 3rd person singular presenwalks, thinksWDTWh-determinerwhich, thatWPWh-pronounwhat, who, whom (wh-pronoun)WP$Possessive wh-pronounwhose, who (wh-words)WRBWh-adverbhow, where, why (wh-adverb)\n\n\nAssigning these pos-tags to words appears to be rather straight forward. However, pos-tagging is quite complex and there are various ways by which a computer can be trained to assign pos-tags. For example, one could use orthographic or morphological information to devise rules such as. . .\n\nIf a word ends in ment, assign the pos-tag NN (for common noun)\nIf a word does not occur at the beginning of a sentence but is capitalized, assign the pos-tag NNP (for proper noun)\n\nUsing such rules has the disadvantage that pos-tags can only be assigned to a relatively small number of words as most words will be ambiguous – think of the similarity of the English plural (-(e)s) and the English 3rd person, present tense indicative morpheme (-(e)s), for instance, which are orthographically identical.Another option would be to use a dictionary in which each word is as-signed a certain pos-tag and a program could assign the pos-tag if the word occurs in a given text. This procedure has the disadvantage that most words belong to more than one word class and pos-tagging would thus have to rely on additional information.The problem of words that belong to more than one word class can partly be remedied by including contextual information such as. .\n\nIf the previous word is a determiner and the following word is a common noun, assign the pos-tag JJ (for a common adjective)\n\nThis procedure works quite well but there are still better options.The best way to pos-tag a text is to create a manually annotated training set which resembles the language variety at hand. Based on the frequency of the association between a given word and the pos-tags it is assigned in the training data, it is possible to tag a word with the pos-tag that is most often assigned to the given word in the training data.All of the above methods can and should be optimized by combining them and additionally including pos–n–grams, i.e. determining a pos-tag of an unknown word based on which sequence of pos-tags is most similar to the sequence at hand and also most common in the training data.This introduction is extremely superficial and only intends to scratch some of the basic procedures that pos-tagging relies on. The interested reader is referred to introductions on machine learning and pos-tagging such as e.g.https://class.coursera.org/nlp/lecture/149.\nThere are several different R packages that assist with pos-tagging texts [see 2]. In this tutorial, we will use the udpipe [3] and the openNLP packages [4]. Each of these has advantages and shortcomings and it is advantageous to try which result best matches one’s needs. That said, the udpipe package is really great as it is easy to use, covers a wide range of languages, is very flexible, and very accurate.\nPreparation and session set up\nThis tutorial is based on R. If you have not installed R or are new to it, you will find an introduction to and more information how to use R here. For this tutorials, we need to install certain packages from an R library so that the scripts shown below are executed without errors. Before turning to the code below, please install the packages by running the code below this paragraph. If you have already installed the packages mentioned below, then you can skip ahead ignore this section. To install the necessary packages, simply run the following code - it may take some time (between 1 and 5 minutes to install all of the libraries so you do not need to worry if it takes some time).\n\n# install packages\n#install.packages(\"dplyr\") \n#install.packages(\"stringr\")\n#install.packages(\"igraph\")\n#install.packages(\"tm\")\n#install.packages(\"NLP\")\n#install.packages(\"openNLP\")\n#install.packages(\"openNLPdata\")\n#install.packages(\"udpipe\")\n#install.packages(\"textplot\") \n#install.packages(\"ggraph\") \n#install.packages(\"ggplot2\") \n#install.packages(\"pacman\")\n#install.packages(\"flextable\")\n#install.packages(\"here\")\n#install.packages(\"http://datacube.wu.ac.at/src/contrib/openNLPmodels.en_1.5-1.tar.gz\", repos=NULL, type=\"source\")\n# install phrasemachine\n#phrasemachineurl <- \"https://cran.r-project.org/src/contrib/Archive/phrasemachine/phrasemachine_1.1.2.tar.gz\"\n#install.packages(phrasemachineurl, repos=NULL, type=\"source\")\n# install parsent\n#pacman::p_load_gh(c(\"trinker/textshape\", \"trinker/coreNLPsetup\",  \"trinker/parsent\"))\n# install klippy for copy-to-clipboard button in code chunks\n#install.packages(\"remotes\")\n#remotes::install_github(\"rlesur/klippy\")\n\nNow that we have installed the packages, we activate them as shown below.\n\n# set options\n#options(stringsAsFactors = F)         # no automatic data transformation\n#options(\"scipen\" = 100, \"digits\" = 4) # suppress math annotation\n# load packages\nlibrary(dplyr)\nlibrary(stringr)\n#library(igraph)\n#library(tm)\n#library(NLP)\n#library(openNLP)\n#library(openNLPdata)\n#library(udpipe)\n#library(textplot) \nlibrary(udpipe) \n#library(ggraph) \n#library(ggplot2) \n#library(igraph)\n#library(phrasemachine)\nlibrary(flextable)\n# load function for pos-tagging objects in R\n#source(\"https://slcladal.github.io/rscripts/POStagObject.r\") \n# syntax tree drawing function\n#source(\"https://slcladal.github.io/rscripts/parsetgraph.R\")\n# activate klippy for copy-to-clipboard button\n#klippy::klippy()\n\nOnce you have installed R and RStudio and initiated the session by executing the code shown above, you are good to go.\n\n\nPOS-Tagging with UDPipe\nUDPipe was developed at the Charles University in Prague and the udpipe R package [3] is an extremely interesting and really fantastic package as it provides a very easy and handy way for language-agnostic tokenization, pos-tagging, lemmatization and dependency parsing of raw text in R. It is particularly handy because it addresses and remedies major shortcomings that previous methods for pos-tagging had, namely\n\nit offers a wide range of language models (64 languages at this point)\nit does not rely on external software (like, e.g., TreeTagger, that had to be installed separately and could be challenging when using different operating systems)\nit is really easy to implement as one only need to install and load the udpipe package and download and activate the language model one is interested in\nit allows to train and tune one’s own models rather easily\n\nThe available pre-trained language models in UDPipe are:\n\n\n\n\n\nLanguages and langauge modesl available via udpipe.\n\n\nLanguagesModelsAfrikaansafrikaans-afriboomsAncient Greekancient_greek-perseus, ancient_greek-proielArabicarabic-padtArmenianarmenian-armtdpBasquebasque-bdtBelarusianbelarusian-hsebulgarian-btbbulgarian-btbBuryatburyat-bdtCatalancatalan-ancoraChinesechinese-gsd, chinese-gsdsimp, classical_chinese-kyotoCopticcoptic-scriptoriumCroatiancroatian-setCzechczech-cac, czech-cltt, czech-fictree, czech-pdtDanishdanish-ddtDutchdutch-alpino, dutch-lassysmallEnglishenglish-ewt, english-gum, english-lines, english-partutEstonianestonian-edt, estonian-ewtFinnishfinnish-ftb, finnish-tdtFrenchfrench-gsd, french-partut, french-sequoia, french-spokenGaliciangalician-ctg, galician-treegalGermangerman-gsd, german-hdtGothicgothic-proielGreekgreek-gdtHebrewhebrew-htbHindihindi-hdtbHungarianhungarian-szegedIndonesianindonesian-gsdIrish Gaelicirish-idtItalianitalian-isdt, italian-partut, italian-postwita, italian-twittiro, italian-vitJapanesejapanese-gsdKazakhkazakh-ktbKoreankorean-gsd, korean-kaistKurmanjikurmanji-mgLatinlatin-ittb, latin-perseus, latin-proielLatvianlatvian-lvtbLithuanianlithuanian-alksnis, lithuanian-hseMaltesemaltese-mudtMarathimarathi-ufalNorth Saminorth_sami-giellaNorwegiannorwegian-bokmaal, norwegian-nynorsk, norwegian-nynorskliaOld Church Slavonicold_church_slavonic-proielOld Frenchold_french-srcmfOld Russianold_russian-torotPersianpersian-serajiPolishpolish-lfg, polish-pdb, polish-szPortugeseportuguese-bosque, portuguese-br, portuguese-gsdRomanianromanian-nonstandard, romanian-rrtRussianrussian-gsd, russian-syntagrus, russian-taigaSanskritsanskrit-ufalScottish Gaelicscottish_gaelic-arcosgSerbianserbian-setSlovakslovak-snkSlovenianslovenian-ssj, slovenian-sstSpanishspanish-ancora, spanish-gsdSwedishswedish-lines, swedish-talbankenTamiltamil-ttbTelugutelugu-mtgTurkishturkish-imstUkrainianukrainian-iuUpper Sorbiaupper_sorbian-ufalUrduurdu-udtbUyghuruyghur-udtVietnamesevietnamese-vtbWolofwolof-wtb\n\n\n\n\n\nThe udpipe R package also allows you to easily train your own models, based on data in CONLL-U format, so that you can use these for your own commercial or non-commercial purposes. This is described in the other vignette of this package which you can view by the command  vignette(\"udpipe-train\", package = \"udpipe\")\n\n\n\n\n\n\nTo download any of these models, we can use the udpipe_download_model function. For example, to download the english-ewt model, we would use the call: m_eng   <- udpipe::udpipe_download_model(language = \"english-ewt\").\nWe start by loading a text\n\n# load text\ntext <- readLines(\"https://slcladal.github.io/data/testcorpus/linguistics06.txt\", skipNul = T)\n# clean data\ntext <- text %>%\n str_squish() \n\nNow that we have a text that we can work with, we will download a pre-trained language model.\n\n# download language model\nm_eng   <- udpipe::udpipe_download_model(language = \"english-ewt\")\n\nIf you have downloaded a model once, you can also load the model directly from the place where you stored it on your computer. In my case, I have stored the model in a folder called udpipemodels\n\n# load language model from your computer after you have downloaded it once\nm_eng <- udpipe_load_model(file = here::here(\"udpipemodels\", \"english-ewt-ud-2.5-191206.udpipe\"))\n\nWe can now use the model to annotate out text.\n\n# tokenise, tag, dependency parsing\ntext_anndf <- udpipe::udpipe_annotate(m_eng, x = text) %>%\n  as.data.frame() %>%\n  dplyr::select(-sentence)\n# inspect\nhead(text_anndf, 10)\n\n   doc_id paragraph_id sentence_id token_id       token      lemma  upos xpos\n1    doc1            1           1        1 Linguistics Linguistic  NOUN  NNS\n2    doc1            1           1        2        also       also   ADV   RB\n3    doc1            1           1        3       deals       deal  NOUN  NNS\n4    doc1            1           1        4        with       with   ADP   IN\n5    doc1            1           1        5         the        the   DET   DT\n6    doc1            1           1        6      social     social   ADJ   JJ\n7    doc1            1           1        7           ,          , PUNCT    ,\n8    doc1            1           1        8    cultural   cultural   ADJ   JJ\n9    doc1            1           1        9           ,          , PUNCT    ,\n10   doc1            1           1       10  historical historical   ADJ   JJ\n                       feats head_token_id  dep_rel deps          misc\n1                Number=Plur             3 compound <NA>          <NA>\n2                       <NA>             3   advmod <NA>          <NA>\n3                Number=Plur             0     root <NA>          <NA>\n4                       <NA>            13     case <NA>          <NA>\n5  Definite=Def|PronType=Art            13      det <NA>          <NA>\n6                 Degree=Pos            13     amod <NA> SpaceAfter=No\n7                       <NA>             8    punct <NA>          <NA>\n8                 Degree=Pos             6     conj <NA> SpaceAfter=No\n9                       <NA>            10    punct <NA>          <NA>\n10                Degree=Pos             6     conj <NA>          <NA>\n\n\nIt can be useful to extract only the words and their pos-tags and convert them back into a text format (rather than a tabular format).\n\ntagged_text <- paste(text_anndf$token, \"/\", text_anndf$xpos, collapse = \" \", sep = \"\")\n# inspect tagged text\ntagged_text\n\n[1] \"Linguistics/NNS also/RB deals/NNS with/IN the/DT social/JJ ,/, cultural/JJ ,/, historical/JJ and/CC political/JJ factors/NNS that/WDT influence/VBP language/NN ,/, through/IN which/WDT linguistic/NN and/CC language/NN -/HYPH based/VBN context/NN is/VBZ often/RB determined/JJ ./. Research/VB on/IN language/NN through/IN the/DT sub-branches/NNS of/IN historical/JJ and/CC evolutionary/JJ linguistics/NNS also/RB focus/RB on/IN how/WRB languages/NNS change/VBP and/CC grow/VBP ,/, particularly/RB over/IN an/DT extended/JJ period/NN of/IN time/NN ./. Language/NN documentation/NN combines/VBZ anthropological/JJ inquiry/NN (/-LRB- into/IN the/DT history/NN and/CC culture/NN of/IN language/NN )/-RRB- with/IN linguistic/JJ inquiry/NN ,/, in/IN order/NN to/TO describe/VB languages/NNS and/CC their/PRP$ grammars/NNS ./. Lexicography/NNP involves/VBZ the/DT documentation/NN of/IN words/NNS that/WDT form/VBP a/DT vocabulary/NN ./. Such/PDT a/DT documentation/NN of/IN a/DT linguistic/JJ vocabulary/NN from/IN a/DT particular/JJ language/NN is/VBZ usually/RB compiled/VBN in/IN a/DT dictionary/NN ./. Computational/JJ linguistics/NNS is/VBZ concerned/JJ with/IN the/DT statistical/NN or/CC rule/NN -/HYPH based/VBN modeling/NN of/IN natural/JJ language/NN from/IN a/DT computational/JJ perspective/NN ./. Specific/JJ knowledge/NN of/IN language/NN is/VBZ applied/VBN by/IN speakers/NNS during/IN the/DT act/NN of/IN translation/NN and/CC interpretation/NN ,/, as/RB well/RB as/IN in/IN language/NN education/NN �/$ the/DT teaching/NN of/IN a/DT second/JJ or/CC foreign/JJ language/NN ./. Policy/NN makers/NNS work/VBP with/IN governments/NNS to/TO implement/VB new/JJ plans/NNS in/IN education/NN and/CC teaching/NN which/WDT are/VBP based/VBN on/IN linguistic/JJ research/NN ./.\"\n\n\n\n\nPOS-Tagging non-English texts\nWe can apply the same method for annotating, e.g. adding pos-tags, to other languages. For this, we could train our own model, or, we can use one of the many pre-trained language models that udpipe provides.\nLet us explore how to do this by using example texts from different languages, here from German and Spanish (but we could also annotate texts from any of the wide variety of languages for which UDPipe provides pre-trained models.\nWe begin by loading a German and a Dutch text.\n\n# load texts\ngertext <- readLines(\"https://slcladal.github.io/data/german.txt\") \nduttext <- readLines(\"https://slcladal.github.io/data/dutch.txt\") \n# inspect texts\ngertext; duttext\n\n[1] \"Sprachwissenschaft untersucht in verschiedenen Herangehensweisen die menschliche Sprache.\"\n\n\n[1] \"Taalkunde, ook wel taalwetenschap of linguïstiek, is de wetenschappelijke studie van de natuurlijke talen.\"\n\n\nNext, we install the pre-trained language models.\n\n# download language model\nm_ger   <- udpipe::udpipe_download_model(language = \"german-gsd\")\nm_dut   <- udpipe::udpipe_download_model(language = \"dutch-alpino\")\n\nOr we load them from our machine (if we have downloaded and saved them before).\n\n# load language model from your computer after you have downloaded it once\nm_ger   <- udpipe::udpipe_load_model(file = here::here(\"udpipemodels\", \"german-gsd-ud-2.5-191206.udpipe\"))\nm_dut   <- udpipe::udpipe_load_model(file = here::here(\"udpipemodels\", \"dutch-alpino-ud-2.5-191206.udpipe\"))\n\nNow, pos-tag the German text.\n\n# tokenise, tag, dependency parsing of german text\nger_pos <- udpipe::udpipe_annotate(m_ger, x = gertext) %>%\n  as.data.frame() %>%\n  dplyr::summarise(postxt = paste(token, \"/\", xpos, collapse = \" \", sep = \"\")) %>%\n  dplyr::pull(unique(postxt))\n# inspect\nger_pos\n\n[1] \"Sprachwissenschaft/NN untersucht/VVFIN in/APPR verschiedenen/ADJA Herangehensweisen/NN die/ART menschliche/NN Sprache/NN ./$.\"\n\n\nAnd finally, we also pos-tag the Dutch text.\n\n# tokenise, tag, dependency parsing of german text\nnl_pos <- udpipe::udpipe_annotate(m_dut, x = duttext) %>%\n   as.data.frame() %>%\n  dplyr::summarise(postxt = paste(token, \"/\", xpos, collapse = \" \", sep = \"\")) %>%\n  dplyr::pull(unique(postxt))\n# inspect\nnl_pos\n\n[1] \"Taalkunde/N|soort|ev|basis|zijd|stan ,/LET ook/BW wel/BW taalwetenschap/N|soort|ev|basis|zijd|stan of/VG|neven linguïstiek/N|soort|ev|basis|zijd|stan ,/LET is/WW|pv|tgw|ev de/LID|bep|stan|rest wetenschappelijke/ADJ|prenom|basis|met-e|stan studie/N|soort|ev|basis|zijd|stan van/VZ|init de/LID|bep|stan|rest natuurlijke/ADJ|prenom|basis|met-e|stan talen/N|soort|mv|basis ./LET\"\n\n\n\n\nDependency Parsing Using UDPipe\nIn addition to pos-tagging, we can also generate plots showing the syntactic dependencies of the different constituents of a sentence. For this, we generate an object that contains a sentence (in this case, the sentence Linguistics is the scientific study of language), and we then plot (or visualize) the dependencies using the textplot_dependencyparser function.\n\n# parse text\nsent <- udpipe::udpipe_annotate(m_eng, x = \"Linguistics is the scientific study of language\") %>%\n  as.data.frame()\n# inspect\nhead(sent)\n\n  doc_id paragraph_id sentence_id\n1   doc1            1           1\n2   doc1            1           1\n3   doc1            1           1\n4   doc1            1           1\n5   doc1            1           1\n6   doc1            1           1\n                                         sentence token_id       token\n1 Linguistics is the scientific study of language        1 Linguistics\n2 Linguistics is the scientific study of language        2          is\n3 Linguistics is the scientific study of language        3         the\n4 Linguistics is the scientific study of language        4  scientific\n5 Linguistics is the scientific study of language        5       study\n6 Linguistics is the scientific study of language        6          of\n       lemma upos xpos                                                 feats\n1 Linguistic NOUN  NNS                                           Number=Plur\n2         be  AUX  VBZ Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin\n3        the  DET   DT                             Definite=Def|PronType=Art\n4 scientific  ADJ   JJ                                            Degree=Pos\n5      study NOUN   NN                                           Number=Sing\n6         of  ADP   IN                                                  <NA>\n  head_token_id dep_rel deps misc\n1             5   nsubj <NA> <NA>\n2             5     cop <NA> <NA>\n3             5     det <NA> <NA>\n4             5    amod <NA> <NA>\n5             0    root <NA> <NA>\n6             7    case <NA> <NA>\n\n\nWe now generate the plot.\n\n# generate dependency plot\ndplot <- textplot::textplot_dependencyparser(sent, size = 3) \n# show plot\ndplot\n\n\n\n\n\n\nPOS-Tagging with openNLP\nIn R we can pos–tag large amounts of text not only by using udpipe but by various means. This section explores pos-tagging using the openNLP package. Using the openNLP package for pos-tagging works particularly well when the aim is to pos-tag newspaper texts as the openNLP package implements the Apache OpenNLPMaxent Part of Speech tagger and it comes with pre-trained models. Ideally, pos-taggers should be trained on data resembling the data to be pos-tagged.However, I do not know how to trained the Apache openNLP pos-tagger via R and it would be great if someone would provide a tutorial on how to do that. Using pre-trained models has the advantage that we do not need to train the pos-tagger ourselves. However, it also means that one has to rely on models trained on data that may not really resemble the data a at hand.This implies that using it for texts that differ from newspaper texts, i.e.the language the models have been trained on, does not work as well, as the model applies the probabilities of newspaper language to the language variety at hand. pos-tagging with the openNLP requires the NLP package and installing the models on which the openNLP package is based.\nTo pos-tag a text, we start by loading an example text into R.\n\n# load corpus data\ntext <- readLines(\"https://slcladal.github.io/data/testcorpus/linguistics07.txt\", skipNul = T)\n# clean data\ntext <- text %>%\n str_squish() \n\n\n\n\nNow that the text data has been read into R, we can proceed with the part-of-speech tagging. To perform the pos-tagging, we load the function for pos-tagging, load the NLP and openNLP packages.\n\n\n\n\nNOTEYou need to change the path that is used in the code below and include the path to en-pos-maxent.bin on your computer!\n\n\n\n\n\n\n\nPOStag <- function(object){\n  require(\"stringr\")\n  require(\"NLP\")\n  require(\"openNLP\")\n  require(\"openNLPdata\")\n  # define paths to corpus files\n  corpus.tmp <- object\n  # define sentence annotator\n  sent_token_annotator <- openNLP::Maxent_Sent_Token_Annotator()\n  # define word annotator\n  word_token_annotator <- openNLP::Maxent_Word_Token_Annotator()\n  # define pos annotator\n  pos_tag_annotator <- openNLP::Maxent_POS_Tag_Annotator(language = \"en\", probs = FALSE, model = NULL)\n    # WARNING: YOU NEED TO INCLUDE YOUR OWN PATH HERE!                                            \n    # model = \"C:\\\\Users\\\\marti\\\\OneDrive\\\\Dokumente\\\\R\\\\win-library\\\\4.1\\\\openNLPdata\\\\models\\\\en-pos-maxent.bin\")\n  # convert all file content to strings\n  Corpus <- lapply(corpus.tmp, function(x){\n    x <- as.String(x)  }  )\n  # loop over file contents\n  lapply(Corpus, function(x){\n    y1 <- NLP::annotate(x, list(sent_token_annotator, word_token_annotator))\n    y2<- NLP::annotate(x, pos_tag_annotator, y1)\n    y2w <- subset(y2, type == \"word\")\n    tags <- sapply(y2w$features, '[[', \"POS\")\n    r1 <- sprintf(\"%s/%s\", x[y2w], tags)\n    r2 <- paste(r1, collapse = \" \")\n    return(r2)  }  )\n  }\n\nWe now apply this function to our text.\n\n# pos tagging data\ntextpos <- POStag(object = text)\n\n\n\n\nThe resulting vector contains the part-of-speech tagged text and shows that the function fulfills its purpose in automatically pos-tagging the text. The pos-tagged text could now be processed further, e.g. by extracting all adjectives in the text or by creating concordances of nouns ending in ment.\n\n\nSyntactic Parsing with openNLP\nParsing refers to another type of annotation in which either structural information (as in the case of XML documents) or syntactic relations are added to text. As syntactic parsing is commonly more relevant in the language sciences, the following will focus only on syntactic parsing. syntactic parsing builds on pos-tagging and allows drawing syntactic trees or dependencies. Unfortunately, syntactic parsing still has relatively high error rates when dealing with language that is not very formal. However, syntactic parsing is very reliable when dealing with written language.\n\ntext <- readLines(\"https://slcladal.github.io/data/english.txt\")\n# convert character to string\ns <- as.String(text)\n# define sentence and word token annotator\nsent_token_annotator <- openNLP::Maxent_Sent_Token_Annotator()\nword_token_annotator <- openNLP::Maxent_Word_Token_Annotator()\n# apply sentence and word annotator\na2 <- NLP::annotate(s, list(sent_token_annotator, word_token_annotator))\n# define syntactic parsing annotator\nparse_annotator <- openNLP::Parse_Annotator()\n# apply parser\np <- parse_annotator(s, a2)\n# extract parsed information\nptexts <- sapply(p$features, '[[', \"parse\")\nptexts\n\n\n# read into NLP Tree objects.\nptrees <- lapply(ptexts, Tree_parse)\n# show frist tree\nptrees[[1]]\n\nThese trees can, of course, also be shown visually, for instance, in the form of a syntax trees (or tree dendrogram).\n\n# load function\nsource(\"https://slcladal.github.io/rscripts/parsetgraph.R\")\n# generate syntax tree\nparse2graph(ptexts[1], leaf.color='red',\n            # to put sentence in title (not advisable for long sentences)\n            #title = stringr::str_squish(stringr::str_remove_all(ptexts[1], \"\\\\(\\\\,{0,1}[A-Z]{0,4}|\\\\)\")), \n            margin=-0.05,\n            vertex.color=NA,\n            vertex.frame.color=NA, \n            vertex.label.font=2,\n            vertex.label.cex=.75,  \n            asp=.8,\n            edge.width=.5, \n            edge.color='gray', \n            edge.arrow.size=0)\n\nSyntax trees are very handy because the allow us to check how reliable the parser performed.\nWe can use the get_phrase_type_regex function from the parsent package written by Tyler Rinker to extract phrases from the parsed tree.\n\npacman::p_load_gh(c(\"trinker/textshape\", \"trinker/parsent\"))\nnps <- get_phrase_type_regex(ptexts[1], \"NP\") %>%\n  unlist()\n# inspect\nnps\n\nWe can now extract the leaves from the text to get the parsed object.\n\nnps_text <- stringr::str_squish(stringr::str_remove_all(nps, \"\\\\(\\\\,{0,1}[A-Z]{0,4}|\\\\)\"))\n# inspect\nnps_text\n\nUnfortunately, we can only extract top level phrases (the NPs with the NPs are not extracted separately).\nIn order to extract all phrases, we can use the phrasemachine from the CRAN archive.\nWe now load the phrasemachine package and pos-tag the text(s) (we will simply re-use the English text we pos-tagged before.)\n\n# pos tag text\ntagged_documents <- phrasemachine::POS_tag_documents(text)\n# inspect\ntagged_documents\n\nIn a next step, we can use the extract_phrases function to extract phrases.\n\n#extract phrases\nphrases <- phrasemachine::extract_phrases(tagged_documents,\n                                          regex = \"(A|N)*N(PD*(A|N)*N)*\",\n                                          maximum_ngram_length = 8,\n                                          minimum_ngram_length = 1)\n# inspect\nphrases\n\nNow, we have all noun phrases that occur in the English sample text.\nThat’s it for this tutorial. We hope that you have enjoyed this tutorial and learned how to annotate texts using language models and perform pos-tagging and dependency parsing of English texts as well as texts in other languages.\n\n\nCitation & Session Info\nSchweinberger, Martin. 2022. POS-Tagging and Syntactic Parsing with R. Brisbane: The University of Queensland. url: https://slcladal.github.io/tagging.html (Version 2022.08.31).\n@manual{schweinberger2022pos,\n  author = {Schweinberger, Martin},\n  title = {pos-Tagging and Syntactic Parsing with R},\n  note = {https://slcladal.github.io/tagging.html},\n  year = {2022},\n  organization = \"The University of Queensland, School of Languages and Cultures},\n  address = {Brisbane},\n  edition = {2022.08.31}\n}\n\nsessionInfo()\n\nR version 4.2.1 (2022-06-23)\nPlatform: x86_64-pc-linux-gnu (64-bit)\nRunning under: Ubuntu 22.04.1 LTS\n\nMatrix products: default\nBLAS:   /usr/lib/x86_64-linux-gnu/blas/libblas.so.3.10.0\nLAPACK: /usr/lib/x86_64-linux-gnu/lapack/liblapack.so.3.10.0\n\nlocale:\n [1] LC_CTYPE=en_AU.UTF-8       LC_NUMERIC=C              \n [3] LC_TIME=en_AU.UTF-8        LC_COLLATE=en_AU.UTF-8    \n [5] LC_MONETARY=en_AU.UTF-8    LC_MESSAGES=en_AU.UTF-8   \n [7] LC_PAPER=en_AU.UTF-8       LC_NAME=C                 \n [9] LC_ADDRESS=C               LC_TELEPHONE=C            \n[11] LC_MEASUREMENT=en_AU.UTF-8 LC_IDENTIFICATION=C       \n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] udpipe_0.8.9    stringr_1.4.0   dplyr_1.0.9     flextable_0.7.3\n\nloaded via a namespace (and not attached):\n [1] ggrepel_0.9.1      Rcpp_1.0.8.3       here_1.0.1         lattice_0.20-45   \n [5] tidyr_1.2.0        assertthat_0.2.1   rprojroot_2.0.3    digest_0.6.29     \n [9] utf8_1.2.2         ggforce_0.3.3      R6_2.5.1           evaluate_0.15     \n[13] ggplot2_3.3.6      pillar_1.7.0       gdtools_0.2.4      rlang_1.0.4       \n[17] uuid_1.1-0         rstudioapi_0.13    data.table_1.14.2  Matrix_1.4-1      \n[21] textplot_0.2.2     rmarkdown_2.14     labeling_0.4.2     htmlwidgets_1.5.4 \n[25] igraph_1.3.2       polyclip_1.10-0    munsell_0.5.0      compiler_4.2.1    \n[29] xfun_0.31          pkgconfig_2.0.3    systemfonts_1.0.4  base64enc_0.1-3   \n[33] htmltools_0.5.2    tidyselect_1.1.2   gridExtra_2.3      tibble_3.1.7      \n[37] graphlayouts_0.8.0 viridisLite_0.4.0  fansi_1.0.3        crayon_1.5.1      \n[41] MASS_7.3-58.1      grid_4.2.1         jsonlite_1.8.0     gtable_0.3.0      \n[45] lifecycle_1.0.1    DBI_1.1.3          magrittr_2.0.3     scales_1.2.0      \n[49] zip_2.2.0          cli_3.3.0          stringi_1.7.8      farver_2.1.1      \n[53] viridis_0.6.2      xml2_1.3.3         ellipsis_0.3.2     generics_0.1.3    \n[57] vctrs_0.4.1        tools_4.2.1        glue_1.6.2         officer_0.4.3     \n[61] tweenr_1.0.2       purrr_0.3.4        ggraph_2.0.5       fastmap_1.1.0     \n[65] yaml_2.3.5         colorspace_2.0-3   tidygraph_1.2.1    knitr_1.39        \n\n\n\nBack to top\nBack to HOME\n\n\n\n\n\n\n\n\n\nReferences\n\n1. Wiedemann, G., Niekler, A.: Hands-on: A five day text mining course for humanists and social scientists in R. In: Proceedings of the workshop on teaching NLP for digital humanities (Teach4DH), berlin, germany, september 12, 2017. pp. 57–65 (2017).\n\n\n2. Kumar, A., Paul, A.: Mastering text mining with r. Packt Publishing Ltd (2016).\n\n\n3. Wijffels, J.: Udpipe: Tokenization, parts of speech tagging, lemmatization and dependency parsing with the ’UDPipe’ ’NLP’ toolkit. (2021).\n\n\n4. Hornik, K.: openNLP: Apache OpenNLP tools interface. (2019)."
  },
  {
    "objectID": "testingrf.html",
    "href": "testingrf.html",
    "title": "How to test if including a random effect is warranted?",
    "section": "",
    "text": "Generate data\nCreate a constant and use it as a random effect, then create two models: one model with the constant as a random effect and another model with variable as random effect. then use anova to compare the models (see below).\n\nset.seed(101)\ndataset <- expand.grid(id = factor(seq_len(10)), \n                       fac1 = factor(c(\"A\", \"B\"), levels = c(\"A\", \"B\")), \n                       trial = seq_len(10))\ndataset$value <- rnorm(nrow(dataset), sd = 0.5) +\n      with(dataset, rnorm(length(levels(id)), sd = 0.5)[id] +\n      ifelse(fac1 == \"B\", 1.0, 0)) + rnorm(1,.5)\ndataset$idconst <- factor(rep(1, each = length(dataset$value)))\n# inspect data\nhead(dataset)\n\n  id fac1 trial    value idconst\n1  1    A     1 1.791537       1\n2  2    A     1 1.621164       1\n3  3    A     1 1.910855       1\n4  4    A     1 1.748506       1\n5  5    A     1 2.796918       1\n6  6    A     1 3.070813       1\n\n\n\n\nGenerate and compare models\n\nlibrary(lme4)\nfm0 <- lmer(value~fac1+(1|id), \n            data = dataset, \n            REML = FALSE)\nfm1 <- lmer(value~fac1+(1|idconst), \n            data = dataset, \n            control=lmerControl(check.nlev.gtr.1=\"ignore\"), \n            REML = FALSE)\n# compare models\nanova(fm1,fm0)\n\nData: dataset\nModels:\nfm1: value ~ fac1 + (1 | idconst)\nfm0: value ~ fac1 + (1 | id)\n    npar    AIC    BIC  logLik deviance  Chisq Df Pr(>Chisq)\nfm1    4 369.42 382.62 -180.71   361.42                     \nfm0    4 309.74 322.94 -150.87   301.74 59.682  0           \n\n\nAccording to the model comparison, the random effect should be included as the fm0 model has the lower AIC as well as BIC.\n\n\n\n\n\n\n\n\nReferences\n\n1. Galwey, N.W.: Introduction to mixed modelling: Beyond regression and analysis of variance. John Wiley & Sons (2007).\n\n\n2. Galwey, N.W.: Introduction to mixed modelling: Beyond regression and analysis of variance. John Wiley & Sons (2014)."
  },
  {
    "objectID": "textanalysis.html#wordclouds",
    "href": "textanalysis.html#wordclouds",
    "title": "Practical Overview of Selected Text Analytics Methods",
    "section": "Wordclouds",
    "text": "Wordclouds\nAlternatively, word frequency lists can be visualized, although less informative, as word clouds.\n\n# create wordcloud\n#wordcloud2(wfreq_wostop[1:100,],\n#           shape = \"diamond\",\n#           color = scales::viridis_pal()(8)\n#          )\n\nAnother variant of word clouds, so-called comparison clouds, Word lists can be used to determine differences between texts. For instance, we can load different texts and check whether they differ with respect to word frequencies. To show this, we load Herman Melville’s Moby Dick, George Orwell’s 1984, and we also use Darwin’s Origin.\nIn a first step, we load these texts and collapse them into single documents.\n\n# load data\norwell_sep <- base::readRDS(url(\"https://slcladal.github.io/data/orwell.rda\", \"rb\"))\norwell <- orwell_sep %>%\n  paste0(collapse = \" \")\nmelville_sep <- base::readRDS(url(\"https://slcladal.github.io/data/melville.rda\", \"rb\"))\nmelville <- melville_sep %>%\n  paste0(collapse = \" \")\ndarwin_sep <- darwin\ndarwin <- paste0(darwin_sep, collapse = \" \")\n\nNow, we generate a corpus object from these texts and create a variable with the author name.\n\ncorp_dom <- quanteda::corpus(c(darwin, orwell, melville)) \nattr(corp_dom, \"docvars\")$Author = c(\"Darwin\", \"Orwell\", \"Melville\")\n\nNow, we can remove so-called stopwords (non-lexical function words) and punctuation and generate the comparison cloud.\n\ncorp_dom  %>%\n    quanteda::tokens(remove_punct = TRUE) %>%\n    quanteda::tokens_remove(stopwords(\"english\")) %>%\n    quanteda::dfm() %>%\n    quanteda::dfm_group(groups = corp_dom$Author) %>%\n    quanteda::dfm_trim(min_termfreq = 200, verbose = FALSE) %>%\n    quanteda.textplots::textplot_wordcloud(comparison = TRUE, \n                                 max_words = 100,\n                                 max_size = 6)"
  },
  {
    "objectID": "textanalysis.html#frequency-changes",
    "href": "textanalysis.html#frequency-changes",
    "title": "Practical Overview of Selected Text Analytics Methods",
    "section": "Frequency changes",
    "text": "Frequency changes\nWe can also investigate the use of the term organism across chapters in Darwin’s Origin. In a first step, we extract the number of words in each chapter.\n\n# extract number of words per chapter\nWords <- darwin_chapters %>%\n  stringr::str_split(\" \")  %>%\n  lengths()\n# inspect data\nWords\n\n [1]  1856 14065  7456  7136 22317 13916 17781 19055 15847 14741 13313 12996\n[13] 13753  9817 20967 12987\n\n\nNext, we extract the number of matches in each chapter.\n\n# extract number of matches per chapter\nMatches <- darwin_chapters %>%\n  stringr::str_count(\"organism[s]{0,1}\")\n# inspect the number of matches per chapter\nMatches\n\n [1]  0  5  3  3  9  3  3  3  0  1  6  6 10  5  8  7\n\n\nNow, we extract the names of the chapters and create a table with the chapter names and the relative frequency of matches per 1,000 words.\n\n# extract chapters\nChapters <- darwin_chapters %>%\n  stringr::str_replace_all(\"(chapter [xvi]{1,7})\\\\.{0,1} .*\", \"\\\\1\")\nChapters <- dplyr::case_when(nchar(Chapters) > 50 ~ \"chapter 0\", TRUE ~ Chapters)\nChapters\n\n [1] \"chapter 0\"    \"chapter i\"    \"chapter ii\"   \"chapter iii\"  \"chapter iv\"  \n [6] \"chapter v\"    \"chapter vi\"   \"chapter vii\"  \"chapter viii\" \"chapter ix\"  \n[11] \"chapter x\"    \"chapter xi\"   \"chapter xii\"  \"chapter xiii\" \"chapter xiv\" \n[16] \"chapter xv\"  \n\n\n\n# create table of results\ntb <- data.frame(Chapters, Matches, Words) %>%\n  dplyr::mutate(Frequency = round(Matches/Words*1000, 2))\n\n\n\n\n\n\nWords and their (relative) freqeuncy across in Charles Darwin’s Origin by frequency.\n\n\nChaptersMatchesWordsFrequencychapter 001,8560.00chapter i514,0650.36chapter ii37,4560.40chapter iii37,1360.42chapter iv922,3170.40chapter v313,9160.22chapter vi317,7810.17chapter vii319,0550.16chapter viii015,8470.00chapter ix114,7410.07chapter x613,3130.45chapter xi612,9960.46chapter xii1013,7530.73chapter xiii59,8170.51chapter xiv820,9670.38\n\n\nWe can now visualize the relative frequencies of our search word per chapter.\n\n# create plot\nggplot(tb, aes(x = Chapters, y = Frequency, group = 1)) + \n  geom_smooth(color = \"purple\") +\n  geom_line(color = \"darkgray\") +         \n  guides(color=guide_legend(override.aes=list(fill=NA))) +\n  theme_bw() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))+\n  scale_y_continuous(name =\"Relative Frequency (per 1,000 words)\")"
  },
  {
    "objectID": "textanalysis.html#dispersion-plots",
    "href": "textanalysis.html#dispersion-plots",
    "title": "Practical Overview of Selected Text Analytics Methods",
    "section": "Dispersion plots",
    "text": "Dispersion plots\nTo show when in a text or in a collection of texts certain terms occur, we can use dispersion plots. The quanteda package offers a very easy-to-use function textplot_xray to generate dispersion plots.\n\n# add chapter names\nnames(darwin_chapters) <- Chapters\n# generate corpus from chapters\ndarwin_corpus <- quanteda::corpus(darwin_chapters)\n# generate dispersion plots\nquanteda.textplots::textplot_xray(kwic(darwin_corpus, pattern = \"organism\"),\n                                  kwic(darwin_corpus, pattern = \"selection\"),\n                                  sort = T)\n\n\n\n\nWe can modify the plot by saving it into an object and then use ggplot to modify it appearance.\n\n# generate and save dispersion plots\ndp <- quanteda.textplots::textplot_xray(kwic(darwin_corpus, pattern = \"organism\"),\n                                        kwic(darwin_corpus, pattern = \"selection\"))\n# modify plot\ndp + aes(color = keyword) + \n  scale_color_manual(values = c('red', 'blue')) +\n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "textanalysis.html#over--and-underuse",
    "href": "textanalysis.html#over--and-underuse",
    "title": "Practical Overview of Selected Text Analytics Methods",
    "section": "Over- and underuse",
    "text": "Over- and underuse\nFrequency information can also tell us something about the nature of a text. For instance, private dialogues will typically contain higher rates of second person pronouns compared with more format text types, such as, for instance, scripted monologues like speeches. For this reason, word frequency lists can be used in text classification and to determine the formality of texts.\nAs an example, below you find the number of the second person pronouns you and your and the number of all words except for these second person pronouns in private dialogues compared with scripted monologues in the Irish component of the International Corpus of English (ICE). In addition, the tables shows the percentage of second person pronouns in both text types to enable seeing whether private dialogues contain more of these second person pronouns than scripted monologues (i.e. speeches).\n\n\n\n\n\n\n\n\nUse of 2nd person pronouns (and all other words) in ICE Ireland.\n\n\n.Private dialoguesScripted monologuesyou, your6761659Other words259625105295Percent2.600.63\n\n\nThis simple example shows that second person pronouns make up 2.6 percent of all words that are used in private dialogues while they only amount to 0.63 percent in scripted speeches. A handy way to present such differences visually are association and mosaic plots.\n\nd <- matrix(c(6761, 659, 259625, 105295), nrow = 2, byrow = T)\ncolnames(d) <- c(\"D\", \"M\")\nrownames(d) <- c(\"you, your\", \"Other words\")\nassocplot(d)\n\n\n\n\nBars above the dashed line indicate relative overuse while bars below the line suggest relative under-use. Therefore, the association plot indicates under-use of you and your and overuse of other words in monologues while the opposite trends holds true for dialogues, i.e. overuse of you and your and under-use of Other words."
  },
  {
    "objectID": "textanalysis.html#finding-collocations",
    "href": "textanalysis.html#finding-collocations",
    "title": "Practical Overview of Selected Text Analytics Methods",
    "section": "Finding collocations",
    "text": "Finding collocations\nThere are various techniques for identifying collocations. To identify collocations without having a pre-defined target term, we can use the textstat_collocations function from the quanteda.textstats package.\nHowever, before we can apply that function and start identifying collocations, we need to process the data to which we want to apply this function. In the present case, we will apply that function to the sentences in Charles Darwin’s Origin which we extract in the code chunk below.\n\ndarwin_sentences <- darwin %>%\n  tolower() %>%\n  paste0(collapse= \" \") %>%\n  stringr::str_split(fixed(\".\")) %>%\n  unlist() %>%\n  tm::removePunctuation() %>%\n  stringr::str_squish()\n\n\n\n\n\n\nFirst 10 sentences in Charles Darwin’s Origin.\n\n\n.the origin of species by charles darwin an historical sketch of the progress of opinion on the origin of species introduction when on board hmsbeagle as naturalist i was much struck with certain facts in the distribution of the organic beings in habiting south america and in the geological relations of the present to the past inhabitants of that continentthese facts as will be seen in the latter chapters of this volume seemed to throw some light on the origin of species — that mystery of mysteries as it has been called by one of our greatest philosopherson my return home it occurred to me in 1837 that something might perhaps be made out on this question by patiently accumulating and reflecting on all sorts of facts which could possibly have any bearing on itafter five years work i allowed myself to specu late on the subject and drew up some short notes these i enlarged in 1844 into a sketch of the conclusions which then seemed to me probable from that period to the present day i have steadily pursued the same objecti hope that i may be excused for entering on these personal details as i give them to show that i have not been hasty in coming to a decisionmy work is now 1859 nearly finished but as it will take me many more years to complete it and as my health is far from strong i have been urged to publish this abstracti have more especially been induced to do this as mrwallace who is now studying the natural history of the malay archipelago has arrived at almost exactly the same general conclusions that i have on the origin of speciesin 1858 he sent me a memoir on this subject with a request that i would forward it to sir charles lyell who sent it to the linnean society and it is published in the third volume of the journal of that societysir clyell and drhooker who both knew of my work— the latter having read my sketch of 1844 — honoured me by thinking it advisable to publish with mr\n\n\nFrom the output shown above, we also see that splitting texts simply by full stops is not optimal as it produces some unwarranted artifacts like the “sentences” that consist of single characters (due to the name of the H.M.S. Beagle - the ship on which Darwin traveled when he explored the southern hemisphere). Fortunately, these errors do not really matter in the case of our example.\nNow that we have split Darwin’s Origin into sentences, we can tokenize these sentences and apply the textstat_collocations function which identifies collocations.\n\n# create a token object\ndarwin_tokens <- tokens(darwin_sentences, remove_punct = TRUE) %>%\n  tokens_remove(stopwords(\"english\"))\n# extract collocations\ndarwin_coll <- textstat_collocations(darwin_tokens, size = 2, min_count = 20)\n\n\n\n\n\n\nTop 10 collocations in Charles Darwin’s Origin.\n\n\ncollocationcountcount_nestedlengthlambdaznatural selection367027.44266257.36943conditions life107025.80767139.91801organic beings105028.17932337.34842closely allied64026.56156233.85212south america41027.41545028.81440widely different49025.31777428.29759modified descendants40026.09297427.68700state nature45025.33128927.38031theory natural54024.86430427.35902distinct species102023.30734726.97796north america31027.09120426.01263forms life57023.84455825.11312struggle existence27026.88921325.08157individual differences30025.92267424.97534united states29028.33014624.94125\n\n\nThe resulting table shows collocations in Darwin’s Origin descending by collocation strength."
  },
  {
    "objectID": "textanalysis.html#visualizing-collocation-networks",
    "href": "textanalysis.html#visualizing-collocation-networks",
    "title": "Practical Overview of Selected Text Analytics Methods",
    "section": "Visualizing Collocation Networks",
    "text": "Visualizing Collocation Networks\nNetwork graphs are a very useful and flexible tool for visualizing relationships between elements such as words, personas, or authors. This section shows how to generate a network graph for collocations of the term organism using the quanteda package.\nIn a first step, we generate a document-feature matrix based on the sentence sin Charles Darwin’s Origin. A document-feature matrix shows how often elements (here these elements are the words that occur in the Origin) occur in a selection of documents (here these documents are the sentences in the Origin).\n\n# create document-feature matrix\ndarwin_dfm <- darwin_sentences %>% \n    quanteda::dfm(remove = stopwords('english'), remove_punct = TRUE) %>%\n    quanteda::dfm_trim(min_termfreq = 10, verbose = FALSE)\n\n\n\n\n\n\nFirst 6 rows and columns of the document-feature matrix.\n\n\ndoc_idoriginspeciescharlesprogressopinionhtext1221111text2000000text3000000text4000000text5110000text6000000\n\n\nAs we want to generate a network graph of words that collocate with the term organism, we use the calculateCoocStatistics function to determine which words most strongly collocate with our target term (organism).\n\n# load function for co-occurrence calculation\nsource(\"https://slcladal.github.io/rscripts/calculateCoocStatistics.R\")\n# define term\ncoocTerm <- \"organism\"\n# calculate co-occurrence statistics\ncoocs <- calculateCoocStatistics(coocTerm, darwin_dfm, measure=\"LOGLIK\")\n# inspect results\ncoocs[1:20]\n\n conditions      nature    relation   relations    physical      action \n  56.666499   34.485146   25.478537   22.836815   21.740633   20.608922 \n       life     bearing   important        many      direct   colonists \n  18.807464   16.246065   15.812686   13.488588   12.705077   11.300306 \ncompetition      nearly     welfare     require     altered   different \n  11.140540   10.945131   10.886868   10.514045   10.174698    9.774481 \nimprovement     rapidly \n   9.576033    9.576033 \n\n\nWe now reduce the document-feature matrix to contain only the top 20 collocates of organism (plus our target word organism).\n\nredux_dfm <- dfm_select(darwin_dfm, \n                        pattern = c(names(coocs)[1:20], \"organism\"))\n\n\n\n\n\n\nFirst 6 rows and columns of the reduced feature co-occurrence matrix.\n\n\ndoc_idrelationsbearingnearlymanyconditionsimportanttext1000000text2000000text3000000text4100000text5000000text6010000\n\n\nNow, we can transform the document-feature matrix into a feature-co-occurrence matrix as shown below. A feature-co-occurrence matrix shows how often each element in that matrix co-occurs with every other element in that matrix.\n\ntag_fcm <- fcm(redux_dfm)\n\n\n\n\n\n\nFirst 6 rows and columns of the feature co-occurrence matrix.\n\n\ndoc_idrelationsbearingnearlymanyconditionsimportantrelations3226116bearing011321nearly00723297many000513427conditions00002718important000009\n\n\nUsing the feature-co-occurrence matrix, we can generate the network graph which shows the terms that collocate with the target term organism with the edges representing the co-occurrence frequency. To generate this network graph, we use the textplot_network function from the quanteda.textplots package.\n\n# generate network graph\ntextplot_network(tag_fcm, \n                 min_freq = 1, \n                 edge_alpha = 0.1, \n                 edge_size = 5,\n                 edge_color = \"purple\",\n                 vertex_labelsize = log(rowSums(tag_fcm))*2)"
  },
  {
    "objectID": "textanalysis.html#keyness",
    "href": "textanalysis.html#keyness",
    "title": "Practical Overview of Selected Text Analytics Methods",
    "section": "Keyness",
    "text": "Keyness\nAnother common method that can be used for automated text summarization is keyword extraction. Keyword extraction builds on identifying words that are particularly associated with a certain text. In other words, keyness analysis aims to identify words that are particularly indicative of the content of a certain text.\nBelow, we identify key words for Charles Darwin’s Origin, Herman Melville’s Moby Dick, and George Orwell’s 1984. We start by creating a weighted document feature matrix from the corpus containing the three texts.\nIn order to create a corpus, we use the text objects that consist out of many different elements rather than the objects which contained the collapsed texts that we used above. Thus, in a first step, we create a corpus of the texts.\n\ncorp_dom <- quanteda::corpus(c(darwin_sep, orwell_sep, melville_sep)) \nattr(corp_dom, \"docvars\")$Author = c(rep(\"Darwin\", length(darwin_sep)), \n                                     rep(\"Orwell\", length(orwell_sep)),\n                                     rep(\"Melville\", length(melville_sep)))\n\nNext, we generate the document feature matrix and we clean it by removing stopwords and selected other words. In addition, we group the documents feature matrix by author.\n\ndfm_authors <- corp_dom %>%\n  quanteda::tokens(remove_punct = TRUE) %>%\n  quanteda::tokens_remove(quanteda::stopwords(\"english\")) %>%\n  quanteda::tokens_remove(c(\"now\", \"one\", \"like\", \"may\", \"can\")) %>%\n  quanteda::dfm() %>%\n  quanteda::dfm_group(groups = Author) %>%\n  quanteda::dfm_weight(scheme = \"prop\")\n\nIn a next step, we use the textstat_frequency function from the quanteda package to extract the most frequent non-stopwords in the three texts.\n\n# Calculate relative frequency by president\nfreq_weight <- quanteda.textstats::textstat_frequency(dfm_authors, \n                                                      n = 10,\n                                                      groups = dfm_authors$Author)\n\n\n\n\n\n\nMost common words across three texts.\n\n\nfeaturefrequencyrankdocfreqgroupspecies0.01808439411Darwinmany0.00605904521Darwinforms0.00539955731Darwinnatural0.00524498941Darwinselection0.00520377151Darwintwo0.00453397961Darwinvarieties0.00421454071Darwinplants0.00402905981Darwinthus0.00392601491Darwinanimals0.003689010101Darwinwhale0.00860250111Melvilleupon0.00537056921Melvilleman0.00427727531Melvilleold0.00421014341Melvilleahab0.00413342051Melville\n\n\nNow, we can simply plot the most common words and most indicative non-stop words in the three texts.\n\nggplot(freq_weight, aes(nrow(freq_weight):1, frequency)) +\n     geom_point() +\n     facet_wrap(~ group, scales = \"free\") +\n     coord_flip() +\n     scale_x_continuous(breaks = nrow(freq_weight):1,\n                        labels = freq_weight$feature) +\n     labs(x = NULL, y = \"Relative frequency\")"
  },
  {
    "objectID": "topicmodels.html#preparation-and-session-set-up",
    "href": "topicmodels.html#preparation-and-session-set-up",
    "title": "Topic Modeling with R",
    "section": "Preparation and session set up",
    "text": "Preparation and session set up\nThis tutorial is based on R. If you have not installed R or are new to it, you will find an introduction to and more information how to use R here. For this tutorials, we need to install certain packages from an R library so that the scripts shown below are executed without errors. Before turning to the code below, please install the packages by running the code below this paragraph. If you have already installed the packages mentioned below, then you can skip ahead and ignore this section. To install the necessary packages, simply run the following code - it may take some time (between 1 and 5 minutes to install all of the packages so you do not need to worry if it takes some time).\n\n# install packages\ninstall.packages(\"tm\")\ninstall.packages(\"topicmodels\")\ninstall.packages(\"reshape2\")\ninstall.packages(\"ggplot2\")\ninstall.packages(\"wordcloud\")\ninstall.packages(\"pals\")\ninstall.packages(\"SnowballC\")\ninstall.packages(\"lda\")\ninstall.packages(\"ldatuning\")\ninstall.packages(\"kableExtra\")\ninstall.packages(\"DT\")\ninstall.packages(\"flextable\")\n# install klippy for copy-to-clipboard button in code chunks\ninstall.packages(\"remotes\")\nremotes::install_github(\"rlesur/klippy\")\n\nNext, we activate the packages.\n\n# set options\noptions(stringsAsFactors = F)         # no automatic data transformation\noptions(\"scipen\" = 100, \"digits\" = 4) # suppress math annotation\n# load packages\nlibrary(knitr) \nlibrary(kableExtra) \nlibrary(DT)\nlibrary(tm)\nlibrary(topicmodels)\nlibrary(reshape2)\nlibrary(ggplot2)\nlibrary(wordcloud)\nlibrary(pals)\nlibrary(SnowballC)\nlibrary(lda)\nlibrary(ldatuning)\nlibrary(flextable)\n# activate klippy for copy-to-clipboard button\nklippy::klippy()\n\n\n\n\nOnce you have installed R and RStudio and once you have initiated the session by executing the code shown above, you are good to go."
  },
  {
    "objectID": "topicmodels.html#model-calculation",
    "href": "topicmodels.html#model-calculation",
    "title": "Topic Modeling with R",
    "section": "Model calculation",
    "text": "Model calculation\nAfter the preprocessing, we have two corpus objects: processedCorpus, on which we calculate an LDA topic model [3]. To this end, stopwords, i.e. function words that have relational rather than content meaning, were removed, words were stemmed and converted to lowercase letters and special characters were removed. The second corpus object corpus serves to be able to view the original texts and thus to facilitate a qualitative control of the topic model results.\nWe now calculate a topic model on the processedCorpus. For this purpose, a DTM of the corpus is created. In this case, we only want to consider terms that occur with a certain minimum frequency in the body. This is primarily used to speed up the model calculation.\n\n# compute document term matrix with terms >= minimumFrequency\nminimumFrequency <- 5\nDTM <- DocumentTermMatrix(processedCorpus, control = list(bounds = list(global = c(minimumFrequency, Inf))))\n# have a look at the number of documents and terms in the matrix\ndim(DTM)\n\n[1] 8833 4278\n\n# due to vocabulary pruning, we have empty rows in our DTM\n# LDA does not like this. So we remove those docs from the\n# DTM and the metadata\nsel_idx <- slam::row_sums(DTM) > 0\nDTM <- DTM[sel_idx, ]\ntextdata <- textdata[sel_idx, ]\n\nAs an unsupervised machine learning method, topic models are suitable for the exploration of data. The calculation of topic models aims to determine the proportionate composition of a fixed number of topics in the documents of a collection. It is useful to experiment with different parameters in order to find the most suitable parameters for your own analysis needs.\nFor parameterized models such as Latent Dirichlet Allocation (LDA), the number of topics K is the most important parameter to define in advance. How an optimal K should be selected depends on various factors. If K is too small, the collection is divided into a few very general semantic contexts. If K is too large, the collection is divided into too many topics of which some may overlap and others are hardly interpretable.\nAn alternative to deciding on a set number of topics is to extract parameters form a models using a rage of number of topics. This approach can be useful when the number of topics is not theoretically motivated or based on closer, qualitative inspection of the data. In the example below, the determination of the optimal number of topics follows [4], but we only use two metrics (CaoJuan2009 and Deveaud2014) - it is highly recommendable to inspect the results of the four metrics available for the FindTopicsNumber function (Griffiths2004, CaoJuan2009, Arun2010, and Deveaud2014).\n\n# create models with different number of topics\nresult <- ldatuning::FindTopicsNumber(\n  DTM,\n  topics = seq(from = 2, to = 20, by = 1),\n  metrics = c(\"CaoJuan2009\",  \"Deveaud2014\"),\n  method = \"Gibbs\",\n  control = list(seed = 77),\n  verbose = TRUE\n)\n\nfit models... done.\ncalculate metrics:\n  CaoJuan2009... done.\n  Deveaud2014... done.\n\n\nWe can now plot the results. In this case, we have only use two methods CaoJuan2009 and Griffith2004. The best number of topics shows low values for CaoJuan2009 and high values for Griffith2004 (optimally, several methods should converge and show peaks and dips respectively for a certain number of topics).\n\nFindTopicsNumber_plot(result)\n\n\n\n\nFor our first analysis, however, we choose a thematic “resolution” of K = 20 topics. In contrast to a resolution of 100 or more, this number of topics can be evaluated qualitatively very easy.\n\n# number of topics\nK <- 20\n# set random number generator seed\nset.seed(9161)\n# compute the LDA model, inference via 1000 iterations of Gibbs sampling\ntopicModel <- LDA(DTM, K, method=\"Gibbs\", control=list(iter = 500, verbose = 25))\n\nK = 20; V = 4278; M = 8810\nSampling 500 iterations!\nIteration 25 ...\nIteration 50 ...\nIteration 75 ...\nIteration 100 ...\nIteration 125 ...\nIteration 150 ...\nIteration 175 ...\nIteration 200 ...\nIteration 225 ...\nIteration 250 ...\nIteration 275 ...\nIteration 300 ...\nIteration 325 ...\nIteration 350 ...\nIteration 375 ...\nIteration 400 ...\nIteration 425 ...\nIteration 450 ...\nIteration 475 ...\nIteration 500 ...\nGibbs sampling completed!\n\n\nDepending on the size of the vocabulary, the collection size and the number K, the inference of topic models can take a very long time. This calculation may take several minutes. If it takes too long, reduce the vocabulary in the DTM by increasing the minimum frequency in the previous step.\nThe topic model inference results in two (approximate) posterior probability distributions: a distribution theta over K topics within each document and a distribution beta over V terms within each topic, where V represents the length of the vocabulary of the collection (V = 4278). Let’s take a closer look at these results:\n\n# have a look a some of the results (posterior distributions)\ntmResult <- posterior(topicModel)\n# format of the resulting object\nattributes(tmResult)\n\n$names\n[1] \"terms\"  \"topics\"\n\nnTerms(DTM)              # lengthOfVocab\n\n[1] 4278\n\n# topics are probability distributions over the entire vocabulary\nbeta <- tmResult$terms   # get beta from results\ndim(beta)                # K distributions over nTerms(DTM) terms\n\n[1]   20 4278\n\nrowSums(beta)            # rows in beta sum to 1\n\n 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 \n 1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 \n\nnDocs(DTM)               # size of collection\n\n[1] 8810\n\n# for every document we have a probability distribution of its contained topics\ntheta <- tmResult$topics \ndim(theta)               # nDocs(DTM) distributions over K topics\n\n[1] 8810   20\n\nrowSums(theta)[1:10]     # rows in theta sum to 1\n\n 1  2  3  4  5  6  7  8  9 10 \n 1  1  1  1  1  1  1  1  1  1 \n\n\nLet’s take a look at the 10 most likely terms within the term probabilities beta of the inferred topics (only the first 8 are shown below).\n\nterms(topicModel, 10)\n\n      Topic 1     Topic 2     Topic 3    Topic 4    Topic 5     Topic 6     \n [1,] \"land\"      \"recommend\" \"measur\"   \"citizen\"  \"great\"     \"claim\"     \n [2,] \"indian\"    \"report\"    \"interest\" \"law\"      \"line\"      \"govern\"    \n [3,] \"territori\" \"congress\"  \"view\"     \"case\"     \"part\"      \"question\"  \n [4,] \"larg\"      \"attent\"    \"subject\"  \"person\"   \"coast\"     \"commiss\"   \n [5,] \"tribe\"     \"secretari\" \"time\"     \"court\"    \"pacif\"     \"spain\"     \n [6,] \"limit\"     \"depart\"    \"present\"  \"properti\" \"construct\" \"island\"    \n [7,] \"popul\"     \"subject\"   \"object\"   \"protect\"  \"import\"    \"made\"      \n [8,] \"portion\"   \"consider\"  \"reason\"   \"natur\"    \"river\"     \"adjust\"    \n [9,] \"general\"   \"present\"   \"adopt\"    \"justic\"   \"complet\"   \"commission\"\n[10,] \"public\"    \"import\"    \"regard\"   \"demand\"   \"south\"     \"final\"     \n      Topic 7     Topic 8     Topic 9      Topic 10     Topic 11   Topic 12   \n [1,] \"public\"    \"state\"     \"govern\"     \"year\"       \"nation\"   \"constitut\"\n [2,] \"offic\"     \"unit\"      \"relat\"      \"amount\"     \"power\"    \"power\"    \n [3,] \"duti\"      \"govern\"    \"receiv\"     \"expenditur\" \"peac\"     \"state\"    \n [4,] \"execut\"    \"mexico\"    \"minist\"     \"increas\"    \"govern\"   \"peopl\"    \n [5,] \"general\"   \"part\"      \"friend\"     \"treasuri\"   \"war\"      \"union\"    \n [6,] \"administr\" \"territori\" \"republ\"     \"end\"        \"foreign\"  \"repres\"   \n [7,] \"give\"      \"texa\"      \"continu\"    \"estim\"      \"independ\" \"govern\"   \n [8,] \"respect\"   \"mexican\"   \"intercours\" \"fiscal\"     \"maintain\" \"presid\"   \n [9,] \"direct\"    \"republ\"    \"hope\"       \"revenu\"     \"polici\"   \"hous\"     \n[10,] \"proper\"    \"author\"    \"inform\"     \"june\"       \"intern\"   \"elect\"    \n      Topic 13   Topic 14   Topic 15    Topic 16   Topic 17     Topic 18  \n [1,] \"great\"    \"treati\"   \"made\"      \"congress\" \"duti\"       \"war\"     \n [2,] \"countri\"  \"great\"    \"appropri\"  \"act\"      \"import\"     \"forc\"    \n [3,] \"peopl\"    \"british\"  \"improv\"    \"law\"      \"increas\"    \"servic\"  \n [4,] \"labor\"    \"britain\"  \"work\"      \"author\"   \"countri\"    \"militari\"\n [5,] \"interest\" \"convent\"  \"purpos\"    \"provis\"   \"foreign\"    \"armi\"    \n [6,] \"condit\"   \"trade\"    \"provid\"    \"session\"  \"product\"    \"navi\"    \n [7,] \"good\"     \"vessel\"   \"make\"      \"legisl\"   \"produc\"     \"men\"     \n [8,] \"system\"   \"port\"     \"establish\" \"execut\"   \"manufactur\" \"offic\"   \n [9,] \"busi\"     \"negoti\"   \"secur\"     \"effect\"   \"revenu\"     \"ship\"    \n[10,] \"individu\" \"american\" \"object\"    \"pass\"     \"larg\"       \"command\" \n      Topic 19   Topic 20  \n [1,] \"nation\"   \"public\"  \n [2,] \"countri\"  \"bank\"    \n [3,] \"peopl\"    \"govern\"  \n [4,] \"prosper\"  \"money\"   \n [5,] \"great\"    \"issu\"    \n [6,] \"institut\" \"treasuri\"\n [7,] \"preserv\"  \"gold\"    \n [8,] \"honor\"    \"note\"    \n [9,] \"happi\"    \"debt\"    \n[10,] \"spirit\"   \"interest\"\n\n\n\nexampleTermData <- terms(topicModel, 10)\nexampleTermData[, 1:8]\n\n      Topic 1     Topic 2     Topic 3    Topic 4    Topic 5     Topic 6     \n [1,] \"land\"      \"recommend\" \"measur\"   \"citizen\"  \"great\"     \"claim\"     \n [2,] \"indian\"    \"report\"    \"interest\" \"law\"      \"line\"      \"govern\"    \n [3,] \"territori\" \"congress\"  \"view\"     \"case\"     \"part\"      \"question\"  \n [4,] \"larg\"      \"attent\"    \"subject\"  \"person\"   \"coast\"     \"commiss\"   \n [5,] \"tribe\"     \"secretari\" \"time\"     \"court\"    \"pacif\"     \"spain\"     \n [6,] \"limit\"     \"depart\"    \"present\"  \"properti\" \"construct\" \"island\"    \n [7,] \"popul\"     \"subject\"   \"object\"   \"protect\"  \"import\"    \"made\"      \n [8,] \"portion\"   \"consider\"  \"reason\"   \"natur\"    \"river\"     \"adjust\"    \n [9,] \"general\"   \"present\"   \"adopt\"    \"justic\"   \"complet\"   \"commission\"\n[10,] \"public\"    \"import\"    \"regard\"   \"demand\"   \"south\"     \"final\"     \n      Topic 7     Topic 8    \n [1,] \"public\"    \"state\"    \n [2,] \"offic\"     \"unit\"     \n [3,] \"duti\"      \"govern\"   \n [4,] \"execut\"    \"mexico\"   \n [5,] \"general\"   \"part\"     \n [6,] \"administr\" \"territori\"\n [7,] \"give\"      \"texa\"     \n [8,] \"respect\"   \"mexican\"  \n [9,] \"direct\"    \"republ\"   \n[10,] \"proper\"    \"author\"   \n\n\nFor the next steps, we want to give the topics more descriptive names than just numbers. Therefore, we simply concatenate the five most likely terms of each topic to a string that represents a pseudo-name for each topic.\n\ntop5termsPerTopic <- terms(topicModel, 5)\ntopicNames <- apply(top5termsPerTopic, 2, paste, collapse=\" \")"
  },
  {
    "objectID": "topicmodels.html#visualization-of-words-and-topics",
    "href": "topicmodels.html#visualization-of-words-and-topics",
    "title": "Topic Modeling with R",
    "section": "Visualization of Words and Topics",
    "text": "Visualization of Words and Topics\nAlthough wordclouds may not be optimal for scientific purposes they can provide a quick visual overview of a set of terms. Let’s look at some topics as wordcloud.\nIn the following code, you can change the variable topicToViz with values between 1 and 20 to display other topics.\n\n# visualize topics as word cloud\ntopicToViz <- 11 # change for your own topic of interest\ntopicToViz <- grep('mexico', topicNames)[1] # Or select a topic by a term contained in its name\n# select to 40 most probable terms from the topic by sorting the term-topic-probability vector in decreasing order\ntop40terms <- sort(tmResult$terms[topicToViz,], decreasing=TRUE)[1:40]\nwords <- names(top40terms)\n# extract the probabilites of each of the 40 terms\nprobabilities <- sort(tmResult$terms[topicToViz,], decreasing=TRUE)[1:40]\n# visualize the terms as wordcloud\nmycolors <- brewer.pal(8, \"Dark2\")\nwordcloud(words, probabilities, random.order = FALSE, color = mycolors)\n\n\n\n\n\n\n\n\nLet us now look more closely at the distribution of topics within individual documents. To this end, we visualize the distribution in 3 sample documents.\nLet us first take a look at the contents of three sample documents:\n\nexampleIds <- c(2, 100, 200)\nlapply(corpus[exampleIds], as.character)\n\n$`2`\n[1] \"I embrace with great satisfaction the opportunity which now presents itself\\nof congratulating you on the present favorable prospects of our public\\naffairs. The recent accession of the important state of North Carolina to\\nthe Constitution of the United States (of which official information has\\nbeen received), the rising credit and respectability of our country, the\\ngeneral and increasing good will toward the government of the Union, and\\nthe concord, peace, and plenty with which we are blessed are circumstances\\nauspicious in an eminent degree to our national prosperity.\"\n\n$`100`\n[1] \"Provision is likewise requisite for the reimbursement of the loan which has\\nbeen made of the Bank of the United States, pursuant to the eleventh\\nsection of the act by which it is incorporated. In fulfilling the public\\nstipulations in this particular it is expected a valuable saving will be\\nmade.\"\n\n$`200`\n[1] \"After many delays and disappointments arising out of the European war, the\\nfinal arrangements for fulfilling the engagements made to the Dey and\\nRegency of Algiers will in all present appearance be crowned with success,\\nbut under great, though inevitable, disadvantages in the pecuniary\\ntransactions occasioned by that war, which will render further provision\\nnecessary. The actual liberation of all our citizens who were prisoners in\\nAlgiers, while it gratifies every feeling of heart, is itself an earnest of\\na satisfactory termination of the whole negotiation. Measures are in\\noperation for effecting treaties with the Regencies of Tunis and Tripoli.\"\n\n\n\nexampleIds <- c(2, 100, 200)\nprint(paste0(exampleIds[1], \": \", substr(content(corpus[[exampleIds[1]]]), 0, 400), '...'))\n\n[1] \"2: I embrace with great satisfaction the opportunity which now presents itself\\nof congratulating you on the present favorable prospects of our public\\naffairs. The recent accession of the important state of North Carolina to\\nthe Constitution of the United States (of which official information has\\nbeen received), the rising credit and respectability of our country, the\\ngeneral and increasing good will ...\"\n\nprint(paste0(exampleIds[2], \": \", substr(content(corpus[[exampleIds[2]]]), 0, 400), '...'))\n\n[1] \"100: Provision is likewise requisite for the reimbursement of the loan which has\\nbeen made of the Bank of the United States, pursuant to the eleventh\\nsection of the act by which it is incorporated. In fulfilling the public\\nstipulations in this particular it is expected a valuable saving will be\\nmade....\"\n\nprint(paste0(exampleIds[3], \": \", substr(content(corpus[[exampleIds[3]]]), 0, 400), '...'))\n\n[1] \"200: After many delays and disappointments arising out of the European war, the\\nfinal arrangements for fulfilling the engagements made to the Dey and\\nRegency of Algiers will in all present appearance be crowned with success,\\nbut under great, though inevitable, disadvantages in the pecuniary\\ntransactions occasioned by that war, which will render further provision\\nnecessary. The actual liberation of all ...\"\n\n\nAfter looking into the documents, we visualize the topic distributions within the documents.\n\nN <- length(exampleIds)\n# get topic proportions form example documents\ntopicProportionExamples <- theta[exampleIds,]\ncolnames(topicProportionExamples) <- topicNames\nvizDataFrame <- melt(cbind(data.frame(topicProportionExamples), document = factor(1:N)), variable.name = \"topic\", id.vars = \"document\")  \nggplot(data = vizDataFrame, aes(topic, value, fill = document), ylab = \"proportion\") + \n  geom_bar(stat=\"identity\") +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +  \n  coord_flip() +\n  facet_wrap(~ document, ncol = N)"
  },
  {
    "objectID": "topicmodels.html#topic-distributions",
    "href": "topicmodels.html#topic-distributions",
    "title": "Topic Modeling with R",
    "section": "Topic distributions",
    "text": "Topic distributions\nThe figure above shows how topics within a document are distributed according to the model. In the current model all three documents show at least a small percentage of each topic. However, two to three topics dominate each document.\nThe topic distribution within a document can be controlled with the Alpha-parameter of the model. Higher alpha priors for topics result in an even distribution of topics within a document. Low alpha priors ensure that the inference process distributes the probability mass on a few topics for each document.\nIn the previous model calculation the alpha-prior was automatically estimated in order to fit to the data (highest overall probability of the model). However, this automatic estimate does not necessarily correspond to the results that one would like to have as an analyst. Depending on our analysis interest, we might be interested in a more peaky/more even distribution of topics in the model.\nNow let us change the alpha prior to a lower value to see how this affects the topic distributions in the model.\n\n# see alpha from previous model\nattr(topicModel, \"alpha\") \n\n[1] 2.5\n\n\n\ntopicModel2 <- LDA(DTM, K, method=\"Gibbs\", control=list(iter = 500, verbose = 25, alpha = 0.2))\n\nK = 20; V = 4278; M = 8810\nSampling 500 iterations!\nIteration 25 ...\nIteration 50 ...\nIteration 75 ...\nIteration 100 ...\nIteration 125 ...\nIteration 150 ...\nIteration 175 ...\nIteration 200 ...\nIteration 225 ...\nIteration 250 ...\nIteration 275 ...\nIteration 300 ...\nIteration 325 ...\nIteration 350 ...\nIteration 375 ...\nIteration 400 ...\nIteration 425 ...\nIteration 450 ...\nIteration 475 ...\nIteration 500 ...\nGibbs sampling completed!\n\ntmResult <- posterior(topicModel2)\ntheta <- tmResult$topics\nbeta <- tmResult$terms\ntopicNames <- apply(terms(topicModel2, 5), 2, paste, collapse = \" \")  # reset topicnames\n\nNow visualize the topic distributions in the three documents again. What are the differences in the distribution structure?\n\n# get topic proportions form example documents\ntopicProportionExamples <- theta[exampleIds,]\ncolnames(topicProportionExamples) <- topicNames\nvizDataFrame <- melt(cbind(data.frame(topicProportionExamples), document = factor(1:N)), variable.name = \"topic\", id.vars = \"document\")  \nggplot(data = vizDataFrame, aes(topic, value, fill = document), ylab = \"proportion\") + \n  geom_bar(stat=\"identity\") +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +  \n  coord_flip() +\n  facet_wrap(~ document, ncol = N)"
  },
  {
    "objectID": "topicmodels.html#topic-ranking",
    "href": "topicmodels.html#topic-ranking",
    "title": "Topic Modeling with R",
    "section": "Topic ranking",
    "text": "Topic ranking\nFirst, we try to get a more meaningful order of top terms per topic by re-ranking them with a specific score [5]. The idea of re-ranking terms is similar to the idea of TF-IDF. The more a term appears in top levels w.r.t. its probability, the less meaningful it is to describe the topic. Hence, the scoring advanced favors terms to describe a topic.\n\n# re-rank top topic terms for topic names\ntopicNames <- apply(lda::top.topic.words(beta, 5, by.score = T), 2, paste, collapse = \" \")\n\nWhat are the defining topics within a collection? There are different approaches to find out which can be used to bring the topics into a certain order.\n\nApproach 1\nWe sort topics according to their probability within the entire collection:\n\n# What are the most probable topics in the entire collection?\ntopicProportions <- colSums(theta) / nDocs(DTM)  # mean probabilities over all paragraphs\nnames(topicProportions) <- topicNames     # assign the topic names we created before\nsort(topicProportions, decreasing = TRUE) # show summed proportions in decreased order\n\n       public object system consider great \n                                   0.06485 \n         nation peopl countri prosper peac \n                                   0.06293 \n         claim govern adjust treati negoti \n                                   0.05979 \ncongress attent report recommend secretari \n                                   0.05888 \n        congress senat state treati repres \n                                   0.05527 \n      year amount treasuri expenditur debt \n                                   0.05500 \n           govern relat state island spain \n                                   0.05493 \n             war mexico peac state citizen \n                                   0.05265 \n         constitut state power peopl union \n                                   0.05128 \n              peopl man labor polit condit \n                                   0.04965 \n        offic servic depart appoint public \n                                   0.04889 \n   product manufactur tariff duti industri \n                                   0.04537 \n              state unit trade vessel port \n                                   0.04491 \n               law court state unit person \n                                   0.04416 \n          year increas mail pension number \n                                   0.04386 \n           bank gold currenc silver circul \n                                   0.04337 \n          navi vessel ship naval construct \n                                   0.04236 \n             armi war forc militari servic \n                                   0.04122 \n          line state territori river pacif \n                                   0.04117 \n           land indian tribe territori acr \n                                   0.03949 \n\n\n\nsoP <- sort(topicProportions, decreasing = TRUE)\npaste(round(soP, 5), \":\", names(soP))\n\n [1] \"0.06485 : public object system consider great\"       \n [2] \"0.06293 : nation peopl countri prosper peac\"         \n [3] \"0.05979 : claim govern adjust treati negoti\"         \n [4] \"0.05888 : congress attent report recommend secretari\"\n [5] \"0.05527 : congress senat state treati repres\"        \n [6] \"0.055 : year amount treasuri expenditur debt\"        \n [7] \"0.05493 : govern relat state island spain\"           \n [8] \"0.05265 : war mexico peac state citizen\"             \n [9] \"0.05128 : constitut state power peopl union\"         \n[10] \"0.04965 : peopl man labor polit condit\"              \n[11] \"0.04889 : offic servic depart appoint public\"        \n[12] \"0.04537 : product manufactur tariff duti industri\"   \n[13] \"0.04491 : state unit trade vessel port\"              \n[14] \"0.04416 : law court state unit person\"               \n[15] \"0.04386 : year increas mail pension number\"          \n[16] \"0.04337 : bank gold currenc silver circul\"           \n[17] \"0.04236 : navi vessel ship naval construct\"          \n[18] \"0.04122 : armi war forc militari servic\"             \n[19] \"0.04117 : line state territori river pacif\"          \n[20] \"0.03949 : land indian tribe territori acr\"           \n\n\nWe recognize some topics that are way more likely to occur in the corpus than others. These describe rather general thematic coherence. Other topics correspond more to specific contents.\n\n\nApproach 2\nWe count how often a topic appears as a primary topic within a paragraph This method is also called Rank-1.\n\ncountsOfPrimaryTopics <- rep(0, K)\nnames(countsOfPrimaryTopics) <- topicNames\nfor (i in 1:nDocs(DTM)) {\n  topicsPerDoc <- theta[i, ] # select topic distribution for document i\n  # get first element position from ordered list\n  primaryTopic <- order(topicsPerDoc, decreasing = TRUE)[1] \n  countsOfPrimaryTopics[primaryTopic] <- countsOfPrimaryTopics[primaryTopic] + 1\n}\nsort(countsOfPrimaryTopics, decreasing = TRUE)\n\n         claim govern adjust treati negoti \n                                       623 \n           govern relat state island spain \n                                       594 \n         nation peopl countri prosper peac \n                                       576 \n       public object system consider great \n                                       525 \n      year amount treasuri expenditur debt \n                                       524 \n        congress senat state treati repres \n                                       521 \n             war mexico peac state citizen \n                                       476 \ncongress attent report recommend secretari \n                                       461 \n           bank gold currenc silver circul \n                                       428 \n        offic servic depart appoint public \n                                       420 \n         constitut state power peopl union \n                                       414 \n               law court state unit person \n                                       383 \n              state unit trade vessel port \n                                       373 \n   product manufactur tariff duti industri \n                                       373 \n          navi vessel ship naval construct \n                                       370 \n              peopl man labor polit condit \n                                       369 \n           land indian tribe territori acr \n                                       368 \n          year increas mail pension number \n                                       357 \n             armi war forc militari servic \n                                       336 \n          line state territori river pacif \n                                       319 \n\n\n\nso <- sort(countsOfPrimaryTopics, decreasing = TRUE)\npaste(so, \":\", names(so))\n\n [1] \"623 : claim govern adjust treati negoti\"         \n [2] \"594 : govern relat state island spain\"           \n [3] \"576 : nation peopl countri prosper peac\"         \n [4] \"525 : public object system consider great\"       \n [5] \"524 : year amount treasuri expenditur debt\"      \n [6] \"521 : congress senat state treati repres\"        \n [7] \"476 : war mexico peac state citizen\"             \n [8] \"461 : congress attent report recommend secretari\"\n [9] \"428 : bank gold currenc silver circul\"           \n[10] \"420 : offic servic depart appoint public\"        \n[11] \"414 : constitut state power peopl union\"         \n[12] \"383 : law court state unit person\"               \n[13] \"373 : state unit trade vessel port\"              \n[14] \"373 : product manufactur tariff duti industri\"   \n[15] \"370 : navi vessel ship naval construct\"          \n[16] \"369 : peopl man labor polit condit\"              \n[17] \"368 : land indian tribe territori acr\"           \n[18] \"357 : year increas mail pension number\"          \n[19] \"336 : armi war forc militari servic\"             \n[20] \"319 : line state territori river pacif\"          \n\n\nWe see that sorting topics by the Rank-1 method places topics with rather specific thematic coherences in upper ranks of the list.\nThis sorting of topics can be used for further analysis steps such as the semantic interpretation of topics found in the collection, the analysis of time series of the most important topics or the filtering of the original collection based on specific sub-topics."
  },
  {
    "objectID": "topicmodels.html#filtering-documents",
    "href": "topicmodels.html#filtering-documents",
    "title": "Topic Modeling with R",
    "section": "Filtering documents",
    "text": "Filtering documents\nThe fact that a topic model conveys of topic probabilities for each document, resp. paragraph in our case, makes it possible to use it for thematic filtering of a collection. AS filter we select only those documents which exceed a certain threshold of their probability value for certain topics (for example, each document which contains topic X to more than 20 percent).\nIn the following, we will select documents based on their topic content and display the resulting document quantity over time.\n\ntopicToFilter <- 6  # you can set this manually ...\n# ... or have it selected by a term in the topic name (e.g. 'children')\ntopicToFilter <- grep('children', topicNames)[1] \ntopicThreshold <- 0.2\nselectedDocumentIndexes <- which(theta[, topicToFilter] >= topicThreshold)\nfilteredCorpus <- corpus[selectedDocumentIndexes]\n# show length of filtered corpus\nfilteredCorpus\n\n<<SimpleCorpus>>\nMetadata:  corpus specific: 1, document level (indexed): 4\nContent:  documents: 0\n\n\nOur filtered corpus contains 0 documents related to the topic NA to at least 20 %."
  },
  {
    "objectID": "topicmodels.html#topic-proportions-over-time",
    "href": "topicmodels.html#topic-proportions-over-time",
    "title": "Topic Modeling with R",
    "section": "Topic proportions over time",
    "text": "Topic proportions over time\nIn a last step, we provide a distant view on the topics in the data over time. For this, we aggregate mean topic proportions per decade of all SOTU speeches. These aggregated topic proportions can then be visualized, e.g. as a bar plot.\n\n# append decade information for aggregation\ntextdata$decade <- paste0(substr(textdata$date, 0, 3), \"0\")\n# get mean topic proportions per decade\ntopic_proportion_per_decade <- aggregate(theta, by = list(decade = textdata$decade), mean)\n# set topic names to aggregated columns\ncolnames(topic_proportion_per_decade)[2:(K+1)] <- topicNames\n# reshape data frame\nvizDataFrame <- melt(topic_proportion_per_decade, id.vars = \"decade\")\n# plot topic proportions per decade as bar plot\nggplot(vizDataFrame, aes(x=decade, y=value, fill=variable)) + \n  geom_bar(stat = \"identity\") + ylab(\"proportion\") + \n  scale_fill_manual(values = paste0(alphabet(20), \"FF\"), name = \"decade\") + \n  theme(axis.text.x = element_text(angle = 90, hjust = 1))\n\n\n\n\n\n\n\n\nThe visualization shows that topics around the relation between the federal government and the states as well as inner conflicts clearly dominate the first decades. Security issues and the economy are the most important topics of recent SOTU addresses."
  },
  {
    "objectID": "tree.html#advantages",
    "href": "tree.html#advantages",
    "title": "Tree-Based Models in R",
    "section": "Advantages",
    "text": "Advantages\nSeveral advantages have been associated with using tree-based models:\n\nTree-structure models are very useful because they are extremely flexible as they can deal with different types of variables and provide a very good understanding of the structure in the data.\nTree-structure models have been deemed particularly interesting for linguists because they can handle moderate sample sizes and many high-order interactions better then regression models.\nTree-structure models are (supposedly) better at detecting non-linear or non-monotonic relationships between predictors and dependent variables. This also means that they are better at finding and displaying interaction sinvolving many predictors.\nTree-structure models are easy to implement in R and do not require the model selection, validation, and diagnostics associated with regression models.\nTree-structure models can be used as variable-selection procedure which informs about which variables have any sort of significant relationship with the dependent variable and can thereby inform model fitting."
  },
  {
    "objectID": "tree.html#problems",
    "href": "tree.html#problems",
    "title": "Tree-Based Models in R",
    "section": "Problems",
    "text": "Problems\nDespite these potential advantages, a word of warning is in order: [4] admits that tree-based models can be very useful but there are some issues that but some serious short-comings of tree-structure models remain under-explored. For instance,\n\nForest-models (Random Forests and Boruta) only inform about the importance of a variable but not if the variable is important as a main effect or as part of interactions (or both)! The importance only shows that there is some important connection between the predictor and the dependent variable. While partial dependence plots (see here for more information) offer a remedy for this shortcoming to a certain degree, regression models are still much better at dealing with this issue.\nSimple tree-structure models have been shown to fail in detecting the correct predictors if the variance is solely determined by a single interaction [4]. This failure is caused by the fact that the predictor used in the first split of a tree is selected as the one with the strongest main effect [5]. This issue can, however, be avoided by hard-coding the interactions as predictors plus using ensemble methods such as random forests rather than individual trees [see 4].\nAnother shortcoming is that tree-structure models partition the data (rather than “fitting a line” through the data which can lead to more coarse-grained predictions compared to regression models when dealing with numeric dependent variables [again, see 4].\n[5] state that high correlations between predictors can hinder the detection of interactions when using small data sets. However, regression do not fare better here as they are even more strongly affected by (multi-)collinearity [see 4].\nTree-structure models are bad a detecting interactions when the variables have strong main effects which is, unfortunately, common when dealing with linguistic data [6].\nTree-structure models cannot handle factorial variables with many levels (more than 53 levels) which is very common in linguistics where individual speakers or items are variables.\nForest-models (Random Forests and Boruta) have been deemed to be better at dealing with small data sets. However, this is only because the analysis is based on permutations of the original small data set. As such, forest-based models only appear to be better at handling small data sets because they “blow up” the data set rather than really being better at analyzing the original data.\n\nBefore we implement a conditional inference tree in R, we will have a look at how decision trees work. We will do this in more detail here as random forests and Boruta analyses are extensions of inference trees and are therefore based on the same concepts."
  },
  {
    "objectID": "tree.html#classification-and-regression-trees",
    "href": "tree.html#classification-and-regression-trees",
    "title": "Tree-Based Models in R",
    "section": "0.1 Classification And Regression Trees",
    "text": "0.1 Classification And Regression Trees\nBelow is an example of a decision tree which shows what what response to expect - in this case whether a speaker uses discourse like or not. Decision trees, like all CARTs and CITs, answer a simple question, namely How do we best classify elements based on the given predictors?. The answer that decision trees provide is the classification of the elements based on the levels of the predictors. In simple decision trees, all predictors, even those that are not significant are included in the decision tree. The decision tree shows that the best (or most important) predictor for the use of discourse like is age as it is the highest node. Among young speakers, those with high status use like more compared with speakers of lower social status. Among old speakers, women use discourse like more than men.\n\ninstall.packages(\"tree\")\ninstall.packages(\"dplyr\")\ninstall.packages(\"grid\")\n\n\n\n\n\n\nThe yes and no at the bottom show if the speaker should be classified as a user of discourse like (yes or no). Each split can be read as true to the left and false to the right. So that, at the first split, if the person is between the ages of 15 and 40, we need to follow the branch to the left while we need to follow to the right if the person is not 15 to 40.\nBefore going through how this conditional decision tree is generated, let us first go over some basic concepts. The top of the decision tree is called root or root node, the categories at the end of branches are called leaves or leaf nodes. Nodes that are in-between the root and leaves are called internal nodes or just nodes. The root node has only arrows or lines pointing away from it, internal nodes have lines going to and from them, while leaf nodes only have lines pointing towards them.\nHow to prune and evaluate the accuracy of decision trees is not shown here. If you are interested in this, please check out chapter 7 of [4] which is a highly recommendable resource that provide a lot of additional information about decision trees and CARTs."
  },
  {
    "objectID": "tree.html#how-tree-based-methods-work",
    "href": "tree.html#how-tree-based-methods-work",
    "title": "Tree-Based Models in R",
    "section": "How Tree-Based Methods Work",
    "text": "How Tree-Based Methods Work\nLet us now go over the process by which the decision tree above is generated. In our example, we want to predict whether a person makes use of discourse like given their age, gender, and social status.\nLet us now go over the process by which the decision tree above is generated. In our example, we want to predict whether a person makes use of discourse like given their age, gender, and social status.\nIn a first step, we load and inspect the data that we will use in this tutorial. As tree-based models require either numeric or factorized data, we factorize the “character” variables in our data.\n\n# load data\ncitdata <- read.delim(\"https://slcladal.github.io/data/treedata.txt\", header = T, sep = \"\\t\") %>%\n  dplyr::mutate_if(is.character, factor)\n\n\n\n\n\n\nFirst 10 rows of the citdata data\n\n\nAgeGenderStatusLikeUser15-40femalehighno15-40femalehighno15-40malehighno41-80femalelowyes41-80malehighno41-80malelowno41-80femalelowyes15-40malehighno41-80malelowno41-80malelowno\n\n\nThe data now consists of factors which two levels each.\nThe first step in generating a decision tree consists in determining, what the root of the decision tree should be. This means that we have to determine which of the variables represents the root node. In order to do so, we tabulate for each variable level, how many speakers of that level have used discourse like (LikeUsers) and how many have not used discourse like (NonLikeUsers).\n\n# tabulate data\ntable(citdata$LikeUser, citdata$Gender)\n\n     \n      female male\n  no      43   75\n  yes     91   42\n\ntable(citdata$LikeUser, citdata$Age)\n\n     \n      15-40 41-80\n  no     34    84\n  yes    92    41\n\ntable(citdata$LikeUser, citdata$Status)\n\n     \n      high low\n  no    33  85\n  yes   73  60\n\n\nNone of the predictors is perfect (the predictors are therefore referred to as impure). To determine which variable is the root, we will calculate the degree of “impurity” for each variable - the variable which has the lowest impurity value will be the root.\nThe most common measure of impurity in the context of conditional inference trees is called Gini (an alternative that is common when generating regression trees is the deviance). The Gini value or gini index was introduced by Corrado Gini as a measure for income inequality. In our case we seek to maximize inequality of distributions of leave nodes which is why the gini index is useful for tree based models. For each level we apply the following equation to determine the gini impurity value:\n\\[\\begin{equation}\n\nG_{x} = 1 - ( p_{1} )^{2} - ( p_{0} )^{2}\n\n\\end{equation}\\]\nFor the node for men, this would mean the following:\n\\[\\begin{equation}\n\nG_{men} = 1-(\\frac{42} {42+75})^{2} - (\\frac{75} {42+75})^{2} = 0.4602235\n\n\\end{equation}\\]\nFor women, we calculate G or Gini as follows:\n\\[\\begin{equation}\n\nG_{women} = 1-(\\frac{91} {91+43})^{2} - (\\frac{43} {91+43})^{2} = 0.4358432\n\n\\end{equation}\\]\nTo calculate the Gini value of Gender, we need to calculate the weighted average leaf node impurity (weighted because the number of speakers is different in each group). We calculate the weighted average leaf node impurity using the equation below.\n\\[\\begin{equation}\n\nG_{Gender} = \\frac{N_{men}} {N_{Total}} \\times G_{men} +  \\frac{N_{women}} {N_{Total}} \\times G_{women}\n\nG_{Gender} = \\frac{159} {303} \\times 0.4602235 +  \\frac{144} {303} \\times 0.4358432 = 0.4611915\n\n\\end{equation}\\]\nWe will now perform the gini-calculation for gender (see below).\n\n# calculate Gini for men\ngini_men <- 1-(42/(42+75))^2 - (75/(42+75))^2\n# calculate Gini for women\ngini_women <- 1-(91/(91+43))^2 - (43/(91+43))^2\n# calculate weighted average of Gini for Gender\ngini_gender <- 42/(42+75)* gini_men +  91/(91+43) * gini_women\ngini_gender\n\n[1] 0.4611915\n\n\nThe gini for gender is 0.4612. In a next step, we revisit the age distribution and we continue to calculate the gini value for age.\n\n# calculate Gini for age groups\ngini_young <- 1-(92/(92+34))^2 - (34/(92+34))^2  # Gini: young\ngini_old <- 1-(41/(41+84))^2 - (84/(41+84))^2    # Gini: old\n# calculate weighted average of Gini for Age\ngini_age <- 92/(92+34)* gini_young +  41/(41+84) * gini_old\ngini_age\n\n[1] 0.4323148\n\n\nThe gini for age is .4323 and we continue by revisiting the status distribution and we continue to calculate the gini value for status.\n\ngini_high <- 1-(73/(33+73))^2 - (33/(33+73))^2   # Gini: high\ngini_low <- 1-(60/(60+85))^2 - (85/(60+85))^2    # Gini: low\n# calculate weighted average of Gini for Status\ngini_status <- 73/(33+73)* gini_high +  60/(60+85) * gini_low\ngini_status\n\n[1] 0.4960521\n\n\nThe gini for status is .4961 and we can now compare the gini values for age, gender, and status.\n\n# compare age, gender, and status ginis\ngini_age; gini_gender; gini_status\n\n[1] 0.4323148\n\n\n[1] 0.4611915\n\n\n[1] 0.4960521\n\n\nSince age has the lowest gini (impurity) value, our first split is by age and age, thus, represents our root node. Our manually calculated conditional inference tree right now looks as below.\n\n\n\n\n\nIn a next step, we need to find out which of the remaining variables best separates the speakers who use discourse like from those that do not under the first node. In order to do so, we calculate the Gini values for Gender and SocialStatus for the 15-40 node.\nWe thus move on and test if and how to split this branch.\n\n# 5TH NODE\n# split data according to first split (only young data)\nyoung <- citdata %>%\n  dplyr::filter(Age == \"15-40\")\n# inspect distribution\ntbyounggender <- table(young$LikeUser, young$Gender)\ntbyounggender\n\n     \n      female male\n  no      17   17\n  yes     58   34\n\n\n\n# calculate Gini for Gender\n# calculate Gini for men\ngini_youngmen <- 1-(tbyounggender[2,2]/sum(tbyounggender[,2]))^2 - (tbyounggender[1,2]/sum(tbyounggender[,2]))^2\n# calculate Gini for women\ngini_youngwomen <- 1-(tbyounggender[2,1]/sum(tbyounggender[,1]))^2 - (tbyounggender[1,1]/sum(tbyounggender[,1]))^2\n# # calculate weighted average of Gini for Gender\ngini_younggender <- sum(tbyounggender[,2])/sum(tbyounggender)* gini_youngmen +  sum(tbyounggender[,1])/sum(tbyounggender) * gini_youngwomen\ngini_younggender\n\n[1] 0.3885714\n\n\nThe gini value for gender among young speakers is 0.3886.\nWe continue by inspecting the status distribution.\n\n# calculate Gini for Status\n# inspect distribution\ntbyoungstatus <- table(young$LikeUser, young$Status)\ntbyoungstatus\n\n     \n      high low\n  no    11  23\n  yes   57  35\n\n\nWe now calculate the gini value for status.\n\n# calculate Gini for status\n# calculate Gini for low\ngini_younglow <- 1-(tbyoungstatus[2,2]/sum(tbyoungstatus[,2]))^2 - (tbyoungstatus[1,2]/sum(tbyoungstatus[,2]))^2\n# calculate Gini for high\ngini_younghigh <- 1-(tbyoungstatus[2,1]/sum(tbyoungstatus[,1]))^2 - (tbyoungstatus[1,1]/sum(tbyoungstatus[,1]))^2\n# # calculate weighted average of Gini for status\ngini_youngstatus <- sum(tbyoungstatus[,2])/sum(tbyoungstatus)* gini_younglow +  sum(tbyoungstatus[,1])/sum(tbyoungstatus) * gini_younghigh\ngini_youngstatus\n\n[1] 0.3666651\n\n\nSince the gini value for status (0.3667) is lower than the gini value for gender (0.3886), we split by status.\nWe would continue to calculate the gini values and always split at the lowest gini levels until we reach a leaf node. Then, we would continue doing the same for the remaining branches until the entire data is binned into different leaf nodes.\nIn addition to plotting the decision tree, we can also check its accuracy. To do so, we predict the use of like based on the decision tree and compare them to the observed uses of like. Then we use the confusionMatrix function from the caret package to get an overview of the accuracy statistics.\n\ndtreeprediction <- as.factor(ifelse(predict(dtree)[,2] > .5, \"yes\", \"no\"))\nconfusionMatrix(dtreeprediction, citdata$LikeUser)\n\nThe conditional inference tree has an accuracy of 72.9 percent which is significantly better than the base-line accuracy of 53.0 percent (No Information Rate \\(*\\) 100). To understand what the other statistics refer to and how they are calculated, run the command ?confusionMatrix."
  },
  {
    "objectID": "tree.html#splitting-numeric-ordinal-and-true-categorical-variables",
    "href": "tree.html#splitting-numeric-ordinal-and-true-categorical-variables",
    "title": "Tree-Based Models in R",
    "section": "Splitting numeric, ordinal, and true categorical variables",
    "text": "Splitting numeric, ordinal, and true categorical variables\nWhile it is rather straight forward to calculate the Gini values for categorical variables, it may not seem quite as apparent how to calculate splits for numeric or ordinal variables. To illustrate how the algorithm works on such variables, consider the example data set shown below.\n\n\n\n\n\n\n\n\nFirst 10 rows of the citdata2 data.\n\n\nAgeLikeUser15yes37no63no42yes22yes27yes\n\n\nIn a first step, we order the numeric variable so that we arrive at the following table.\n\n\n\n\n\n\n\n\nFirst 10 rows of the citdata2 data arranged by age.\n\n\nAgeLikeUser15yes22yes27yes37no42yes63no\n\n\nNext, we calculate the means for each level of “Age”.\n\n\n\n\n\n\n\n\nFirst 10 rows of the citdata3 data arranged by age.\n\n\nAgeLikeUser15.0yes18.522.0yes24.527.0yes32.037.0no39.542.0yes52.5\n\n\nNow, we calculate the Gini values for each average level of age. How this is done is shown below for the first split.\n\\[\\begin{equation}\n\nG_{x} = 1 - ( p_{1} )^{2} - ( p_{0} )^{2}\n\n\\end{equation}\\]\nFor an age smaller than 18.5 this would mean:\n\\[\\begin{equation}\n\nG_{youngerthan18.5} = 1-(\\frac{1} {1+0})^{2} - (\\frac{0} {1+0})^{2} = 0.0\n\n\\end{equation}\\]\nFor an age greater than 18.5, we calculate G or Gini as follows:\n\\[\\begin{equation}\n\nG_{olerthan18.5} = 1-(\\frac{2} {2+3})^{2} - (\\frac{3} {2+3})^{2} = 0.48\n\n\\end{equation}\\]\nNow, we calculate the Gini for that split as we have done above.\n\\[\\begin{equation}\n\nG_{split18.5} = \\frac{N_{youngerthan18.5}} {N_{Total}} \\times G_{youngerthan18.5} +  \\frac{N_{olderthan18.5}} {N_{Total}} \\times G_{olderthan18.5}\n\nG_{split18.5} = \\frac{1} {6} \\times 0.0 +  \\frac{5} {6} \\times 0.48 = 0.4\n\n\\end{equation}\\]\nWe then have to calculate the gini values for all possible age splits which yields the following results:\n\n# 18.5\n1-(1/(1+0))^2 - (0/(1+0))^2\n1-(2/(2+3))^2 - (3/(2+3))^2\n1/6 * 0.0 +  5/6 * 0.48\n# 24.4\n1-(2/(2+0))^2 - (0/(2+0))^2\n1-(3/(3+1))^2 - (2/(3+1))^2\n2/6 * 0.0 +  4/6 * 0.1875\n# 32\n1-(3/(3+0))^2 - (0/(3+0))^2\n1-(1/(1+2))^2 - (2/(1+2))^2\n3/6 * 0.0 +  3/6 * 0.4444444\n# 39.5\n1-(3/(3+1))^2 - (1/(3+1))^2\n1-(1/(1+1))^2 - (1/(1+1))^2\n4/6 * 0.375 +  2/6 * 0.5\n# 52.5\n1-(4/(4+1))^2 - (1/(4+1))^2\n1-(0/(0+1))^2 - (1/(0+1))^2\n5/6 * 0.32 +  1/6 * 0.0\n\n\n\n\n\n\n\n\n\nFirst 10 rows of the citdata3 data with Gini coefficients.\n\n\nAgeSplitGini18.50.40024.50.50032.00.44439.50.41052.50.267\n\n\nThe split at 52.5 years of age has the lowest Gini value. Accordingly, we would split the data between speakers who are younger than 52.5 and speakers who are older than 52.5 years of age. The lowest Gini value for any age split would also be the Gini value that would be compared to other variables.\nThe same procedure that we have used to determine potential splits for a numeric variable would apply to an ordinal variable with only two differences:\n\nThe Gini values are calculated for the actual levels and not the means between variable levels.\nThe Gini value is nor calculated for the lowest and highest level as the calculation of the Gini values is impossible for extreme values. Extreme levels can, therefore, not serve as a potential split location.\n\nWhen dealing with categorical variables with more than two levels, the situation is slightly more complex as we would also have to calculate the Gini values for combinations of variable levels. While the calculations are, in principle, analogous to the ones performed for binary of nominal categorical variables, we would also have to check if combinations would lead to improved splits. For instance, imagine we have a variable with categories A, B, and C. In such cases we would not only have to calculate the Gini scores for A, B, and C but also for A plus B, A plus C, and B plus C. Note that we ignore the combination A plus B plus C as this combination would include all other potential combinations."
  },
  {
    "objectID": "tree.html#prettifying-your-cit-tree",
    "href": "tree.html#prettifying-your-cit-tree",
    "title": "Tree-Based Models in R",
    "section": "Prettifying your CIT tree",
    "text": "Prettifying your CIT tree\nThe easiest and most common way to visualize CITs is to simply use the plot function from base R. However, using this function does not allow to adapt and customize the visualization except for some very basic parameters. The ggparty function allows to use the ggplot syntax to customize CITs which allows more adjustments and is more aesthetically pleasing.\nTo generate this customized CIT, we activate the ggparty package and extract the significant p-values from the CIT object. We then plot the CIT and define the nodes, edges, and text elements as shown below.\n\n# extract p-values\npvals <- unlist(nodeapply(citd.ctree, ids = nodeids(citd.ctree), function(n) info_node(n)$p.value))\npvals <- pvals[pvals <.05]\n# plotting\nggparty(citd.ctree) +\n  geom_edge() +\n  geom_edge_label() +\n  geom_node_label(line_list = list(aes(label = splitvar),\n                                   aes(label = paste0(\"N=\", nodesize, \", p\", \n                                                      ifelse(pvals < .001, \"<.001\", paste0(\"=\", round(pvals, 3)))), \n                                       size = 10)),\n                  line_gpar = list(list(size = 13), \n                                   list(size = 10)), \n                  ids = \"inner\") +\n  geom_node_label(aes(label = paste0(\"Node \", id, \", N = \", nodesize)),\n    ids = \"terminal\", nudge_y = -0.0, nudge_x = 0.01) +\n  geom_node_plot(gglist = list(\n    geom_bar(aes(x = \"\", fill = LikeUser),\n             position = position_fill(), color = \"black\"),\n      theme_minimal(),\n      scale_fill_manual(values = c(\"gray50\", \"gray80\"), guide = FALSE),\n      scale_y_continuous(breaks = c(0, 1)),\n    xlab(\"\"), \n    ylab(\"Probability\"),\n    geom_text(aes(x = \"\", group = LikeUser,\n                  label = stat(count)),\n              stat = \"count\", position = position_fill(), vjust = 1.1)),\n    shared_axis_labels = TRUE)\n\n\n\n\nWe can also use position_dodge (instead of position_fill) to display frequencies rather than probabilities as shown below.\n\n# plotting\nggparty(citd.ctree) +\n  geom_edge() +\n  geom_edge_label() +\n  geom_node_label(line_list = list(aes(label = splitvar),\n                                   aes(label = paste0(\"N=\", nodesize, \", p\", \n                                                      ifelse(pvals < .001, \"<.001\", paste0(\"=\", round(pvals, 3)))), \n                                       size = 10)),\n                  line_gpar = list(list(size = 13), \n                                   list(size = 10)), \n                  ids = \"inner\") +\n  geom_node_label(aes(label = paste0(\"Node \", id, \", N = \", nodesize)),\n    ids = \"terminal\", nudge_y = 0.01, nudge_x = 0.01) +\n  geom_node_plot(gglist = list(\n    geom_bar(aes(x = \"\", fill = LikeUser),\n             position = position_dodge(), color = \"black\"),\n      theme_minimal(),\n    theme(panel.grid.major = element_blank(), \n            panel.grid.minor = element_blank()),\n      scale_fill_manual(values = c(\"gray50\", \"gray80\"), guide = FALSE),\n      scale_y_continuous(breaks = seq(0, 100, 20),\n                         limits = c(0, 100)),\n    xlab(\"\"), \n    ylab(\"Frequency\"),\n      geom_text(aes(x = \"\", group = LikeUser,\n                    label = stat(count)),\n                stat = \"count\", \n                position = position_dodge(0.9), vjust = -0.7)),\n    shared_axis_labels = TRUE)"
  },
  {
    "objectID": "tree.html#problems-of-conditional-inference-trees",
    "href": "tree.html#problems-of-conditional-inference-trees",
    "title": "Tree-Based Models in R",
    "section": "Problems of Conditional Inference Trees",
    "text": "Problems of Conditional Inference Trees\nLike other tree-based methods, CITs are very intuitive, multivariate, non-parametric, they do not require large data sets, and they are easy to implement. Despite these obvious advantages, they have at least one major short coming compared to other, more sophisticated tree-structure models (in addition to the general issues that tree-structure models exhibit as discussed above: they are prone to overfitting which means that they fit the observed data very well but preform much worse when being applied to new data.\nAn extension which remedies this problem is to use a so-called ensemble method which grows many varied trees. The most common ensemble method is called a Random Forest Analysis and will have a look at how Random Forests work and how to implement them in R in the next section."
  },
  {
    "objectID": "tree.html#random-forests-in-r",
    "href": "tree.html#random-forests-in-r",
    "title": "Tree-Based Models in R",
    "section": "Random Forests in R",
    "text": "Random Forests in R\nThis section shows how a Random Forest Analysis can be implemented in R. Ina first step, we load and inspect the data.\n\n# load random forest data\nrfdata <- read.delim(\"https://slcladal.github.io/data/mblrdata.txt\", header = T, sep = \"\\t\")\n\n\n\n\n\n\nFirst 10 rows of the rfdata data.\n\n\nIDGenderAgeConversationTypePrimingSUFlikeS1A-061$CWomenYoungMixedGenderNoPrime0S1A-023$BWomenYoungMixedGenderNoPrime0S1A-054$AWomenYoungSameGenderNoPrime0S1A-090$BWomenYoungMixedGenderNoPrime0S1A-009$BWomenOldSameGenderPrime0S1A-085$EMenYoungMixedGenderPrime1S1A-003$CWomenYoungMixedGenderNoPrime1S1A-084$CWomenYoungSameGenderNoPrime0S1A-076$AWomenYoungSameGenderNoPrime0S1A-083$DMenOldMixedGenderNoPrime1\n\n\nThe data consists of four categorical variables (Gender, Age, ConversationType, and SUFlike). Our dependent variable is SUFlike which stands for speech-unit final like (a pragmatic marker that is common in Irish English and is used as in A wee girl of her age, like). While Age and Gender are pretty straight forward what they are called, ConversationType encodes whether a conversation has taken place between interlocutors of the same or of different genders.\nBefore going any further, we need to factorize the variables as tree-based models require factors instead of character variables (but they can, of course, handle numeric and ordinal variables). In addition, we will check if the data contains missing values (NAs; NA stands for not available).\n\n# factorize variables (rf require factors instead of character vectors)\nrfdata <- rfdata %>%\n  dplyr::mutate_if(is.character, factor) %>%\n  dplyr::select(-ID)\n\n\n\n\n\n\nFirst 10 rows of the factorized rfdata data.\n\n\nGenderAgeConversationTypePrimingSUFlikeWomenYoungMixedGenderNoPrime0WomenYoungMixedGenderNoPrime0WomenYoungSameGenderNoPrime0WomenYoungMixedGenderNoPrime0WomenOldSameGenderPrime0MenYoungMixedGenderPrime1WomenYoungMixedGenderNoPrime1WomenYoungSameGenderNoPrime0WomenYoungSameGenderNoPrime0MenOldMixedGenderNoPrime1\n\n\nWe now check if the data contains missing values and remove those (if necessary).\n\n# check for NAs\nnatest <- rfdata %>%\n  na.omit()\nnrow(natest) # no NAs present in data (same number of rows with NAs omitted)\n\n[1] 2000\n\n\nIn our case, the data does not contain missing values. Random Forests offer a very nice way to deal with missing data though. If NAs are present, they can either be deleted OR their values for any missing values can be imputed using proximities. In this way, such data points do not have to be removed which can be problematic especially when dealing with relatively small data sets. For imputing values, you could run the code below but as our data does not have NAs, we will skip this step and just show it here so you can have a look at how it is done.\n\n# replacing NAs with estimates\ndata.imputed <- rfImpute(SUFlike ~ ., data = rfdata, iter=6)\n\nThe argument iter refers to the number of iterations to run. According to [7], 4 to 6 iterations is usually good enough. With this data set (if it had NAs) and when we were to execute the code, the resulting OOB-error rates lie somewhere around 17 and 18 percent. When we were to set iter to 20, we get values a little better and a little worse, so doing more iterations doesn’t improve the situation.\nAlso, if you want to customize the rfImpute function, you can change the number of trees it uses (the default is 300) and the number of variables that it will consider at each step.\nWe will now generate a first random forest object and inspect its model fit. As random forests rely on re-sampling, we set a seed so that we arrive at the same estimations.\n\n# set.seed\nset.seed(2019120204)\n# create initial model\nrfmodel1 <- cforest(SUFlike ~ .,  data = rfdata, \n                    controls = cforest_unbiased(ntree = 50, mtry = 3))\n# evaluate random forest (model diagnostics)\nrfmodel1_pred <- unlist(party::treeresponse(rfmodel1))#[c(FALSE,TRUE)]\nsomers2(rfmodel1_pred, as.numeric(rfdata$SUFlike))\n\n           C          Dxy            n      Missing \n   0.7112131    0.4224262 2000.0000000    0.0000000 \n\n\nThe model parameters are good but not excellent: remember that if the C-value is 0.5, the predictions are random, while the predictions are perfect if the C-value is 1. C-values above 0.8 indicate real predictive capacity [8]. Somers’ Dxy is a value that represents a rank correlation between predicted probabilities and observed responses. Somers’ Dxy values range between 0, which indicates complete randomness, and 1, which indicates perfect prediction [8].\nIn a next step, we extract the variable importance conditional=T adjusts for correlations between predictors).\n\n# extract variable importance based on mean decrease in accuracy\nrfmodel1_varimp <- varimp(rfmodel1, conditional = T) \n# show variable importance\nrfmodel1_varimp\n\n          Gender              Age ConversationType          Priming \n     0.003770260      0.000542920      0.002520164      0.022095496 \n\n\nWe can also calculate more robust variable importance using the varimpAUC function from the party package which calculates importance statistics that are corrected towards class imbalance, i.e. differences in the number of instances per category. The variable importance is easily visualized using the dotplot function from base R.\n\n# extract more robust variable importance \nrfmodel1_robustvarimp <- party::varimp(rfmodel1)  \n# plot result\ndotchart(sort(rfmodel1_robustvarimp), pch = 20, main = \"Conditional importance of variables\")\n\n\n\n\nThe plot shows that Age is the most important predictor and that Priming is not really important as a predictor for speech-unit final like. Gender and ConversationType are equally important but both much less so than Age.\nWe will now use an alternative way to calculate RFs which allows us to use different diagnostics and pruning techniques by using the randomForest rather than the cforest function.\nA few words on the parameters of the randomForest function: if the thing we’re trying to predict is a numeric variable, the randomForest function will set mtry (the number of variables considered at each step) to the total number of variables divided by 3 (rounded down), or to 1 if the division results in a value less than 1. If the thing we’re trying to predict is a “factor” (i.e. either “yes/no” or “ranked”), then randomForest() will set mtry to the square root of the number of variables (rounded down to the next integer value).Again, we start by setting a seed to store random numbers and thus make results reproducible.\n\n# set.seed\nset.seed(2019120205)\nrfmodel2 <- randomForest::randomForest(SUFlike ~ ., \n                                       data=rfdata, \n                                       mtry = 2,\n                                       proximity=TRUE)\n# inspect model\nrfmodel2 \n\n\nCall:\n randomForest(formula = SUFlike ~ ., data = rfdata, mtry = 2,      proximity = TRUE) \n               Type of random forest: regression\n                     Number of trees: 500\nNo. of variables tried at each split: 2\n\n          Mean of squared residuals: 0.1277716\n                    % Var explained: 10.28\n\n\nThe output tells us that the model explains less than 15 percent of the variance. It is recommendable to check if changing parameters causes and increase in the amount of variance that is explained by a model (which is desirable). In this case, we can try different values for mtry and for ntree as shown below and then compare the performance of the random forest models by inspecting the amount of variance that they explain. Again, we begin by setting a seed and then continue by specifying the random forest model.\n\n# set.seed (to store random numbers and thus make results reproducible)\nset.seed(2019120206)\n# create a new model with fewer trees and that takes 2 variables at a time\nrfmodel3 <- randomForest(SUFlike ~ ., data=rfdata, ntree=30, mtry = 4, proximity=TRUE)\n# inspect model\nrfmodel3\n\n\nCall:\n randomForest(formula = SUFlike ~ ., data = rfdata, ntree = 30,      mtry = 4, proximity = TRUE) \n               Type of random forest: regression\n                     Number of trees: 30\nNo. of variables tried at each split: 4\n\n          Mean of squared residuals: 0.1283899\n                    % Var explained: 9.85\n\n\nDespite optimization, the results have not changed but it may be very useful for other data. To evaluate the tree, we create a confusion matrix.\n\n# save what the model predicted in a new variable\nrfdata$Probability <- predict(rfmodel3, rfdata)\nrfdata$Prediction <- ifelse(rfdata$Probability >=.5, 1, 0)\n# create confusion matrix to check accuracy\nconfusionMatrix(as.factor(rfdata$Prediction), as.factor(rfdata$SUFlike))  \n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction    0    1\n         0 1622  297\n         1   34   47\n                                             \n               Accuracy : 0.8345             \n                 95% CI : (0.8175, 0.8505)   \n    No Information Rate : 0.828              \n    P-Value [Acc > NIR] : 0.2303             \n                                             \n                  Kappa : 0.1665             \n                                             \n Mcnemar's Test P-Value : <0.0000000000000002\n                                             \n            Sensitivity : 0.9795             \n            Specificity : 0.1366             \n         Pos Pred Value : 0.8452             \n         Neg Pred Value : 0.5802             \n             Prevalence : 0.8280             \n         Detection Rate : 0.8110             \n   Detection Prevalence : 0.9595             \n      Balanced Accuracy : 0.5580             \n                                             \n       'Positive' Class : 0                  \n                                             \n\n\nThe RF performs significantly better than a no-information base-line model but the base-line model already predicts 78.18 percent of cases correctly (compared to the RF with a prediction accuracy of 82.5 percent).\nUnfortunately, we cannot easily compute robust variable importance for RF models nor C or Somers’ Dxy which is why it is advisable to create analogous models using both the cforest and the randomForest functions. In a last step, we can now visualize the results of the optimized RF.\n\n# plot variable importance\nvarImpPlot(rfmodel3, main = \"\", pch = 20)\n\n\n\n\nHere is an alternative way of plotting variable importance using the vip function from the vip package.\n\n# generate vip plot\nvip::vip(rfmodel3, geom = \"point\", horizontal = FALSE)\n\n\n\n\nUsing the vip package, you can also generate variable importance barplots.\n\n# generate vip plot\nvip::vip(rfmodel3, horizontal = FALSE)\n\n\n\n\nA second type of visualization that can provide insights in the partial function from the pdp package which shows the effects of individual predictors - but remember that this still does not provide information about the way that the predictor interacts with other predictors.\n\n# extract importance of individual variables\nrfmodel3 %>%  # the %>% operator is read as \"and then\"\n  partial(pred.var = \"Age\") %>%\n  autoplot(smooth = TRUE, ylab = expression(f(Age))) +\n  theme_light() +\n  ggtitle(\"Partial Depencence Plot: Age\") + \n  ylim(-20, 0)\n\n\n\n\nYou can however use the partial function to show how the effect of predictors interacts with other predictors (see below).\n\npartial(rfmodel3, pred.var = c(\"Age\", \"Gender\"), plot = TRUE, plot.engine = \"ggplot2\")\n\n\n\n\nAnother common way to evaluate the performance of RFs is to split the data into a test and a training set. the model is then fit to the training set and, after that, applied to the test set. This allows us to evaluate how well the RF performs on data that it was not trained on. This approach is particularly common in machine learning contexts."
  },
  {
    "objectID": "tree.html#advantages-1",
    "href": "tree.html#advantages-1",
    "title": "Tree-Based Models in R",
    "section": "Advantages",
    "text": "Advantages\nBoruta outperforms random forest analyses because:\n\nBoruta does not provide merely a single value for each predictor but a distribution of values leading to higher reliability.\nBoruta provides definitive cut-off points for variables that have no meaningful relationship with the dependent variable. This is a crucial difference between RF and Boruta that make Boruta particularly interesting from a variable selection point of view."
  },
  {
    "objectID": "tree.html#procedure",
    "href": "tree.html#procedure",
    "title": "Tree-Based Models in R",
    "section": "Procedure",
    "text": "Procedure\nThe Boruta procedure consists out of five steps.\n\nIn a first step, the Boruta algorithm copies the data set and adds randomness to the data by (re-)shuffling data points and thereby creating randomized variables. These randomized variables are referred to as shadow features.\nSecondly, a random forest classifier is trained on the extended data set.\nIn a third step, a feature importance measure (Mean Decrease Accuracy represented by z-scores) is calculated to determine the relative importance of all predictors (both original or real variables and the randomized shadow features).\nIn the next step, it is checked at each iteration of the process whether a real predictor has a higher importance compared with the best shadow feature. The algorithm keeps track of the performance of the original variables by storing whether they outperformed the best shadow feature or not in a vector.\nIn the fifth step, predictors that did not outperform the best shadow feature are removed and the process continues without them. After a set number of iterations, or if all the variables have been either confirmed as outperforming the best shadow feature, the algorithm stops.\n\nDespite its obvious advantages of Boruta over random forest analyses and regression modeling, it can neither handle multicollinearity not hierarchical data structures where data points are nested or grouped by a given predictor (as is the case in the present analysis as data points are grouped by adjective type). As Boruta is a variable selection procedure, it is also limited in the sense that it provides information on which predictors to include and how good these predictors are (compared to the shadow variables) while it is neither able to take hierarchical data structure into account, nor does it provide information about how one level of a factor compares to other factors. In other words, Boruta shows that a predictor is relevant and how strong it is but it does not provide information on how the likelihood of an outcome being used differs between variable levels, for instance between men and women."
  },
  {
    "objectID": "tree.html#boruta-in-r",
    "href": "tree.html#boruta-in-r",
    "title": "Tree-Based Models in R",
    "section": "Boruta in R",
    "text": "Boruta in R\nWe begin by loading and inspecting the data.\n\n# load data\nborutadata <- read.delim(\"https://slcladal.github.io/data/ampaus05_statz.txt\", header = T, sep = \"\\t\")\n\n\n\n\n\n\nFirst 10 rows of the borutadata data.\n\n\nAgeAdjectiveFileSpeakerFunctionPrimingGenderOccupationConversationTypeAudienceSizeveryreallyFreqGradabiltySemanticCategoryEmotionality26-40good<S1A-001:1$B>AttributiveNoPrimeMenAcademicManagerialProfessionalsSameSexMultipleInterlocutors0027.84810NotGradableValuePositiveEmotional26-40good<S1A-001:1$B>AttributiveNoPrimeMenAcademicManagerialProfessionalsSameSexMultipleInterlocutors0027.84810NotGradableValuePositiveEmotional26-40good<S1A-001:1$B>PredicativeNoPrimeMenAcademicManagerialProfessionalsSameSexMultipleInterlocutors0027.84810NotGradableValuePositiveEmotional17-25nice<S1A-003:1$B>AttributiveNoPrimeMenAcademicManagerialProfessionalsSameSexDyad107.29282NotGradableHumanPropensityNonEmotional41-80other<S1A-003:1$A>PredicativeNoPrimeMenAcademicManagerialProfessionalsSameSexDyad000.61728NotGradableValueNonEmotional41-80other<S1A-004:1$C>PredicativeNoPrimeMenMixedSexMultipleInterlocutors102.46914NotGradableValuePositiveEmotional41-80good<S1A-004:1$B>AttributiveNoPrimeWomenAcademicManagerialProfessionalsMixedSexMultipleInterlocutors0020.98765NotGradableValuePositiveEmotional41-80other<S1A-005:1$B>PredicativeNoPrimeWomenMixedSexMultipleInterlocutors100.61728GradabilityUndeterminedHumanPropensityNegativeEmotional17-25other<S1A-006:1$B>AttributiveNoPrimeMenAcademicManagerialProfessionalsMixedSexMultipleInterlocutors014.64088GradabilityUndeterminedDimensionNonEmotional17-25other<S1A-006:1$B>AttributivePrimeMenAcademicManagerialProfessionalsMixedSexMultipleInterlocutors010.44199NotGradablePhysicalPropertyNonEmotional\n\n\nAs the data contains non-factorized character variables, we convert those into factors.\n\n# factorize variables (boruta - like rf - require factors instead of character vectors)\nborutadata <- borutadata %>%\n  dplyr::filter(complete.cases(.)) %>%\n  dplyr::mutate_if(is.character, factor)\n\n\n\n\n\n\nFirst 10 rows of the factorized borutadata data.\n\n\nAgeAdjectiveFileSpeakerFunctionPrimingGenderOccupationConversationTypeAudienceSizeveryreallyFreqGradabiltySemanticCategoryEmotionality26-40good<S1A-001:1$B>AttributiveNoPrimeMenAcademicManagerialProfessionalsSameSexMultipleInterlocutors0027.84810NotGradableValuePositiveEmotional26-40good<S1A-001:1$B>AttributiveNoPrimeMenAcademicManagerialProfessionalsSameSexMultipleInterlocutors0027.84810NotGradableValuePositiveEmotional26-40good<S1A-001:1$B>PredicativeNoPrimeMenAcademicManagerialProfessionalsSameSexMultipleInterlocutors0027.84810NotGradableValuePositiveEmotional17-25nice<S1A-003:1$B>AttributiveNoPrimeMenAcademicManagerialProfessionalsSameSexDyad107.29282NotGradableHumanPropensityNonEmotional41-80other<S1A-003:1$A>PredicativeNoPrimeMenAcademicManagerialProfessionalsSameSexDyad000.61728NotGradableValueNonEmotional41-80good<S1A-004:1$B>AttributiveNoPrimeWomenAcademicManagerialProfessionalsMixedSexMultipleInterlocutors0020.98765NotGradableValuePositiveEmotional17-25other<S1A-006:1$B>AttributiveNoPrimeMenAcademicManagerialProfessionalsMixedSexMultipleInterlocutors014.64088GradabilityUndeterminedDimensionNonEmotional17-25other<S1A-006:1$B>AttributivePrimeMenAcademicManagerialProfessionalsMixedSexMultipleInterlocutors010.44199NotGradablePhysicalPropertyNonEmotional17-25other<S1A-006:1$B>PredicativePrimeMenAcademicManagerialProfessionalsMixedSexMultipleInterlocutors010.44199NotGradablePhysicalPropertyNonEmotional17-25nice<S1A-007:1$A>AttributiveNoPrimeWomenAcademicManagerialProfessionalsSameSexDyad017.29282NotGradableHumanPropensityNonEmotional\n\n\nWe can now create our initial Boruta model and set a seed for reproducibility.\n\n# set.seed \nset.seed(2019120207)\n# initial run\nboruta1 <- Boruta(really~.,data=borutadata)\nprint(boruta1)\n\nBoruta performed 99 iterations in 7.042017 secs.\n 8 attributes confirmed important: Adjective, AudienceSize,\nConversationType, Emotionality, FileSpeaker and 3 more;\n 5 attributes confirmed unimportant: Age, Gender, Occupation, Priming,\nSemanticCategory;\n 1 tentative attributes left: Gradabilty;\n\n# extract decision\ngetConfirmedFormula(boruta1)\n\nreally ~ Adjective + FileSpeaker + Function + ConversationType + \n    AudienceSize + very + Freq + Emotionality\n<environment: 0x561537d0aa10>\n\n\nIn a next step, we inspect the history to check if any of the variables shows drastic fluctuations in their importance assessment.\n\nplotImpHistory(boruta1)\n\n\n\n\nThe fluctuations are do not show clear upward or downward trends (which what we want). If predictors do perform worse than the shadow variables, then these variables should be excluded and the Boruta analysis should be re-run on the data set that does no longer contain the superfluous variables. Tentative variables can remain but they are unlikely to have any substantial effect. We thus continue by removing variables that were confirmed as being unimportant, then setting a new seed, re-running the Boruta on the reduced data set, and again inspecting the decisions.\n\n# remove irrelevant variables\nrejected <- names(boruta1$finalDecision)[which(boruta1$finalDecision == \"Rejected\")]\n# update data for boruta\nborutadata <- borutadata %>%\n  dplyr::select(-rejected)\n# set.seed (to store random numbers and thus make results reproducible)\nset.seed(2019120208)\n# 2nd run\nboruta2 <- Boruta(really~.,data=borutadata)\nprint(boruta2)\n\nBoruta performed 99 iterations in 7.846175 secs.\n 8 attributes confirmed important: Adjective, AudienceSize,\nConversationType, Emotionality, FileSpeaker and 3 more;\n No attributes deemed unimportant.\n 1 tentative attributes left: Gradabilty;\n\n# extract decision\ngetConfirmedFormula(boruta2)\n\nreally ~ Adjective + FileSpeaker + Function + ConversationType + \n    AudienceSize + very + Freq + Emotionality\n<environment: 0x561539916328>\n\n\nOnly adjective frequency and adjective type are confirmed as being important while all other variables are considered tentative. However, no more variables need to be removed as all remaining variables are not considered unimportant. In a last step, we visualize the results of the Boruta analysis.\n\nborutadf <- as.data.frame(boruta2$ImpHistory) %>%\n  tidyr::gather(Variable, Importance, Adjective:shadowMin) %>%\n  dplyr::mutate(Type = ifelse(str_detect(Variable, \"shadow\"), \"Control\", \"Predictor\")) %>%\n  dplyr::mutate(Type = factor(Type),\n                Variable = factor(Variable))\nggplot(borutadf, aes(x = reorder(Variable, Importance, mean), y = Importance, fill = Type)) + \n  geom_boxplot() +\n  geom_vline(xintercept=3.5, linetype=\"dashed\", color = \"black\") +\n  scale_fill_manual(values = c(\"gray80\", \"gray40\")) +\n  theme_bw() + \n  theme(legend.position = \"top\",\n        axis.text.x = element_text(angle=90)) +\n  labs(x = \"Variable\")\n\n\n\n\nOf the remaining variables, adjective frequency and adjective type have the strongest effect and are confirmed as being important while syntactic function fails to perform better than the best shadow variable. All other variables have only a marginal effect on the use of really as an adjective amplifier."
  },
  {
    "objectID": "tutorials.html",
    "href": "tutorials.html",
    "title": "Tutorials",
    "section": "",
    "text": "This page lists the tutorials offered by LADAL - if you want to check out a tutorial, simply click on the title of the tutorial and you will taken directly sent to the respective website.\n\n1 Data Science Basics\n\n\n\n\n\nWorking with Computers\nThis tutorial provides advice and general tips on how to keep your computer clean and running smoothly, how to organize files and folders, and how to store your data safely and orderly.\nData Management and Reproducibility\nThis tutorial introduces basic data management techniques, version control measures, and issues relating to reproducible research.\nIntroduction to Quantitative Reasoning\nThis tutorial takes a philosophical or history-of-ideas approach and introduces the logical and cognitive underpinnings of the the scientific method.\nBasic Concepts in Quantitative Research\nThis tutorial introduces basic concepts of data analysis and quantitative reserach.\n\n\n2 R Basics\n\n\n\n\n\nWhy R?\nThis site provides our reasoning for focusing (almost exclusively) on R in LADAL.\nGetting started\nThis tutorial shows how to get started with R and it specifically focuses on R for analyzing language data but it offers valuable information for anyone who wants to get started with R.\nString Processing\nThis tutorial introduces string processing and this can be used when working with language data.\nRegular Expressions\nThis tutorial introduces regular expressions and how they can be used when working with language data.\nHandling tables in R\nThis tutorial shows how to work with tables and how to tabulate data in R.\n\n\n3 Data Visualization\n\n\n\n\n\nIntroduction to Data Viz\nThis tutorial introduces data visualization using R and shows how to modify different types of visualizations in the ggplot framework in R.\nData Visualization with R\nThis tutorial introduces different types of data visualization and how to prepare your dat for different plot types.\nIntroduction to Geospatial Data Visualization\nThis tutorial introduces geo-spatial data visualization in R.\nInteractive Charts\nThis tutorial shows how to generate interactive data visualizations in R.\n\n\n4 Statistics\n\n\n\n\n\nDescriptive Statistics\nThis tutorial focuses on how to describe and summarize data in R.\nBasic Inferential Statistics\nThis tutorial introduces basic inferential procedures for null-hypothesis hypothesis testing.\nRegression Analysis\nThis tutorial introduces regression analyses (also called regression modeling) using R. Regression models are among the most widely used quantitative methods in the language sciences to assess if and how predictors (variables or interactions between variables) correlate with a certain response.\n\n\n\n\n\nTree-Based Models\nThis tutorial focuses on tree-based models and their implementation in R.\nCluster and Correspondence Analysis\nThis tutorial introduces classification and clustering using R. Cluster analyses fall within the domain of classification methods which are used to find groups or patterns in data or to predict group membership.\n\n\n\n\n\n\nIntroduction to Lexical Similarity\nThis tutorial introduces Text Similarity, i.e. how close or similar two pieces of text are with respect to either their use of words or characters (lexical similarity) or in terms of meaning (semantic similarity).\nSemantic Vector Space Models\nThis tutorial introduces Semantic Vector Space (SVM) modeling R. SVMs are used to find groups or patterns in data or to predict group membership.\nPower Analysis\nThis tutorial introduces power analysis using R. Power analysis is a method primarily used to determine the appropriate sample size for empirical studies.\n\n\n5 Text Analytics\n\n\n\n\n\nIntroduction to Text Analysis\nThis tutorial introduces Text Analysis, i.e. computer-based analysis of language data or the (semi-)automated extraction of information from text.\nPractical Overview of Selected Text Analytics Methods\nThis tutorial showcases some basic but useful methods for text analysis and serves as a practical overview or introduction to Text analytics and distant reading.\nConcordancing (keywords-in-context)\nThis tutorial introduces how to find words or phrases in text and display concordances, a so-called keyword-in-context (KWIC) display, with R.\nNetwork Analysis\nThis tutorial introduces network analysis using R. Network analysis is a method for visualization that can be used to represent various types of data.\nCo-occurrence and Collocation Analysis\nThis tutorial introduces collocation and co-occurrence analysis with R and shows how to extract and visualize semantic links between words.\n\n\n\n\n\nTopic Modeling\nThis tutorial introduces topic modeling using R.\nSentiment Analysis\nThis tutorial introduces sentiment analysis (SA) and shows how to perform a SA in R.\nTagging and Parsing\nThis tutorial introduces part-of-speech tagging and syntactic parsing using R.\nAutomated Text Summarization\nThis tutorial shows how to summarize texts automatically using R by extracting the most prototypical sentences.\n\n\n6 Case Studies\n\n\n\n\n\nCorpus Linguistics with R\nThis section presents different case studies or use cases that highlight how to do corpus-based analyses by implementing procedures shown in other LADAL tutorials.\nAnalyzing learner language using R\nThis tutorial focuses on learner language and how to analyze differences between learners and L1 speakers of English using R.\nLexicography and Creating Dictionaries with R\nThis tutorial introduces lexicography with R and shows how to use R to create dictionaries and find synonyms through determining semantic similarity in R.\nVisualizing and Analyzing Questionnaire and Survey Data\nThis tutorial offers some advice on what to consider when creating surveys and questionnaires, provides tips on visualizing survey data, and exemplifies how survey and questionnaire data can be analyzed.\n\n\n\n\n\nCreating Vowel Charts in R\nThis tutorial exemplifies how to create a vowel chart with Praat and R.\nComputational Literary Stylistics with R\nThis tutorial focuses on computational literary stylistics (also digital literary stylistics) and shows how fictional texts can be analyzed by using computational means.\nPractical phylogenetic methods for linguistic typology\nThis tutorial shows how you can do phylogenetic analysis in R.\nReinforcement Learning and Text Summarization in R\nThis tutorial introduces the concept of Reinforcement Learning (RL), and how it can be applied in the domain of Natural Language Processing (NLP) and linguistics.\n\n\n7 How-Tos\n\n\n\n\n\nConverting PDFs to txt\nThis tutorial shows how to extract text from one or more pdf-files using optical character recognition (OCR) and then saving the text(s) in txt-files on your computer.\nDownloading Texts from Project Gutenberg\nThis tutorial shows how to download and clean works from the Project Gutenberg archive using R. Project Gutenberg is a data base which contains roughly 60,000 texts for which the US copyright has expired.\n\nBack to top\nBack to HOME"
  },
  {
    "objectID": "txtsum.html",
    "href": "txtsum.html",
    "title": "Automated Text Summarization with R",
    "section": "",
    "text": "Introduction\nThis tutorial shows how to summarize texts automatically using R by extracting the most prototypical sentences.\n\n\n\n\n\nThis tutorial is aimed at beginners and intermediate users of R with the aim of showcasing how to summarize textual data using R. The aim is not to provide a fully-fledged analysis but rather to show and exemplify selected useful methods associated with summarizing texts.\n\n\n\nThe entire R Notebook for the tutorial can be downloaded here. If you want to render the R Notebook on your machine, i.e. knitting the document to html or a pdf, you need to make sure that you have R and RStudio installed and you also need to download the bibliography file and store it in the same folder where you store the Rmd file. \n\n\n\n\n\n\nPreparation and session set up\nThis tutorial is based on R. If you have not installed R or are new to it, you will find an introduction to and more information how to use R here. For this tutorials, we need to install certain packages from an R library so that the scripts shown below are executed without errors. Before turning to the code below, please install the packages by running the code below this paragraph. If you have already installed the packages mentioned below, then you can skip ahead and ignore this section. To install the necessary packages, simply run the following code - it may take some time (between 1 and 5 minutes to install all of the libraries so you do not need to worry if it takes some time).\n\n# set options\noptions(stringsAsFactors = F)          # no automatic data transformation\noptions(\"scipen\" = 100, \"digits\" = 12) # suppress math annotation\n# install packages\ninstall.packages(\"xml2\")\ninstall.packages(\"rvest\")\ninstall.packages(\"lexRankr\")\ninstall.packages(\"textmineR\")\ninstall.packages(\"tidyverse\")\ninstall.packages(\"quanteda\")\ninstall.packages(\"igraph\")\ninstall.packages(\"here\")\n# install klippy for copy-to-clipboard button in code chunks\ninstall.packages(\"remotes\")\nremotes::install_github(\"rlesur/klippy\")\n\nNext we activate the packages.\n\n# activate packages\nlibrary(xml2)\nlibrary(rvest)\nlibrary(lexRankr)\nlibrary(textmineR)\nlibrary(tidyverse)\nlibrary(quanteda)\nlibrary(igraph)\nlibrary(here)\n# activate klippy for copy-to-clipboard button\nklippy::klippy()\n\n\n\n\nOnce you have installed RStudio and have also initiated the session by executing the code shown above, you are good to go.\n\n\nBasic Text summarization\nThis section shows an easy to use text summarizing method which extracts the most prototypical sentences from a text. As such, this text summarizer does not generate sentences based on prototypical words but evaluates how prototypical or central sentences are and then orders the sentences in a text according to their prototypicality (or centrality).\nFor this example, we will download text from a Guardian article about a meeting between Angela Merkel and Donald Trump at the G20 summit in 2017. In a first step, we define the url of the webpage hosting the article.\n\n# url to scrape\nurl = \"https://www.theguardian.com/world/2017/jun/26/angela-merkel-and-donald-trump-head-for-clash-at-g20-summit\"\n\nNext, we extract the text of the article using thexml2  and thervest` packages.\n\n# read page html\npage = xml2::read_html(url)\n# extract text from page html using selector\npage %>%\n  # extract paragraphs\n  rvest::html_nodes(\"p\") %>%\n  # extract text\n  rvest::html_text() %>%\n  # remove empty elements\n  .[. != \"\"] -> text\n# inspect data\nhead(text)\n\n[1] \"German chancellor plans to make climate change, free trade and mass migration key themes in Hamburg, putting her on collision course with US\"                                                                                                        \n[2] \"A clash between Angela Merkel and Donald Trump appears unavoidable after Germany signalled that it will make climate change, free trade and the management of forced mass global migration the key themes of the G20 summit in Hamburg next week.\"   \n[3] \"The G20 summit brings together the world’s biggest economies, representing 85% of global gross domestic product (GDP), and Merkel’s chosen agenda looks likely to maximise American isolation while attempting to minimise disunity amongst others. \"\n[4] \"The meeting, which is set to be the scene of large-scale street protests, will also mark the first meeting between Trump and the Russian president, Vladimir Putin, as world leaders.\"                                                               \n[5] \"Trump has already rowed with Europe once over climate change and refugees at the G7 summit in Italy, and now looks set to repeat the experience in Hamburg but on a bigger stage, as India and China join in the criticism of Washington. \"          \n[6] \"Last week, the new UN secretary-general, António Guterres, warned the Trump team if the US disengages from too many issues confronting the international community it will be replaced as world leader.\"                                             \n\n\nNow that we have the text, we apply the lexRank function from the lexRankr package to determine the prototypicality (or centrality) and extract the three most central sentences.\n\n# perform lexrank for top 3 sentences\ntop3sentences = lexRankr::lexRank(text,\n                          # only 1 article; repeat same docid for all of input vector\n                          docId = rep(1, length(text)),\n                          # return 3 sentences\n                          n = 3,\n                          continuous = TRUE)\n\nParsing text into sentences and tokens...DONE\nCalculating pairwise sentence similarities...DONE\nApplying LexRank...DONE\nFormatting Output...DONE\n\n# inspect\ntop3sentences\n\n  docId sentenceId\n1     1        1_2\n2     1        1_5\n3     1       1_16\n                                                                                                                                                                                                                                                       sentence\n1             A clash between Angela Merkel and Donald Trump appears unavoidable after Germany signalled that it will make climate change, free trade and the management of forced mass global migration the key themes of the G20 summit in Hamburg next week.\n2                     Trump has already rowed with Europe once over climate change and refugees at the G7 summit in Italy, and now looks set to repeat the experience in Hamburg but on a bigger stage, as India and China join in the criticism of Washington.\n3 But the G7, and Trump’s subsequent decision to shun the Paris climate change treaty, clearly left a permanent mark on her, leading to her famous declaration of independence four days later at a Christian Social Union (CSU) rally in a Bavarian beer tent.\n       value\n1 0.06017053\n2 0.05656337\n3 0.04974733\n\n\nNext, we extract and display the sentences from the table.\n\ntop3sentences$sentence\n\n[1] \"A clash between Angela Merkel and Donald Trump appears unavoidable after Germany signalled that it will make climate change, free trade and the management of forced mass global migration the key themes of the G20 summit in Hamburg next week.\"            \n[2] \"Trump has already rowed with Europe once over climate change and refugees at the G7 summit in Italy, and now looks set to repeat the experience in Hamburg but on a bigger stage, as India and China join in the criticism of Washington.\"                    \n[3] \"But the G7, and Trump’s subsequent decision to shun the Paris climate change treaty, clearly left a permanent mark on her, leading to her famous declaration of independence four days later at a Christian Social Union (CSU) rally in a Bavarian beer tent.\"\n\n\nThe output show the three most prototypical (or central) sentences of the article. The articles are already in chronological order - if the sentences were not in chronological order, we could also have ordered them by sentenceId before displaying them using dplyr and stringr package functions as shown below (in our case the order does not change as the prototypicality and the chronological order are identical).\n\ntop3sentences %>%\n  dplyr::mutate(sentenceId = as.numeric(stringr::str_remove_all(sentenceId, \".*_\"))) %>%\n  dplyr::arrange(sentenceId) %>%\n  dplyr::pull(sentence)\n\n[1] \"A clash between Angela Merkel and Donald Trump appears unavoidable after Germany signalled that it will make climate change, free trade and the management of forced mass global migration the key themes of the G20 summit in Hamburg next week.\"            \n[2] \"Trump has already rowed with Europe once over climate change and refugees at the G7 summit in Italy, and now looks set to repeat the experience in Hamburg but on a bigger stage, as India and China join in the criticism of Washington.\"                    \n[3] \"But the G7, and Trump’s subsequent decision to shun the Paris climate change treaty, clearly left a permanent mark on her, leading to her famous declaration of independence four days later at a Christian Social Union (CSU) rally in a Bavarian beer tent.\"\n\n\n\n\n\n\nEXERCISE TIME!\n\n\n\n\n\n\n`\n\nExtract the top 10 sentences from every chapter of Charles Darwin’s On the Origin of Species. You can download the text using this command: darwin <- base::readRDS(url(\"https://slcladal.github.io/data/origindarwin.rda\", \"rb\")). You will then have to paste the whole text together, split it into chapters, create a list of sentences in each chapter, and then apply text summarization to each element in the list. \n\n\n\nAnswer\n\n::: {.cell}\n  darwin <- base::readRDS(url(\"https://slcladal.github.io/data/origindarwin.rda\", \"rb\")) %>%\n  # collapse into single document\n  paste0(collapse = \" \") %>%\n  # split into chapters\n  stringr::str_split(\"CHAPTER\")\n  \n  # split chapters into sentences\n  chapters <- sapply(darwin, function(x){\n    x <- stringi::stri_split_boundaries(x, type = \"sentence\")\n  })\n  \n  chapters_clean <- lapply(chapters, function(x){\n    # remove chapter headings\n    x <- stringr::str_remove_all(x, \"[A-Z]{2,} {0,1}[0-9]{0,}\")\n  })\n  \n  # extract top 3 sentences from each chapter\n  top3s <- lapply(chapters_clean, function(x){\n    x <- lexRankr::lexRank(x,\n                          # only 1 article; repeat same docid for all of input vector\n                          #docId = rep(1, length(text)),\n                          # return 3 sentences\n                          n = 3,\n                          continuous = TRUE) %>%\n                          dplyr::pull(sentence) %>%\n    # remove special characters\n    stringr::str_remove_all(\"[^[:alnum:] ]\") %>%\n    # remove superfluous white spaces\n    stringr::str_squish()\n  })\n  \n  # inspect top 3 sentences of first 5 chapters\n  top3s[1:5]\n:::\n\n\n`\n\nYou can go ahead and play with the text summarization and see if it is useful for you or if you can trust the results based on your data.\n\n\nCitation & Session Info\nSchweinberger, Martin. 2022. Automated text summarization with R. Brisbane: The University of Queensland. url: https://slcladal.github.io/txtsum.html (Version 2022.08.31).\n@manual{schweinberger2022txtsum,\n  author = {Schweinberger, Martin},\n  title = {Automated Text Summarization with R},\n  note = {https://slcladal.github.io/txtsum.html},\n  year = {2022},\n  organization = \"The University of Queensland, Australia. School of Languages and Cultures},\n  address = {Brisbane},\n  edition = {2022.08.31}\n}\n\nsessionInfo()\n\nR version 4.2.1 (2022-06-23)\nPlatform: x86_64-pc-linux-gnu (64-bit)\nRunning under: Ubuntu 22.04.1 LTS\n\nMatrix products: default\nBLAS:   /usr/lib/x86_64-linux-gnu/blas/libblas.so.3.10.0\nLAPACK: /usr/lib/x86_64-linux-gnu/lapack/liblapack.so.3.10.0\n\nlocale:\n [1] LC_CTYPE=en_AU.UTF-8       LC_NUMERIC=C              \n [3] LC_TIME=en_AU.UTF-8        LC_COLLATE=en_AU.UTF-8    \n [5] LC_MONETARY=en_AU.UTF-8    LC_MESSAGES=en_AU.UTF-8   \n [7] LC_PAPER=en_AU.UTF-8       LC_NAME=C                 \n [9] LC_ADDRESS=C               LC_TELEPHONE=C            \n[11] LC_MEASUREMENT=en_AU.UTF-8 LC_IDENTIFICATION=C       \n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] here_1.0.1      igraph_1.3.2    quanteda_3.2.1  forcats_0.5.1  \n [5] stringr_1.4.0   dplyr_1.0.9     purrr_0.3.4     readr_2.1.2    \n [9] tidyr_1.2.0     tibble_3.1.7    ggplot2_3.3.6   tidyverse_1.3.2\n[13] textmineR_3.0.5 Matrix_1.4-1    lexRankr_0.5.2  rvest_1.0.2    \n[17] xml2_1.3.3     \n\nloaded via a namespace (and not attached):\n [1] httr_1.4.3          jsonlite_1.8.0      modelr_0.1.8       \n [4] RcppParallel_5.1.5  assertthat_0.2.1    selectr_0.4-2      \n [7] googlesheets4_1.0.0 cellranger_1.1.0    yaml_2.3.5         \n[10] pillar_1.7.0        backports_1.4.1     lattice_0.20-45    \n[13] glue_1.6.2          digest_0.6.29       colorspace_2.0-3   \n[16] htmltools_0.5.2     pkgconfig_2.0.3     broom_1.0.0        \n[19] haven_2.5.0         scales_1.2.0        tzdb_0.3.0         \n[22] googledrive_2.0.0   generics_0.1.3      ellipsis_0.3.2     \n[25] withr_2.5.0         klippy_0.0.0.9500   cli_3.3.0          \n[28] magrittr_2.0.3      crayon_1.5.1        readxl_1.4.0       \n[31] evaluate_0.15       stopwords_2.3       fs_1.5.2           \n[34] fansi_1.0.3         SnowballC_0.7.0     RcppProgress_0.4.2 \n[37] tools_4.2.1         hms_1.1.1           gargle_1.2.0       \n[40] lifecycle_1.0.1     munsell_0.5.0       reprex_2.0.1       \n[43] compiler_4.2.1      rlang_1.0.4         grid_4.2.1         \n[46] rstudioapi_0.13     htmlwidgets_1.5.4   rmarkdown_2.14     \n[49] gtable_0.3.0        DBI_1.1.3           curl_4.3.2         \n[52] R6_2.5.1            lubridate_1.8.0     knitr_1.39         \n[55] fastmap_1.1.0       utf8_1.2.2          fastmatch_1.1-3    \n[58] rprojroot_2.0.3     stringi_1.7.8       Rcpp_1.0.8.3       \n[61] vctrs_0.4.1         dbplyr_2.2.1        tidyselect_1.1.2   \n[64] xfun_0.31          \n\n\n\nBack to top\nBack to HOME"
  },
  {
    "objectID": "vc.html",
    "href": "vc.html",
    "title": "Creating Vowel Charts in R",
    "section": "",
    "text": "Introduction\nThis tutorial exemplifies how to create a vowel chart with Praat and R.\n\n\n\n\n\nThis tutorial is aimed at beginners and intermediate users of R with the aim of showcasing how to create personalized vowel chart using Praat and R. The aim is not to provide a fully-fledged analysis but rather to show and exemplify easily generate these vowel charts without much prior knowledge.\n\n\n\nThe entire R Notebook for the tutorial can be downloaded here. If you want to render the R Notebook on your machine, i.e. knitting the document to html or a pdf, you need to make sure that you have R and RStudio installed and you also need to download the bibliography file and store it in the same folder where you store the Rmd file. \n\n\n\n\n\n\nPreparation and session set up\nThis tutorial is based on R. If you have not installed R or are new to it, you will find an introduction to and more information how to use R here. For this tutorials, we need to install certain packages from an R library so that the scripts shown below are executed without errors. Before turning to the code below, please install the packages by running the code below this paragraph. If you have already installed the packages mentioned below, then you can skip ahead and ignore this section. To install the necessary packages, simply run the following code - it may take some time (between 1 and 5 minutes to install all of the packages so you do not need to worry if it takes some time).\n\n# install packages\ninstall.packages(\"vowels\")\ninstall.packages(\"tidyverse\")\ninstall.packages(\"flextable\")\n# install klippy for copy-to-clipboard button in code chunks\ninstall.packages(\"remotes\")\nremotes::install_github(\"rlesur/klippy\")\n\nWe now load the packages.\n\n# load packages\nlibrary(tidyverse)\nlibrary(vowels)\nlibrary(flextable)\n# activate klippy for copy-to-clipboard button\nklippy::klippy()\n\n\n\n\nOnce you have installed R and RStudio and once you have also initiated the session by executing the code shown above, you are good to go.\n\n\nVowel sounds\nWhen learning or studying a language - the case in point here being English - it is likely that you are confronted with different classes of sounds, e.g. consonants and vowels [1]. Consonants differ from vowels in that they are formed with an obstruction of the air stream coming from the lungs and they cannot form the nucleus of a syllable [2]. In fact, consonants are classified according to the manner and place of the obstruction of the air stream. As vowels are produced without obstruction of the air stream, other criteria for differentiating between vowel sounds are needed. The criteria for differentiating between different vowel sounds are\n\nthe number of tongue positions during vowel production (to differentiate between mono-, diph-, and triphthongs),\nheight of the tongue,\nposition of the tongue,\nroundedness of the lips.\n\nThe latter two features are used in the production of vowel charts which show where in the mouth the tongue is located during the production of monophthongal vowel phones. A vowel chart for the monophthongal vowel phones in Received Pronunciation (RP) is shown below.\n\n\n\n\n\nFigure 1: Vowel Chart of monophthongal vowel sounds in Received Pronunciation (RP) (left); Tongue positions for the vowels /i, e, E, a/ (right)\n\n\n\n\n\n\n\nFigure 1: Vowel Chart of monophthongal vowel sounds in Received Pronunciation (RP) (left); Tongue positions for the vowels /i, e, E, a/ (right)\n\n\n\n\nInterestingly, a very similar figure can be created by plotting the Hertz frequency of the first formant of monophthongal vowel sound against the Hertz frequency of second formant minus the Hertz frequency of the first formant of a monophthongal vowel sound. Formants are frequencies of air waves that, if collapsed, form a complex vowel sound [3, 4]. In other words, vowels are periodic, i.e. rhythmic, compressions and decompressions of air and to create a vowel sound, i.e. a complex periodic wave, one needs to produce several simple periodic waves simultaneously. During acoustic analysis, the complex wave is deconstructed into its component parts, i.e. the simple periodic waves that make up that sound. This means that we do not necessarily have to plot the position of the tongue of a speaker when he or she produces vowels to create a vowel chart but that analyses of audio recordings of words in which vowels occur, can be utilized to plot a personalized vowel chart of a speaker. Such vowel charts can then be used in language learning as corrective feedback [see 5].\nTo produce a personalized vowel chart, the following steps are necessary:\n\nInstall Praat\nRecord words in which all monophthongal vowel sounds of a given variety occur;\nMeasure and extract the first and second formant of each vowel;\nVisualize the vowel sounds.\n\nThe subsequent sections elaborate the above steps. However, before continuing a word of warning is in order. The example focuses on extracting and plotting vowel formants in an easy but also very uncontrolled way. In case vowel formant extraction is part of a proper research project, some additional steps are warranted. For instance, in a serious research project, it were necessary to control and reduce environmental noise and to optimize the recording situation, one would have to randomize the test items (words with the required phonetic environment and the respective vowel sounds) and use filler items (words that are not relevant for the analysis proper) in order to avoid participants guessing which items are relevant for the analysis, one would also use text grids in Praat to guarantee replicability instead of the simple measurements we use in the example here, etc. However, in case you are only interested in an approximation of your own vowel production and how native-like it is, the example fulfills its purpose and provides the reader with a step-by-step guide on how to plot your personalized vowel chart.\nDownloading and installing PRAAT The first step is thus to download Praat form www.praat.org and to install it on your machine by following the instructions provided on the website and by the Praat installation script. Praat is an open{source software for acoustic analysis that was developed by Paul Boersma at the University of Amsterdam.\nAfter having installed Praat we need to record the words in which the monophthongal vowel phones occur. In this example, we will simply record the words shown below.\n\n\n\n\n\nWord selection with controlled environments in which monophthongal vowels occur.\n\n\nwordipa_symbolsphonemic_contexthadæ h_dhardɑ h_dheadeh_dheedih_dherdɜ h_dhidɪ h_dhoardɔ h_dhodɒ h_dhoodʊ h_dhudʌ h_dwho'duh_d\n\n\nThe following section describes how to record data in Praat [see 6 for a more elaborate description of how this can be done].\n\n\n1 Recording words in PRAAT\nTo record these words, start Praat with a double click on the Praat symbol which - after installation - appears on your Desktop. Two windows will appear: the main object window to the left and the picture window to the right (cf. Figure 2). Close the picture window on the right and choose New from the menu at the top of the main object window and select Record mono sound from the menu which pops up. For the recording it is, of course, necessary that a microphone is hook up to your machine { the better the microphone, the better the recording and thus the more accurate the graphical display we are going to produce.\n\n\n\n\n\nFigure 2: Praat’s main object window (left) and Praat’s picture window (right)\n\n\n\n\nSelecting Record mono sound opens Praat’s SoundRecorder window (cf. Figure 4). Select Record, label the recording by entering a title, e.g. vowels, in the Name field and read the words form the list shown in Table .\n\n\n\n\n\nFigure 3: Praat’s main object window\n\n\n\n\n\n\n\n\n\nFigure 4: Praat’s recording window\n\n\n\n\nEach word should be repeated at least three times with a short break between the individual items so that what you record is had, had, had … pause … hard, hard, hard, etc. Try to sound natural, i.e. avoid speaking too fast or too slow, and try not to sound artificial or too careful.\nWhile recording, there should be some green bouncing up and down in the vertical white ” stripe (no bouncing indicates that your machine is not recording properly from the microphone).Once you are finished with your recording, select Stop and next select Save to list & close (cf. Figure 8).\n\n\n\n\n\nFigure 5: Praat’s recording window during recording\n\n\n\n\n\n\n\n\n\nFigure 6: Praat’s recording window after recording\n\n\n\n\nSaving has created an object in Praat’s main object window - in case you have named your recording vowels, the new object will be called 1. Sound vowels (cf. Figure 7). Before editing the data, it is advisable to save them on your machine. To save the data select the Save option from the upper menu, then select Save as WAV file... and navigate to the directory in which you want to save the recorded data.\n\n\n\n\n\nFigure 7: Praat’s main object window with saved object\n\n\n\n\n\n\n\n\n\nFigure 8: Save the recording as a .wav file\n\n\n\n\nNext, select View & Edit in Praat’s main menu in the main object window. This will open Praat’s edit window (cf. Figure 9) - the object represents a recording of the word heed repeated three times for sake of simplicity.\n\n\n\n\n\nFigure 9: Praat’s edit window with the word heed repeated three times\n\n\n\n\nAfter recording and saving the data necessary for the task at hand, we continue by extracting the vowel formants.\n\n\n2 Measure and extract vowel formants\nBefore extracting of the vowel formants, some parameters need adjusting. In a first step, go to Formant from the menu at the top of the edit window and select Formant settings.... Next, select the option Show formant and then, depending on whether the recording represents a male, a female or a child, adjust the Maximum formant (Hz) to 5000 Hz (male), 5500 Hz (female) or up to 8000 Hz (for a child) (cf. http://www.haskins.yale.edu/staff/gafos_ downloads/AcouToyPraat(1).pdf). It may also be necessary to adjust the number of formants that Praat aims to find: the default is 5, but it may be set to any number between 3 and 7 depending on the data. To elaborate, if the formants do not exhibit a regular horizontal pattern but they are somewhat unsteady or the dots are all over the place, try to find the number of formants that provide the best results (i.e. steady horizontal lines).\n\n\n\n\n\nFigure 10: Praat’s edit window with the word heed repeated three times and formants shown\n\n\n\n\nAfter having set the parameters, listen to the recording and highlight the section which represents the vowel sound you want to extract the formants from. Highlightling is done by selecting the start and end point of the vowel sound - the beginning and end of the steady line during which the vowel is produced - within the edit window as done for the first of the three instances of heed in Figure 11.\n\n\n\n\n\nFigure 11: Praat’s edit window with the word heed repeated three times and formants shown and steady state selected\n\n\n\n\nThe vowel formants can be extracted by going to Formant in the edit window and selecting Get first formant. Having done so, a window with the mean Hertz frequency of the first formant during the steady state is shown (cf. Figure 12). Please note that you should additionally extract the start and end time of the highlighted section from the display in the edit window.\n\n\n\n\n\nFigure 12: The mean Hertz frequency of first formant of the word heed during the steady state\n\n\n\n\nTo extract the second (and in case you want to use your data in other analysis also the third formant) simply choose Get third formant (and Get second formant), note down the Hertz frequencies in a table, and also note down the start and end time of the steady state. The final table should look like Table below (some columns are removed for sake of simplicity).\n\n\n\n\n\nVowel formants extracted from PRAAT.\n\n\nfilesubjecttrialitemF1F2vowelsms1had717.33611,868.1754vowelsms1had743.48351,903.7152vowelsms1had720.97401,938.6928vowelsms1hard734.52751,493.3289vowelsms1hard832.92281,407.8247vowelsms1hard797.28421,498.2064vowelsms1head610.89432,062.8820vowelsms1head722.25192,130.6322vowelsms1head625.11172,009.6507vowelsms1heed263.38302,833.0017vowelsms1heed301.41762,745.8471vowelsms2heed286.96562,822.5988vowelsms2herd532.79251,704.9954vowelsms2herd537.79621,819.8916vowelsms2herd524.71371,704.2321vowelsms2hid451.87662,390.7996vowelsms2hid417.03302,483.3900vowelsms2hid410.68172,360.0382vowelsms2hoard540.3306951.1443vowelsms2hoard549.9205927.0956vowelsms2hoard648.04821,093.3466vowelsms2hod698.40691,144.4669vowelsms3hod615.16211,086.4479vowelsms3hod751.01901,452.4663vowelsms3hood431.29931,478.1930vowelsms3hood404.18841,453.1036vowelsms3hood470.14691,216.3027vowelsms3hud646.05141,700.0030vowelsms3hud622.53021,510.4514vowelsms3hud749.35401,581.7578vowelsms3whod346.88121,013.0007vowelsms3whod353.82651,285.8341vowelsms3whod366.81371,016.9800\n\n\nThe next section describes how to plot the data and compare the vowels to equivalent vowels produced by native-RP speakers.\n\n\n3 Visualizing the vowel sounds\nWe will now process the data so that we can plot the F1 against the F2 values by speaker and word. In a first step, we load the data from the learner (nns) and the native speakers (ns).\n\n# load data\nns <- read.table(\"https://slcladal.github.io/data/rpvowels.txt\", header = T, sep = \"\\t\")\nnns <- read.table(\"https://slcladal.github.io/data/vowels.txt\", header = T, sep = \"\\t\") %>%\n  dplyr::select(-file)\n\nThe data of the native speakers, i.e the reference data, is shown below.\n\n\n\n\n\nVowel formants of native RP speakers.\n\n\nsubjectitemcontextF1F2F1sdF2sdrpspkhadwordlist916.351,473.15124.29815119.43696rpspkhardwordlist604.151,040.1570.9197340.06478rpspkheadwordlist599.951,925.70102.22858143.60476rpspkheedwordlist276.152,337.6025.48328223.42440rpspkherdwordlist493.551,372.4047.4091795.94648rpspkhidwordlist392.852,174.3540.83893166.85868rpspkhoardwordlist391.65629.6039.7071881.19074rpspkhodwordlist483.10864.9035.4800248.49948rpspkhoodwordlist412.851,286.6532.98209193.69870rpspkhudwordlist658.201,208.05116.1494572.51677rpspkwhodwordlist288.701,616.3030.18905225.73858\n\n\nThe reference data is taken from from [7] (see here) and represents the first and second formant for the words heed, hid, head, had, hard, hod, hoard, hood, who’d, hud, and herd produced by 5 20 to 25 year old L1-speakers of Received Pronunciation.\nWe now combine the two data sets, rename the subject and item columns to Speaker and Word, add a column which holds the ipa symbols of the vowel sounds that the word represent, and we calculate the means of the F1 (F1_mean) and F2 (F2_mean) by Word and Speaker.\n\n\n\n\n\nFirst 15 rows of the vowel formants data extracted from PRAAT.\n\n\nSpeakerWordF1F2ipaF1_meanF2_meanLearnerhad717.33611,868.175æ727.26451,903.528Learnerhad743.48351,903.715æ727.26451,903.528Learnerhad720.97401,938.693æ727.26451,903.528Learnerhard734.52751,493.329ɑ788.24481,466.453Learnerhard832.92281,407.825ɑ788.24481,466.453Learnerhard797.28421,498.206ɑ788.24481,466.453Learnerhead610.89432,062.882e652.75262,067.722Learnerhead722.25192,130.632e652.75262,067.722Learnerhead625.11172,009.651e652.75262,067.722Learnerheed263.38302,833.002i283.92202,800.483Learnerheed301.41762,745.847i283.92202,800.483Learnerheed286.96562,822.599i283.92202,800.483Learnerherd532.79251,704.995ɜ531.76751,743.040Learnerherd537.79621,819.892ɜ531.76751,743.040Learnerherd524.71371,704.232ɜ531.76751,743.040\n\n\nWe can now generate the vowel chart by plotting the F1 values against the F2 values. In addition, we will differentiate between different vowel sounds as well as between the learner (Learner) and native speakers (NS).\n\nns <- voweldata %>% dplyr::filter(Speaker == \"NS\")\nnns <- voweldata %>% dplyr::filter(Speaker == \"Learner\")\nggplot(voweldata, aes(F2, F1, color = Speaker, group = Word, fill = Speaker)) +\n  geom_point(alpha = .1) +\n  geom_text(data = voweldata, aes(x = F2_mean, y = F1_mean, label = ipa), fontface = \"bold\")  +\n  stat_ellipse(data = ns, level = 0.50, geom = \"polygon\", alpha = 0.05, aes(fill = Speaker)) +\n  stat_ellipse(data = nns, level = 0.95, geom = \"polygon\", alpha = 0.05, aes(fill = Speaker)) +\n  scale_x_reverse(breaks = seq(500, 3000, 500), labels = seq(500, 3000, 500)) + scale_y_reverse() +\n  scale_color_manual(breaks = c(\"Learner\", \"NS\"), values = c(\"orange\", \"gray40\")) +\n  theme_bw() +\n  theme(legend.position = \"top\",\n        panel.grid.major = element_blank(), \n        panel.grid.minor = element_blank())\n\n\n\n\nThe vowel chart shows that the i-sounds by the L1-German speaker are more fronted and that the o-sounds are substantially higher by the non-native speaker compared to the RP reference vowel spaces. The short u-sound, however, is very similar, indicating that this L1-German speaker produces the short u-sound in English very native-like while the long u-sound is higher and more fronted in the speech of the L1-German speaker. Interestingly, the vowel space of the ash differs quite dramatically between the native speakers and the L1 German speaker which could be caused by the fact that German does not have an ash vowel. I hope this short tutorial helps you in creating your own personalized vowel charts with Praat and R.\n\n\nCitation & Session Info\nSchweinberger, Martin. 2022. Creating Vowel Charts in R. Brisbane: The University of Queensland. url: https://slcladal.github.io/vc.html (Version 2022.08.31).\n@manual{schweinberger2022vc,\n  author = {Schweinberger, Martin},\n  title = {Creating Vowel Charts in R},\n  note = {https://slcladal.github.io/vc.html},\n  year = {2022},\n  organization = \"The University of Queensland, Australia. School of Languages and Cultures},\n  address = {Brisbane},\n  edition = {2022.08.31}\n}\n\nsessionInfo()\n\nR version 4.2.1 (2022-06-23)\nPlatform: x86_64-pc-linux-gnu (64-bit)\nRunning under: Ubuntu 22.04.1 LTS\n\nMatrix products: default\nBLAS:   /usr/lib/x86_64-linux-gnu/blas/libblas.so.3.10.0\nLAPACK: /usr/lib/x86_64-linux-gnu/lapack/liblapack.so.3.10.0\n\nlocale:\n [1] LC_CTYPE=en_AU.UTF-8       LC_NUMERIC=C              \n [3] LC_TIME=en_AU.UTF-8        LC_COLLATE=en_AU.UTF-8    \n [5] LC_MONETARY=en_AU.UTF-8    LC_MESSAGES=en_AU.UTF-8   \n [7] LC_PAPER=en_AU.UTF-8       LC_NAME=C                 \n [9] LC_ADDRESS=C               LC_TELEPHONE=C            \n[11] LC_MEASUREMENT=en_AU.UTF-8 LC_IDENTIFICATION=C       \n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] flextable_0.7.3 vowels_1.2-2    forcats_0.5.1   stringr_1.4.0  \n [5] dplyr_1.0.9     purrr_0.3.4     readr_2.1.2     tidyr_1.2.0    \n [9] tibble_3.1.7    ggplot2_3.3.6   tidyverse_1.3.2\n\nloaded via a namespace (and not attached):\n [1] Rcpp_1.0.8.3        lubridate_1.8.0     assertthat_0.2.1   \n [4] digest_0.6.29       utf8_1.2.2          R6_2.5.1           \n [7] cellranger_1.1.0    backports_1.4.1     reprex_2.0.1       \n[10] evaluate_0.15       httr_1.4.3          pillar_1.7.0       \n[13] gdtools_0.2.4       rlang_1.0.4         uuid_1.1-0         \n[16] googlesheets4_1.0.0 readxl_1.4.0        rstudioapi_0.13    \n[19] data.table_1.14.2   klippy_0.0.0.9500   rmarkdown_2.14     \n[22] labeling_0.4.2      googledrive_2.0.0   htmlwidgets_1.5.4  \n[25] munsell_0.5.0       broom_1.0.0         compiler_4.2.1     \n[28] modelr_0.1.8        xfun_0.31           systemfonts_1.0.4  \n[31] pkgconfig_2.0.3     base64enc_0.1-3     htmltools_0.5.2    \n[34] tidyselect_1.1.2    fansi_1.0.3         crayon_1.5.1       \n[37] tzdb_0.3.0          dbplyr_2.2.1        withr_2.5.0        \n[40] MASS_7.3-58.1       grid_4.2.1          jsonlite_1.8.0     \n[43] gtable_0.3.0        lifecycle_1.0.1     DBI_1.1.3          \n[46] magrittr_2.0.3      scales_1.2.0        zip_2.2.0          \n[49] cli_3.3.0           stringi_1.7.8       farver_2.1.1       \n[52] fs_1.5.2            xml2_1.3.3          ellipsis_0.3.2     \n[55] generics_0.1.3      vctrs_0.4.1         tools_4.2.1        \n[58] glue_1.6.2          officer_0.4.3       hms_1.1.1          \n[61] fastmap_1.1.0       yaml_2.3.5          colorspace_2.0-3   \n[64] gargle_1.2.0        rvest_1.0.2         knitr_1.39         \n[67] haven_2.5.0        \n\n\n\nBack to top\nBack to HOME\n\n\n\n\n\n\n\n\n\nReferences\n\n1. Rogers, H.: The sounds of language: An introduction to phonetics. Routledge (2014).\n\n\n2. Zsiga, E.C.: The sounds of language: An introduction to phonetics and phonology. John Wiley & Sons (2012).\n\n\n3. Johnson, K.: Acoustic and auditory phonetics. Blackwell, Malden, MA (2003).\n\n\n4. Ladefoged, P.: Elements of acoustic phonetics. University of Chicago Press, Chigago (1996).\n\n\n5. Paganus, A., Mikkonen, V.-P., Mäntylä, T., Nuuttila, S., Isoaho, J., Aaltonen, O., Salakoski, T.: The vowel game: Continuous real-time visualization for pronunciation learning with vowel charts. In: International conference on natural language processing (in finland). pp. 696–703. Springer (2006).\n\n\n6. Styler, W.: Using praat for linguistic research. University of Colorado at Boulder Phonetics Lab. (2013).\n\n\n7. Hawkins, S., Midgley, J.: Formant frequencies of RP monophthongs in four age groups of speakers. Journal of the International Phonetic Association. 35, 183–199 (2005)."
  },
  {
    "objectID": "webinars2022.html",
    "href": "webinars2022.html",
    "title": "LADAL Webinar Series 2022",
    "section": "",
    "text": "The LADAL Webinar Series 2022 consists of 6 webinars | online presentations from speakers with backgrounds in linguistics, data science, or computational humanities and it covers topics related to the computational handling of language data! All recordings of the webinar series are available on the LADAL YouTube channel.\nDetails about upcoming and past webinars that are part of the LADAL Webinar Series 2022 can be found below.\nAll events were announced on Twitter (@slcladal), via the UQ School of Languages and Cultures, and via our collaborators) - so please follow us if you like to catch up with the activities at LADAL. Below are links to recordings of past webinars on our YouTube channel.\nSpread of Lexical Innovations (Jack Grieve)\nAnalyzing Longitudinal Data (Dimitrios Vagenas)"
  },
  {
    "objectID": "webinars2022.html#the-travels-of-marco-polo-andreas-niekler",
    "href": "webinars2022.html#the-travels-of-marco-polo-andreas-niekler",
    "title": "LADAL Webinar Series 2022",
    "section": "The travels of Marco Polo (Andreas Niekler)",
    "text": "The travels of Marco Polo (Andreas Niekler)\nSeptember 26, 2022, 8pm (Brisbane)\n\n\nThe travels of Marco Polo: Information extraction and visualization of historic travel literature\n\n\n\n\n\nZoom link: https://uqz.zoom.us/j/86849442143\nAbstract\nMarco Polo was born into a wealthy Venetian merchant family in 1254 and at the age of 17 he embarked on an epic journey to Asia, as one of the first westerners to ever visit China. When he returned 24 years later he recorded his extensive travels in a book – publishing possibly the first travel guide ever – and introducing Europeans to Central Asia and China.\nIn our talk we show our preliminary work on the analysis of travel literature using Marco Polo’s travel diaries as an example. First, we show the annotation and automatic extraction of important landmarks. On the other hand, we propose an extraction procedure that can uncover possible movements and route segments from the text. This is complemented by a geo-visualization that does not only show a map, but rather works with the spatial situation and terrain models, so that the landscape context can provide complementary information for the text itself. With this work we hope to contribute to a better understanding of historical journeys.\n\n\n\n\n\n\nAbout Andreas\nAndreas Niekler is a research associate at the Institute of Computer Science at the University of Leipzig. He develops computer-based methods in the field of semantic properties in language and language-based AI. He develops computer-based algorithmic methods for computational social science and digital humanities research. Here he has contributed primarily text mining methods to political science, cultural studies, and communication research. Within several research projects he was also involved in the development of the interactive Leipzig Corpus Miner. An interactive and graphical tool for intuitive work with large text corpora and modern methods of text mining."
  },
  {
    "objectID": "webinars2022.html#bayesian-glmms-with-brms-bodo-winter",
    "href": "webinars2022.html#bayesian-glmms-with-brms-bodo-winter",
    "title": "LADAL Webinar Series 2022",
    "section": "Bayesian GLMMs with brms (Bodo Winter)",
    "text": "Bayesian GLMMs with brms (Bodo Winter)\nNovember 7, 2022, 8pm (Brisbane)\n\n\nBayesian generalized linear mixed models with brms\n\n\n\n\n\nZoom link: https://uqz.zoom.us/j/86849442143\nAbstract\nLinguistics is undergoing a rapid shift away from significance tests towards approaches emphasizing parameter estimation, such as linear mixed effects models. Alongside this shift, another revolution is underway: away from using p-values as part of a “null ritual” (Gigerenzer, 2004) towards Bayesian models. Both shifts can nicely be dealt with the ‘brms’ package (Bürkner, 2017). After briefly reviewing why we shouldn’t blindly follow the “null ritual” of significance testing, I will demonstrate how easy it is to fit quite complex models using this package. I will also talk about how mixed models are used in different subfields of linguistics (Winter & Grice, 2021), and why established practices such as dropping random slopes for non-converging models are a further reason to go Bayesian. Finally, I will briefly touch on issues relating to prior specification, especially the importance of weakly informative priors to prevent overfitting.\n\n\n\n\n\n\nAbout Bodo\nBodo Winter is a Senior Lecturer at the Department of Linguistics at the University of Birmingham, a UKRI Future Leaders Fellow, a Fellow of the Institute for Interdisciplinary Data Science and AI, and Editor-in-Chief at the journal Language and Cognition. Dr. Winter has received his PhD in Cognitive and Information Sciences from the University of California, Merced. His research focuses on multimodality, sound symbolism, gesture, and metaphor."
  },
  {
    "objectID": "webinars2022.html#found-in-translation-jörg-tiedemann",
    "href": "webinars2022.html#found-in-translation-jörg-tiedemann",
    "title": "LADAL Webinar Series 2022",
    "section": "Found in Translation (Jörg Tiedemann)",
    "text": "Found in Translation (Jörg Tiedemann)\nDecember 5, 2022, 8pm (Brisbane)\n\n\nFound in Translation - What can we learn from translations about languages and human communication\n\n\n\n\n\nZoom link: https://uqz.zoom.us/j/86849442143\nAbstract\nThe goal of language technology is to create computational models that can understand and generate language in a way humans can do. One of the strategies is to learn such communication abilities from real-world data and in that way somewhat resemble humans and their capability of picking up language skills through practical experience. However, the crucial question is what kind of experience is needed and what kind of tasks have to be practiced to build an understanding of human language signals. We are currently running a project that studies the use of translations that form natural semantic mirrors of original texts in other languages as a means of providing information about the underlying latent meaning that corresponds to the observable language string. The big question is what kind of abstractions can be learned from this cross-lingual signal and how much does that reflect our knowledge about linguistic properties on various levels. Part of this question is how much language diversity can be used to push abstraction levels even further. In this talk I will present some of our results and try to connect this kind of neural “black-box” NLP with questions in general linguistics and cognition.\n\n\n\n\n\n\nAbout Jörg\nJörg Tiedemann is professor of language technology at the Department of Digital Humanities at the University of Helsinki. He received his PhD in computational linguistics for work on bitext alignment and machine translation from Uppsala University before moving to the University of Groningen for 5 years of post-doctoral research on question answering and information extraction. His main research interests are connected with massively multilingual data sets and data-driven natural language processing and he currently runs an ERC-funded project on representation learning and natural language understanding.\nWebsite: https://blogs.helsinki.fi/tiedeman/"
  },
  {
    "objectID": "webinars2022.html#spread-of-lexical-innovations-jack-grieve",
    "href": "webinars2022.html#spread-of-lexical-innovations-jack-grieve",
    "title": "LADAL Webinar Series 2022",
    "section": "Spread of Lexical Innovations (Jack Grieve)",
    "text": "Spread of Lexical Innovations (Jack Grieve)\n\n\nThe Spread of Lexical Innovation is Constrained by Cultural Patterns\n\n\n\n\n\nThis talk was recorded March 7, 2022, as part of the LADAL Webinar Series 2022.\nRecording link: https://youtu.be/sW70Y6XDiRA\nAbstract\nIn this talk I discuss the results of three studies on the geographical diffusion of lexical innovation, all of which are based on the analysis of multibillion word corpora of Twitter data collected between 2013 and 2014. In this first study, I track the spread of new words across the US. In the second study, I zoom in and look at how these same words spread out across New York City. In the third study, I consider how lexical items from Multicultural London English have diffused across the UK. In all cases, I show that the spread of lexical innovation is not only constrained by physical distance and population density, as predicted by the Wave and Gravity Models, but by cultural patterns and boundaries.\n\n\n\n\n\n\nAbout Jack\nJack Grieve is Professor of Corpus Linguistics at the University of Birmingham and Turing Fellow at the Alan Turing Institute. His research involves analysing large corpora of natural language to understand language variation and change. He is especially interested in grammatical and lexical variation in the English language across time, space and communicative context, as well as developing methods for quantitative linguistic analysis. Jack also conducts research on authorship analysis and sometimes consult on casework as a forensic linguist. You can get in touch with Jack at j.grieve@bham.ac.uk or via his Twitter handle @JWGrieve."
  },
  {
    "objectID": "webinars2022.html#archives-as-subject-not-source-cedric-courtois",
    "href": "webinars2022.html#archives-as-subject-not-source-cedric-courtois",
    "title": "LADAL Webinar Series 2022",
    "section": "Archives as Subject not Source (Cedric Courtois)",
    "text": "Archives as Subject not Source (Cedric Courtois)\n\n\nThe Archive as Subject rather than Source: a Roadmap to Constructing and Disseminating a Digital Archive\n\n\n\n\n\nThis talk was recorded May 9, 2022, as part of the LADAL Webinar Series 2022.\nRecording link: https://youtu.be/VZnV3pI8wyw\nAbstract\nArchives are valuable resources in historical media and communication research. Unfortunately, many collections are hard to find and access, and have not been properly digitised and described. In this paper we argue that computational methods are instrumental in engaging with and digitising archive materials, even though these methods introduce problems of their own. More specifically, we articulate critical thinking on the concept of ‘the archive’ with a discussion of the key decisions and practical hurdles encountered in the construction and digitisation of an archive. This is illustrated by a mid-sized project based on records of communications between the Pentagon and CIA Entertainment Liaison Offices (ELO) and audiovisual productions companies around the world, which are key in revealing influence of the US military on the entertainment industry. However, rather than discussing every step in the project in minute detail, we maximise the relevance for a broad readership by distilling a generic roadmap and key recommendations that cover all stages of strategic decision-making in archive construction and digitisation, as well as demonstrating the required generic technical implementations.\n\n\n\n\n\n\nAbout Cedric\nCedric Courtois is a senior lecturer in the School of Communication and Arts in the Faculty of Humanities and Social Sciences at the University of Queensland. He is both an audience researcher and a methodologist. His research interests include algorithmic impact in digital culture and data science applications in (digital) media and communication research (including text mining and image processing)."
  },
  {
    "objectID": "webinars2022.html#analyzing-longitudinal-data-dimitrios-vagenas",
    "href": "webinars2022.html#analyzing-longitudinal-data-dimitrios-vagenas",
    "title": "LADAL Webinar Series 2022",
    "section": "Analyzing Longitudinal Data (Dimitrios Vagenas)",
    "text": "Analyzing Longitudinal Data (Dimitrios Vagenas)\n\n\nAnalyzing Longitudinal Data\n\n\n\n\n\nThis talk was recorded July 4, 2022, as part of the LADAL Webinar Series 2022.\nRecording link: https://youtu.be/oKZHRHqU0YY\nZoom link: https://uqz.zoom.us/j/86849442143\nAbstract\nThe focus of the current seminar will be a (very) brief introduction to longitudinal data and their analysis focusing on regression. To start with we will look at longitudinal data and different designs for collecting such data. We will then look at some empirical observations and why they occur before turning our attention to simple linear regression and why it is generally not appropriate to use it for the analysis of such data. Understanding the above is very important but also a neglected aspect of longitudinal data. We will then, very briefly, introduce two methods for appropriately analysing such data: (i) mixed models and (ii) Generalised Estimating Equations.\n\n\n\n\n\n\nAbout Dimitrios\nDimitrios is a biostatistician and the head of the Research Methods Groups at the Queensland University of Technology. He is also the President of the QLD branch of the Statistical Society of Australia and a member of the Accreditation committee of the same society. He also holds the highest professional accreditation of the American Statistical Association and the Royal Statistical Society. He has also studied animal science, quantitative genetics, and digital biology. His main job is to help researchers do better research and he enjoys working in multidisciplinary teams."
  },
  {
    "objectID": "webinars2022.html#griffith-digital-humanities-webinar-series",
    "href": "webinars2022.html#griffith-digital-humanities-webinar-series",
    "title": "LADAL Webinar Series 2022",
    "section": "Griffith Digital Humanities Webinar Series",
    "text": "Griffith Digital Humanities Webinar Series\n\n\n\n\n\nMartin Schweinberger and Michael Haugh presented about ATAP and LADAL at the Griffith Digital Humanities Webinar Series on Friday May 20, 2022. For more information about this webinar series and the Griffith Centre for Social and Cultural Research see here.\n\n\nBack to top\nBack to HOME"
  },
  {
    "objectID": "whyr.html#full-flexibility",
    "href": "whyr.html#full-flexibility",
    "title": "Why R?",
    "section": "Full flexibility",
    "text": "Full flexibility\n\n\n\n\n\nR represents a fully-fledged programming environment that is extremely versatile and allows to perform a huge variety of things - from handling and processing data, performing text analytics, statistical analyses, generating very beautiful and even interactive data visualization,generating websites and apps, and creating questionnaires and surveys for gathering data online to for creating behavioral experiments (to name just a very few applications). As such, R is cool because it allows you to do really awesome, exciting, fancy things!\nSo, rather than having to learn how to use many different tools that are all very specific and limited in their applicability, R can do it all! And you only need to learn it once!\nInitially, it may seem like a better option to learn how to use specific tools for specific things but over time, having to learn more tools will turn out to be much more time consuming and much less efficient!"
  },
  {
    "objectID": "whyr.html#great-for-learners",
    "href": "whyr.html#great-for-learners",
    "title": "Why R?",
    "section": "Great for learners!",
    "text": "Great for learners!\n\n\n\n\n\nR is actually not that hard to learn - R is actually quite very easy to learn (once you have covered the basics). Particularly the tidyverse style or dialect of R - is commonly deemed to be very easy to learn especially for anyone who is new to programming. Although base R is probably easier to pick up for people who already have some programming experience and there are certain things where base R is easier than the tidyverse style (for example when it comes to generating plots), R is a really beginner-friendly, quite forgiving fully fledged programming environment. Indeed, R is so versatile while easy to get into that Google has recently completely switched to R in its Data Analytics certificate."
  },
  {
    "objectID": "whyr.html#free-and-open-source",
    "href": "whyr.html#free-and-open-source",
    "title": "Why R?",
    "section": "Free and Open Source",
    "text": "Free and Open Source\nR and RStudio are free and open source which means that everybody who has a computer, internet access, and is literate in a major world language can learn and work with R without any additional costs everywhere on this planet. This makes R the ultimate equity guarantee as one’s financial and even language language background do not bar anyone from exploring programming and data when using R - it’s like the green and environment-friendly gardening alternative to buying plastic wrapped tomatoes in the supermarket that have no taste anyway."
  },
  {
    "objectID": "whyr.html#rstudio-and-ease-of-use",
    "href": "whyr.html#rstudio-and-ease-of-use",
    "title": "Why R?",
    "section": "RStudio and ease of use",
    "text": "RStudio and ease of use\n\n\n\n\n\nWith RStudio, the use of R has become even simpler and user-friendly with RStudio allowing to have easy and incredibly efficient work flows that tie in to great version control and documentation options (by having easy access to Git and GitHub and containerization with renv. With RStudio, R is intuitive, and entire analyses can be performed in one single integrative development environment (IDE) - this is great because one can do everything in R and does not need to use many different tools in one project (which makes projects and work flows much for transparent and reproducible)."
  },
  {
    "objectID": "whyr.html#r-is-a-community-effort",
    "href": "whyr.html#r-is-a-community-effort",
    "title": "Why R?",
    "section": "R is a community effort",
    "text": "R is a community effort\n\n\n\n\n\nOn a related note, R is great because anyone can contribute and add to the multitude of packages, functions, and resources that are available when working with R. The community is really fantastic and super helpful with tips and tricks being publicized on many different channels and platforms - from YouTube tutorials, over online courses, help sites like StackOverflow, to facebook groups and Reddit channels.\nOne really great thing that is also a major achievement of RStudio and the RStudio community is that the R community is really welcoming, tolerant, and forgiving! You can post any question in a forum or on a discussion board and you will receive help and advice - this is really something that sets the R community apart from other programming and software communities (in a good way)."
  },
  {
    "objectID": "whyr.html#reproducibilty-and-transparency",
    "href": "whyr.html#reproducibilty-and-transparency",
    "title": "Why R?",
    "section": "Reproducibilty and Transparency",
    "text": "Reproducibilty and Transparency\n\n\n\n\n\nA really major advantage for using R when you are involved in research is that it is a fantastic - and maybe even the optimal - tool for sharing and documenting your analyses and making your research transparent, reproducible and replicable! When you generate a folder and create an Rproject with a project specific library using renv, use R notebooks to document what you have done, and enable version control with Git, and then connect that project to GitHub, your research is fully transparent and reproducible! Due to the in-built options in RStudio, this is easily done with a few mouse clicks if you have a GitHub account and installed Git on your machine.\nThis is really fantastic because is represents real or true reproducibility and transparency rather than just theoretical reproducibility that still requires researchers to manually re-perform analyses based on (often not ideal or accurate) descriptions of how data was handled in and by different tools - which is so time-consuming that it is almost never done (and that is why research done like this is only theoretically reproducible)."
  },
  {
    "objectID": "whyr.html#tools-versus-scripts",
    "href": "whyr.html#tools-versus-scripts",
    "title": "Why R?",
    "section": "Tools versus Scripts",
    "text": "Tools versus Scripts\n\n\n\n\n\nIt is perfectly fine to use tools for the analyses exemplified on LADAL. Almost anyone I know - including myself - started off with using fantastic tools like AntConc! However, the aim of LADAL is not primarily to show how to perform certain tasks or analyses but how to perform these tasks in a way that complies with practices that guarantee sustainable, transparent, reproducible research. As R code can be readily shared and optimally contains all the data extraction, processing, visualization, and analysis steps, using scripts is preferable over using (commercial) software.\nIn addition to being not as transparent and hindering reproduction of research, using tools can also lead to dependencies on third parties which does not arise when using open source software. Finally, the widespread use of R particularly among data scientists, engineers, and analysts reduces the risk of software errors as a very active community corrects flawed functions typically quite rapidly."
  },
  {
    "objectID": "whyr.html#widely-used",
    "href": "whyr.html#widely-used",
    "title": "Why R?",
    "section": "Widely used",
    "text": "Widely used\nR is a really widely used and it is becoming ever more popular - not only among data scientists. Ever more creative ways of working with data, discovering the use on ever more exciting topics, and filling niches are filled with applications of R- from sport analytics to improving communication with patients in hospitals to performing experiments and designing art!"
  },
  {
    "objectID": "whyr.html#employability",
    "href": "whyr.html#employability",
    "title": "Why R?",
    "section": "Employability",
    "text": "Employability\n\n\n\n\n\nR is extremely useful for many tasks as it is a fully fledged programming language or environment!. This means that you are flexible in what you can do and this offers you many options of what jobs you want to or can do.\nAlso, this makes you and your skills appealing and interesting for employers - not only because of its versatility but also because it allows you to do many different and complex things without having to buy expensive software packages (e.g. SPSS, Microsoft Excel, Stata, or MatLab) and you thus have an advantage over other candidates that may be able to do what you can do with R - but the employers would have to buy the software these candidates need in addition to paying a salary!"
  },
  {
    "objectID": "whyr.html#what-about-python",
    "href": "whyr.html#what-about-python",
    "title": "Why R?",
    "section": "What about Python?",
    "text": "What about Python?\nPython is a really great tool (at least I’m told so by Python users whenever they have the chance to let everyone know). Python has traditionally been stronger in Natural Language Processing (NLP) although recent developments have R at least on the same level.\nJokes aside, Python is really great for NLP and everything involving the web - when your focus is doing things like those, Python may be a better option for you. However, when it comes to data visualization and data analysis - particularly visualizations and statistical analyses as they are required, used, and performed in research, R is reeeeally hard to beat."
  },
  {
    "objectID": "workshops.html",
    "href": "workshops.html",
    "title": "Workshops",
    "section": "",
    "text": "LADAL and its collaborators offer workshops of matter relating to text analytics, statistics, data visualization, and related topics. This page contains links, information, and details about upcoming and past workshops offered by LADAL or its collaborators and partners. Also, LADAL members present their research or information relevant to LADAL at conferences. Below are links to upcoming events (conferences/workshops/presentations) and presentations containing information about LADAL or research based on LADAL."
  },
  {
    "objectID": "workshops.html#june-2022---acqva-aurora-workshop-from-tables-to-forests",
    "href": "workshops.html#june-2022---acqva-aurora-workshop-from-tables-to-forests",
    "title": "Workshops",
    "section": "17 June 2022 - AcqVA Aurora workshop: From Tables to Forests",
    "text": "17 June 2022 - AcqVA Aurora workshop: From Tables to Forests\n\n\n\n\n\nThis workshop consists of two parts. The first part of the workshop focuses on loading and working with tabular data R. In this part of the workshop, we will learn how to load, inspect, process, and summarize tabulated data. The second part of the workshop focuses tree-based models in R, such as conditional inference trees, random forests, and Boruta, which have become more prominent in the language science over the past decade. This part will discuss the underlying logic of tree-based models, how they can be implemented in R, as well as their strengths and weaknesses.\nThe materials for this workshop can be accessed via this GitHub repo and the rendered script is available here."
  },
  {
    "objectID": "workshops.html#network-analysis-and-topic-modeling-on-twitter-data-using-r",
    "href": "workshops.html#network-analysis-and-topic-modeling-on-twitter-data-using-r",
    "title": "Workshops",
    "section": "Network analysis and Topic Modeling on Twitter data using R",
    "text": "Network analysis and Topic Modeling on Twitter data using R\nThis online workshop is offered free to Australian researchers and research students and will cover:\n\nIntroduction to network theory.\n\nFundamental principles of using R for network analysis.\n\nTopic modeling of tweet text using R.\n\nThe data used for the workshop will be an open source dataset of Twitter data relating to the 2019 Federal Election.\nDate: May 18 2022 Time: 9:00AM - 12:00PM AEST Venue: Online\nRegistration is required and the workshop is brought to you by the teams at the Australian Digital Observatory (ADO) and the Australian Text Analytics Platform (ATAP) via the Australian Research Data Commons (ARDC)."
  },
  {
    "objectID": "workshops.html#monotreme-mania",
    "href": "workshops.html#monotreme-mania",
    "title": "Workshops",
    "section": "Monotreme Mania!",
    "text": "Monotreme Mania!\n\n\n\n\n\nComparative text analytics on Twitter data\nThis online workshop is offered free to Australian researchers and research students and will cover:\n\nCollecting and transforming Twitter data using twarc.\n\nAnalysing the collected Twitter data using the R ecosystem for text analytics.\n\nRegister via Eventbrite\nUpdate - 09/03/2022 The workshop is now fully booked. If you missed out, do check back next week in case of cancellations, and let us know if you’re keen for us to run the workshop again later.\nBrought to you by the teams at the Australian Digital Observatory (ADO) and the Australian Text Analytics Platform (ATAP) via the Australian Research Data Commons (ARDC)."
  },
  {
    "objectID": "workshops.html#power-analysis-with-r",
    "href": "workshops.html#power-analysis-with-r",
    "title": "Workshops",
    "section": "Power Analysis with R",
    "text": "Power Analysis with R\n\n\n\n\n\nTitle: Power Analysis with R (pwr and simr)\nDate: 1 February 2022\nEvent: AcqVA Aurora Lab Workshop\nLength: 4 hours\nFacilitators: Martin Schweinberger\nResources: GitHub Repo"
  },
  {
    "objectID": "workshops.html#data-visualization-with-r-ggplot2-and-likert",
    "href": "workshops.html#data-visualization-with-r-ggplot2-and-likert",
    "title": "Workshops",
    "section": "Data Visualization with R (ggplot2 and likert)",
    "text": "Data Visualization with R (ggplot2 and likert)\n\n\n\n\n\nTitle: Data Visualization with R (ggplot2 and likert)\nDate: 25 January 2022\nEvent: AcqVA Aurora Lab Workshop\nLength: 4 hours\nFacilitators: Martin Schweinberger\nResources: GitHub Repo"
  },
  {
    "objectID": "workshops.html#an-introduction-to-jupyter-notebooks-for-text-analysis",
    "href": "workshops.html#an-introduction-to-jupyter-notebooks-for-text-analysis",
    "title": "Workshops",
    "section": "An introduction to Jupyter notebooks for text analysis",
    "text": "An introduction to Jupyter notebooks for text analysis\n\n\n\n\n\nTitle: An introduction to Jupyter notebooks for text analysis - Virtual workshop for absolute beginners\nDate: 24 November 2021\nEvent: Digital Humanities Australasia 2021 Conference\nLength: 3 hours\nFacilitators: Sara King, Simon Musgrave"
  },
  {
    "objectID": "workshops.html#best-practices-in-corpus-linguistics",
    "href": "workshops.html#best-practices-in-corpus-linguistics",
    "title": "Workshops",
    "section": "Best Practices in Corpus Linguistics",
    "text": "Best Practices in Corpus Linguistics\n\n\n\n\n\nTitle: Best Practices in Corpus Linguistics – What lessons should we take from the Replication Crisis and how can we guarantee high quality in our research?\nDate: 20–24 May 2020\nEvent: ICAME 41 (41th Meeting of the International Computer Archive of Modern and Medieval English). Heidelberg, Germany.\nSpeaker: Martin Schweinberger\nMaterials: slides, video"
  },
  {
    "objectID": "workshops.html#the-language-technology-and-data-analysis-laboratory-ladal",
    "href": "workshops.html#the-language-technology-and-data-analysis-laboratory-ladal",
    "title": "Workshops",
    "section": "The Language Technology and Data Analysis Laboratory (LADAL)",
    "text": "The Language Technology and Data Analysis Laboratory (LADAL)\n\n\n\n\n\nTitle: Implementing school-based support infrastructure for digital humanities research at UQ - The Language Technology and Data Analysis Laboratory (LADAL)\nDate: 30 October 2019\nSpeakers: Michael Haugh & Martin Schweinberger\nPresentation at the Australian Research Data Commons (ARDC): The Australian eResearch Skilled Workforce Summit. Sydney, Australia, 29-30/7/2019.\nMaterials: slides"
  },
  {
    "objectID": "workshops.html#using-r-for-corpus-linguistics",
    "href": "workshops.html#using-r-for-corpus-linguistics",
    "title": "Workshops",
    "section": "Using R for Corpus Linguistics",
    "text": "Using R for Corpus Linguistics\n\n\n\n\n\nTitle: Using R for Corpus Linguistics – an Introduction and Discussion Note on Sustainability and Replicability in Corpus Linguistics\nDate: 2 April 2019\nSpeaker: Martin Schweinberger\nPresentation at the Center of Excellence for the Dynamics of Language (CoEDL) Corpus Workshop. Melbourne, Australia, 2–3/4/2019.\nMaterials: slides\n\nBack to top\nBack to HOME"
  }
]