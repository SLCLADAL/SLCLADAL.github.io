<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />


<meta name="author" content="Martin Schweinberger" />


<title>Introduction to Text Analysis</title>

<script src="site_libs/header-attrs-2.26/header-attrs.js"></script>
<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cosmo.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/jqueryui-1.13.2/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<link rel="stylesheet" href="styles.css" />

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-VSGK4KYDQZ"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-VSGK4KYDQZ');
</script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>





<style type="text/css">
/* for pandoc --citeproc since 2.11 */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>




<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark the anchor link active (and if it's in a dropdown, also mark that active)
  var dropdown = menuAnchor.closest('li.dropdown');
  if (window.bootstrap) { // Bootstrap 4+
    menuAnchor.addClass('active');
    dropdown.find('> .dropdown-toggle').addClass('active');
  } else { // Bootstrap 3
    menuAnchor.parent().addClass('active');
    dropdown.addClass('active');
  }

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before, .tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "\e259";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "\e258";
  font-family: 'Glyphicons Halflings';
  border: none;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">



<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  
  <!-- Added by SKC - LADAL image and thicker top with   -->
  <div class="container-fluid navbar-top" >
    <a href="index.html"> <!-- Make entire top row and text clickable home link  -->
        <div class="row">
            <div class="navbar-brand col-md-12">
              <img src="/content/ladal_icon_cas_tran_white_trimed.png" class="navbar-icon" alt="LADAL"/>
              <span class="navbar-title-note navbar-collapse collapse" >Language Technology and Data Analysis Laboratory</span>
            </div>
        </div>
    </a>
  </div>
  
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <!-- SKC removed  navbar brand -->
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">HOME</a>
</li>
<li>
  <a href="about.html">ABOUT</a>
</li>
<li>
  <a href="events.html">EVENTS</a>
</li>
<li>
  <a href="tutorials.html">TUTORIALS</a>
</li>
<li>
  <a href="tools.html">TOOLS</a>
</li>
<li>
  <a href="resources.html">RESOURCES</a>
</li>
<li>
  <a href="contact.html">CONTACT</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">



<h1 class="title toc-ignore">Introduction to Text Analysis</h1>
<h4 class="author">Martin Schweinberger</h4>

</div>


<p><img src="https://slcladal.github.io/images/uq1.jpg" width="100%" /></p>
<div id="introduction" class="section level1 unnumbered">
<h1 class="unnumbered">Introduction</h1>
<p>This tutorial focuses on Text Analysis (TA) <span
class="citation">Bernard and Ryan (<a
href="#ref-bernard1998text">1998</a>)</span>, i.e. computer-based
extraction, processing, and analysis of text(s). TA is associated with a
divers set of computational methods that enable researchers to explore
and analyse unstructured data, i.e., text (unstructured is used here in
contrast to structured, i.e., tabular data). Due to the increasing
availability of large amounts of textual data, TA methods and techniques
are becoming increasingly relevant to a larger body of researchers and
disciplines.</p>
<div class="warning"
style="padding:0.5em; background-color:rgba(215,209,204,.3); color:#51247a">
<span>
<p style="margin-top:1em; text-align:center">
Please cite as: <br>Schweinberger, Martin. 2023. <em>Introduction to
Text Analysis</em>. Brisbane: The Language Technology and Data Analysis
Laboratory (LADAL). url: <a href="https://ladal.edu.au/introta.html"
class="uri">https://ladal.edu.au/introta.html</a> (Version
2023.09.24).<br>
</p>
<p></span></p>
</div>
<p><br></p>
<p><img src="https://slcladal.github.io/images/gy_chili.jpg" width="15%" style="float:right; padding:10px" /></p>
<p>This tutorial introduces basic concepts of Text Analysis. The aim is
not to provide a fully-fledged analysis but rather to discuss and
explore selected useful methods associated with text analysis and
distant reading.</p>
</div>
<div id="what-is-text-analysis" class="section level1 unnumbered">
<h1 class="unnumbered">What is Text Analysis?</h1>
<p><img src="https://slcladal.github.io/images/ta.png" width="35%" style="float:right; padding:10px" /></p>
<p>Text Analysis (TA) refers to the process of examining, processing,
and interpreting unstructured data (texts) to uncover actionable
knowledge using computational methods. Unstructured data (text) can, for
example, include emails, literary texts, letters, articles,
advertisements, official documents, social media content, transcripts,
and product reviews. Actionable knowledge refers to insights and
patterns used to classify, sort, extract information, determine
relationships, identify trends, and make informed decisions.</p>
<p>Sometimes, Text <em>Analysis</em> is distinguished from Text
<em>Analytics</em>. In this context, Text Analysis refers to manual,
close-reading, and qualitative interpretative approaches, while Text
Analytics refers to quantitative, computational analysis of text.
However, in this tutorial, we consider Text Analysis and Text Analytics
to be synonymous, encompassing any computer-based qualitative or
quantitative method for analyzing text.</p>
<p>While TA, Distant Reading (DR), Corpus Linguistics (CL), Natural
Language Processing (NLP), and Text Mining (TM) share the common goal of
using textual data, they differ in their approaches, methodologies, and
objectives. TA can be seen as an umbrella term encompassing the other
types of analyses or approaches mentioned before. The following sections
provide details of these approaches so it becomes clearer how they
relate and differ from TA.</p>
<p><strong>Distant Reading</strong> (DR) is an approach to analyzing
(literary) texts pioneered by Franco Moretti <span
class="citation">Moretti (<a
href="#ref-moretti2013distant">2013</a>)</span>. It involves analyzing
large corpora of literary texts using computational methods to identify
broad patterns and trends. Distant reading focuses on the quantitative
analysis of texts rather than close, qualitative reading. It allows for
the exploration of large-scale patterns and trends that would be
difficult to discern through traditional close-reading techniques.
Distant reading is a cover term for TA applications that investigate
literary and cultural trends by analyzing large amounts of textual data.
In contrast, close reading refers to the traditional method of reading
texts in detail to interpret their meanings. While both TA and distant
reading utilize similar computational methods, they differ in their
outlooks. The outlook of DR is to extract information from text without
engaging in close reading, i.e., without reading the document(s)
themselves, but rather focusing on emerging patterns in the language
used.</p>
<p><strong>Corpus Linguistics (CL)</strong> <span
class="citation">Amador Moreno (<a
href="#ref-amador2010how">2010</a>)</span> is a branch of linguistics
that involves the study of language using large collections of texts
known as corpora. It aims to analyze linguistic phenomena by examining
patterns and frequencies of words and structures within a corpus. Corpus
linguistics provides empirical data and insights into language use,
variation, and change over time.</p>
<p><strong>Natural Language Processing</strong> (NLP) <span
class="citation">Mitkov (<a
href="#ref-mitkov2022oxford">2022</a>)</span> is a field of computer
science that focuses on the interaction between computers and human
languages. It focuses on developing and evaluating methods that aim to
enable computers to understand, interpret, and generate human language
in a meaningful way. Key aspects of NLP include understanding the
meaning and context of text or speech, extracting information from large
volumes of unstructured data, and generating human-like text. NLP
techniques utilise machine and deep learning and are used in various
applications such as machine translation, speech recognition, chatbots,
information retrieval, and text classification.</p>
<p><strong>Text Mining</strong> (TM) is a data science field focused on
extracting information and insights from large volumes of unstructured
text data. Text Mining is decidedly data-driven and typically applies
automated methods without substantial human supervision. This allows for
the efficient processing of vast text data sets using techniques from
NLP, machine learning (ML), and statistics, such as text classification,
clustering, sentiment analysis, and entity recognition. TM is
particularly associated with social media and Big Data, where manual
analysis is impractical due to the sheer volume of data. Businesses use
TM to analyze customer feedback from social media, online reviews, and
surveys to gain insights into consumer sentiment and preferences.</p>
<p>Text Analysis (TA) is broader than Corpus Linguistics (CL) and
Distant Reading (DR) as it is not limited to literary texts or the
understanding of language. Additionally, TA is less focused on
developing and testing computational methods, as is common in Natural
Language Processing (NLP) and it is more common in TA to have
human-in-the-loop workflows compared to TM. TA uses computational
analyses of text to address a wide range of topics, falling within the
realm of computational humanities research. This field represents the
application of computational methods in the humanities, leveraging these
techniques to explore and analyze diverse humanities-related
questions.</p>
<p>The advantages of Text Analysis include:</p>
<ul>
<li><p>Extraction of information from large textual data sets</p></li>
<li><p>Replicability and reproducibility of analyses</p></li>
</ul>
<p><img src="https://slcladal.github.io/images/GoogleNgram.png" width="60%" style="float:right; padding:10px" /></p>
<p>Text Analysis is rapidly gaining popularity in the humanities because
textual data is readily available and because computational methods can
be applied to a huge variety of research questions. The attractiveness
of computational text analysis based on digitally available texts and in
their capability to provide insights that cannot be derived from close
reading techniques.</p>
<p><img src="https://slcladal.github.io/images/romeonet.png" width="40%" style="float:right; padding:10px" /></p>
<p>While rapidly growing as a valid approach to analyzing textual data,
Text Analysis is <a
href="https://www.chronicle.com/article/The-Digital-Humanities-Debacle/245986">critizised</a>
for lack of “quantitative rigor and because its findings are either
banal or, if interesting, not statistically robust (see <a
href="https://www.chronicle.com/article/The-Digital-Humanities-Debacle/245986">here</a>.
This criticism is correct in that most of the analysis that performed in
<em>Computational Literary Studies</em> (CLS) are not yet as rigorous as
analyses in fields that have a longer history of computational based,
quantitative research, such as, for instance, corpus linguistics.
However, the practices and methods used in CLS will be refined, adapted
and show a rapid increase in quality if more research is devoted to
these approaches. Text Analysis simply offers an alternative way to
analyze texts that is not in competition to traditional techniques but
rather complements them.</p>
<p>So far, most of the applications of Text Analysis are based upon a
relatively limited number of key procedures or concepts
(e.g. concordancing, word frequencies, annotation or tagging, parsing,
collocation, text classification, Sentiment Analysis, Entity Extraction,
Topic Modeling, etc.). In the following, we will explore these
procedures and introduce some basic tools that help you perform the
introduced tasks.</p>
<p><strong>Tools versus Scripts</strong></p>
<p>It is perfectly fine to use tools such as <a
href="https://www.laurenceanthony.net/software/antconc/">AntConc</a>
<span class="citation">(<a href="#ref-anthony2024antconc">Anthony
2024</a>)</span> or <a
href="https://www.sketchengine.eu/">SketchEngine</a> <span
class="citation">(<a href="#ref-kilgarriff2014sketch">Kilgarriff et al.
2014</a>)</span> for the analyses exemplified below. However, the aim of
LADAL is not primarily to show how to perform text analyses but how to
perform text analyses in a way that complies with practices that
guarantee sustainable, transparent, reproducible research. As R code can
be readily shared and optimally contains all the data extraction,
processing, visualization, and analysis steps, using scripts is
preferable over using (commercial) software. In addition to being not as
transparent and hindering reproduction of research, using tools can also
lead to dependencies on third parties which does not arise when using
open source software. Finally, the widespread use of R particularly
among data scientists, engineers, and analysts reduces the risk of
software errors as a very active community corrects flawed functions
typically quite rapidly.</p>
</div>
<div id="glossary-of-important-concepts"
class="section level1 unnumbered">
<h1 class="unnumbered">Glossary of Important Concepts</h1>
<p>Below, you will find explanations of concepts and methods that are
important in Text Analysis and also links to relevant resources
(including LADAL tutorials).</p>
<div id="word" class="section level2 unnumbered">
<h2 class="unnumbered">Word</h2>
<p>What a word is is actually very tricky. For instance, How many words
are there in this sentence?</p>
<blockquote>
<p><em>The cat sat on the mat.</em></p>
</blockquote>
<p>One answer is that there are <em>six words</em>; that is, there are
six <em>groups of characters</em> which are separated according to
typographical convention.</p>
<p>But there is another answer: There are <em>five words</em>, that is
five <em>distinct sequences</em> of characters and one of those
sequences (the) occurs twice.</p>
<p>The terms commonly used to make this distinction are
<strong>type</strong> and <strong>token</strong>. Tokens are instances
of types, therefore if we count tokens, we count without considering
repetition, while if we count types, we do consider repetition. In our
example, there are five types (<em>the</em>, <em>cat</em>, <em>sat</em>,
<em>on</em>, <em>mat</em>) but six tokens, because there are two tokens
of one of the types (<em>the</em>).</p>
<p>There is a further distinction we may need to make which we can see
if we consider another question: are <em>cat</em> and <em>cats</em> the
same word? They are distinct types, and therefore must also be distinct
as tokens. But we have an intuition that at some level they are related,
that there is some more abstract item which underlies both of them. This
concept is usually referred to as a <strong>lemma</strong>.</p>
<p><img src="https://slcladal.github.io/images/AntConcConcordance.png" width="40%" style="float:right; padding:10px" /></p>
</div>
<div id="concordancing" class="section level2 unnumbered">
<h2 class="unnumbered">Concordancing</h2>
<p>In Text Analysis, concordancing refers to the extraction of words
from a given text or texts <span class="citation">(<a
href="#ref-lindquist2009corpus">Lindquist 2009</a>)</span>. Commonly,
concordances are displayed in the form of key-word in contexts (KWIC)
where the search term is shown with some preceding and following
context. Thus, such displays are referred to as key word in context
concordances. A more elaborate tutorial on how to perform concordancing
with R is available <a
href="https://slcladal.github.io/kwics.html">here</a>. If you do not
want to use coding to extract concordances, a highly recommendable tool
for extracting concordances (and many other TA tasks) is <a
href="https://www.laurenceanthony.net/software/antconc/">AntConc</a>.</p>
<p>Concordancing is helpful for seeing how the term is used in the data,
for inspecting how often a given word occurs in a text or a collection
of texts, for extracting examples, and it also represents a basic
procedure and often the first step in more sophisticated analyses of
language data.</p>
</div>
<div id="corpus-pl.-corpora" class="section level2 unnumbered">
<h2 class="unnumbered">Corpus (pl. Corpora)</h2>
<p>A corpus is a machine readable and electronically stored collection
of natural language texts representing writing or speech chosen to be
characteristic of a variety or the state of a language <span
class="citation">(see <a href="#ref-sinclair1991corpus">Sinclair
1991</a>)</span>. Corpora are great for extracting examples of natural
examples and testing research hypotheses as it is easy to obtain
information on frequencies, grammatical patterns, or collocations and
they are commonly publicly available so the research results can be
contrasted, compared and repeated.</p>
<p>There are four main types of corpora:</p>
<ol style="list-style-type: decimal">
<li><p><strong>Monitor corpora</strong>: large collections of texts from
different genres/modes that aim at representing a language or language
variety, e.g., <a
href="https://www.ice-corpora.uzh.ch/en.html"><em>International Corpus
of English</em> (ICE)</a>, <a
href="https://www.english-corpora.org/coca/"><em>Corpus of Contemporary
Corpus of American English</em> (COCA)</a>, that are, e.g., used to
analyse the use of certain linguistic phenomena or to investigate
collocations of certain words/topics</p></li>
<li><p><strong>Learner corpora</strong>: Contain data from language
learners - these can be either L1 learners, e.g., <a
href="https://childes.talkbank.org/"><em>Child Language Data Exchange
System</em> (CHILDES)</a>, and/or L2 learners, e.g., <a
href="https://uclouvain.be/en/research-institutes/ilc/cecl/icle.html">the
<em>International Corpus of Learner English</em> (ICLE)</a>) - to study,
e.g., how L1 and/or L2 speakers learn/acquire (aspects of) a language
and to see how learners differ from native speakers.</p></li>
<li><p><strong>Historical or diachronic corpora</strong>: Contain data
from different points in time that allow to analyse the development of a
language or language variety (e.g., <a
href="https://www.ling.upenn.edu/hist-corpora/"><em>Penn Parsed Corpora
of Historical English</em></a>,<a href="http://icame.uib.no/hc/"><em>The
Helsinki Corpus of English Texts</em></a>) to study, e.g., how language
changes or how genres develop over time.</p></li>
<li><p><strong>Specialized corpora</strong>: Contain data representing a
specific genre/text type (e.g., <a
href="https://www.coventry.ac.uk/research/research-directories/current-projects/2015/british-academic-written-english-corpus-bawe/"><em>British
Academic Written English Corpus</em> (BAWE)</a>) to study, e.g.,
(linguistic) features of a genre (e.g. academic writing) or language in
class rooms.</p></li>
</ol>
<p><img src="https://slcladal.github.io/images/collocates.png" width="40%" style="float:right; padding:10px" /></p>
</div>
<div id="collocations" class="section level2 unnumbered">
<h2 class="unnumbered">Collocations</h2>
<p>Collocations are words that are attracted to each other (and that
co-occur or co-locate together), e.g., <em>Merry Christmas</em>,
<em>Good Morning</em>, <em>No worries</em>, or <em>Fuck off</em>.
Collocations are important because any word in any given language has
collocations, i.e., others words that are attracted to that word or
words that that word is attracted to allow us to anticipate what word
comes next and collocations are context/text type specific. It is
important to note that collocations to not have to appear/occur right
next to each other but that other words can be in between. There are
various different statistical measures are used to define the strength
of the collocations, like the Mutual Information (MI) score and
log-likelihood (<a href="http://www.collocations.de/AM/index.html">see
here for an over view of different association strengths
measures</a>).</p>
</div>
<div id="document-classification" class="section level2 unnumbered">
<h2 class="unnumbered">Document Classification</h2>
<p>Document or Text Classification (also referred to as text
categorization) generally refers to process of grouping texts or
documents based on similarity. This similarity can be based on word
frequencies or other linguistics features but also on text external
features such as genre labels or polarity scores.</p>
</div>
<div id="document-term-matrix" class="section level2 unnumbered">
<h2 class="unnumbered">Document-Term Matrix</h2>
<p>Document-Term Matrices (DTM) and Term- Document Matrices (TDM)
contain the frequencies of words per document. DTM and TDM differ in
whether the words or the documents are represented as rows. Thus, the
words (terms) are listed as row names and the documents represent the
column names while the matrix itself contains the frequencies of the
words in the documents.</p>
<p><img src="https://slcladal.github.io/images/speciescloud.png" width="40%" style="float:right; padding:10px" /></p>
</div>
<div id="frequency-analysis" class="section level2 unnumbered">
<h2 class="unnumbered">Frequency Analysis</h2>
<p>Frequency Analysis is a suit of methods which extract and compare
frequencies of different words (tokens and/or types), collocations,
phrases, sentences, etc. These frequencies are the often tabulated to
show lists of words, phrases, etc. descending by frequency, visualized
to show distributions, and/or compared and analyzed statistically to
find differences between texts or collections fo texts.</p>
</div>
<div id="keyword-analysis" class="section level2 unnumbered">
<h2 class="unnumbered">Keyword Analysis</h2>
<p>Keyword Analysis refers to a suit of methods that allow to detect
words that are characteristic of on text or collection of texts compared
to another text/collection of texts. There are various keyness measures
such as Log-Likelihood or the term frequency–inverse document frequency
(tf-idf).</p>
</div>
<div id="lemma-lemmatization" class="section level2 unnumbered">
<h2 class="unnumbered">Lemma (Lemmatization)</h2>
<p>Lemma refers to the base form of a word (example: <em>walk</em>,
<em>walked</em>, and <em>walking</em> are word forms of the lemma
<em>WALK</em>). Lemmatization refers to a annotation process in which
word forms are associated with their base form (lemma). Lemmatization is
a very common and sometimes useful processing step for further analyses.
In contrast to stemming - which is a related process - lemmatization
also takes into account semantic differences (differences in the word
meaning), while stemming only takes the orthography of words into
consideration.</p>
</div>
<div id="n-gram" class="section level2 unnumbered">
<h2 class="unnumbered">N-Gram</h2>
<p>N-grams are combinations/sequences of words, e.g. the sentence <em>I
really like pizza!</em> has the bi-grams (2-grams): <em>I really</em>,
<em>really like</em>, and <em>like pizza</em> and the tri-grams
(3-grams) <em>I really like</em> and <em>really like pizza</em>. N-grams
play an important part in natural language processing
(e.g. part-of-speech tagging), language learning, psycholinguistics
models of language production, and genre analysis.</p>
</div>
<div id="natural-language-processing" class="section level2 unnumbered">
<h2 class="unnumbered">Natural Language Processing</h2>
<p>Natural Language Processing (NLP) is an interdisciplinary field in
computer science that has specialized on processing natural language
data using computational and mathematical methods. Many methods used in
Text Analysis have been developed in NLP.</p>
<p><img src="https://slcladal.github.io/images/romeonet.png" width="40%" style="float:right; padding:10px" /></p>
</div>
<div id="network-analysis" class="section level2 unnumbered">
<h2 class="unnumbered">Network Analysis</h2>
<p>Network Analysis is the most common way to visualize relationships
between entities. Networks, also called graphs, consist of nodes
(typically represented as dots) and edges (typically represented as
lines) and they can be directed or undirected networks.</p>
<p>In directed networks, the direction of edges is captured. For
instance, the exports of countries. In such cases the lines are directed
and typically have arrows to indicate direction. The thickness of lines
can also be utilized to encode information such as frequency of
contact.</p>
</div>
<div id="part-of-speech-tagging" class="section level2 unnumbered">
<h2 class="unnumbered">Part-of-Speech Tagging</h2>
<p>Part-of-Speech (PoS) Tagging identifies the word classes of words
(e.g., noun, adjective, verb, etc.) in a text and adds part-of-speech
tags to each word. There are various part-of-speech tagsets, e.g. the
Penn Treebank is the most frequently used tagset used for English. A
more detailed tutorial on how to perform part-of-speech tagging in R can
be found <a href="https://slcladal.github.io/tagging.html">here</a>.</p>
</div>
<div id="project-gutenberg" class="section level2 unnumbered">
<h2 class="unnumbered">Project Gutenberg</h2>
<p>The Project Gutenberg is a excellent resource for accessing digitized
literary texts. The Project Gutenberg library contains over 60,000
ebooks that are out of copyright in the US. A tutorial on how to
download texts form the Project Gutenberg library using the
<code>GutenbergR</code> package can be found <a
href="https://slcladal.github.io/gutenberg.html">here</a>.</p>
</div>
<div id="regular-expression" class="section level2 unnumbered">
<h2 class="unnumbered">Regular Expression</h2>
<p>Regular Expressions - often simply referred to as regex - are symbols
or sequence of symbols utilized to search for patterns in textual data.
Regular Expressions are very useful and widely used in Text Analysis and
often different programming languages will have very similar but
slightly different Regular Expressions. A tutorial on how to use regular
expression in R can be found <a
href="https://slcladal.github.io/regex.html">here</a> and here is a link
to a <a
href="https://raw.githubusercontent.com/rstudio/cheatsheets/main/regex.pdf">regex
in R cheat sheet</a>.</p>
</div>
<div id="semantic-analysis" class="section level2 unnumbered">
<h2 class="unnumbered">Semantic Analysis</h2>
<p>Semantic Analysis refers to a suit of methods that allow to analyze
the semantic (semantics) fo texts. Such analyses often rely on semantic
tagsets that are based on word meaning or meaning families/categories.
Two examples of such semantic tagsets are the <a
href="https://ucrel.lancs.ac.uk/claws7tags.html">URCEL tagset</a> and
the <a href="http://eprints.gla.ac.uk/115024/"><em>Historical Thesaurus
Semantic Tagger</em></a> <span class="citation">(<a
href="#ref-alexander2015tagger">Alexander and Wattam 2015</a>)</span>
developed at the University of Glasgow.</p>
<p><img src="https://slcladal.github.io/images/senti.png" width="40%" style="float:right; padding:10px" /></p>
</div>
<div id="sentiment-analysis" class="section level2 unnumbered">
<h2 class="unnumbered">Sentiment Analysis</h2>
<p>Sentiment Analysis is a computational approach to determine if words
or texts are associated with (positive or negative) polarity or
emotions.Commonly, sentiments analyses are based on sentiment
dictionaries (words are annotated based on whether they occur in a list
of words associated with, e.g., positive polarity or emotion, e.g.,
<em>fear</em>, <em>anger</em>, or <em>joy.</em> A tutorial on how to
perform sentiment analysis in R can be found <a
href="https://slcladal.github.io/sentiment.html">here</a>.</p>
</div>
<div id="string" class="section level2 unnumbered">
<h2 class="unnumbered">String</h2>
<p>In computational approaches, a string is a specific type of data that
represents text and is often encoded in specific format, e.g., Latin1 or
UTF8. Strings may also be present in other data types such as lists or
data frames. A tutorial on how to work with strings in R can be found <a
href="https://slcladal.github.io/string.html">here</a>.</p>
</div>
<div id="term-frequencyinverse-document-frequency-tf-idf"
class="section level2 unnumbered">
<h2 class="unnumbered">Term Frequency–Inverse Document Frequency
(tf-idf)</h2>
<p>Term Frequency–Inverse Document Frequency is a statistical measure of
keyness which reflects how characteristic a word is of a specific text.
Term Frequency–Inverse Document Frequency is based on the frequencies of
words in a text compared to the frequency of documents in which it
occurs</p>
<p><img src="https://slcladal.github.io/images/topic.png" width="40%" style="float:right; padding:10px" /></p>
</div>
<div id="topic-modeling" class="section level2 unnumbered">
<h2 class="unnumbered">Topic Modeling</h2>
<p>Topic modelling is a machine learning method seeks to answer the
question: given a collection of documents, can we identify what they are
<em>about</em>?</p>
<p>Topic model algorithms look for patterns of co-occurrences of words
in documents. We assume that, if a document is about a certain topic,
one would expect words that are related to that topic to appear in the
document more often than in documents that deal with other topics. Topic
model commonly use Latent Dirichlet Allocation (LDA) to find
<em>topics</em> in textual data.</p>
<p>There are two basic types of Topic models</p>
<ul>
<li><p>supervised or seeded topics models where the researchers provides
seed terms around which the LDS looks for topics (collections of
correlating terms)</p></li>
<li><p>unsupervised or unseeded topic models which try to find a
predefined number of topics (collections of correlating terms)</p></li>
</ul>
<p>A tutorial on how to work with strings in R can be found <a
href="https://slcladal.github.io/topicmodels.html">here</a>.</p>
<p><img src="https://slcladal.github.io/images/netta.png" width="40%" style="float:right; padding:10px" /></p>
</div>
</div>
<div id="text-analysis-at-uq" class="section level1 unnumbered">
<h1 class="unnumbered">Text Analysis at UQ</h1>
<p>As LADAL has been established at The University of Queensland, we
have listed selected resources on Text Analysis offered by UQ.</p>
<p>The <a href="https://www.library.uq.edu.au/">UQ Library</a> offers a
very handy and attractive summary of <a
href="https://guides.library.uq.edu.au/research-techniques/text-mining-analysis/introduction">resources,
concepts, and tools</a> that can be used by researchers interested in
Text Analysis and Distant Reading. Also, the UQ library site offers
short video introductions and addresses issues that are not discussed
here such as <a
href="https://guides.library.uq.edu.au/research-techniques/text-mining-analysis/considerations">copyright
issues</a>, <a
href="https://guides.library.uq.edu.au/research-techniques/text-mining-analysis/sources-of-text-data">data
sources available at the UQ library</a>, as well as <a
href="https://guides.library.uq.edu.au/research-techniques/text-mining-analysis/sources-of-text-data">social
media</a> and <a
href="https://guides.library.uq.edu.au/research-techniques/text-mining-analysis/web-scraping">web
scaping</a>.</p>
<p>In contrast to the UQ library site, the focus of this introduction
lies on the practical how-to of text analysis. this means that the
following concentrates on how to perform analyses rather than discussing
their underlying concepts or evaluating their scientific merits.</p>
</div>
<div id="citation-session-info" class="section level1 unnumbered">
<h1 class="unnumbered">Citation &amp; Session Info</h1>
<p>Schweinberger, Martin. 2024. <em>Introduction to Text Analysis</em>.
Brisbane: The Language Technology and Data Analysis Laboratory (LADAL).
url: <a href="https://ladal.edu.au/introta.html"
class="uri">https://ladal.edu.au/introta.html</a> (Version
2024.05.26).</p>
<pre><code>@manual{schweinberger2024introta,
  author = {Schweinberger, Martin},
  title = {Introduction to Text Analysis},
  note = {https://ladal.edu.au/introta.html},
  year = {2024},
  organization = {The Language Technology and Data Analysis Laboratory (LADAL)},
  address = {Brisbane},
  edition = {2024.05.26}
}</code></pre>
<pre class="r"><code>sessionInfo()</code></pre>
<pre><code>## R version 4.3.2 (2023-10-31 ucrt)
## Platform: x86_64-w64-mingw32/x64 (64-bit)
## Running under: Windows 11 x64 (build 22621)
## 
## Matrix products: default
## 
## 
## locale:
## [1] LC_COLLATE=English_Australia.utf8  LC_CTYPE=English_Australia.utf8   
## [3] LC_MONETARY=English_Australia.utf8 LC_NUMERIC=C                      
## [5] LC_TIME=English_Australia.utf8    
## 
## time zone: Australia/Brisbane
## tzcode source: internal
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## loaded via a namespace (and not attached):
##  [1] digest_0.6.35     R6_2.5.1          fastmap_1.1.1     xfun_0.43        
##  [5] cachem_1.0.8      knitr_1.46        htmltools_0.5.8.1 rmarkdown_2.26   
##  [9] lifecycle_1.0.4   cli_3.6.2         sass_0.4.9        jquerylib_0.1.4  
## [13] compiler_4.3.2    highr_0.10        rstudioapi_0.16.0 tools_4.3.2      
## [17] evaluate_0.23     bslib_0.7.0       yaml_2.3.8        rlang_1.1.3      
## [21] jsonlite_1.8.8</code></pre>
<p><a href="#introduction">Back to top</a></p>
<p><a href="https://ladal.edu.au">Back to HOME</a></p>
</div>
<div id="references" class="section level1 unnumbered">
<h1 class="unnumbered">References</h1>
<div id="refs" class="references csl-bib-body hanging-indent"
entry-spacing="0">
<div id="ref-alexander2015tagger" class="csl-entry">
Alexander, Baron, M., and S. Wattam. 2015. <span>“The Historical
Thesaurus Semantic Tagger.”</span>
</div>
<div id="ref-amador2010how" class="csl-entry">
Amador Moreno, Carolina P. 2010. <span>“How Can Corpora Be Used to
Explore Literary Speech Representation?”</span> In <em>The Routledge
Handbook of Corpus Linguistics</em>, edited by Anne O’Keeffe and Michael
McCarthy, 531–44. Abingdon (UK) &amp; New York: Routeledge.
https://doi.org/<a
href="https://doi.org/10.4324/9780203856949-38">https://doi.org/10.4324/9780203856949-38</a>.
</div>
<div id="ref-anthony2024antconc" class="csl-entry">
Anthony, Laurence. 2024. <span>“AntConc (Version 4.2.4) [Computer
Software].”</span> Tokyo, Japan: Waseda University. <a
href="https://www.laurenceanthony.net/software">https://www.laurenceanthony.net/software</a>.
</div>
<div id="ref-bernard1998text" class="csl-entry">
Bernard, H. Russell, and Gery Ryan. 1998. <span>“Text Analysis.”</span>
<em>Handbook of Methods in Cultural Anthropology</em> 613.
</div>
<div id="ref-chowdhary2020natural" class="csl-entry">
Chowdhary, KR1442, and KR Chowdhary. 2020. <span>“Natural Language
Processing.”</span> <em>Fundamentals of Artificial Intelligence</em>,
603–49.
</div>
<div id="ref-eisenstein2019introduction" class="csl-entry">
Eisenstein, Jacob. 2019. <em>Introduction to Natural Language
Processing</em>. MIT Press.
</div>
<div id="ref-gries2009whatiscorpuslinguistics" class="csl-entry">
Gries, Stefan Th. 2009. <span>“What Is Corpus Linguistics?”</span>
<em>Language and Linguistics Compass</em> 3: 1–17. https://doi.org/<a
href="https://doi.org/10.1111/j.1749-818x.2009.00149.x">https://doi.org/10.1111/j.1749-818x.2009.00149.x</a>.
</div>
<div id="ref-indurkhya2010handbook" class="csl-entry">
Indurkhya, Nitin, and Fred J Damerau. 2010. <em>Handbook of Natural
Language Processing</em>. 2nd edition. Routledge.
</div>
<div id="ref-jockers2020text" class="csl-entry">
Jockers, Matthew L, and Rosamond Thalken. 2020. <em>Text Analysis with
r</em>. Springer.
</div>
<div id="ref-kabanoff1997introduction" class="csl-entry">
Kabanoff, Boris. 1997. <span>“Introduction: Computers Can Read as Well
as Count: Computer-Aided Text Analysis in Organizational
Research.”</span> <em>Journal of Organizational Behavior</em>, 507–11.
</div>
<div id="ref-kilgarriff2014sketch" class="csl-entry">
Kilgarriff, Adam, Vít Baisa, Jan Bušta, Miloš Jakubíček, Vojtěch Kovář,
Jan Michelfeit, Pavel Rychlý, and Vít Suchomel. 2014. <span>“The Sketch
Engine: Ten Years On.”</span> <em>Lexicography</em> 1: 7–36.
</div>
<div id="ref-lindquist2009corpus" class="csl-entry">
Lindquist, Hans. 2009. <em>Corpus Linguistics and the Description of
English</em>. Vol. 104. Edinburgh: Edinburgh University Press.
</div>
<div id="ref-mitkov2022oxford" class="csl-entry">
Mitkov, Ruslan. 2022. <em>The Oxford Handbook of Computational
Linguistics</em>. Oxford: Oxford University Press.
</div>
<div id="ref-moretti2005graphs" class="csl-entry">
Moretti, Franco. 2005. <em>Graphs, Maps, Trees: Abstract Models for a
Literary History</em>. Vol. 43. Verso. https://doi.org/<a
href="https://doi.org/10.5860/choice.43-2646">https://doi.org/10.5860/choice.43-2646</a>.
</div>
<div id="ref-moretti2013distant" class="csl-entry">
———. 2013. <em>Distant Reading</em>. Verso.
</div>
<div id="ref-popping2000computer" class="csl-entry">
Popping, Roel. 2000. <em>Computer-Assisted Text Analysis</em>. Sage.
https://doi.org/<a
href="https://doi.org/10.4135/9781849208741">https://doi.org/10.4135/9781849208741</a>.
</div>
<div id="ref-sinclair1991corpus" class="csl-entry">
Sinclair, John. 1991. <em>Corpus, Concordance, Collocation</em>. Oxford:
Oxford University Press.
</div>
<div id="ref-stafanowitsch2020corpus" class="csl-entry">
Stefanowitsch, Anatol. 2020. <em>Corpus Linguistics. A Guide to the
Methodology</em>. Textbooks in Language Sciences. Berlin: Language
Science Press. https://doi.org/<a
href="https://doi.org/10.5281/zenodo.3735822">https://doi.org/10.5281/zenodo.3735822</a>.
</div>
<div id="ref-welbers2017text" class="csl-entry">
Welbers, Kasper, Wouter Van Atteveldt, and Kenneth Benoit. 2017.
<span>“Text Analysis in r.”</span> <em>Communication Methods and
Measures</em> 11 (4): 245–65. https://doi.org/<a
href="https://doi.org/10.1080/19312458.2017.1387238">https://doi.org/10.1080/19312458.2017.1387238</a>.
</div>
</div>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // temporarily add toc-ignore selector to headers for the consistency with Pandoc
    $('.unlisted.unnumbered').addClass('toc-ignore')

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
