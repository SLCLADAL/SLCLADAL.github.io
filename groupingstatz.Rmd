---
title: "Statistics: Pattern-Detection"
author: "UQ SLC Digital Team"
date: "`r format(Sys.time(), '%Y-%m-%d')`"
output: html_document
bibliography: bibliography.bib
---
```{r uq1, echo=F, fig.cap="", message=FALSE, warning=FALSE, out.width='100%'}
knitr::include_graphics("images/uq1.jpg")
```

# Introduction

This section deals with methods that are used to find groups or patterns in data. 

# Cluster Analysis

The most common method in linguistics that is sued to detect groups in data are cluster analyses. Cluster analyses are common in linguistics because they not only detect commonalities based on the frequency or occurrence of featutres but they also allow to visualize when splits between groups have occurred and are thus the method of choice in historical linguistics to determine and show genealogical relationships. 

## Underlying Concepts

The next section focuses on the basic idea that underlies all cluster analyses. WE will have a look at some very basic examples to highlight and discuss the principles that cluster analyses rely on. 

The underlying idea of cluster analysis is very simple and rather intuitive as we ourselves perform cluster analyses everyday in our our lives. This is so because we group things together under certain lables and into concepts. The first example to exemplyfy this deals with types of trees and how we group these types of trees based on their outward appearance. 

Imagine you see six trees representing different types of trees: a pine tree, a fir tree, an oak tree, a beech tree, a phoenix palm tree, and a nikau palm tree. Now, you were asked to group these trees accroing to similarity. Have a look at the plot below and see whether you would have come up with a similar type of grouping.

```{r echo = F, results = 'asis'}
x <- 1:10
y <- 1:10
plot(x, y, type = "n", ylim = c(-.5,10), xlim = c(0,5), axes = F, xlab = "", ylab = "")
text("Trees", x = 2.25, y = 10, cex = 1.5)
text("Conifers", x = .5, y = 6.5, cex = 1.5)
text("Broad leaf", x = 2.25, y = 6.5, cex = 1.5)
text("Palms", x = 4, y = 6.5, cex = 1.5)
text("Pine tree", x = .25, y = 1.5, srt=90, cex = 1.5)
text("Fir tree", x = .75, y = 1.5, srt=90, cex = 1.5)
text("Oak tree", x = 2, y = 1.5, srt=90, cex = 1.5)
text("Beech tree", x = 2.5, y = 1.5, srt=90, cex = 1.5)
text("Phoenix palm", x = 3.75, y = 1.75, srt=90, cex = 1.5)
text("Nikau palm", x = 4.25, y = 1.5, srt=90, cex = 1.5)
#
lines(x = c(.5, 1.75), y = c(7, 9), lwd = 2)
lines(x = c(2.25, 2.25), y = c(7, 9), lwd = 2)
lines(x = c(4, 2.75), y = c(7, 9), lwd = 2)
#
lines(x = c(.5, .5), y = c(6, 4.5), lwd = 2)
lines(x = c(2.25, 2.25), y = c(6, 4.5), lwd = 2)
lines(x = c(4, 4), y = c(6, 4.75), lwd = 2)
#
lines(x = c(.25, .75), y = c(4.5, 4.5), lwd = 2)
lines(x = c(2, 2.5), y = c(4.5, 4.5), lwd = 2)
lines(x = c(3.75, 4.25), y = c(4.75, 4.75), lwd = 2)
#
lines(x = c(.25, .25), y = c(4.5, 4), lwd = 2)
lines(x = c(.75, .75), y = c(4.5, 4), lwd = 2)
lines(x = c(2, 2), y = c(4.5, 4), lwd = 2)
lines(x = c(2.5, 2.5), y = c(4.5, 4), lwd = 2)
```

An alternative way to group the trees would be the follwoing.

```{r echo = F, results = 'asis'}
x <- 1:10
y <- 1:10
plot(x, y, type = "n", ylim = c(-.5,15), xlim = c(0,5), axes = F, xlab = "", ylab = "")
text("Trees", x = 2.25, y = 15, cex = 1)
text("Conifers", x = .5, y = 6.5, cex = 1)
text("Broad leaf", x = 2.25, y = 6.5, cex = 1)
text("Palm Trees", x = 3.5, y = 10, cex = 1)
text("Pine tree", x = .25, y = 1.5, srt=90, cex = 1)
text("Fir tree", x = .75, y = 1.5, srt=90, cex = 1)
text("Oak tree", x = 2, y = 1.5, srt=90, cex = 1)
text("Beech tree", x = 2.5, y = 1.5, srt=90, cex = 1)
text("Phoenix palm", x = 3.25, y = 1.75, srt=90, cex = 1)
text("Nikau palm", x = 3.75, y = 1.5, srt=90, cex = 1)
#
lines(x = c(1.5, 2.15), y = c(11, 13.5), lwd = 2)
lines(x = c(3.5, 2.5), y = c(11, 13.5), lwd = 2)
lines(x = c(.5, 1.5), y = c(7.25, 11), lwd = 2)
lines(x = c(1.5, 2.25), y = c(11, 7.25), lwd = 2)
#
lines(x = c(.5, .5), y = c(6, 4.5), lwd = 2)
lines(x = c(2.25, 2.25), y = c(6, 4.5), lwd = 2)
lines(x = c(3.5, 3.5), y = c(8.75, 6.25), lwd = 2)
#
lines(x = c(.25, .75), y = c(4.5, 4.5), lwd = 2)
lines(x = c(2, 2.5), y = c(4.5, 4.5), lwd = 2)
lines(x = c(3.25, 3.75), y = c(6.25, 6.25), lwd = 2)
#
lines(x = c(.25, .25), y = c(4.5, 4), lwd = 2)
lines(x = c(.75, .75), y = c(4.5, 4), lwd = 2)
lines(x = c(2, 2), y = c(4.5, 4), lwd = 2)
lines(x = c(2.5, 2.5), y = c(4.5, 4), lwd = 2)
```

In this display, conifers and broad-leaf trees are grouped together because their are more similar to each other compared to palm trees. This poses the question of what is meant by similarity. Consider the display below. 

```{r echo = F, results = 'asis'}
# generate data
y <- c(1, 3.1, 1.2, 2.3, 3.4, 2.5, 1.6, 2.7, 3.8, 2.9)
x <- c(1:10)
plot(x, y, 
     type = "l", 
     ylim = c(0,11), 
     xaxt='n', 
     yaxt='n', 
     ann=FALSE, 
     lwd = 2, 
     ylab = "", 
     xlab = "")
lines(x = 1:10, y = c(5, 5.1, 5.2, 5.3, 5.4, 5.5, 5.6, 5.7, 5.8, 5.9), col = "blue", lwd = 2)
lines(x = 1:10, y = c(8, 10.1, 8.2, 9.3, 10.4, 9.5, 8.6, 9.7, 10.8, 9.9), col = "red", lwd = 2)
```

Are the red and the blue line more similar because they have the same shape or are the red and the black line more similar becaus etheir are closer together? Ther is no single correct answer here. Rather the plot indends to raise awarness about the fact that how cluster analyses group data depends on how similarity is defined in the respective algorithm.

Let's consider another example to better understand how cluster analyses determine which data points should be merged when. Imagine you have five students adn want to group them togehter based on their overall performance in school. The data that you rely on are their grades in math, music, and biology (with 1 being the best grade and 6 being the worst).

```{r echo = F, results = 'asis'}
# similarity
students <- matrix(c(2,  3,  2, 1,  3,  2, 1,  2,  1, 2,  4,  4, 3,  4,  3),
  nrow = 5, byrow = T)
students <- as.data.frame(students)
colnames(students) <- c("Math", "Music", "Biology")
rownames(students) <- c("StudentA", "StudentB", "StudentC", "StudentD", "StudentE")
```

```{r echo = F, results = 'asis'}
library(knitr)
kable(students, caption = "Sample of five students and their grades in math, music, and biology")
```
 
 The first step in determining the similarity among students is to cerate a distance matrix. 
 
```{r echo = T, results = 'asis'}
diststudents <- dist(students, method = "manhattan") # create a distance matrix
```

The distance matrix below shows that Student A and Student B only differ by one grade. Student B and Student C differ by 2 grades. Student A and Student C differ by 3 grades and so on.

```{r echo = F, results = 'asis'}
library(knitr)
diststudentstb <- matrix(c("1", "3", "3","3", "", "2", "4", "4","", "", "6", "6", "", "", "", "2"), nrow = 4, byrow = F)
# add column and row names
colnames(diststudentstb) <- c("StudentA", "StudentB", "StudentC", "StudentD")
rownames(diststudentstb) <- c("StudentB", "StudentC", "StudentD", "StudentE")
kable(diststudentstb, caption = "Distance matrix based of students based on grades in math, music, and biology.")
```

Based on this distance matrix, we can now implement a cluster analysis in `R`. 

# Cluster Analysis: Numeric Data

To create a simple cluster object in `R`, we use the `hclust` function from the `cluster` package. The resulting object is then plotted to create a dentrogram which shows how students have been amalgamated (combined) by the clustering algorithm (which, in the present case, is called "ward.D").

```{r echo = T, results = 'asis'}
library("cluster")                         # load library for clustering
clusterstudents <- hclust(diststudents,    # create cluster object (ward.D linkage)
                          method="ward.D") # define amalgamation method (ward.D linkage)
plot(clusterstudents, hang = 0)            # plot result as dendrogram
```

```{r echo = F, results = 'asis'}
students2 <- matrix(c(1.5, 3, 2, 1,  2,  1, 2,  4,  4, 3,  4,  3),
  nrow = 4, byrow = T)
students2 <- as.data.frame(students2)
rownames(students2) <- c("Cluster1", "StudentC", "StudentD", "StudentE")
diststudents2 <- dist(students2, method = "manhattan")
diststudents2

#
students3 <- matrix(c(1.5, 3, 2, 1,  2,  1, 2.5, 4, 3.5),
  nrow = 3, byrow = T)
students3 <- as.data.frame(students3)
rownames(students3) <- c("Cluster1", "StudentC", "Cluster2")
diststudents3 <- dist(students3, method = "manhattan")
# inspect distance object
diststudents3
```

# Distances

To understand how a cluster analysis determines to which cluster a given data points belongs, we need to understand what different distance measures represent.

```{r echo = F, results = 'asis'}
par(mar=c(1,1,1,1))  # define margine width of the plot
x <- c(1,5)          # define an x value
y <- c(1,5)          # define a y value
plot(x, y, 
     pch = 20, 
     cex = 1, 
     axes = F, 
     las = 1, 
     xlab = "", 
     ylab = "", 
     xlim = c(0,7), 
     ylim = c(0,10))
text(0.5, .5, "vowel", cex = 1)
text(5, 5.5, "target vowel", cex = 1)
lines(x = c(1, 5), y = c(1, 5), type = "l", lty = 3, lwd = 2, col = "red")
lines(x = c(1, 5), y = c(1, 1), type = "l", lty = 2, lwd = 2, col = "blue")
lines(x = c(5, 5), y = c(1, 5), type = "l", lty = 4, lwd = 2, col = "green")
lines(x = c(.9, 5), y = c(.9, .9), type = "l", lty = 4, lwd = 2, col = "green")
legend("topleft", inset=.05, title="", bty = "n", lty = c(3, 2, 4), lwd = 2,
   c("euclidean distance", "maximum distance", "manhatten distance"), col=c("red", "blue", "green"), horiz=F, cex = 1)
par(mar=c(5.1,4.1,4.1,2.1))

```


```{r echo = T, results = 'asis'}
# generate data
ire <- round(sqrt((rnorm(10, 9.5, .5))^2), 3)
sce <- round(sqrt((rnorm(10, 9.3, .4))^2), 3)
bre <- round(sqrt((rnorm(10, 6.4, .7))^2), 3)
aus <- round(sqrt((rnorm(10, 6.6, .5))^2), 3)
nze <- round(sqrt((rnorm(10, 6.5, .4))^2), 3)
ame <- round(sqrt((rnorm(10, 4.6, .8))^2), 3)
can <- round(sqrt((rnorm(10, 4.5, .7))^2), 3)
jam <- round(sqrt((rnorm(10, 1.4, .2))^2), 3)
phi <- round(sqrt((rnorm(10, 1.5, .4))^2), 3)
ind <- round(sqrt((rnorm(10, 1.3, .5))^2), 3)
clus <- data.frame(ire, nze, can, ame, phi, jam, bre, sce, aus, ind)
# add row names
rownames(clus) <- c("nae_neg", "like", "clefts", "tags", "youse", "soitwas", "dt", "nsr", "invartag", "wh_cleft")
# inspect data
clus

str(clus)

summary(clus)

```


```{r echo = T, results = 'asis'}
# clean data
clust <- t(clus)            # transpose data
clust <- na.omit(clust)     # remove missing values
#clusts <- scale(clust)     # standardize variables
clusts <- as.matrix(clust)  # convert into matrix
```

We now assess if data is clusterable by testing whether or not the data includes nonrandom structures. To means to determine whether the data conatins nonrandomness, we calculate the Hopkins statistic which informs how similar the data is to a random distribution. If the values of the Hopkins are higher than 0.5 then this indicates that the data is random and that there are no inherent clusters. However, if the Hopkins statistic is close to 0, then the data is clusterable. In addition, we will test the optimal number of clusters. 

```{r echo =  T, results = 'asis'}
library("factoextra")         # load library to extract cluster tendency
get_clust_tendency(clusts,    # apply get_clust_tendency to cluster object
                   n = 8,     # define number of clusters 
                   gradient = list(low = "steelblue",  # define color for low values 
                                   high = "white"))    # define color for high values
```

In addition, we will test the optimal number of clusters.

```{r echo = T, results = 'asis'}
clustd <- dist(clusts,                 # create distance matrix
               method = "euclidean")   # use eucledian (!) distance
round(clustd, 2)                       # display distance matrix
```

Next, we cerate a distance plot using the `distplot` fucntion. If the distance plot shows different regions (non random, non uniform grey areas) then clustering the data is permitable as the data contains actual structures.

```{r echo = T, results = 'asis'}
dissplot(clustd)  # create distance plot
```


```{r echo = T, results = 'asis'}

# create distance matrix
clustd <- dist(clusts, method = "euclidean") # create distance matrix (eucledian method: not good when dealing with many dimensions)
#clustd <- dist(clusts, method = "maximum")   # create distance matrix (maximum method: here the difference between points dominates)
#clustd <- dist(clusts, method = "manhattan") # create distance matrix (manhattan method: most popular choice)
#clustd <- dist(clusts, method = "canberra")  # create distance matrix (canberra method: for count data - focuses on small differences and neglects larger differences)
#clustd <- dist(clusts, method = "binary")    # create distance matrix (binary method: for binary data only!)
#clustd <- dist(clusts, method = "minkowski") # create distance matrix (minkowski method: is not a true distance measure)

# distance method for words: daisy(data, method = "euclidean") # other possible distances are "manhattan" and "gower"

#cd <- hclust(clustd, method="single")    # create cluster object (single linkage)    : cluster with nearest data point
#cd <- hclust(clustd, method="ward.D")    # create cluster object (ward.D linkage)
cd <- hclust(clustd, method="ward.D2")   # create cluster object (ward.D2 linkage)   : cluster in a way to achieve minimum variance
#cd <- hclust(clustd, method="average")   # create cluster object (average linkage)   : cluster with closest mean
#cd <- hclust(clustd, method="mcquitty")  # create cluster object (mcquitty linkage)
#cd <- hclust(clustd, method="median")    # create cluster object (median linkage)    : cluster with closest median
#cd <- hclust(clustd, method="centroid")  # create cluster object (centroid linkage)  : cluster with closest prototypical point of target cluster
#cd <- hclust(clustd, method="complete")  # create cluster object (complete linkage)  : cluster with nearest furthest data point of target cluster

# plot result as dendrogram
plot(cd, hang = -1)              # display dendogram

```

Now, we determine the optimal number of clusters based on silhouette widths which shows the ratio of internal similarity of clusters against the similarity between clusters. If the silhuette widths have values lower than .2 then this indicates that clustering is not appropriate ([@levshina2015linguistics] 311). The function below displays the silhouette width values of 2 to 8 clusters. 
```{r echo =  T, results = 'asis'}
optclus <- sapply(2:8, function(x) summary(silhouette(cutree(cd, k = x), clustd))$avg.width)
optclus # inspect results
# cut tree into 4 clusters
groups <- cutree(cd, k=4)
```

Since the highest value is provided for 4 clusters, 4 clusters are the optimal solution given the present data and, accordingly, we cut the tree into 4 clusters and plot the result.

```{r echo =  T, results = 'asis'}
groups <- cutree(cd, k=4)          # cut tree into 4 clusters
plot(cd, hang = -1)                # plot result as dendrogram
rect.hclust(cd, k=4, border="red") # draw red borders around clusters
```

In a next step, we aim to determine which factors are particularly improtant for the clustering - this step is soemwhat comparable to measuring the effect size in inferential designs.

```{r echo =  T, results = 'asis'}

# which factors are particularly important
celtic <- clusts[c(1,8),]
others <- clusts[-c(1,8),]
# calculate column means
celtic.cm <- colMeans(celtic)
others.cm <- colMeans(others)
# calcualte difference between celtic and other englishes
diff <- celtic.cm - others.cm
sort(diff, decreasing = F)
```

```{r echo =  T, results = 'asis'}
plot(sort(diff), 1:length(diff), type= "n", xlab ="cluster 2 (others) <-> cluster 1 (celtic)", yaxt = "n", ylab = "")
text(sort(diff), 1:length(diff), names(sort(diff)), cex = 1)

```

```{r echo =  T, results = 'asis'}

# which factors are particularly important
nam <- clusts[c(3,4),]
others <- clusts[-c(3,4),]
# calculate column means
nam.cm <- colMeans(nam)
others.cm <- colMeans(others)
# calcualte difference between celtic and other englishes
diff <- nam.cm - others.cm
sort(diff, decreasing = F)
```

```{r echo =  T, results = 'asis'}
plot(sort(diff), 1:length(diff), type= "n", xlab ="cluster 2 (others) <-> cluster 1 (nam)", yaxt = "n", ylab = "")
text(sort(diff), 1:length(diff), names(sort(diff)), cex = 1)

```

```{r echo=T, message=FALSE, warning=FALSE, paged.print=FALSE}

# we see that wh-clefts and the frequency of like is typical for other varieties
# and that the use of youse as 2nd pl pronoun and inve√°riant tags are typical for
# celtic englishes

# validate clustering
# compute pvclust to check how reliable our clusters are
res.pv <- pvclust(clus, method.dist="euclidean", method.hclust="ward.D2", nboot = 100)
```

```{r echo=T, message=FALSE, warning=FALSE, paged.print=FALSE}
# plot (provides Approximately Unbiased p-value and Bootstrap Probability value, cf. Levshina 2015: 316)
plot(res.pv, cex = 1)
pvrect(res.pv)
```

```{r echo =  T, results = 'asis'}
# load package ape; to install type: install.packages("ape")
library(ape)
# plot basic tree
plot(as.phylo(cd), cex = 0.9, label.offset = 1)

```

```{r echo =  T, results = 'asis'}
# plot as unrooted tree
plot(as.phylo(cd), type = "unrooted")

```

# Cluster Analysis: Nominal Data

```{r echo =  T, results = 'asis'}
# generate data
ire <- c(1,1,1,1,1,1,1,1,1,1)
sce <- c(1,1,1,1,1,1,1,1,1,1)
bre <- c(0,1,1,1,0,0,1,0,1,1)
aus <- c(0,1,1,1,0,0,1,0,1,1)
nze <- c(0,1,1,1,0,0,1,0,1,1)
ame <- c(0,1,1,1,0,0,0,0,1,0)
can <- c(0,1,1,1,0,0,0,0,1,0)
jam <- c(0,0,1,0,0,0,0,0,1,0)
phi <- c(0,0,1,0,0,0,0,0,1,0)
ind <- c(0,0,1,0,0,0,0,0,1,0)
clus <- data.frame(ire, nze, can, ame, phi, jam, bre, sce, aus, ind)
# add row names
rownames(clus) <- c("nae_neg", "like", "clefts", "tags", "youse", "soitwas", "dt", "nsr", "invartag", "wh_cleft")
# convert into factors
clus <- apply(clus, 1, function(x){
  x <- as.factor(x) })
# inspect data
clus

```


```{r echo =  T, message=FALSE, warning=FALSE}

# clean data
clusts <- as.matrix(clus)
# create distance matrix
clustd <- dist(clusts, method = "binary")   # create a distance object with binary (!) distance
# display distance matrix
round(clustd, 2)
```


```{r echo=T, message=FALSE, warning=FALSE}

# create cluster object (ward.D2 linkage)   : cluster in a way to achieve minimum variance
cd <- hclust(clustd, method="ward.D2")
# plot result as dendrogram
plot(cd, hang = -1)              # display dendogram

```

```{r echo=T, message=FALSE, warning=FALSE}

# create factor with celtic varieties on one hand and other varieties on other
cluster <- as.factor(ifelse(as.character(rownames(clusts)) == "ire", "1",
  ifelse(as.character(rownames(clusts)) == "sce", "1", "0")))
# load library
library(vcd)
clsts.df <- as.data.frame(clusts)
# determine significance
library(exact2x2)
pfish <- fisher.exact(table(cluster, clsts.df$youse))
pfish[[1]]

# determine effect size
assocstats(table(cluster, clsts.df$youse))

assocstats(table(cluster, clsts.df$like))

```


```{r echo=T, message=FALSE, warning=FALSE}
library("factoextra")
library("seriation")
library("NbClust")
library("pvclust")
```

# References

