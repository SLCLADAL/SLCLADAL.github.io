---
title: "Computational Literary Stylistics with R"
author: "Dattatreya Majumdar and Martin Schweinberger"
date: "`r format(Sys.time(), '%Y-%m-%d')`"
output:
  bookdown::html_document2: default
bibliography: bibliography.bib
link-citations: yes
---

<!--html_preserve-->
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-130562131-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-130562131-1');
</script>
<!--/html_preserve-->

```{r uq1, echo=F, fig.cap="", message=FALSE, warning=FALSE, out.width='100%'}
knitr::include_graphics("https://slcladal.github.io/images/uq1.jpg")
```


# Introduction{-}

This tutorial focuses on computational literary stylistics (also digital literary stylistics) and exemplifies how computational literary stylistic analyses can be done in R. This tutorial builds on and expands  @silge2017text (see [here](https://www.tidytextmining.com/)) and the entire code for the sections below can be downloaded [here](https://slcladal.github.io/litsty.Rmd). 

**Literary stylistics** refers to analyses of the language of literary texts by computational means using linguistic concepts and categories, with the goal of finding patters among the literary texts and explaining how literary meaning/s is/are created by specific language choices.

## Preparation and session set up{-}

This tutorial is based on R. If you have not installed R or are new to it, you will find an introduction to and more information how to use R [here](https://slcladal.github.io/IntroR_workshop.html). For this tutorials, we need to install certain *packages* from an R *library* so that the scripts shown below are executed without errors. Before turning to the code below, please install the packages by running the code below this paragraph. If you have already installed the packages mentioned below, then you can skip ahead and ignore this section. To install the necessary packages, simply run the following code - it may take some time (between 1 and 5 minutes to install all of the libraries so you do not need to worry if it takes some time).

```{r prep1, eval = F, message=FALSE, warning=FALSE}
# clean current workspace
rm(list=ls(all=T))
# set options
options(stringsAsFactors = F)
# install libraries
install.packages(c("tidytext","janeaustenr","tidyverse"))
```

Once you have installed R and RStudio and initiated the session by executing the code shown above, you are good to go.

# Getting started{-}

For this tutorial we will be primarily requiring 5 packages: `tidyverse`, `tidytext`, `janeaustenr`, `forcats`, and `quanteda`. The `tidytext` package provides functions and supporting data sets to allow conversion of text to and from tidy formats, and to switch seamlessly between tidy tools and existing text mining packages. The `janeaustenr` package is used to load the collection of novels by Jane Austen. The `tidyverse` package provides functions for data processing and visualization. The `forcats` and `quanteda` packages provide additional functions for extracting  and visualizing information from textual data. We will start by loading the required packages. 

```{r pack_01, message=FALSE, warning=FALSE}
library(tidyverse)
library(janeaustenr)
library(tidytext)
library(forcats)
library(quanteda)
```


We can now begin to load, process, and extract information from the texts under investigation (Jane Austen's novels). 

# Extracting words{-}

The most basic but also most common task is to extract instances of individual words and seeing how they are used in context. This is also called *concordancing*. When extracting words, they are typically displayed in context which is why their display is called a *keyword in context concordance* or kwic, for short.

The code below extracts the word *pride* from the novel *Pride and Prejudice* and displays the resulting instances of this keyword in a kwic.

```{r kwic_01}
# extract text
austen_texts <- austen_books() %>%
  dplyr::filter(book == "Pride & Prejudice") %>%
  dplyr::summarise(text = paste0(text, collapse = " ")) %>%
  stringr::str_squish()
# give text a name
names(austen_texts)  <- "Pride & Prejudice"
# extract instances of pride
pride <- quanteda::kwic(austen_texts, "pride") %>%
  as.data.frame()
# inspect data
head(pride)
```

The kwic display could now be processed further or could be inspected to see how the keyword in question (pride) is used in this novel.

# Identifying Keywords{-}

Another common task in literary stylistics is to extract terms that are particularly characteristic of a given text. The problem underlying the identification of keywords is to figure out the importance of words in each document. We can assign weights to words that are more characteristic for a text if these terms are used more frequently than expected in a given text. We can then show terms ordered by their relative weight. Using the `bind_tf_idf()` function from the *tidytext* package, we can extract the *term frequency - inverse document frequency*, or tf-idf, scores which represent these relative weights and we can also  report other parameters such as number of occurrences of that word, total number of words and term frequency.

Before we continue, we need to define certain terms of concepts that rae related to literary stylistics and that we will use repeatedly in this tutorials and that we need to define so that the analysis shown below makes sense.

*Term Frequency* is the measure of importance of a word in a document or how frequently it appears in that document. However there are some words such as "the","is", "of", etc. which appear frequently even though they might not be important. An approach of using a list of stop-words and removing them before analysis can be useful but in case of some documents these words might be highly relevant. 

The *Inverse Document Frequency* decreases the weight for most used words and increases the weight for words that are not much used in a collection of documents. This together with the Term Frequency can be used to calculate a term's *tf-idf* (the multiplication of both the terms) which adjusts the frequency of the term based on how rarely it is used. Mathematically *idf* can be expressed as follows:

\begin{equation}
  idf_{(term)}=  ln (\frac{n_{documents}}{n_{documents\; containing\; term}})
\end{equation}

```{r book_tf_idf, warning=F, message = F}
book_words <- austen_books() %>%
  tidytext::unnest_tokens(word, text) %>%
  dplyr::count(book, word, sort = TRUE) %>% 
  dplyr::group_by(book) %>% 
  dplyr::mutate(total = sum(n))
book_tf_idf <- book_words %>%
  tidytext::bind_tf_idf(word, book, n)
# inspect data
book_tf_idf
```

From the above table it is evident that the extremely common words have a very low inverse document frequency and thus a low tf-idf score. The inverse document frequency will be a higher number for words that occur in fewer documents in the collection of novels.

```{r high_tf_idf}
book_tf_idf %>%
  dplyr::select(-total) %>%
  dplyr::arrange(desc(tf_idf))
```

Next, we plot the 15 words with the highest tf-idf scores for each novel to show which words are particularly charateristic of each of the novels.

```{r high_tf_idfplot,  fig.height=8, fig.width=6, fig.cap="Highest tf-idf words in each of Jane Austen's Novels"}
book_tf_idf %>%
  dplyr::group_by(book) %>%
  slice_max(tf_idf, n = 15) %>%
  dplyr::ungroup() %>%
  ggplot(aes(tf_idf, fct_reorder(word, tf_idf), fill = book)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~book, ncol = 2, scales = "free") +
  labs(x = "tf-idf", y = NULL)
```

As you can see, the method has indeed extracted words (and by extension concepts) that are characteristic of the texts. 

# Extracting Structural Features{-}

Extracting structural features of texts is a very common and has a wide range of applications such as determining if texts belong to the same genre or if texts represent a real language or a made up nonsensical language, for example.

## Word-Frequency Distributions{-}

Word-frequency distributions can be used to determine if a text represents natural language (or a simple replacement cipher) or if the text does not represent natural language or a more complex cipher. In the following, we will check if the language used in Jane Austen's novels aligns with distributions that we would expect when dealing with natural language. In a first step, we load Jane Austen's novels and determine both the term-frequency and the idf.

```{r book_words, message=FALSE, warning=FALSE}
book_words <- austen_books() %>%
  tidytext::unnest_tokens(word, text) %>%
  dplyr::count(book, word, sort = TRUE)  %>% 
  dplyr::group_by(book) %>% 
  dplyr::mutate(total = sum(n))
# inspect data
head(book_words)
```

From the above table it is evident that the usual suspects *the*, *and*, *to* and so-forth are leading in terms of their usage frequencies in Jane Austen's novels. Now let us look at the distribution of *n/total* for each term in each of the novels (which represents the normalized term frequency).

```{r tfplot, message = F, warning = F, fig.height=6, fig.width=6, fig.cap="Term frequency distribution in Jane Austen's novels"}
ggplot(book_words, aes(n/total, fill = book)) +
  geom_histogram(show.legend = FALSE) +
  xlim(NA, 0.0009) +
  facet_wrap(~book, ncol = 2, scales = "free_y")
```

From the plots it is clear that we are dealing with a negative exponential distribution and that there many words which occur only rarely and that only few words occur frequently. 


## Zipf's Law{-}

Zipf's Law represents an empirical power law or power function that was established in the 1930s and it is the most fundamental law in linguistics [see @george1935zipf]. This law states that the frequency that a word is inversely proportional to its rank in a text or collection of texts. 

Let
* N be the number of elements in a text (or collection of texts);
* k be their rank;
* s be the value of the exponent characterizing the distribution.

Zipf's law then predicts that out of a population of N elements, the normalized frequency of the element of rank k, f(k;s,N), is:

\begin{equation}
f(k;s,N)={\frac {1/k^{s}}{\sum \limits _{n=1}^{N}(1/n^{s})}} 
\end{equation}

In the code chunk below, we check if Zipf's Law applies to the words that occur in Jane Austen's novels.

```{r Zipf}
freq_by_rank <- book_words %>% 
  dplyr::group_by(book) %>% 
  dplyr::mutate(rank = row_number(), 
         `term frequency` = n/total) %>%
  dplyr::ungroup()
# inspect data
freq_by_rank
```

To get a better understanding of Zipf's law, let us visualize the distribution by plotting on the logged rank of elements on the x-axis and logged frequency of the terms on the y-axis.

```{r fr, fig.width=5, fig.height=4.5, fig.cap="Zipf's law for Jane Austen's novels"}
freq_by_rank %>% 
  ggplot(aes(rank, `term frequency`, color = book)) + 
  geom_line(size = 1.1, alpha = 0.8, show.legend = FALSE) + 
  scale_x_log10() +
  scale_y_log10()
```

We can see that the plot has a negative slope which corroborates the inverse relationship of rank with respect to term frequency which shows that the words in Jane Austin's novels follow Zipf's law. This would ascertain that we are dealing with natural language and not a made up nonsense language or a complex cipher. 

## Lexical Diversity{-}

Lexical diversity is a complexity measure that provides information about the lexicon size of a text, i.e. how many different words occur in a text given the size of the text. Typically higher values indicate higher lexical diversity. For example, the Type-Token-Ratio (TTR) provides information about the number of word tokens (individual instances of a word) divided by the number of different word types (word forms).

To elaborate, the sentence *The dog chased the cat* contains five tokens but only 4 types because *the* occurs twice. More complex texts or more advanced learners of a language commonly have higher TTRs compared to simpler texts or less advanced language learners. As such, we can use lexical diversity measures to analyze the complexity of the language in which a text is written which can be used to inspect the advances a language learner makes when acquiring a language: initially, the learner will have high TTR as they do not have a large vocabulary. The TTRs will increase as lexicon of the learner grows.

In the following example, we calculate the TTRs for the novels of Jane Austen.

```{r ttr, message=F, warning=F}
austen_books <- austen_books() %>%
  dplyr::group_by(book) %>%
  dplyr::summarise(text = paste(text, collapse = " "))
austen <- austen_books %>%
  dplyr::pull(text)
names(austen) <- austen_books %>%
  dplyr::pull(book)
tokens_austen <- austen %>%
  quanteda::corpus() %>%
  quanteda::tokens()
# inspect data
head(tokens_austen)
```


```{r}
dfm(tokens_austen, remove = stopwords("en")) %>%
  quanteda::textstat_lexdiv(measure = "all") %>%
  tidyr::gather(Measure, Value, TTR:lgeV0) %>%
  dplyr::mutate(Measure = factor(Measure)) %>%
  dplyr::filter(Measure %in% c("TTR")) %>%
  ggplot(aes(x = Value, y = reorder(document, Value))) + 
  geom_point() +
  xlab("Type-Token-Ratio (TTR)") +
  ylab("")
```

We can see that *Emma* has the lowest lexical diversity while *Northanger Abbey* has the highest. This would suggest that the language in *Northanger Abbey* is more complex than the language of *Emma*.

### Average Sentence Length{-}

The average sentence length (ASL) is another measure of textual complexity with more sophisticated language use being associated with longer and more complex sentences. As such, we can use the ASL as an alternative measure of the linguistic complexity of a text or texts. 

```{r asl, warning=F, message=F}
library(lexRankr)
austen_sentences <- austen_books() %>%
  dplyr::group_by(book) %>%
  dplyr::summarise(text = paste(text, collapse = " ")) %>%
  lexRankr::unnest_sentences(sentence, text)
# inspect data
head(austen_sentences)
```


```{r}
austen_sentences %>%
  dplyr::mutate(sentlength = stringr::str_count(sentence, '\\w+')) %>%
  ggplot(aes(x = sentlength, y = book, group = book)) +
  stat_summary(fun = mean, geom = "point")   +          
  stat_summary(fun.data = mean_cl_boot, geom = "errorbar", width = 0.2) +
  xlab("Average Sentence Length (ASL)") +
  ylab("")
```

## Similarity among literary texts{-}

```{r sim01}
feature_mat <- austen %>%
  quanteda::corpus() %>%
  quanteda::dfm(remove_punct = TRUE, remove_url = TRUE, remove_symbols = TRUE) %>% 
  quanteda::dfm_remove(pattern = stopwords("en"))
# inspect data
ndoc(feature_mat)
```
```{r sim02}
topfeatures(feature_mat)
```

```{r sim03}
austen_dist <- as.dist(textstat_dist(feature_mat))
austen_clust <- hclust(austen_dist)
plot(austen_clust)
```

## Networks of Personas{-}

```{r net_01, message=FALSE, warning=FALSE}
# load data
romeo <- read.delim("https://slcladal.github.io/data/romeo.txt", sep = "\t")
# convert into feature co-occurrence matrix
romeo_fcm <- as.fcm(as.matrix(romeo))
# inspect data
romeo_fcm
```




```{r net_05, message=FALSE, warning=FALSE}
textplot_network(romeo_fcm, min_freq = 0.1, edge_alpha = 0.1, edge_size = 5)
```



# Citation & Session Info {-}

Majumdar, Dattatreya and Martin Schweinberger. 2021. *Literary Stylistics with R*. Brisbane: The University of Queensland. url: https://slcladal.github.io/litsty.html (Version `r format(Sys.time(), '%Y.%m.%d')`).

```
@manual{Majumdar2021ta,
  author = {Majumdar, Dattatreya and Martin Schweinberger},
  title = {Literary Stylistics with R},
  note = {https://slcladal.github.io/litsty.html},
  year = {2021},
  organization = "The University of Queensland, Australia. School of Languages and Cultures},
  address = {Brisbane},
  edition = {`r format(Sys.time(), '%Y.%m.%d')`}
}
```



```{r fin}
sessionInfo()
```

***

[Back to top](#introduction)

[Back to HOME](https://slcladal.github.io/index.html)

***

# References {-}


